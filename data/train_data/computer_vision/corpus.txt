This article treats a component-based approach for the prototyping of Tai Chi-based physical therapy games. The research group of the authors has already proposed a component-based three-dimensional (3D) software development system called IntelligentBox. One of the application fields of IntelligentBox is the development of 3D games. In this article, the authors validate the availability of IntelligentBox for the prototype development of Tai Chi-based physical therapy games that require several functionalities, for example, body action input, sound play, movie play, and so on. For them, IntelligentBox has already supported a video-based motion input, a motion capture system input, a data-glove input, and so on. Also, physical therapy games need fine-tuning, according to each of the rehabilitants, because their physical disability levels differ. Therefore, therapists have to frequently change many parameters such as a body action speed, difficulty levels of body actions, and so on. For this point, the component-based approach is significant. To validate this, the authors have been developing physical therapy games using IntelligentBox and they have just developed a practical Tai Chi-based dance game as one of the physical therapy games. In this article, the authors explain how easily such a game can be developed using IntelligentBox. The authors also explain new functionalities of several components of IntelligentBox extended for this development. In addition, this article shows the results of the performance evaluations to indicate that the developed game practically can be used as a physical therapy game. The authors also propose another framework enables network collaboration of the developed ?Tai Chi? game.
In this article, we present a novel approach for modulating the shape of transitions between terrain materials to produce detailed and varied contours where blend resolution is limited. Whereas texture splatting and blend mapping add detail to transitions at the texel level, our approach addresses the broader shape of the transition by introducing intermittency and irregularity. Our results have proven that enriched detail of the blend contour can be achieved with a performance competitive to existing approaches without additional texture, geometry resources, or asset preprocessing. We achieve this by compositing blend masks on-the-fly with the subdivision of texture space into differently sized patches to produce irregular contours from minimal artistic input. Our approach is of particular importance for applications where GPU resources or artistic input is limited or impractical.
Collaboration in visual sensor networks is essential not only to compensate for the limitations of each sensor node but also to tolerate inaccurate information generated by faulty sensors. This article focuses on the design of a collaborative target localization algorithm that is resilient to sensor faults. We first develop a distributed solution to fault-tolerant target localization based on a so-called certainty map. To tolerate potential sensor faults, a voting mechanism is adopted and a threshold value needs to be specified which is the key to the realization of the distributed solution. Analytical study is conducted to derive the lower and upper bounds for the threshold such that the probability of faulty sensors negatively impacts the localization performance is less than a small value. Second, we focus on the detection and correction of one type of sensor faults, error in camera orientation. We construct a generative image model in each camera based on the detected target location to estimate camera's orientation, detect inaccuracies in camera orientations and correct them before they cascade. Based on results obtained from both simulation and real experiments, we show that the proposed method is effective in localization accuracy as well as fault detection and correction performance.
Recent work has shown that, despite the minimal information provided by a binary proximity sensor, a network of these sensors can provide remarkably good target tracking performance. In this article, we examine the performance of such a sensor network for tracking multiple targets. We begin with geometric arguments that address the problem of counting the number of distinct targets, given a snapshot of the sensor readings. We provide necessary and sufficient criteria for an accurate target count in a one-dimensional setting, and provide a greedy algorithm that determines the minimum number of targets that is consistent with the sensor readings. While these combinatorial arguments bring out the difficulty of target counting based on sensor readings at a given time, they leave open the possibility of accurate counting and tracking by exploiting the evolution of the sensor readings over time. To this end, we develop a particle filtering algorithm based on a cost function that penalizes changes in velocity. An extensive set of simulations, as well as experiments with passive infrared sensors, are reported. We conclude that, despite the combinatorial complexity of target counting, probabilistic approaches based on fairly generic models of trajectories yield respectable tracking performance.
Coverage estimation is one of the fundamental problems in sensor networks. Coverage estimation in visual sensor networks (VSNs) is more challenging than in conventional 1-D (omnidirectional) scalar sensor networks (SSNs) because of the directional sensing nature of cameras and the existence of visual occlusion in crowded environments. This article represents a first attempt toward a closed-form solution for the visual coverage estimation problem in the presence of occlusions. We investigate a new target detection model, referred to as the certainty-based target detection (as compared to the traditional uncertainty-based target detection) to facilitate the formulation of the visual coverage problem. We then derive the closed-form solution for the estimation of the visual coverage probability based on this new target detection model that takes visual occlusions into account. According to the coverage estimation model, we further propose an estimate of the minimum sensor density that suffices to ensure a visual K-coverage in a crowded sensing field. Simulation is conducted which shows extreme consistency with results from theoretical formulation, especially when the boundary effect is considered. Thus, the closed-form solution for visual coverage estimation is effective when applied to real scenarios, such as efficient sensor deployment and optimal sleep scheduling.
This article describes a novel approach to localizing networks of embedded cameras and sensors. In this scheme, the cameras and the sensors are equipped with controllable light sources (either visible or infrared), which are used for signaling. Each camera node can then determine automatically the bearing to all of the nodes that are visible from its vantage point. By fusing these measurements with the measurements obtained from onboard accelerometers, the camera nodes are able to determine the relative positions and orientations of other nodes in the network. The method uses angular measurements derived from images, rather than range measurements derived from time-of-flight or signal attenuation. The scheme can be implemented relatively easily with commonly available components, and it scales well since the localization calculations exploit the sparse structure of the system of measurements. Additionally, the method provides estimates of camera orientation which cannot be determined solely from range measurements. The localization technology could serve as a basic capability on which higher-level applications could be built. The method could also be used to automatically survey the locations of sensors of interest, to implement distributed surveillance systems, or to analyze the structure of a scene, based on images obtained from multiple registered vantage points. It also provides a mechanism for integrating the imagery obtained from the cameras with the measurements obtained from distributed sensors.
The problem of online selection of monocular view sequences for an arbitrary task in a calibrated multi-camera network is investigated. An objective function for the quality of a view sequence is derived from a novel task-oriented, model-based instantaneous coverage quality criterion and a criterion of the smoothness of view transitions over time. The former is quantified by a priori information about the camera system, environment, and task generally available in the target application class. The latter is derived from qualitative definitions of undesirable transition effects. A scalable online algorithm with robust suboptimal performance is presented based on this objective function. Experimental results demonstrate the performance of the method?and therefore the criteria?as well as its robustness to several identified sources of nonsmoothness.
Given a hybrid camera layout?one containing, for example, static and active cameras?and people moving around following established traffic patterns, our goal is to predict a subset of cameras, respective camera parameter settings, and future time windows that will most likely lead to success the vision tasks, such as, face recognition when a camera observes an event of interest. We propose an adaptive probabilistic model that accrues temporal camera correlations over time as the cameras report observed events. No extrinsic, intrinsic, or color calibration of cameras is required. We efficiently obtain the camera parameter predictions using a modified Sequential Monte Carlo method. We demonstrate the performance of the model in an example face detection scenario in both simulated and real environment experiments, using several active cameras.
One of the fundamental requirements for visual surveillance with smart camera networks is the correct association of camera's observations with the tracks of objects under tracking. Most of the current systems work in a centralized manner in that the observations on all cameras need to be transmitted to a central server where some data association algorithm is running. Recently some works have been shown for distributed data association based solely on appearance observation. However, how to perform distributed association inference using both appearance and spatio-temporal information is still unclear. In this article, we present a novel method for estimating the posterior distribution of the label of each observation, indicating which of the objects it comes from, based on belief propagation between neighboring cameras. We develop distributed forward and backward inference algorithms for online and offline application, respectively, and further extend the algorithms to the case of unreliable detection. We also incorporate the proposed inference algorithms into distributed EM framework to simultaneously solve the problem of data association and appearance model learning in a completely distributed manner. The proposed method is verified on artificial data and on real world observations collected by a camera networks in an office building.
Camera network systems generate large volumes of potentially useful data, but extracting value from multiple, related videos can be a daunting task for a human reviewer. Multicamera video summarization seeks to make this task more tractable by generating a reduced set of output summary videos that concisely capture important portions of the input set. We present a system that approaches summarization at the level of detected activity motifs and shortens the input videos by compacting the representation of individual activities. Additionally, redundancy is removed across camera views by omitting from the summary activity occurrences that can be predicted by other occurrences. The system also detects anomalous events within a unified framework and can highlight them in the summary. Our contributions are a method for selecting useful parts of an activity to present to a viewer using activity motifs and a novel framework to score the importance of activity occurrences and allow transfer of importance between temporally related activities without solving the correspondence problem. We provide summarization results for a two camera network, an eleven camera network, and data from PETS 2001. We also include results from Amazon Mechanical Turk human experiments to evaluate how our visualization decisions affect task performance.
In a wide-area camera network, cameras are often placed such that their views do not overlap. Collaborative tasks such as tracking and activity analysis still require discovering the network topology including the extrinsic calibration of the cameras. This work addresses the problem of calibrating a fixed camera in a wide-area camera network in a global coordinate system so that the results can be shared across calibrations. We achieve this by using commonly available mobile devices such as smartphones. At least one mobile device takes images that overlap with a fixed camera's view and records the GPS position and 3D orientation of the device when an image is captured. These sensor measurements (including the image, GPS position, and device orientation) are fused in order to calibrate the fixed camera. This article derives a novel maximum likelihood estimation formulation for finding the most probable location and orientation of a fixed camera. This formulation is solved in a distributed manner using a consensus algorithm. We evaluate the efficacy of the proposed methodology with several simulated and real-world datasets.
Using neuroimaging, researchers are succesfully mapping neural connectivity and in the process creating vivid "brainbows."
Kodu Game Lab is a complete, 3-D game development environment designed to be accessible to children as young as 9 years old. The core of Kodu is a custom visual programming language, which blends ease of use with expressibility.
In this study, we developed an algorithmic method to analyze late contrast-enhanced (CE) magnetic resonance (MR) images, revealing the so-called hibernating myocardium. The algorithm is based on an efficient and robust image registration algorithm. Using our method, we are able to integrate the static late CE MR image with its corresponding cardiac cine MR images, constructing cardiac motion CE MR images, which are referred to as cardiac cine CE MR images. This method appears promising as an improved cardiac viability assessment tool
Fans of PC role-playing games need no introduction to Bioware-the Edmonton, Alberta based developer of Baldur's Gate, Neverwinter Nights, and Jade Empire, among others. The company recently opened a studio in Austin, Texas to develop a massively multiplayer online role-playing game (MMORPG, or simply MMO) for an unannounced intellectual property. Ben Earhart, client technology lead on the new project, took a few hours out of his busy schedule to discuss with Crossroads the future of real-time rendering-3-D graphics that render fast enough to respond to user input, such as those required for video games.
This paper describes a protocol for efficient mutual authentication (via a mutually trusted third party) that assures both principal parties of the timeliness of the interaction without the use of clocks or double encipherment. The protocol requires a total of only four messages to be exchanged between the three parties concerned.
This paper examines the impact of deficiencies in data quality on the results generated for spreadsheet applications. The purpose is to describe a framework which can be systematically used to determine the relative importance of potential errors in operational and judgmental data. Special emphasis is placed on analyzing the implications of deficiencies in data quality on projected spreadsheet results.
"If you're working on actual products you can't say that 90 percent is good enough and just move to something else."
Image interpolation is an important image processing operation applied in diverse areas ranging from computer graphics, rendering, editing, medical image reconstruction, to online image viewing. Image interpolation techniques are referred in literature by many terminologies, such as image resizing, image resampling, digital zooming, image magnification or enhancement, etc. Basically, an image interpolation algorithm is used to convert an image from one resolution (dimension) to another resolution without loosing the visual content in the picture. Image interpolation algorithms can be grouped in two categories, non-adaptive and adaptive. The computational logic of an adaptive image interpolation technique is mostly dependent upon the intrinsic image features and contents of the input image whereas computational logic of a non-adaptive image interpolation technique is fixed irrespective of the input image features. In this paper, we review the progress of both non-adaptive and adaptive image interpolation techniques. We also proposed a new algorithm for image interpolation in discrete wavelet transform domain and shown its efficacy. We describe the underlying computational foundations of all these algorithms and their implementation techniques. We present some experimental results to show the impact of these algorithms in terms of image quality metrics and computational requirements for implementation.
Universities and training organizations increasingly use technology to record and distribute original material, bringing on a new class of technological and legal issues.
A new global optimization algorithm for functions of continuous variables is presented, derived from the ?Simulated Annealing? algorithm recently introduced in combinatorial optimization. The algorithm is essentially an iterative random search procedure with adaptive moves along the coordinate directions. It permits uphill moves under the control of a probabilistic criterion, thus tending to avoid the first local minima encountered. The algorithm has been tested against the Nelder and Mead simplex method and against a version of Adaptive Random Search. The test functions were Rosenbrock valleys and multiminima functions in 2,4, and 10 dimensions. The new method proved to be more reliable than the others, being always able to find the optimum, or at least a point very close to it. It is quite costly in term of function evaluations, but its cost can be predicted in advance, depending only slightly on the starting point.
Consider a network of communicating finite state machines that exchange messages over unbounded FIFO channels. Each machine in the network can be defined by a directed graph whose nodes represent the machine states and whose edges represent its transitions. In general, for a node in one of the machines to be live (i.e., encountered infinitely often during the course of communication), each machine in the network should progress in some fair fashion. We define three graduated notions of fair progress (namely, node fairness, edge fairness, and network fairness), and on this basis we define three corresponding degrees of node liveness. We discuss techniques to verify that a given node is live under each of these fairness assumptions. These techniques can be automated; and they are effective even if the network under consideration has an infinite number of reachable states. We use our techniques to establish the liveness of some practical communication protocols; these include an unbounded start-stop protocol, an unbounded alternating bit protocol, and a simplified version of the CSMA/CD protocol for local area networks.
DNP (dynamic networks of processes) is a variant of the language introduced by Kahn and MacQueen [11, 12]. In the language it is possible to create new processes dynamically. We present a complete, formal denotational semantics for the language, along the lines sketched by Kahn and MacQueen. An informal explanation of the formal semantics is also given.
This paper introduces a performance evaluation method for algorithms that generates a depth map using an image from a stereo endoscopic camera for image processing of laparoscope operations. The depth image was created by using a space-time stereo method by illuminating various patterns on scenes consisting of models of the 3D-printed organ model and actual organs from a pig, and the ground truth image was generated for each sub-pixel unit to achieve high accuracy and high precision. Different algorithms were evaluated using the ground truth image data. The number of effective depth pixels compared to the ground truth and the distance error was measured from algorithms based on an edge-preserving filter as real-time algorithms and quasi-dense algorithms. This paper presents an analysis of each algorithm from its evaluation indices to determine which algorithm is appropriate to compute the depth map from laparoscopic images.
Recently, vehicle detection methods have been popularly used in the field of intelligent vehicles. The performance and processing time of vehicle detection is very important because it is associated with the life of a driver. However, all vehicle detection methods generate missing detections and false detections because of different vehicle appearances. However, in a general road environment, the appearance of most of these vehicles has a front and a rear. In this paper, we propose a training method to detect the front and rear of the vehicle. Our vehicle detection integrates state-of-the-art feature-based detection.
Practicing and playing a sport causes athletes' bodies to adapt to the movements they regularly perform. Unfortunately, this can cause muscle imbalances, which might impair performance or worse, cause an injury. It is always best to find the root cause of a muscle imbalance, and to make a precise effort to fix it. Muscle imbalance shouldn't be taken lightly-it could create bigger problems, from posture to spinal positioning, which can ultimately lead to issues in walking, sitting and even lying down, as time progresses. However, muscle imbalances can't be easily evaluated using X-rays, CT scans, or other high-tech devices. But it's possible to address the problem in other ways. In general, the "strong" muscle is measured against the "weaker" muscle. Using the infrared (IR) camera, Kinect can recognize users and track their skeletons in the field of view of the sensor. Kinect sensor can locate the joints of the tracked users in space and track their movements over time. This allows Kinect sensor to recognize people (postures) and follow their actions (movements). Hence, the primary aim of this research is to investigate patterns of muscle imbalance among athletes and evaluate those patterns based on the posture, balance, gait and movement variations using Kinect sensor. Ideally the expected outcome of this research would be a physically meaningful & robust method to identify the muscle imbalance of an athlete.
Pedestrian detection requires both reliable performance and fast processing. Stereo-based pedestrian detectors meet these requirements due to a hypotheses generation processing. However, noisy depth images increase the difficulty of robustly estimating the road line in various road environments. This problem results in inaccurate candidate bounding boxes and complicates the correct classification of the bounding boxes. In this letter, we propose a dynamic ground plane estimation method to manage this problem. Our approach estimates the ground plane optimally using a posterior probability that combines a prior probability and several uncertain observations due to cluttered road environments. Our approach estimates a ground plane optimally using a posterior probability which combines a prior probability and several uncertain observations due to cluttered road environments. The experimental results demonstrate that the proposed method estimates the ground plane robustly and accurately in noisy depth images and also a stereo-based pedestrian detector using the proposed method outperforms previous state-of-the art detectors with less complexity.
This paper presents a novel convolution neural network for classifying the orientation (or viewpoint) of a vehicle in a given image. Current equipping sensors in self-driving car is able to produce bounding box of vehicles in the proximity, but it does not recognize the viewpoint of them. Analyzing surrounding cars' direction in very complex environment has a significant role for autonomous driving. Utilizing nothing but a captured image, the purpose of this research is to classify viewpoint of vehicle: (1) front; (2) rear; (3) side; (4) front-side; and (5) rear-side. Deep convolutional neural network is used as the tool in performing classification task. The approach involves examining different CNN architectures using a large scale car dataset. In addition to that, the goal of the model is to be small and fast enough for limited hardware resource. We are able to achieve 95% accuracy, 57ms inference time on Nvidia GRID K520 GPU, and 1.6 MB Caffe model size.
The performance of the semantic concept detection method depends on, the selection of the low-level visual features used to represent key-frames of a shot and the selection of the feature-fusion method. This paper proposes a set of low-level visual features of considerably smaller size and also proposes novel 'hybrid-fusion' and 'mixed-hybrid-fusion' approaches which are formulated by combining contemporary early and late-fusion strategies. In the proposed hybrid-fusion approach, the features from the same feature group are combined using early-fusion before classifier training; and the concept probability scores from multiple classifiers are merged using late-fusion approach, to get final detection scores. A feature group is defined as the features from the same feature family like color moments. The hybrid-fusion approach is refined and the 'mixed-hybrid-fusion' approach is proposed additionally to further improve the detection rate. Neural Network is used to build classifiers that produce concept probabilities for a test frame. The proposed approaches are evaluated on TRECVID development dataset which contains multi-labeled key-frames. Results show that, the proposed approaches outperform early-fusion and late-fusion approaches by large margins with respect to feature set dimensionality and mean Average Precision (mAP) values.
This paper presents an image hashing algorithm for authentication and tampering based on texture features. Center Symmetric Local Binary Pattern (CSLBP) feature is computationally simple, rotation invariant which works in spatial domain. In CSLBP, number of histogram bin for each sub block of an image is 16, unlike 256 bin in Local Binary Pattern (LBP). In our proposed method, flipped difference is used to generate a histogram of only 8 bin, for each sub block. Resultant method with 8 bin histogram has less discrimination power. To enhance discrimination power, Laplacian of Gaussian (LoG) is used as a weight factor during histogram construction. LoG is used to find a characteristic scale for a given image location. LoG is a second order derivative edge detection operator which performs well in presence of noise. In our previous papers, we tried various local descriptors like magnitude of difference, standard deviation, coefficient correlation as a weight factor, to enhance the success rate of compressed CSLBP. Proposed LoG-QCSLBP gives good results for JPEG, salt & pepper noise, brightness plus, increase/decrease contrast. In the results section, we compared all variants of compressed CSLBP. Results clearly show that by incorporating the weight of a local descriptor, discrimination power of compressed CSLBP is enhanced.
The damage of the accident between a pedestrian and a vehicle is most serious in the kind of traffic accidents. According to the statistics, 38% of road fatalities occur in an accident between a pedestrian and a vehicle, and the night accident is accounted for 64% in that number. This paper proposes pedestrian recognition algorithm with the far-infrared image sensor mounted vehicle at night time. We propose recognition algorithm with noble features which are Local Binary Pattern Haar-like (LBP-Haar_like), Advanced Histogram Oriented Gradient-Local Binary Pattern_histogram (adv_HOG- LBP _histogram) features. The features are extracted from big database (DB) using Adaptive Boosting (ada-boost) classification. The experimental results show that the proposed algorithm can detect and track pedestrian with 97% accuracy at average 20 frames per second.
In order to improve the quality and efficiency of printing, a new digital screening method based on the concept of prefabricated functions is proposed in this paper. Nine functions corresponding to different screening schemes are established in advance according to the direction relations of pixels. Then one of the screening scheme could be called to complete the screening operations on the basis of the gradient values of pixel to be screened and the around eight pixels. The simulation results show that compared with the traditional digital screening methods, the prefabricated functions algorithm can effectively improve the performance and execution speed.
Texture feature analysis has undergone tremendous growth in recent years. It plays an important role for the analysis of many kinds of images. More recently, the use of texture analysis techniques for historical document image segmentation has become a logical and relevant choice in the conditions of significant document image degradation and in the context of lacking information on the document structure such as the document model and the typographical parameters. However, previous work in the use of texture analysis for segmentation of digitized historical document images has been limited to separately test one of the well-known texture-based approaches such as autocorrelation function, Grey Level Co-occurrence Matrix (GLCM), Gabor filters, gradient, wavelets, etc. In this paper we raise the question of which texture-based method could be better suited for discriminating on the one hand graphical regions from textual ones and on the other hand for separating textual regions with different sizes and fonts. The objective of this paper is to compare some of the well-known texture-based approaches: autocorrelation function, GLCM, and Gabor filters, used in a segmentation of digitized historical document images. Texture features are briefly described and quantitative results are obtained on simplified historical document images. The achieved results are very encouraging.
This paper presents a character segmentation system from ancient palm leaf manuscripts written in ancient Thai language. This aims to develop an automated system for the digitization and processing of ancient manuscripts. In this paper, the preprocessing stage of noise reduction is carried out. An optimal binarization is selected in order to reduce the unrelated noise and background information on the document. The proposed approach can improve the readability of the documents and enable selection of the optimal binarization technique. Text line segmentation is then applied to partial projection profiles, and the characters are separated by using the contour tracing algorithm and a trace of background skeleton. The experiment results have shown that this proposed system can be used to support subsequent steps such as automatic recognition of characters from Thai ancient palm leaf.
The first version of the HistDoc platform was designed as an ImageJ plugin to process images of historical documents. This paper presents the second version of HistDoc that besides updating the image processing capabilities of HistDoc in a number of ways, including processing images of monochromatic documents and incorporating newer and better algorithms for the old functionality, it allows document images to be batch processed in standalone mode in a single machine and in parallel distributed architectures in cluster and grids.
Some of the sliding window features commonly used in off-line handwritten text recognition are inherently noisy or sensitive to image noise. In this paper, we investigate the effects of several de-noising filters applied in the feature space and not in the image domain. The purpose is to target the intrinsic noise of these features, stemming from the complex shapes of handwritten characters. This noise is present even if the image has been captured without any kind of artefacts or noise. An evaluation, using a public database, is presented showing that the recognition of word-spotting can be improved considerably by using de-noising filters in the feature space.
A major focus of recent large scale digitisation initiatives has been historical texts, primarily in the form of out-of-copyright newspapers and books. However, the Optical Character Recognition (OCR) software used to translate the scanned images to machine-readable text does not provide satisfactory results for historical documents. This is due to issues inherent in the material such as warped pages, bleed-through, historical fonts, broken and irregular characters, complex layouts, and spelling variants. In the large scale project Improving Access to Text (IMPACT), a European team of scientists, industry partners and digitisation professionals have been working together to enhance existing and develop new approaches to the extraction of text content from historical documents. The project facilitates a successful collaboration between digitisation professionals, based at institutions digitising millions of historical text documents, and scientists in document analysis, language technologies and OCR. This session will detail the work of IMPACT in the context of real life problems faced in the large scale digitisation programmes of libraries and the legacy that the project will leave to foster further research in advancing the state of the art in extracting textual content from historical documents.
In this paper, we present the first effort in preprocessing and character segmentation on digitized Nom document pages toward their digital archiving. Nom is an ideographic script to represent Vietnamese, used from the 10th century to 20th century. Because of various complex layouts, we propose an efficient method based on connected component analysis for extraction of characters from images. The area Voronoi diagram is then employed to represent the neighborhood and boundary of connected components. Based on this representation, each character can be considered as a group of extracted adjacent Voronoi regions. To improve the performance of segmentation, we use the recursive x-y cut method to segment separated regions. We evaluate the performance of this method on several pages in different layouts. The results confirm that the method is effective for character segmentation in Nom documents.
We describe a rule-line removal algorithm for handwritten document images in this paper. Compared to the existing approaches, our algorithm obtains more scalability to higher-resolution images and thicker rule-lines. Derived from the simple gap-filling methods using line-drawing algorithms, we present a novel approach to regenerating the missing portions of text strokes. Using this approach, the deformed text can be restored to its original shape. We also explore the noise filtering method for binarized document images, in particular by choosing the morphological operator in accordance with the noise power of the input image. Our approach has proven to be effective by experiments on both real and synthetic handwritten document images.
Death certificates provide important data such as causa mortis, age of death, birth and death places, parental information, etc. Such information may be used to analyze not only what caused the death of the person, but also a large number of demographic information such as internal migration, the relation of death cause with marital status, sex, profession, etc. Thanatos is a platform designed to extract information from the Death Certificate Records in Pernambuco (Brazil), a collection of "books" kept by the local authorities from the 16th century onwards. The current phase of the Thanatos project focus on the books from the 19th century.
Our previous work has shown that the error correction of optical character recognition (OCR) on degraded historical machine-printed documents is improved with the use of multiple information sources and multiple OCR hypotheses including from multiple document image binarizations. The contributions of this paper are in demonstrating how diversity among multiple binarizations makes those improvements to OCR accuracy possible. We demonstrate the degree and breadth to which the information required for correction is distributed across multiple binarizations of a given document image. Our analysis reveals that the sources of these corrections are not limited to any single binarization and that the full range of binarizations holds information needed to achieve the best result as measured by the word error rate (WER) of the final OCR decision. Even binarizations with high WERs contribute to improving the final OCR. For the corpus used in this research, fully 2.68% of all tokens are corrected using hypotheses not found in the OCR of the binarized image with the lowest WER. Further, we show that the higher the WER of the OCR overall, the more the corrections are distributed among all binarizations of the document image.
This paper presents novel results for word spotting based on dynamic time warping applied to medieval manuscripts in Latin and Old Swedish. A target word is marked by a user, and the method automatically finds similar word forms in the document by matching them against the target. The method automatically identifies pages and lines. We show that our method improves accuracy compared to earlier proposals for this kind of handwriting. An advantage of the new method is that it performs matching within a text line without presupposing that the difficult problem of segmenting the text line into individual words has been solved. We evaluate our word spotting implementation on two medieval manuscripts representing two script types. We also show that it can be useful by helping a user find words in a manuscript and present graphs of word statistics as a function of page number.
We introduce the Concurrent Activity Recognizer (CAR) - an efficient deep learning structure that recognizes complex concurrent teamwork activities from multimodal data. We implemented the system in a challenging medical setting, where it recognizes 35 different activities using Kinect depth video and data from passive RFID tags on 25 types of medical objects. Our preliminary results showed our system achieved an 84% average accuracy with 0.20 F1-Score.
Unmanned aerial vehicle (UAV) swarms provide situation awareness in tasks such as emergency response, search and rescue, etc. However, most of these scenarios take place in GPS-denied environments, where accurately localizing each UAV is challenging. Heterogeneous UAV swarms, in which only a subset of the drones carry cameras, face the additional challenge of identifying each individual UAV to avoid sending position updates to the wrong drone, thus crashing. This work presents an identification mechanism based on the correlation between motion observed from external camera, and acceleration measured on each UAV's accelerometer.
Simultaneous Localization and Mapping (SLAM) is the process of learning about both the environment and about a robot's location with respect to the environment and is essential for robots to autonomously navigate. A variety of algorithms using many different sensors such as RGB-D cameras, laser range finders, ultrasonic sensors and others have been proposed to perform SLAM. However, these algorithms face common challenges are that of computational complexity, wrong loop closure detection and failure to localize correctly when robot loses state (kidnapped robot problem). In this work, we utilize Wi-Fi signal strength sensing to aid the SLAM process in indoor environments and address the challenges mentioned above.
Cities have long been considered as complex entities with nonlinear and dynamic properties. Pervasive urban sensing and crowd sourcing have become prevailing technologies that enhance the interplay between the cyber space and the physical world. In this paper, a spectral graph based manifold learning method is proposed to alleviate the impact of noisy, sparse and high-dimensional dataset. Correlation analysis of two physical processes is enhanced by semi-supervised machine learning. Preliminary evaluations on the correlation of traffic density and air quality reveal great potential of our method in future intelligent evironment study.
We are developing an extensible, open-source framework that can localize and track rigid bodies using a network of cameras (both RGB and depth). This system is motivated by two design goals - (i) ease of setup, and (ii) ability to be agnostic of individual cameras and recognition algorithms. The goal of this implementation is to be a poor man's motion capture system that can be quickly set up for experimentation and provide accurate 3-D pose of the rigid body and scalable across cameras and the volume of coverage.
We present a deep learning framework for fast 3D activity localization and tracking in a dynamic and crowded real world setting. Our training approach reverses the traditional activity localization approach, which first estimates the possible location of activities and then predicts their occurrence. Instead, we first trained a deep convolutional neural network for activity recognition using depth video and RFID data as input, and then used the activation maps of the network to locate the recognized activity in the 3D space. Our system achieved around 20cm average localization error (in a 4m &times; 5m room) which is comparable to Kinect's body skeleton tracking error (10--20cm), but our system tracks activities instead of Kinect's location of people.
Lighting plays a major role in photography. Professional photographers use elaborate installations to light their subjects and achieve sophisticated styles. However, lighting moving subjects performing dynamic tasks presents significant challenges and requires expensive manual intervention. A skilled additional assistant might be needed to reposition lights as the subject changes pose or moves, and the extra logistics significantly raises costs and time. We present a new approach to lighting dynamic subjects where an aerial robot equipped with a portable light source lights the subject to automatically achieve a desired lighting effect. We focus on rim lighting, a particularly challenging effect to achieve with dynamic subjects, and allow the photographer to specify a required rim width. Our algorithm processes the images from the photographer's camera and provides necessary motion commands to the aerial robot to achieve the desired rim width in the resulting photographs. We demonstrate a control approach that localizes the aerial robot with reference to the subject and tracks the subject to achieve the necessary motion. Our proof-of-concept results demonstrate the utility of robots in computational lighting.
The demanding nature of human robot collaboration (HRC) in terms of robustness and efficiency requirements make object tracking a tough challenge. Instead of treating the area of HRC as an adversary, the idea is to exploit the abundance of context information available in a human robot collaboration scenario to enhance tracking. In this work, a multi object tracking system that uses context information and which is capable of tracking the 3D pose of multiple objects using RGBD data is presented. In order to showcase the importance of context and the enhancement possible for an object tracker when integrated into a cognitive architecture, several experiments are performed and evaluated. This approach is one of the first to apply and evaluate the concept of multiple object tracking in 3D to a human robot collaborative assembly process.
The paradigm shift of Robotics moving toward autonomous robots meeting human needs in real world settings requires developing robot interaction skills. So, a synergy of different disciplines is essential to achieve that goal. In particular, Human-Robot Interaction (HRI), the study of how humans interact with robots in daily environments, aims to endow robots with the necessary interactive abilities. In this context, efficient people detection and tracking is an imperative preliminary step to increase efficiency by selecting relevant parts of an image to be processed by other systems such as: gesture detection and recognition; face detection, recognition and tracking; understanding human activity, etc. In this paper, we propose to analyze the motion detection problem by providing a solution for a good environment adaptation of the robot system and real-time visual perception, including an accurate people/object distinction
In this paper, we propose a human-robot interaction system in which the robot detects and classifies the target human's gaze pattern into either spontaneous looking or scene-relevant looking. If the gaze pattern is detected as the spontaneous looking, the robot waits for the target human without disturbing his/her attention. However, if the gaze pattern is detected as the scene-relevant looking, the robot establishes a communication channel with him/her in order to explain about the scene. We have implemented the proposed system into a robot, Robovie-R3 as a museum guide robot and tested the system to confirm its effectiveness.
We introduce a new conversational Human-Robot-Interaction (HRI) dataset with a real-behaving robot inducing interactive behavior with and between humans. Our scenario involves a humanoid robot NAO1 explaining paintings in a room and then quizzing the participants, who are naive users. As perceiving nonverbal cues, apart from the spoken words, plays a major role in social interactions and socially-interactive robots, we have extensively annotated the dataset. It has been recorded and annotated to benchmark many relevant perceptual tasks, towards enabling a robot to converse with multiple humans, such as speaker localization and speech segmentation; tracking, pose estimation, nodding, visual focus of attention estimation in visual domain; and an audio-visual task such as addressee detection. NAO system states are also available. As compared to recordings done with a static camera, this corpus involves the head-movement of a humanoid robot (due to gaze change, nodding), posing challenges to visual processing. Also, the significant background noise present in a real HRI setting makes auditory tasks challenging.
This report describes our ongoing attempt to evaluate the previously proposed method of response obligation estimation [1] in a realtime human-robot interaction with a fully automated conversational robot system based on HALOGEN, which is an infrastructural distributed framework for a multimodal multiparty dialogue system. HALOGEN has a two-stage information fusion strategy and it potentially has merits to implement multiparty human-robot interactions. We discuss interaction-oriented compensatory mechanisms for failures in response obligation estimation.
Two HMM-based threshold models are suggested for recognition and incremental learning of scenario-oriented human behavior patterns. One is the expected behavior threshold model to discriminate if a monitored behavior pattern is normal or not. The other model is the registered behavior threshold model to detect whether such behavior pattern is already learned. If a behavior patten is detected as a new one, an HMM is generated to represent the pattern, and then the HMM is used to update behavior clusters by hierarchical clustering process.
In this paper, we present a method for the automatic detection of F-formations for mobile robot telepresence (MRP). The method consists of two phases a) estimating face orientation in video frames and b) estimating the F-formation based on detected faces. The method works in real time and is tailored for images of lower resolution that are typically collected from MRP units.
An intuitive and robust user recognition system is at the key of a natural interaction between a social robot and its users. The gender of a new user can be guessed without explicitly asking it of her, which can then be used to personalize the interaction flow. In this LBR, a novel algorithm is used to estimate the gender of a person based on its morphological shape. More specifically, the vertical outline of the breast of the user is used to estimate his or her gender, based on similar shapes seen during training. On early benchmarks with databases that represent well the diversity of human body shapes, the accuracy rate is close to 90% and outperforms a state-of-the-art algorithm. Our algorithm provides a fast and seamless estimation flow and needs limited computation resources, which tailor it for HRI. Its usefulness has been proved by integrating it in a social robot. However, its use raises concerns among the users about their privacy, which will lead to further study.
The current trend in computer vision is development of data-driven approaches where the use of large amounts of data tries to compensate for the complexity of the world captured by cameras. Are these approaches also viable solutions in robotics? Apart from 'seeing', a robot is capable of acting, thus purposively change what and how it sees the world around it. There is a need for an interplay between processes such as attention, segmentation, object detection, recognition and categorization in order to interact with the environment. In addition, the parameterization of these is inevitably guided by the task or the goal a robot is supposed to achieve. In this talk, I will present the current state of the art in the area of robot vision and discuss open problems in the area. I will also show how visual input can be integrated with proprioception, tactile and force-torque feedback in order to plan, guide and assess robot's action and interaction with the environment. Interaction between two agents builds on the ability to engage in mutual prediction and signaling. Thus, human-robot interaction requires a system that can interpret and make use of human signaling strategies in a social context. Our work in this area focuses on developing a framework for human motion prediction in the context of joint action in HRI. We base this framework on the idea that social interaction is highly influences by sensorimotor contingencies (SMCs). Instead of constructing explicit cognitive models, we rely on the interaction between actions the perceptual change that they induce in both the human and the robot. This approach allows us to employ a single model for motion prediction and goal inference and to seamlessly integrate the human actions into the environment and task context. We employ a deep generative model that makes inferences over future human motion trajectories given the intention of the human and the history as well as the task setting of the interaction. With help predictions drawn from the model, we can determine the most likely future motion trajectory and make inferences over intentions and objects of interest.
The reliable perception of a human in a dynamic environment is the most critical issue for interactive human-robot services. In human-robot interaction, a camera on a robot naturally captures the low-body-part of human because robots are usually shorter than the human. Conventionally, a two-dimensional laser range finder is used in low-body-part detection [1, 2]. However, these methods may cause errors when there are similar structures with legs. This video demonstrates a low-body-part detection scheme that not only exploits three-dimensional characteristics and but also the RGB features of the low-body-part. We build the low-body-part candidates by clustering from the legs to the heap. In the results, spurious candidates are eliminated by the proposed method.
Gaze direction is an important communicative cue. In order to use this cue for human-robot interaction, software needs to be developed that enables the estimation of head pose. We began by designing an application that is able to make a good estimate of the head pose, and, contrary to earlier head pose estimation approaches, that works for non-optimal lighting conditions. Initial results show that our approach using multiple networks trained with differing datasets, gives a good estimate of head pose, and it works well in poor lighting conditions and with low-resolution images. We validated our head pose estimation method using a custom built database of images of human heads. The actual head poses were measured using a trakStar (Ascension Technologies) six-degrees-of-freedom sensor. The head pose estimation algorithm allows us to assess a person's focus of attention, which allows robots to react in a timely fashion to dynamic human communicative cues.
In this paper, we present a core technology to enable robot recognition of human activities during human-robot interactions. In particular, we propose a methodology for early recognition of activities from robot-centric videos (i.e., first-person videos) obtained from a robot's viewpoint during its interaction with humans. Early recognition, which is also known as activity prediction, is an ability to infer an ongoing activity at its early stage. We present an algorithm to recognize human activities targeting the camera from streaming videos, enabling the robot to predict intended activities of the interacting person as early as possible and take fast reactions to such activities (e.g., avoiding harmful events targeting itself before they actually occur). We introduce the novel concept of 'onset' that efficiently summarizes pre-activity observations, and design a recognition approach to consider event history in addition to visual features from first-person videos. We propose to represent an onset using a cascade histogram of time series gradients, and we describe a novel algorithmic setup to take advantage of such onset for early recognition of activities. The experimental results clearly illustrate that the proposed concept of onset enables better/earlier recognition of human activities from first-person videos collected with a robot.
In Advanced Driving Assistance Systems (ADASs), monitoring the driver's cognitive status during driving is considered as an important issue. Because, most of the accidents in the automotive sector occur due to the driver's misinterpretation or lack of sufficient information regarding the situation. In order to prevent these accidents, current ADASs include lane departure warning systems, vehicle detection systems, advanced cruise control systems, etc. In a particular driving scenario, the amount of information available to the driver regarding a situation can be judged by monitoring the driver's gaze (internal information) and distributions corresponding to the forward traffic (external information). Therefore, to provide sufficient information to the driver regarding a driving scenario it is essential to integrate the internal and external information which is lacking in the current ADASs. In this paper, we use 3D pose estimate algorithm (POSIT) to estimate driver's attention area. In order to estimate the distributions corresponding to the forward traffic we employ Bottom-up Saliency map. To integrate the internal and external information we use conditional mutual information.
Palmprint recognition is a challenging problem, mainly due to low quality of the patterns, variation in focal lens distance, large nonlinear deformations caused by contactless image acquisition system, and computational complexity for the large image size of typical palmprints. This paper proposes a new contactless biometric system using features of palm texture extracted from the single hand image acquired from a digital camera. In this work, we propose to apply convolutional neural network (CNN) for palmprint recognition. The results demonstrate that the extracted local and general features using CNN are invariant to image rotation, translation, and scale variations.
This paper presents a real-time hand gesture detection and recognition method. Proposed method consists of three steps - detection, validation and recognition. In the detection stage, several areas, estimated to contain hand shapes are detected by random forest hand detector over the whole image. The next steps are validation and recognition stages. In order to check whether each area contains hand or not, we used Linear Discriminant Analysis. The proposed work is based on the assumption that samples with similar posture are distributed near each other in high dimensional space. So, training data used for random forest are also analyzed in three dimensional space. In the reduced dimensional space, we can determine decision conditions for validation and classification. After detecting exact area of hand, we need to search for hand just in the nearby area. It reduces processing time for hand detection process.
Pedestrian detection is used in video surveillance systems and driver assistance systems. The purpose is to build automated vision systems for detecting pedestrians as shown in figure 1. We use Histograms of Oriented Gradients (HOG), which are one of the well-known features for object recognition. HOG features are calculated by taking orientation histograms of edge intensity in a local region [1]. In this paper we select the interesting point in the image by using FAST features detector and extracted HOG features around these strongest corners and use them as an input vector of linear Support Vector Machine (SVM) to classify the given input into pedestrian/non-pedestrian. By using FAST detector we reduce the number of features less than half without lowering the performance.
The importance of researches in the old days is the retrieval of similar objects. Content-Based Image Retrieval (CBIR) system uses low-level features such as texture, color, and shape to extract the features, but when it comes to food images it is hard to get satisfactory accurate results. Recognition of food images has recently become very important and challenging due to people's health care, religious or cultural reasons. In this paper, we propose a system that recognizes small food images consist of 10 categories by using bag of features (BoF) based on SURF detection features. In addition, we achieved up to 78% accuracy rate, and try to improve the feature detection by using color features at the same time with the SURF feature detection. This experiment shows that more accurate rate of results will be obtained than the existing methods.
This paper proposes an agent to assist human cognition in memorizing multiple human faces by analyzing user's eye gaze points. The gaze point which is the direction of sight is obtained by the infrared camera on a glass-type agent with the help of an embedded module. The gaze information is then combined with the image captured by the frontal camera to identify the location of the face that the user is looking at among several faces. The gaze detection and face selection with tracking are performed in embedded modules attached to the glass-type agent, and the recognition of the selected facial images is performed and shown on a mobile computer connected via wireless network. The major contribution of the proposed work is the use of eye gaze direction to select faces of interest, and provide information regarding the faces to improve human memory capability in recalling the faces.
This abstract introduces an efficient method for identifying various facial expressions from image inputs. To recognize the emotions of the facial expressions, a number of facial feature points were extracted. The extracted feature points are then transformed to 49-dimensional feature vectors which are robust to scale and translational variations, and the facial emotions are recognized by a support vector machine (SVM). Based on the experimental results, SVM performance was obtained by 50.8% for 6 emotion classification, and 78.0% for 3 emotions.
Face recognition is used to identify humans by their face image. Recently it becomes most common application in information security. Bag of features has been successfully applied in face recognition. In our research we use SURF features and try to improve it by using block-based bag of feature models. In this method we partition the image into multiple blocks and we extract SURF features densely on each block. We compare the performance of the original bag of feature model with Grid/Detector method and bag of block-based feature model.
Knowing where a person is looking is an important parameter of every human-human interaction. Detecting a person's gaze could significantly improve the interaction capabilities of today's robotic agents. But many robots' visual systems are limited by data bandwidth and optical hardware. We propose a low-cost high-def pan/tilt/zoom active vision system that could significantly improve the robot's eye tracking capabilities. We tested the proposed system for improving mutual gaze detection in a human-robot interaction scenario and found significant results compared to systems without zoom capability.
Due to food culture, religion, allergy and food intolerance we have to find a good system to help us recognize our food. In this paper, we propose methods to recognize food and to show the ingredients using a bag-of-features (BoF) based on SURF detection features. We also propose bag of SURF features and bag-of HOG Features at the same time with the SURF feature detection to recognize the food items. In the experiment, we have achieved up to 72% of accuracy rate on a small food image dataset of 10 categories. Our experiments show that the proposed representation is significantly accurate at identifying food in the existing methods. Moreover, the enhancement of the visual dataset with more images will improve the accuracy rates, especially for the classes with high diversity.
Image classification, in general, is considered a hard problem, though it is necessary for many useful applications such as automatic target recognition. Indeed, no general methods exist that can work in varying scenarios and still achieve good performance across the board. In this paper, we actually identify a very interesting problem, where image classification is dangerously easy. We look at the problem of image classification, in the specific context of accurately classifying images containing highly sensitive data such as drivers licenses, credit cards and passports. Our key contribution is to build a Hierarchical Temporal Memory (HTM) network that is able to classify many sensitive images with over 90% accuracy, and use this to develop a system to automatically derive and transcribe sensitive information from image data. Our system classifies images into two groups -- sensitive and non-sensitive. The group of sensitive images can then be further analyzed. This is a real world security issue that could easily lead to privacy problems such as identity theft, since scans of passports and drivers licenses are routinely emailed or kept in digital form, and many local documents are left unencrypted. Essentially, an attacker can use data mining and machine learning techniques very effectively to breach individual privacy. Thus, our main contribution is to demonstrate the efficacy of image classification for deriving sensitive information, which could also serve as a guide for other interesting applications such as document detection and analysis. Thus, it also serves as a warning against leaving data unencrypted and again proves that security through obscurity is simply not enough.
Selecting clothes and taking trials of them at the stores are always being time consuming job. It becomes, sometimes, difficult to find clothes of desired color, fabric and size. It is more difficult for customers who use e-shopping for their shopping as visualization is not possible. In this paper we are proposing a web-based system based on 3D human modeling and its visualization. System is deployed in two parts, one on client system and other on the server. Client system accepts customer's dimensions of their exposed body part, taken in traditional fashion, and 2D image of their face as input. Client system generates a 3D model using these inputs along with the face of the customer. Color of skin of the model is same as that of the 2D face image. Customers can customize their clothing style using digital styles and accessories present in the digital library at client system. After completion of clothing modification user can place orders. Client system sends all information, size and model, to the server.
In this paper, an image steganography system implemented is described, in which the data hiding (embedding) is realized in bit planes by using image segmentation based on a local complexity measure. A complexity measure is defined in Steganography for discriminating noisy regions in an image. This paper presents a revised method for embedding data into an image by dynamic variation in complexity threshold measure. The principle of the method is based on that of bit plane complexity based steganography. High embedding rates are achieved with low distortion based on the theory that noise-like regions in an image's bit-planes can be replaced with noise-like secret data without significant loss in image quality Percentage embeddable area of complex images is around. 50% without visual distortion showing PSNR above 30dB, which is the unique salient feature of this technique.
Segmenting a mammographic images into homogeneous texture regions representing disparate tissue types is often a useful preprocessing step in the computer-assisted detection of breast cancer. That is why we proposed new algorithm to detect cancer in mammogram breast cancer images. In this paper we proposed segmentation using vector quantization technique. Here we used Kekre's Fast Codebook Generation algorithm (KFCG) for segmentation of mammographic images. Initially a codebook of size 128 was generated for mammographic images. These code vectors were further clustered in 8 clusters using same KFCG algorithm. Eight segmented images were obtained for each code vector. These 8 images were displayed as a result. This approach does not leads to over segmentation or under segmentation, as is the case for watershed segmentation and entropy segmentation using Gray Level Co-occurrence Matrix. Results of these algorithms are shown for comparison.
In this paper we analyze and test several steganographic techniques on still images. We show that embedding a large amount of data into the picture can modify its visible properties. We compare the LSB, DCT and Wavelet steganographic techniques; we also analyze their advantages and disadvantages in hiding secret data. In steganography it is important that the embedded data size should be minimum so that host image will not annihilate.
Adaptive Neuro-Fuzzy system for automatic multilevel image segmentation and edge detection. This system consists of a multilayer perceptron (MLP)-like network that performs image segmentation using thresholds automatically pre selected by Fuzzy C-means clustering algorithm. The learning technique employed is self supervised allowing, therefore, automatic adaptation of the neural network. This system does not require a priori assumptions whatsoever are made about the image (type, features, contents, stochastic model, etc.). Such algorithms are most useful for applications that are supposed to work with different (and possibly initially unknown) types of images. This system is also useful for applications dealing with more complex scenes, where several objects have to be detected. This system produces much smoother results as compared to region growing and histogram thresholding techniques and it is (type, features, contents, stochastic model, etc.). Such algorithms are most useful for applications that are supposed to work with different (and possibly initially unknown) types of images. This system is also useful for applications dealing with more complex scenes, where several objects have to be detected. This system produces much smoother results as compared to region growing and histogram thresholding techniques and it is robust to noise and un illumination conditions.
In this paper we propose novel Face Recognition method based on vector quantization (VQ) using Kekre's Median Code Book Generation (KMCG) algorithm. The performance of the proposed method is compared with the well known face recognition method based on Discrete Cosine Transform (DCT). Both the methods are implemented on Georgia Tech Database of 750 images consisting of 15 images for each of 50 individuals. From the results it is observe that our proposed method gives 92.67 % accuracy as compared to DCT. Further it is observed that KMCG requires 99.45% computation less than DCT.
The proposed method detects the exact location of masses and circumscribed masses in mammograms based on RBFNN (Redial Basis Function Neural Network) with accuracy of 62% and 50% respectively for mammograms containing masses. The recognition rate for the normal one reaches 94.89% in MIAS (Mammography Image Analysis Society) database. Also the results are independent of preprocessing. This procedure is implemented by performing sub-image windowing analysis. The evaluation of the proposed mass and circumscribed mass detection was carried out in the MIAS database, giving reliable detection.
The objective of segmentation of medical image is to extract and characterize anatomical structures from the images. Segmentation of medical is quiet difficult task because most images contain large noise. Canny operator has decent anti-noise ability. However Edge based canny operator is not consecutive and applying the canny operator on total image make reduces the performance of the system. In this paper, a new method of segmentation by the integration of 2D Otsu method with Canny edge detector and Region Growing is proposed. Here the first the low pass filter to reduce the noise and then Otsu thresholding method is used to extract the region of interest. In this system, Region Growing and Edge detection algorithm are executed parallel. This parallel executing system is used to get the edge map of image. This system is used to identify the Brain tumor. It is also used for Bone Fracture identification and Classification of Blood Cells. Experiments have shown that this system gives best segmentation results for brain tumor identification.
In this paper, we present the working of various preprocessing and representation techniques that are being used for illumination invariant 3 D face recognition. Face recognition is a well known biometric technique specially used for human identification and mostly applicable in security applications, user identification, and enrollment/authentication systems. Generally most of the applications use the 2D face recognition systems, but these systems are prone to various factors like pose, expression, lighting conditions, etc. Now a day's 3D face recognition has become an evergrowing and promising biometric technique because of its robust features. We are presenting the techniques that are used for representation of the 3D data and matching faces for illumination compensated face recognition.
With the result of advancement in today's technology, digital content can be easily copied, modified, or distributed. Digital watermarking provides the solution to this problem. Most of the digital watermarking methods are divided into two categories: Robust watermarking and fragile watermarking. As a special subset of fragile watermarking, reversible watermarking (lossless or invertible watermarking) enables us to recover the image which is same as the original image pixel by pixel after the content is authenticated. This type of lossless recovery is compulsory in sensitive imagery applications like medical and military purposes. An efficient watermarking algorithm has been implemented using Matlab which uses the concept of difference expansion of high pass transform coefficients with watermark bits. This work was to find a reversible watermarking algorithm for JPEG2000 standard for medical applications, a (5, 3) wavelet transform is used which is considered as lossless transform in the JPEG2000 standard. In the algorithm, (5, 3) Integer wavelet transformed high pass coefficients are difference expanded instead of Haar wavelet transformed coefficients in Mark Tian's algorithm. Based on the Algorithm developed for Matlab modeling, a new architecture for Reversible watermarking was designed and the hardware modeling for that architecture was done using Verilog HDL. By difference expanding the high pass coefficients of the image and embedding the watermark in those high pass coefficient, maximum embedding capacity over 90000 bits is achieved for a 256x256 image. The Watermark embedding block is synthesized using Xilinx ISE and implemented on Spartan3 FPGA. The Reversible Watermarking Block operates at a maximum clock frequency of 62.073 MHz with a minimum period of 16.110ns. The Latency of the system is N+2 clock cycles for a total of N pixels macro block. The embedding capacity of 2bits at a time are used to embed in the high frequency coefficients, as number of bits to be embedded increases the Peak Signal to Noise Ratio decreases up to 31.3%.
In today's world, the scope and applications of face recognition systems either as standalone or as an integrated module into a larger system cannot be belittled. Remarkable face recognition algorithms have been proposed in last decade offering towering levels of accuracy and precision. However, the flexibility and robustness of the existing systems remain highly questionable. To further intricate these problems, most of the face recognitions systems are designed under strict background constraints, conveniently ignoring variations in illumination and at the apex of the predicament, lies the intricacy involved in the creation and maintenance of training sets in the databases. Even the most sophisticated systems designed posed grave difficulties in deployment and required maintenance by skilled personnel, leading to poor response from the global market. With an objective to realize an intelligent, maintenance-free system with impeccable design, our research team developed an immaculate real-time face recognition system with dynamic training and enhanced multi-algorithm face recognition. The system would facilitate user-friendly, dynamic creation of training sets by means of an innovative approach which would make the need of skilled maintenance personnel obsolete. The proposed system has been comprehensively tested to achieve remarkable precision and accuracy of 99.9%. Proposed system archetype design would result in manifold applications in developing simplified face recognition systems (for enrolling masses) for institutions, business organizations, research labs, military applications, etc. for authentication and authorization purposes.
With the advances in Information, Networking and Communication technologies, the Multimedia services are gaining high popularity. Especially applications like Video-On-Demand (VoD) systems are becoming inherently popular. VoD is a data-intensive application because clients frequently retrieve data stored on high end servers. As the numbers of users are exponentially growing from thousands to millions severing such large request is a great challenge to the multimedia companies. Hence VoD service providers are intensely interested in high QoS services to users by using efficient algorithms or strategies or architectures. Numerous solutions in terms of algorithms or architectures are available to reduce response times. Among these an efficient techniques for replication, storage and retrieval/streaming has attracted much attention from researchers due to its effectiveness and low cost. This paper focuses on proposing an optimal video replication, storage and retrieval strategies for VoD systems.
Looking at the expected technical improvements as to the spatial and spectral resolution, satellite imagery could more and more provide a basis for complex information systems for recognizing and extracting even small-scale and short-term structural features of interests within nuclear facilities. The analysis of large volumes of multisensor satellite data will then definitely require a high degree of automation for processing, analysis and interpretation in order to extract the features of interest. Against this background, the present paper focuses on the automated extraction of various objects like Waterbodies, Roads, Buildings etc in high resolution remotely sensed images using Definiens eCognition software.
The science of tracking an object has seen a constant evolution in order to find more efficient means. Tracking of an object should be fast and accurate. When the object is moving with respect to the end user its parameters like shape and pixel values keep changing continuously. This paper tries to track the path of the moving objects using edge detection with respect to the end user using MATLAB programming.
Watershed algorithm is powerful tool for image segmentation. But classical watershed segmentation is sensitive to noise and can leads to serious over-segmentation. This paper presents an improved algorithm of watershed transformation based on opening-closing operation and webber perception principle.
The classification of various land cover features using fully polarimetric Synthetic Aperture Radar (SAR) data sets is an important application of radar remote sensing. The data acquired is over various parts of India are SIR-C L- and C-band data over Kolkata city and its surroundings. The field work was carried out in April 2009. Similarly, PALSAR quad pol data over several areas is acquired. The proposed classifier is based on the artificial neural network which is developed in Matlab and it makes use of backscattering values. It is a supervised classification technique which is applied on the ALOS PALASR and SIR-C data. The classification accuracy after applying different speckle filters is compared with the classification accuracy obtained without applying filter. It is also compared with minimum distance and maximum likelihood techniques.
Iris recognition has become a popular research in recent years due to its reliability and nearly perfect recognition rates. Iris recognition system has three main stages: Image preprocessing, Feature extraction and Template matching. In the preprocessing stage, iris segmentation is critical to the success of subsequent feature extraction and template matching stages. Most recent algorithm on template matching proposed by Libor Masek shows an improvement of 3.6 % over existing algorithm like Hamming Distance. This paper addresses for improvement to Libor Masek algorithm of Template matching method for Iris Recognition. The method evaluates on iris images taken from the CASIA iris image database version 1.0 and version 3. Experimental results show that the proposed approach has more efficient than to Libor Masek in terms of Template matching Time of about 99%, Creation of template is of about 10 % and False Rejection Ratio (FRR) is of about 10 %.
Volume segmentation is an important part of computer based medical applications for diagnosis and analysis of anatomical data. With rapid advances in medical imaging modalities and volume visualization techniques, computer based diagnosis is fast becoming a reality. These computer based tools allow scientists and physician to understand and diagnose anatomical structures by virtually interacting with them. Volume segmentation plays a critical role by facilitating automatic or semiautomatic extraction of the anatomical organ or region of interest. In this review we provide an introduction to various segmentation algorithms found in the literature. We classify the algorithms in to three categories: structural techniques, statistical technique and hybrid techniques. Under structural techniques we will review algorithms which take into consideration structural information for segmentation stochastic techniques are those which perform segmentation based on statistical analysis methods and under hybrid techniques we will review algorithms which make use of structural information in addition to statistical analysis.
This paper proposes an improved reversible color to textured gray image mapping. The method was originally devised using the wavelet transform, where high-frequency subbands are replaced by subsampled chrominance plane. The problem with this method was lower Peak Signal to Noise Ratio (PSNR). Proposed method is an attempt to improve the original method by embedding the content of both horizontal and vertical subbands into diagonal subbands and making a place to embed Cb and Cr chrominance planes. To embed two subbands into one we have used two dimensional Discrete Cosine Transform (2D-DCT). Our aim is to reduce the loss in subband replacement. The experimental results shows that we have achieved higher PSNR, almost up to 2--3 dB gain compared to the original method. This makes the proposed method a good candidate for application that needs to preserve the original color fidelity and sharpness.
Quality inspection of surface mount capacitor is an offline process and usually done by inspecting some capacitors in a lot using compound microscopes. We propose to use location M estimator with any edge detection methods to inspect the basic dimensions of multi-layer ceramic chip capacitors (MLCC) like width, length, separation distance between two end terminations and the local deviations on the termination boundaries. Usually the distances are calculated by an average distance. The average operator is not robust to outliers in the data. In this paper, we propose to use the combination of location M estimator with any type of edge detection technique which will remove the need of a specific optimal edge detection technique and thus can result into easy hardware realization to inspect the basic dimensions of MLCC.
Human face detection is concerned with finding location and size of every human face in a given image. Locating skin pixels in images or video sequences where people appear has many applications, especially those related to Human-Computer Interaction. Face detection plays a very important role in human computer interaction field. It represents the first step in a fully automatic face recognition, facial features detection, and expression recognition. There are many techniques used in face detection, each one has its advantages and disadvantages. An algorithm is proposed to improve the performance of face detection and eye detection using skin color model under poor illumination condition. Skin regions are extracted using a set of bounding rules based on the skin color distribution obtained from a training set. After detecting the face region, the Hough transform is used in order to detect the facial feature i. e. eye, specially circular Hough transform is used in order to detect the circular object in the image.
In present era the multi/hype spectral images are being increasingly used in traditional and key application areas such as remote sensing and geosciences. These images contain geographical information and reflect the complexity of geographical features and spatial structures. As the means of observing and describing geographical phenomena, the rapid development of remote sensing has provided an enormous amount of geographical information which is highly correlated. The massive wealth of information is very useful in a variety of applications but the sheer bulk of this information has increased beyond what can be analyzed and used efficiently and effectively. This uneven increase in the technologies of gathering and analyzing information has created difficulties in its storage, transfer, and processing. The paper attempts to develop an application-specific data compression technique that exploits inter-band correlation and then applies HAAR wavelet to selected bands of the multispectral image 'orlea_s.lan'. We finally calculate the loss functions in statistics like MSE, MAE, RMSE and PSNR.
Handwriting character recognition (HCR) for Indian Languages is an important problem where there is relatively little work has been done. In this paper, we investigate the moments features on Kannada handwritten basic character set of 49 letters. Moments features are extracted from the preprocessed original images by most of the researchers. Kannada characters are curved in nature with some symmetry observed in the shape. This information can be best extracted as a feature if we extract moment features from the directional images. So we are finding 4 directional images using Gabor wavelets from the dynamically preprocessed original images. We then extract moments features from them. The comparison of moments features of 4 directional images with original images when tested on Multi Layer Perceptron with Back Propagation Neural Network shows an average improvement of 13% from 72% to 85%. The mean performance of the system with these two features together is 92%.
This paper describes the methods to increase the capacity of the cover image for information hiding. In this paper we have proposed two novel multiple least significant bit replacement techniques KIMLA and KAMLA to increase the information hiding capacity of the cover image. Comparative results of KIMLA, KAMLA and their previous version KMLA [1] are also shown. These novel techniques use multiple LSB'S bit replacement technique to hide the secret information. The proposed methods KAMLA and KIMLA have shown better results for AFCPV, PBC ratios and they have been tested for secret information in image as well as text form.
The advancements in the field of Computers and other related technologies are becoming a boon to multimedia industry. One of the multimedia service that is gaining attraction from millions of customers across the world is Video-on-Demand (VoD) systems. Hence, effective strategies for distribution of load among the servers are essential. This paper focuses on solving the load imbalance problem by using a dynamic load balancing strategy. In particular, it uses distributed dynamic load balancing strategy because of its efficiency upto 93% when compared to the centralized dynamic load balancing strategy which has 90% of efficiency. Thus, a comparative analysis of centralized and distributed strategies for dynamic load balancing in VoD systems is carried out.
This paper gives an overview of J2ME application for mobile devices and also gives a detail of our work in the field of a web browser having streaming capability for mobile devices, especially multimedia cell phones. This web browser solves the problem of tabbed-browsing with style, freedom, and customization. The paper gives a basic overview of the progress achieved and future advancements in our project we are looking forward to. If you like using Internet Explorer, Safari, Mozilla Firefox, Opera, or Chrome on your desktop, then you're going to love this browser on your hand held devices (i.e. mobile devices). Full control bar (back, forward, refresh, etc), that only displays when appropriate and it also save any history. The entire application works in either orientation. At the present stage, the number of mobile devices with different form factors and unique input and output facilities is growing substantially. Cell phones have their web browsers with limited functionalities and low multimedia support. Our current work has reached web page parsing, downloading documents, multimedia files and video streaming. Also, video streaming is tested on local server (Apache Tomcat) and this application can run on low bandwidth GPRS network.
Security has become a rising concern for every field in computing info media. The earlier use of passwords and number codes do not suffice to the cause nowadays. This paper deals with various biometric measures being adopted in technology. Focussing on Face recognition and its applications, the various possibilities in different areas are explored.
Wetland is one of the important ecosystems in the world. It provides a valuable habitat for a great variety of hydrologic plants, fishes, wildlife and insets. The data over wet regions of India has been processed for classification of various land features like mangrove, ocean water, and clear water. In this study the utility of NASA's Shuttle Imaging Radar-C (SIR-C) data is evaluated for classifying wetlands in India. Minimum Distance and Decision tree classification techniques are used. The SIR-C data is acquired over Kolkata region of West Bengal, India. The results show that multipolarization SAR data helps to classify wetlands effectively. The combinations of different polarizations from L- and C- band helps to improve the classification accuracy. The combinations of L-HV, C-HH, C-HV and L-HH, C-HH, C-HV had the best overall accuracies. These two 3 band combination can differentiate well the six classes. The five band combination L-HH, L-HV, L-VV, CHH, C-HV gives the highest classification accuracy. It is greater than the three band combinations which are mentioned above. By applying enhanced Lee filter the accuracy can be further increased. The enhanced Lee filter removes the speckle effectively.
This paper presents the design of a Service Oriented Architecture (SOA) -based platform for image processing on web. In service oriented architectures, the most important element is the service, a resource provided to remote clients via a service contract. SOA-based systems may prove to be a good solution to address problems like large storage and expensive computational requirements faced in image processing applications. We propose a component-based platform which is not tied to a specific programming language or a specific technology. The Web service provides various functions of pre-processing, noise removal, segmentation and feature extraction. Each phase in image processing system is viewed as a Web service operated on the Internet. Each algorithm is implemented as a Web service. Set of algorithms are implemented for each phase and the phases of image processing are linked dynamically. The dynamically linking of web service is based on the quality defining attributes of the input image.
Image morphing now a day has received a lot of attention. Still photographs convey limited information, but using 2D image morphing animations can be added to the still photographs. Color transition methods used in image morphing control the rate of color blending and hence directly decide the quality of the morphs generated. The middle image in the entire morph sequence is a key. If it look good then entire sequence looks good. In this paper a new color transition method is proposed, which generates a better quality middle image and entire morph sequence than most commonly used cross dissolve method of color transition.
This method is based upon the image registration process and the application is when the text which is to be identified is behind the mesh which works as a hurdle. We know that the mesh as hurdle can be made less irritating by either moving the camera or the source itself. The method uses Radon Transform for extracting the mesh lines and capturing the position of the mesh lines. The final process of filling the deformed image is through the registration. The method is adaptive to movement in any direction. Quaternion method is used to remove the scaling and rotational errors. It was tested on a number of images approximately and gave excellent results.
Hyperspectral imagery provides richer information about materials than multispectral imagery. The new larger data volumes from hyperspectral sensors present a challenge for traditional processing techniques. Principal component analysis (PCA) has been the technique of choice for dimension reduction. Spectral data reduction and estimation using wavelet filter bank with perfect reconstruction can be considered for better results.
In this paper, a novel accurate method for detection of copy-move forgery is proposed. The proposed method compresses digital image using Discrete Wavelet Transform (DWT) up to the certain desired level based on the image size. This reduced dimensional image is then divided into overlapping blocks of fixed size. These blocks are sorted using lexicographic sorting and duplicated blocks are identified by searching for rows that get closer to other rows after sorting. The comparative analysis with different methods shows that the proposed scheme gives better time complexity and accuracy of detection. Even the method is able to detect forgeries on images where attacker has applied various retouching operations to make forgery harder to detect.
This paper proposes an improved method for attacking the LSB (Least Significant Bit) matching steganography. In LSB matching steganography the least two or more significant bit-planes of the cover image would be changed during the embedding and thus the pairs of values do not exist in stego image. In the proposed method, an image is obtained by combining the least three significant bit-planes and is divided into 3x3 overlapped subimages. The subimages are grouped into eight types, i. e. T1, T2, T3, T4, T5, T6, T7 and T8 according to the count of gray levels. A message is embedded by LSB matching. The alteration rate of the number of elements belonging to T1 is computed. It is found that normally the alteration rate is higher in cover image than in the corresponding stego image. This is used as the discrimination rule in the method. The literature consists of the method which focuses on the combination of least two significant bit-planes. Experimental results demonstrate that the proposed method gives better results than those obtained by combining the least two significant bit-planes. It is an efficient method to detect the LSB matching stegonagraphy in gray scale images.
In this paper we propose fast face recognition system based on the Kekre's Fast Code book Generation (KFCG) method. This algorithm can be easily implemented and number of coefficients required for recognition reduces drastically compared to Eigen face approach. Thus computational burden decreases. We have compared the performance of KFCG with PCA. The algorithm is tested for different percentage of occlusion applied on various codebook sizes. Here we have tested proposed scheme on standard ORL database and locally generated unconstrained database of Indian faces. ORL database gave the accurate results and local database gave accuracy of 89% which is higher than PCA approach.
Fingerprints being unique and life-long characteristics of human, are the most popular way of identification. With the extent of database increasing and the requirement of reduced processing time, there is a continual demand and extended scope for research in this area. In this paper, a method which deals with fingerprint identification in the transform domain is considered. The one-step Walsh transform i. e. either the row or the column transform of the fingerprint is subjected to sectorization to generate the feature vector. Sectorization is done in the complex plane after the sequency components are separated. The scores of both the row and column transform techniques are fused together used MAX and OR rules. The algorithm has been tested on a database of 168 images of 21 individuals. The results with accuracy of more than 96% show that the method can be satisfactorily used in fingerprint identification.
Segmentation of liver tumor on Computed Tomography (CT) images is a challenging task due to anatomic complexity and the imaging system noise. The conventional region growing method has a widespread use in medical image segmentation because of its robustness to noise. However, region growing algorithm is semi-automatic in which the initial seed point and threshold value have to be manually identified. To avoid these problems, in this paper we propose a automatic region growing method that incorporates fuzzy c-means clustering algorithm to find the threshold value and modified region growing algorithm to find seed point automatically. In this paper, we also describe a framework to create a three dimensional (3D) model of the liver which can be used by the surgeons for tumor volume measurement, liver transplant and surgical planning. The proposed method has been tested on several CT images of liver. The results show that the algorithm successfully detects the edges of the liver tumor distinguishing it from the background without manual intervention.
We propose a multi step motion estimation algorithm (MSME) that encompasses techniques such as motion vector prediction through initial search, refinement of motion vector to locate true motion vector and early termination criteria that suits to all type of video characteristic. This approach allows us to exploit random distribution of motion vector in successive video frames from which the initial candidate predictors are derived. The derived predictors are the most probable points in search window, which will assure that, the motion vectors in the vicinity of center point and at the edge of the search window does not miss out, as it does for earlier algorithms like Three step search(TSS), Four step search(FSS), Diamond(DS), etc and refinement stage used in the algorithm will allow us to extract true motion vector so that the picture quality is as good as Full search(FS) which is the optimal algorithm. The novelty of the proposed MSME algorithm is that the search pattern derived is not static but can dynamically shrink or enlarge to account for small and large motion. Fixed threshold used improves speed without sacrificing the quality of video. The Simulation result shows that our proposed algorithm outperforms all sub-optimal algorithms in terms of quality and speed up performance and in many cases PSNR of proposed algorithm is comparable to Full Search.
The paper is devoted to the use of Wavelet transform for feature extraction associated with image pixels and their classification in comparison with the watershed transform. Since watershed algorithm was applied to an image segmentation then it will have over clusters in segmentation. To solve this first we applied wavelet transform then watershed algorithm was applied to segment the image and then applying the inverse of wavelet transform to get the segmented image with high resolution. A specific attention is paid to the use of Haar transform as a tool for image compression and image pixels feature extraction. Wavelets provide a mathematical way of encoding information in such a way that it is layered according to level of detail.
This paper presents, efficient transform based face recognition technique which considers full and partial feature vector of an image. 2D-DCT and Walsh transform is applied on the resized image of size 128x128, to obtain its feature vector. Partial feature vector is obtained by selecting 75% rows and columns of feature vector, 50% rows and columns of feature vector and so on. The smallest size of partial feature vector is selected as 4x4. Proposed technique is tested on two different databases. Georgia Tech Face Database contains JPEG color images and Indian Face Database contains Bitmap color images of varying size. Recognition rate is calculated for varying size of selected feature vector using DCT and Walsh transform and compared. Also computational complexity in terms of number of CPU units is compared in both the cases: with full feature vector and with partial feature vector. Results show that, Walsh transform gives better recognition rate than DCT and number of CPU units required using 2D- Walsh transform is almost 9 times less than that of required by using 2D-DCT. This is because the multiplications required in Walsh transform are zero.
The new technique for image retrieval using the color-texture features extracted from images based on vector quantization with Kekre's fast codebook generation is proposed. This gives better discrimination capability for CBIR. Here the database image is divided into 2x2 pixel windows to obtain 12 color descriptors per window (Red, Green and Blue per pixel) to form a vector. Collection of all such vectors is a training set. Then the Kekre's Fast Codebook Generation (KFCG) is applied on this set to get 16 codevectors. The Walsh transform is applied on each column of the codebook, followed by Kekre's transform applied on each row of the Walsh transformed codebook. This transform vector then is used as the image signature (feature vector) for image retrieval. The method takes lesser computations as compared to conventional Walsh applied on complete image. The method gives the color-texture features of the image database at reduced feature set size. Proposed method gives better precision and recall as compared to full Walsh based CBIR. Proposed method avoids resizing of images which is required for any transform based feature extraction method.
Signature identification and verification is considered among the most popular biometric methods in the area of personal authentication. The proposed method is based on offline verification of signature by two different algorithms. Before extracting different features from the signature, some preprocessing of the signature is done. In preprocessing, the signature is colour normalized and scaled into a standard format. The first algorithm is based on discrete cosine transform and it takes into consideration all the Discrete Cosine Transform (DCT) coefficients while finding match between test signature and signature stored in the database. The second algorithm is improved version of the first algorithm; it considers only significant DCT coefficients while matching. Both the algorithms use Euclidean distance classifier for comparing test signature with database. The algorithms have shown promising results while dealing with random forgeries and simple forgeries; also it gives good recognition rates.
Advancements in computing and networking have paved a path for the growth of the multimedia industry. Today, multimedia has a number of applications that can significantly attract a wide range of customers towards it. Applications such as digital libraries and Video-on-Demand (VoD) services are becoming more popular in the current era. Thus, online access to multimedia information has become possible and cost efficient. As the number of customers increase, the load on the server(s) also increases and may future lead to degradation of the performance of the system as well as the QoS being offered. This is mainly due to some of the effects of heavy load on the servers. Thus, load balancing is becoming more a trivial issue in the design of VoD systems. This paper focuses on proposing a multithreaded dynamic load balancing strategy for Video-on-Demand (VoD) systems.
With advancement in technology and also increase in consumer requirements, mobile phones are no longer just a mere communication tool. More and more applications are embedded onto the mobile phones nowadays. Here we introduce a novel technique for handheld mobile devices which enables the user interface to be controlled by the motion of the user's hand. A feature --based approach is proposed for local motion estimation that exploits the direction in which the handheld device is moved and the phone's motion is then used to trigger some events on the phone for example photo browsing, music track changing, games control etc. Special motion sensors such as accelerometers provide a straight forward solution but require extra hardware to be installed. Today, computer vision is a more natural choice, because current mobile phones are often are often equipped with cameras that can provide visual input for estimating motion.
Computed tomography (CT) images have been widely used for liver disease diagnosis. The CT images have been used for the analysis as these images are more clear compared to other imaging techniques. This paper focuses on developing method for classifying liver tumor from CT images using texture analysis and neural network classifier. The Co-occurrence matrices are used to extract feature like contrast, correlation, entropy, homogeneity, energy. Then the probabilistic neural network is trained to classify liver tumors as malignant and benign. The proposed system was evaluated by several liver images. It produces accuracy of 93.75%. The performance of the proposed system is also evaluated by calculating specificity, sensitivity.
Touch-Less Fingerprint Recognition Is Considered As A Feasible Alternative To Touch-Based Fingerprint Recognition Technology. Touch-Less Fingerprint Technology Provides A Near Perfect Solution To The Problems In Terms Of Hygienic, Maintenance And Latent Fingerprints. This Paper Presents An Introduction To Touch-Less Fingerprint Recognition System Using A Digital Camera. This Paper Also Discusses Both The Disadvantages And The Advantages Of Touch-Less Fingerprint Systems Along With The Approaches To Capture Touch-Less Fingerprint.
The paper addresses a commonly encountered motion blur problem in photographic images, when there is relative motion between the camera and the object being captured. When a photograph is taken in low light conditions or of a fast moving object, motion blur can cause significant degradation of the image. Both the moving object and camera shake contribute to this blurring. The overall approach comprises of taking a standard (non-blurred) image, creating a known blurring function (point spread function-PSF) and then filtering the image with this function so as to add blur into it. This image is further corrupted by different amount of additive Gaussian noise. The aim is to deblur this image by various deblurring algorithms viz., direct and pseudo-inverse filtering, Wiener and parametric Wiener filtering, constrained least squares filtering and Richardson-Lucy algorithm, then analyze and compare their properties. Experimental evaluation is carried out in MATLAB environment on standard lena and cameraman images and these methods are compared in a variety of blur and noise conditions. Both qualitative and quantitative assessment based on popular performance metrics in image processing i. e., peak signal-to-noise ratio (PSNR) and mean squared error (MSE), provides an objective and subjective standards to compare the deblurring methods.
Content Based Image Retrieval System retrieves images using color, texture and shape properties of the image. Different methods which are implemented in this paper are Discrete wavelet transform (DWT), Gabor wavelet transform (GWT), Color histogram (CH), color autocorrelogram (CA). Integration of color and texture features is done using different methods and their comparison is done using precision and recall as performance measures. .DWT (D) method is implemented using the combination of statistical mean and standard deviation features and perceptual feature directionality. The best results are obtained with GWT (A) + CH method as compared to all other ten methods as phase information from the Gabor transformed coefficients is taken into consideration.
This paper analyzes the concept of robot mimicking in the field of Human-Machine Interaction (HMI). Gestures are investigated for HMI applications. Detection of a moving human arm is done by a video image and calculating the orientation of the arm. The angle of orientation found is passed to robot arm in order to orient the robot arm. The simulations show that it is possible to determine human arm orientation by initial background image information or tracking of features.
Steganographic method for embedding secret messages into a gray- valued cover image is proposed. It uses the fact that, Human visual system is having low sensitivity to small changes in digital data. It modifies pixel values of image for data hiding. Cover images is partitioned into non-Overlapping blocks of two consecutive pixels. Difference between the two consecutive pixel values is calculated. These difference values are classified into number of ranges. Range intervals are selected according to the characteristics of human vision's sensitivity to gray value variations from smoothness to contrast. A small difference value indicates that the block is in a smooth area and a large one indicates that the block is in a edged area. The pixels in edged area can tolerate larger changes of pixel values than those in the smooth area. So, in the proposed method we can embed more data in the edged areas than the smooth areas. The difference value then is replaced by a new value to embed the value of a sub-stream of the secret message. The number of bits which can be embedded in a pixel pair is decided by the width of the range that the difference value belongs to. The method is designed in such a way that the modification is never out of the range interval. This method not only provides a better way for embedding large amounts of data into cover images with imperceptions, but also offers an easy way to accomplish secrecy. This method provides an easy way to accomplish secrecy. This method provides an easy way to produce a more imperceptible result than those yielded by simple least-significant bit replacement methods. The embedded secret message can be extracted from the resulting stego-image without referencing the original cover image. Experimental results show the feasibility of the proposed method.
In recent years, thousands of images are generated everyday, which implies the necessity to classify, organize and access them by easy and faster way. The need for image classification is becoming increasingly important. Classifying the images into semantic categories is a challenging problem. Although this is usually not a very difficult task for humans, it has been proved to be an extremely difficult problem for computer programs. This paper presents the idea of using Discrete Cosine Transform to generate the feature vector for Image Classification. The various sizes of feature vectors are generated such as 8X8, 16X16, 32X32, 64X64 and 128X128. The proposed algorithm is worked over database of 1000 images spread over 10 different classes. The Euclidean distance is used as similarity measure. A threshold value is set to determine to which category the query image belongs to.
Signature identification and verification is considered among the most popular biometric methods in the area of personal authentication. In proposed method, we deal with offline verification of signature by two different algorithms. Before extracting different features from the signature, some preprocessing of the signature is done. In preprocessing, the signature is colour normalized and scaled into a standard format. The algorithm is quite different and it deals with extraction of features based on mean, standard deviation and moment. The algorithm uses Euclidean distance classifier for comparing test signature with database. The algorithm have shown promising results while dealing with random forgeries and simple forgeries; also it gives good recognition rate.
FFT algorithms are mainly used for the calculation of DFT because they are preferred due to their increased speed and higher efficiency, which arises due to the fact that for the calculation of an N-pt DFT, the sequence is broken into several segments and the DFT for each segment is calculated. However, for this many redundant memory spaces and the Butterfly structures are required for DFT calculations. Grouping the identical twiddle factors of different stages together reduces the number of memory references and the storage space due to twiddle factors, therein reducing the number of clock cycles needed for the complete implementation of the algorithm. For further reduction combine the DIT at the initial stage and DIF at the final stage. This technique is referred as DITF. It combines both the benefits of DIT and DIF and thus reducing the memory reference. The DITF algorithm reduces the number of the complex additions and multiplications in the calculation and hence in turn reduces the computational time. Prime Factor Algorithm provides an unified derivation of the Cooley-Tukey FFT and the Winograd Fourier Transform Algorithm (WFTA) FFT. In this method of reduction, arithmetic is used as an index mapping (a change of variables) to change the one-dimensional DFT into a two or higher dimensional DFTs. This is done in order to change a large problem into several easier smaller ones which makes its computing quicker and simpler and thereby reducing the computational time and does not require any butterfly structures. This algorithm is implemented in C language to verify their reduction in computation time. The same can also be verified using Code composer studio (CCS) and implemented in DSP Processors for the verification of reduced clock cycles. The reduction in memory spaces and memory references of the twiddle factors is evident in all the methods implicitly from the nature of the methods. Thus this can be an efficient method for the calculation of FFT algorithms.
The proposed system showed high hiding rates with reasonable imperceptibility compared to other steganographic systems, DCT and better audio quality. The results shown gives detail comparison between DWT and DCT. In this paper a novel method for digital audio Steganography with security i. e. cryptography is presented where covert data is embedded into the coefficients of host audio (cover signal) in integer wavelet domain using quantization to reduce embedding error. Performance analysis based on quantization and wavelet decomposition is explained in result section. The characteristics of this method are imperceptibility, robustness, large payload, high audio quality and full recovery.
Finger-Knuckle-Print is an emerging biometric trait. Because of high degree of uniqueness and low requirement of user co-operation it has got potential to become building block of future biometric security systems. In this paper we propose finger-knuckle-print verification using kekre's wavelets. Kekre's wavelets are based on kekre's transform and easy to construct. We discuss the feasibility of using kekre's wavelet to extract spectral features of the finger-knuckle-print and use them for verification. In this paper we have used wavelet energy feature, we have compared the results with Haar wavelets. Kekre's wavelet based features give moderate accuracy and performance is same as Haar wavelets.
As research in computer vision shifted from processing single, static images to the manipulation of video sequences the concept of movement recognition has become important. Movements are the most atomic primitives, requiring no contextual or sequence knowledge to be recognized. Movement is often addressed using either view-invariant or view specific geometric techniques. Here an attempt is made to develop a view-based approach to recognition of movement that is designed to support the direct recognition of the motion itself without reference to underlying static poses of the body. The basis of the representation is motion history image (MHI) -- a static image where intensity is a function of the recency of motion in a sequence. Then recognition is performed by using both binary and scalar valued versions of the MHI as temporal templates to match against obtained instances of movement.
In this paper, the fetal sonographic head markers for estimating the gestational age for Indian population has been evaluated and presented. Combinational Nonlinear Mean Median (CNLMM) filter along with the Shape Sensitive Derivative Segmentation Scheme has been utilized to single out the region of interest. It has been inferred that using the proposed scheme it is possible to estimate the gestational age effectively upto 27 weeks of gestation with an estimated deviation of ?3 days, which is relatively high compared to the previous studies. Regression and trend analysis on observed data has also been carried out for obtaining the 10th, 50th and 90th percentile values and plotted. The strength of fit has been evaluated using SSE, R Squared and RMSE values to find the association of the estimated percentile values with the actual data.
As the use of digital media increases, effective retrieval and management techniques become more important. In order to extract useful information from this huge amount of data, many content based image retrieval (CBIR) systems have been developed. A typical CBIR system captures image features such as color, texture, or shape of objects in the query image and tries to retrieve images from the database with similar features. The main advantage of CBIR systems with relevance feedback is that these systems take into account the gap between the high-level concepts and low-level features and subjectivity of human perception of visual content. In this paper, a CBIR system based on color histogram has been presented and a new approach for image storage and retrieval called association-based image retrieval (ABIR) has been studied. The results of both systems have been shown.
We have developed a machine vision based liquid level inspection system which decides the liquid level of bottle to be under or overfilled using ISEF edge detection technique. The system is having a conveyer belt controlled by Siemens LOGO24RLC PLC. MATLAB image acquisition toolbox along with a normal web camera is used for image acquisition purpose. We apply ISEF Edge detection technique and an average distance algorithm to decide about the level of the liquid in the bottles. A GUI based interfacing software is made to display the over and under filled status of the bottles on the screen.
SPECK has all the desirable properties of embeddedness, progressive transmission, low computational complexity, low dynamic memory requirements, fast decoding/encoding and provides excellent performance. In this paper SPECK algorithm is discussed and implemented.
Iris recognition has been a fast growing, challenging and interesting area in real-time applications. A large number of iris recognition algorithms have been developed for decades. The paper presents novel Walshlet Pyramid based iris recognition technique. Here iris recognition is done using the image feature set extracted from Walsh Wavelets at various levels of decomposition. Analysis was performed of the proposed method, consisting of the False Acceptance Rate and the Genuine Acceptance Rate. The proposed technique is tested on an iris image database having 384 images. The results show that Walshlet at level-5 outperforms other Walshlets, because the higher level Walshlets are giving very fine texture features while the lower level Walshlets are representing very coarse texture features which are less useful for discrimination of images in iris recognition.
This work proposes a new filter based on progressive decision using Dempster-Shafer theory, to suppress the impulse noise to preserve details of image and to restore image corrupted by random valued impulse noise. The new filter mechanism is composed of an efficient D-S impulse detector and a noise filter. The D-S evidence theory provides a way to deal with uncertainty in the evidence. The pixel detection and noise filtering are applied progressively through several iteration. If a pixel is noisy then the proposed filter will replace it with the central noise free ordered mean value otherwise is kept unchanged. The final image is further refined by using median filter which improves the image quality. Simulation results reveal that proposed algorithm outperforms other existing median filters for random noise.
Face detection has been an attractive field of research for both neuroscientists and computer vision scientists. Humans are able to identify reliably a large number of faces and neuroscientists are interested in understanding the perceptual and cognitive mechanisms at the base of the face detection process. Face detection helps human being to focus on the face recognition system, optimizing the system speed and performance. In this paper to detect the face Gabor filters are used. It performs dimensionality reduction and linear discriminate analysis on the Gabor wavelet faces. Gabor wavelet faces combined with neural network classifier shows very good performance, and achieves maximum correct recognition rate on different databases.
Extensive research on continuous speech recognition (CSR) marks the current trend in Automatic Speech Recognition (ASR) course. Topics concerning CSR include acoustic modeling, segmentation, language modeling and word decoding. This paper describes an efficient method for recognizing words derived from continuous speech signal. Speech boundaries are very obscure due to overlapping context of the acoustic units, designated as the coarticulation effects. Considering larger segments such as words, helps tapering off much of this issue. Nevertheless, the problem still exists between interconnected words thus addressing the aim of our study. Like any other static classifier, Support Vector Machines (SVM) inherently unable to receive variable input length. Our solution is to extend the size of acoustic segment incrementally rather than shifting a fixed block over the utterance. Recognition is determined via voting the highest posterior probability score for a word segment by means of confidence measure. We proposed a two-fold embedded grammar strategy that eliminates both the decoding process and out of vocabulary errors. We argue that these notions have some advantages over previous works, hence could yield better result.
In this paper, the algorithm is developed by combining advantages of the median filters for filtration of noisy pixels with noise detection step. The algorithm works well for suppressing impulse noise with noise density ranging from 10 to 70% while preserving image details. The proposed algorithm is based on the two schemes:- 1) Impulse noise detection scheme and 2) Adaptive filtering scheme. In the beginning, the noisy pixel in the image is detected by Iterative way. Finally, the noise is suppressed by estimating the values of the noisy pixels with a switching based median filter applied exclusively to those neighborhood pixels not labeled as noisy. The size of filtering window is adaptive in nature, and it depends on the number of noise-free pixels in current filtering window. Simulation results prove that the proposed method outperforms other existing Median filters for removing of Impulse noise with preserving image details.
This paper presents the development of a three level unsupervised segmentation framework based on color and texture features. An important contribution of this work consists of a new formulation of three different clustering algorithms at three different levels. In the first level, a multiclass clustering algorithm using binary quaternion moment preserving thresholding algorithm is applied in order to quantize the colors. In the second level, clustering is performed on the quantized image using Self-organizing map for the estimation of the optimal number of components in the image and to resolve the initialization problem of mixture model based clustering which is carried out in the third level. The clusters obtained in the second level are then, refined and modelled using an adaptive spatial finite mixture model in the color-texture feature space. Since the dimensionality and the complexity of the image space is reduced at every level the proposed algorithm is fast and efficient. The proposed algorithm is applied on the Berkeley database images and complex natural images. The results are competent with the JSEG and CTex algorithms.
Skin detection plays an important role in a wide range of image processing applications ranging from Hand and face detection, tracking in gesture analysis. This work deals with skin color identification algorithm using color segmentation to detect human hands and face in color images. For color segmentation both Single Gaussian Model (SGM) and Gaussian Mixture Model (GMM) are used on different images. Experimental results on images presenting variations in lighting condition and background, and variation in age of the person, demonstrate the efficiency of described skin-segmentation algorithm. This work evolves and compares four GMM's with respect to the SGM.
In wireless scenarios, the image is transmitted over the wireless channel block by block. Due to severe fading, we may lose an entire block, even several consecutive blocks of an image. We aim to reconstruct the lost data using correlation between the lost block and its neighbors. The basic idea is to first automatically classify the block as textured or structured (containing edges), and then fill-in the missing block with information propagated from the surrounding pixels. If the lost block contained structure, it is reconstructed using an image inpainting algorithm, while texture synthesis is used for the textured blocks. We also combine this approach with JPEG compression itself, where the encoder voluntarily skips blocks, and these are reconstructed at the decoder in the same fashion as in the wireless scenario. The switch between the two schemes is done in a fully automatic fashion based on the surrounding available blocks. The performance of this method is tested for various images and combinations of lost blocks.
Road extraction in urban areas from high resolution satellite images helps in creating a database of a city and in cartography. The extraction results are intended to be used for updating a road database. The high dimensionality of aerial and satellite imagery presents a challenge to the human analysis based on the traditional classification algorithms using statistical assumptions. Artificial Neural Networks (ANNs) on the other hand may represent a valuable alternative approach for land cover mapping for such highly dimensional imagery. The urban areas contain roads of different shapes, sizes and lengths. In this paper, the extraction algorithm performs edge detection, morphological reconstruction, feature extraction and classification. The road features are classified using ANNs.
In this paper, we discussed Graph matching based approach for signature verification and cross-validation for same. Database signature
Handwritten Signatures are one of the widely used biometric traits for document authentication as well as human authorization. Various techniques have been implemented for Automatic Signature Recognition. In this paper we discuss the application of vector quantization to the problem of signature recognition. Vector quantization based methodology is used here to detect intra and inter-class variations in signatures. Here we discuss a method for the codebook generation; this method is fast and simple. We use the codebook to generate a codevector histogram specific to the signature template. The spatial moments related to the codevectors are also calculated. These parameters are used to classify the signature. Feasibility of this technique for signature recognition is discussed in his paper.
NOTE FROM ACM: It has been determined that the author of this article plagiarized the contents from a previously published paper. Therefore ACM has shut off access to this paper. The paper by Joshi, Rajput, and Jadhav plagiarizes the following paper: Feature Extraction Based on Moment Invariants for Handwriting Recognition by R. J. Ramteke and S. C. Mehrotra which may be found here For further information, contact the ACM Director of Publications.
Technology is proliferating. Many methods are used for medical imaging. The important methods used here are fast marching and fast level set in comparison with the watershed transform. Since watershed algorithm was applied to an image has over clusters in segmentation. Both methods are applied to segment the medical images. First, fast marching method is used to extract the rough contours. Then fast level set method is utilized to finely tune the initial boundary. Moreover, Traditional fast marching method was modified by the use of watershed transform. The method is feasible in medical imaging and deserves further research. In the future, we will integrate level set method with statistical shape analysis to make it applicable to more kinds of medical images and have better robustness to noise.
Handwritten signatures are one of the oldest biometric traits for human authorization and authentication of documents. Majority of commercial application area deal with static form of signature. In this paper we present a method for off-line signature recognition. We have used morphological dilation on signature template for measurement of the pixel variance and hence the inter class and intra class variations in the signature. The proposed feature extraction mechanism is fast enough so that it can be applied for on-line signature verification also.
In this paper we recommend a block-wise fragile watermarking scheme based on histogram segmentation which is capable to rebuild tampered content of an image. First we divide the image into blocks having 4 X 4 dimension then calculate Authentication bit by extracting five MSBs from each pixels and pass it to a hash function. Do bit-wise Ex-OR of those hash bits with pseudo random matrix generated by a secrete key. These Authentication bits are used to replace first LSB of corresponding pixel. Then using histogram segmentation technique, generate four clusters for each block and estimate average of each cluster. Now using bit encoding method, generate two Recovery bits for each pixel. These three LSBs for each pixel within a block will be swapped by its mapping block. At the receiver end using Authentication bit with a particular threshold value, one can be acquainted with the tampered block and extracted Recovery bits from mapping pixel gives adequate information to recover principal content of host image.
Image segmentation is a decisive and fundamental step for remote sensing information retrieval and classification. High-resolution satellite image classification using standard per-pixel approaches is difficult because of the high volume of data, as well as high spatial variability within the objects. One approach to deal with this problem is to reduce the image complexity by dividing it into homogenous segments prior to classification. This has the added advantage that segments can not only be classified on basis of spectral information but on a host of other features such as neighborhood, size, texture and so forth. Segmentation of the images is carried out using the region based algorithms such as marker-based watershed transform by taking the advantage of multi-resolution and multi-scale gradient algorithms. This paper presents an efficient method for image segmentation based on a multi-resolution application of a wavelet transform and marker-based watershed segmentation algorithm. Experimental result of proposed technique gives promising result on QuickBird images. It can be applied to the segmentation of noisy or degraded images as well as reduce over-segmentation.
The present work focuses on design and development of approaches for intelligent vehicle systems with application centered in recognition of road signs. The traffic signs captured from a digital camera is processed to extract various unique identifiable features and compared with standard signs. The algorithm is tested using image set of different traffic signs and non traffic signs. The recognition rate is 90% for mandatory signals and 85% for cautionary signals. The method can easily be modified to include new classes of signs.
Signatures are extensively used as a means of personal verification. Manual signature-based authentication of a large number of documents is a very difficult and time consuming task. Consequently for many years, in the field of protected communication and financial applications, we have observed an explosive growth in biometric personal authentication systems that are closely connected with measurable physical unique characteristics (hand geometry, iris scan, finger prints or DNA) or behavioural features. Human signatures provide secure means for confirmation and authorization in legal documents. So nowadays, automatic signature verification becomes an essential component. In order to convey the state-of-the-art in the field to researchers, in this paper we present a survey of off-line signature verification systems.
In this paper we develop a computational model to identify the unknown person's face by comparing characteristics of face to those of known individuals. Principal Component Analysis, based on information theory concepts, seek a computational model that best describe a face. Eigenface approach is the principal component analysis method, in which small set of characteristic pictures are used to describe the variation between face images. Goal is to find out the eigenvectors (eigenfaces) of the covariance matrix of the distribution, spanned by a training set of face images. Later, every face image is represented by a linear combination of these eigenvectors. The eigenface algorithm has been applied to extract the basic face of the human face images stored in database of faces (e.g. ORL face database). Recognition is performed by projecting a new image into the subspace spanned by the eigenfaces and then classifying the face by comparing its position in face space with the positions of known individuals. In this approach we treat the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by 2-D characteristic views.
The field of image processing field is inundated with new ideas and concepts put forth by various researchers with an intention to overcome the challenges encountered during face detection. Among the various techniques proposed to achieve optimum face detection, skin segmentation has been most popular for its coherence and its minimalism in ascertaining faces from photos. However, the major obstacle while attempting to achieve exceptional face detection using skin segmentation is the disparity of the skin color when an image is captured under different lighting conditions. The hues of the captured skin vary from natural light to artificial indoor lighting to partial lighting and shadow effect of light. Also, considering the diverse skin textures of people worldwide, it is intricate to achieve immaculate skin detection in the presence of such disparity. The paper presents a novel technique designed to perform skin segmentation based face tracking independent of lighting conditions. The proposed skin segmentation algorithm has been implemented and tested to identify all possible skin tones under light variant conditions. Integration of the novel skin segmentation algorithm into a larger real-time face detection system yielded 99.9% accuracy and precision, thus achieving a flawless skin-segmentation based face tracking system.
Vector Quantization is a technique of compressing data based on grouping blocks having similar data. These blocks are called Code Vectors and all the code vectors grouped together is called a Codebook. The key to VQ data compression is a good codebook. In order to reduce bandwidth overhead it is necessary to generate Global Codebook for a particular class of images. Otherwise local codebook has to be transferred every time before the transmission of image. In this paper various global codebook generation algorithms for vector quantization for color images are presented.
Preprocessing of document image is a very important step to handle the deformations namely noise, different handwriting complexities that may result in base line skew, word skew, character skew, accents may be cited either above or below the text line and parts of neighboring text lines may be connected, etc. The paper proposes a novel preprocessing technique for handwritten document to handle some of the deformations usually present in the document like touching components, overlapping components, skewed lines, words with individual skews etc. and build a proper text image with all these deformations removed. Based on the analysis of Indian script character shapes and literature survey, it proposes a new sequence of preprocessing methods. A binarized image is sub-sampled and connected components are extracted. These components are dilated and thinned and is given to Hough transform for both global skew and local skew detection for line extraction. The word segmentation is done with the computation of the distances of adjacent components in the text line image and classification of the previously computed distances as either inter-word gaps or inter-character gaps. The extracted words can be used for producing properly aligned text image or for text conversion using OCR.
Patients undergoing physical therapy go through a series of sessions performing exercises to help improve the range of motion (RoM) in affected regions of the body due to disease or injury. However, patients find these tasks repetitive and boring and end up not completing the prescribed therapy program. It has been shown that game based therapy exercises have led to increased rates of compliance. In this paper, we provide a continuation of previous work in VR-based therapy and present Pot Hunter, and one type of RoM analysis for when a person reaches above their head.
Over the last years, automotive industry has shown a tremendous interest in Advanced Driver Assistance Systems (ADASs), especially the ones based on driver's bio-features. As most car producers strive to meet the increasing needs of high-end and even average consumers, more and more complex systems are being developed. The current trend is to maximize the synergy between humans and machines by designing better user interfaces (UIs) which can anticipate the behavior of the driver. Great research efforts are put into inferring car position, traffic environment, driver's condition (e.g. degree of drowsiness, driving skill, emotional state) and intentions (e.g. changing lanes, overtaking other cars). The goal of this study is to design and implement an ADAS based on a dual camera mobile phone. We address the mathematical models, the main software modules and the system architecture implied by such a system. Within this paper, we focus on developing the system architecture, as seen from different perspectives: User View, Logic Segmentation View and Process View.
Users of written languages have the ability to quickly and easily look up the meaning of an unknown word. Those who use sign languages, however, lack this advantage, and it can be a challenge to find the meaning of an unknown sign. While some sign-to-written language dictionaries do exist, they are cumbersome and slow to use. We present an improved American Sign Language video dictionary system that allows a user to perform an unknown sign in front of a sensor and quickly retrieve a ranked list of similar signs with a video example of each. Earlier variants of the system required the use of a separate piece of software to record the query sign, as well as user intervention to provide bounding boxes for the hands and face in the first frame of the sign. The system presented here integrates all functionality into one piece of software and automates head and hand detection with the use of an RGB-D sensor, eliminating some of the shortcomings of the previous system, while improving match accuracy and shortening the time required to perform a query.
In this paper, we present results of joint segmentation and classification of sequences in the framework of conditional random field (CRF) models. We use a recently proposed dual-functionality CRF model: on the first level, the proposed model conducts sequence segmentation, while, on the second level, the whole observed sequences are classified into one of the available learned classes. We evaluate the efficacy of our approach considering a real-world application, and we compare its performance to popular alternatives.
This paper demonstrates the assembly and verification of an inexpensive and straightforward stepping dynamics assessment system capable of simultaneously recording human lower limb motions, vertical ground reaction forces (GRF), and two dimensional foot center of pressures (COP) during the gait stance phase. This proposed system uses a single webcam video camera for motion analysis in the sagittal (side) plane. A color detection image processing Python script enables the webcam to track distinctly colored marker tape placed on the ankle and knee joint while stepping on and over a Nintendo? Wii Balance Board (WBB). The WBB is used to measure vertical GRF and foot COP. Marker positions and COP are used to construct a foot roll-over shape (ROS), the effective rocker shape that a lower limb system conforms to during a step. The accuracy of our WBB-webcam system is evaluated by the comparing marker motion, GRF, COP, and derived ROS measurements to a commercial force plate (FP) and eight-camera infrared motion capture (IRMC) system.
Rheumatoid Arthritis is a chronic disease that leads to swelling and inflammation of the joints and even spread to surrounding tissues and blood vessels. Physical therapy has been used successfully to slow the effects of this degenerative disease. Patients, however, do not want to do these exercises due to the fact they are boring and repetitive. In this paper, we introduce the first steps in creating a virtual environment using a CAVE System for the physical therapy sessions where the user will be engaged and motived to complete the exercises prescribed by his or her doctor.
The purpose of this paper is twofold. First, we introduce our Microsoft Kinect--based video dataset of American Sign Language (ASL) signs designed for body part detection and tracking research. This dataset allows researchers to experiment with using more than 2-dimensional (2D) color video information in gesture recognition projects, as it gives them access to scene depth information. Not only can this make it easier to locate body parts like hands, but without this additional information, two completely different gestures that share a similar 2D trajectory projection can be difficult to distinguish from one another. Second, as an accurate hand locator is a critical element in any automated gesture or sign language recognition tool, this paper assesses the efficacy of one popular open source user skeleton tracker by examining its performance on random signs from the above dataset. We compare the hand positions as determined by the skeleton tracker to ground truth positions, which come from manual hand annotations of each video frame. The purpose of this study is to establish a benchmark for the assessment of more advanced detection and tracking methods that utilize scene depth data. For illustrative purposes, we compare the results of one of the methods previously developed in our lab for detecting a single hand to this benchmark.
Automatic patient though record categorization (TR) is important in Cognitive Behavior Therapy (CBT), which is an useful augmentation of standard clinic treatment for Major Depressive Disorder (MDD). Because both collection and labeling of TR data are expensive, it is cost prohibitive to require a large amount of TR data, as well as their cor-responding category labels, to train a classification model with high classification accuracy. As in practice we only have very limited amount of labeled and unlabeled training TR data, traditional semi-supervised learning methods and transfer learning methods, which are the most commonly used strategies to deal with the lack of training data in statistical learning, can not work well in the task of automatic TR categorization. With the recognition of these challenges, in this paper we propose to approach the TR categorization problem from a new perspective via self-taught learning, an emerging topic in machine learning. Self-taught learning is a special type of transfer learning, instead of requiring labeled data from an auxiliary domain that are relevant to the classification task of interest as in traditional transfer learning methods, it learns the inherent structures of the auxiliary data and does not require their labels. Consequently, we may learn a classifier using the limited amount of labeled TR texts to achieve decent classification accuracy, with the assistance from the large amount text data obtained from some cheap, or even no-cost, resources. That is, a cost effective TR categorization system can be built up, which is of particular use in practical diagnosis and training of new therapists. We demonstrate the proposed method in the task of classifying the real depression homework texts, where promising experimental results validate the effectiveness of our new method.
Advances in robotic surgery and growing use of robots in minimal access surgery (MAS), has increased the need of adapting computer-vision algorithms for surgical-vision applications. While methods for 3-D reconstruction are extensively investigated on man-made environments, the surgical-vision lacks such studies on 3-D reconstruction methods and their pros and cons on endoscopic images. In this paper we extensively compared several dense stereo reconstruction methods on a mock-up model using videos acquired from the daVinci endoscope. Also, the advantages and disadvantages of each method for different stages of stereo reconstruction are mentioned and supported by exhaustive experiments on endoscopic images.
We propose a novel framework for online analysis of visual structured processes, using fusion from multiple cameras. Online recognition is performed through particle filters supported by hidden Markov models. We evaluate three fusion methods, an early fusion, a simple multiplication of the observation probabilities and a multi-stream one implying cross-stream coupling of observations and states. The performance is thoroughly evaluated under two complex visual behavior understanding scenarios: a visual process for table preparation in a kitchen and a real life manufacturing process in an industrial plant. The obtained results are compared and discussed.
This paper displays the possible uses of facial metrics to create custom fitted POG goggles. This custom device will improve usage and enhance conformability for the end user.
Evacuation is a complex process influenced by multiple parameters that have significant impact on the design and execution of an efficient Active Evacuation Route (AER). Computer vision algorithms are critical for an effective AER, since it indicates the current situation awareness of the environment. Thermal imaging is an alternative effective computer vision mechanisms for the analysis of the crowd behavior either at the micro or macro scale. Thermal imaging allows efficient determination of people from the background even if highly dynamic scenes, illumination, occlusions or content alterations. This allows micro-scale analysis of the crowds resulting in an efficient active evacuation design. Experiments on thermal data from Athens International Airport indicate the assistive performance of our method.
Using video game technology in physical rehabilitation has shown many positive results in the past few years. The release of the Microsoft Kinect has presented many new opportunities for development in physical rehabilitation technologies. However, there have been questions about the Kinect's accuracy in actual experimentation. In this paper, we compare skeleton data obtained by a Kinect to that obtained by a VICON system in order to determine the accuracy of the Kinect while a tracked subject is moving their arm around. This is the first steps towards a much larger physical rehabilitation system.
Social interactive robots require sophisticated perception and cognition abilities to behave and interact in a natural human-like way. The proper perception of behavior of interaction partner plays a crucial role in social robotics. The interpretation of these behaviors and mapping them to their exact meanings is also an important aspect that interactive robots should have. This paper proposes an interaction model for communicating verbally and nonverbally with human. Human behavior, during the interaction with the robot, is perceived and then interpreted depending on the situation in which the behavior has been detected. In this model, head gestures are used as a back channel (feedback) for the robot to adapt the interaction scenario. The back channel signals can be consciously or unconsciously generated by human. Simultaneously, the eye gazes are also detected to ensure right interpretation of head gestures. In order to recognize the human head gestures, head poses have been tracked over time. A stream of images with their corresponding depth information, acquired from a Kinect sensor, are used to find, track, and estimate the head poses of human. The proposed model has been tested in various experiments with different scenarios in interaction with human.
In this paper, we present our approach towards developing visual competencies for socially assistive robots within the framework of the HOBBIT project. We show how we integrated several vision modules using a layered architectural scheme. Our goal is to endow the mobile robot with visual perception capabilities so that it can interact with the users. We present the key modules of independent motion detection, object detection, body localization, person tracking, head pose estimation and action recognition and we explain how they serve the goal of natural integration of robots in social environments.
An interesting challenge in e-Health is to develop tools and software in order to benefit the healthcare services. Our applicative context is Magnetic Resonance Imaging (MRI). The main purpose of this paper is to propose a regularization framework for solving an inverse reconstruction problem in MRI. We focus on the Split Bregman method, which is a well known efficient tool for solving a wide variety of optimization problems e.g. total variation minimization problems arising from image denoising. The proposed denoising approach, based on the TV/ROF model, involves a second-order derivative penalty term and, accordingly, introduces some modifications to the Split Bregman scheme. Our iterative regularization strategy has interesting features in highlighting the image contrasts and in the noise removal. Numerical experiments prove the goodness of the proposed approach.
An essential component of any hand gesture recognition system is the hand detector and tracker. While a system with a small vocabulary of sufficiently dissimilar gestures may work well with approximate estimations of hand locations, more accurate hand position information is needed for the best results with a large vocabulary of complex two-handed gestures, such as those found in sign languages. In this paper we assess the feasibility of using a popular commercial skeleton tracking software solution in a large vocabulary gesture recognition system using an RGB-D gesture dataset. We also provide a discussion of where improvements in existing methods utilizing the advantages of depth-sensing technology can be made in order to achieve the best possible results in complex gesture recognition.
Automatic human action recognition is a research topic that has attracted significant attention lately, mainly due to the advancements in sensing technologies and the improvements in computational systems' power. However, complexity in human movements, input devices' noise and person-specific pattern variability impose a series of challenges that still remain to be overcome. In the proposed work, a novel human action recognition method using Microsoft Kinect depth sensing technology is presented for handling the above mentioned issues. Each action is represented as a basis vector and spectral analysis is performed on an affinity matrix of new action feature vectors. Using simple kernel regressors for computing the affinity matrix, complexity is reduced and robust low-dimensional representations are achieved. The proposed scheme loosens action detection accuracy demands, while it can be extended for accommodating multiple modalities, in a dynamic fashion.
The iris signature is used in "Iridology", for person identification and calculation of torsional eye movements in video-oculography. Iris data are expensive and difficult to be acquired and their amount plays an important role in recognition accuracy when data mining methods are used. However, privacy issues often restrict open exchange of data between stakeholders. The present article presents a privacy-preserving neural network protocol, for horizontally-partitioned datasets, i.e. datasets that share common attributes but contain different records at each party. The proposed protocol assumes a malicious user model and does not use homomorphic cryptographic methods which are inherently only suited for a semi-trusted user environment. The performance analysis shows that the communication overhead is low enough to warrant its use while the computational complexity is identical in most cases with the centralized computation scenario (e.g. a trusted third party). The accuracy of the output model is only marginally subpar to a centralized computation on the union of all datasets. Another important aim of this work is to search proper choice of the part of the iris signature for person identification and calculating torsional eye movements. Also, estimate changes of the iris contour, sections of the iris and elements of the iris signature. The mathematical model of formation of the iris image on the plane was compared with the real image of the iris.
Integrating robotic platforms in smart home environments can improve the monitoring quality of daily activities. In this study, we explore a scenario where a robot provides a service to the users, which in our case is delivering a cup of coffee. The users place their order via an application, which at the same time captures a short video from their upper-body and their face to obtain information about their identity and to recognize them during the delivery phase. The proposed approach comprises three distinct steps. At a first step the robot detects groups of people, then it captures information from their faces and their upper body and measures the distance with the probe and identifies the person with the higher probability. Finally it approaches this person, performs an additional identification and delivers the cup of coffee. Through real-time preliminary tests under different illumination conditions, we verified that the robot can execute the task with high accuracy.
This paper presents an Unmanned Aerial Vehicle (UAV), based on the AR.Drone platform, which can perform an autonomous navigation in indoor (e.g. corridor, hallway) and industrial environments (e.g. production line). It also has the ability to avoid pedestrians while they are working or walking in the vicinity of the robot. The only sensor in our system is the front camera. For the navigation part our system rely on the vanishing point algorithm, the Hough transform for the wall detection and avoidance, and the HOG descriptors for pedestrian detection using SVM classifier. Our experiments show that our vision navigation procedures are reliable and enable the aerial vehicle to fly without humans intervention and coordinate together in the same workspace. We are able to detect human motion with high confidence of 85% in a corridor and to confirm our algorithm in 80% successful flight experiments.
In this paper we propose a vision based fall detection algorithm. Our scheme exploits a deep learning paradigm in order to isolate human's object from the background and then to perform tracking. Deep learning better emulates our brain by propagating the raw sensory inputs into "deep" levels of hierarchies. Network adaptation dynamically re-configures the network to fit current environment visual properties. This way, object classification accuracy and tracking is enriched. In the following step, geometrically properties from the detected human object takes place. This is performed by extracting real 3D measurements from the captured 2D image planes. Camera self-calibration methods through the extraction of vanishing points are considered in this context. The derived features are filtered using autoregressive models and filtered feature sequence are exploited as feature in a time delay neural network for performing the final fall detection. Semi-supervised learning strategies are exploited to enhance classification efficiency. Experimental results indicate the efficiency of our proposed algorithm.
In this paper, we propose a novel feature descriptor based on Gabor filters, called Topological Gabor Descriptor. We build a filter bank in such a way that the descriptors are invariant to rotation and scale changes. The filter bank topology enables a simple matching scheme based on the circular shift of descriptors. We evaluate the effectiveness of our approach to feature description in an object/scene recognition setting. The descriptors were evaluated with synthetic and real images. The performance of the descriptors was measured by computing the average matching rate. Our experiments with synthetic data show a robust invariance property for a high degree of rotation and scale variations. Our experimental results shows a 93.50% matching rate for synthetic images subjected to rotation. The matching rate for a scale variation of up to two times the original scale is 81.11%. The methods discussed in this paper were also tested on three different datasets of real images of buildings where we obtained an average matching rate of 41.33%.
Brand Associations, one of central concepts in marketing, describe customers' top-of-mind attitudes or feelings toward a brand. Thus, this consumer-driven brand equity often attains the grounds for purchasing products or services of the brand. Traditionally, brand associations are measured by analyzing the text data from consumers' responses to the survey or their online conversation logs. In this paper, we propose to go beyond text data and leverage large-scale online photo collections contributed by the general public, which have not been explored so far. As a first technical step toward the study of photo-based brand associations, we aim to jointly achieve the following two visualization tasks in a mutually-rewarding way: (i) detecting and visualizing core visual concepts associated with brands, and (ii) localizing the regions of brand in the images. With experiments on about five millions of images of 48 brands crawled from five popular online photo sharing sites, we demonstrate that our approach can discover complementary views on the brand associations that are hardly mined from the text data. We also quantitatively show that our approach outperforms other candidate methods on the both visualization tasks.
Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using additional images and videos to express their opinions and share their experiences. Sentiment analysis of such large-scale textual and visual content can help better extract user sentiments toward events or topics. Motivated by the needs to leverage large-scale social multimedia content for sentiment analysis, we propose a cross-modality consistent regression (CCR) model, which is able to utilize both the state-of-the-art visual and textual sentiment analysis techniques. We first fine-tune a convolutional neural network (CNN) for image sentiment analysis and train a paragraph vector model for textual sentiment analysis. On top of them, we train our multi-modality regression model. We use sentimental queries to obtain half a million training samples from Getty Images. We have conducted extensive experiments on both machine weakly labeled and manually labeled image tweets. The results show that the proposed model can achieve better performance than the state-of-the-art textual and visual sentiment analysis algorithms alone.
In this paper, a movie title recognition system is proposed to extract the movie title from a movie poster and transform it into words. First, the title region in a poster image is located using gradient-conversion method, and then all character images in the region are segmented using the watershed approach. Next, two kinds of features about the texture and shape defined in the MPEG-7 specification are extracted to train an SVM classifier used in recognizing those character images into words. Finally, we employ PyEnchant on each word for spelling check or even spelling revision. The experimental results show that the correct rate of title recognition would reach 90%, if the title region in a poster image can be located and segmented successfully.
This paper designs and implements a corner detection algorithm for generic shapes. The proposed corner detector detects corners by using combination of two ellipses and rectangle with different parameters sliding along the boundary of the shape and record number of boundary points in each ellipse and rectangle. This relationship represents both local and global view of the image which is a natural corners detection methodology to detect all true corners accurately. Proposed corner detection technique is consistent with human vision system.
This paper presents an effective approach to estimate the fixed internal and varying external parameters of the camera for real time experiments using 2D-3D point correspondences. Images are acquired at each time step, a pose estimation algorithm is then employed to determine the camera pose w.r.t the object. A simple homogenous transformation is derived between the camera and end-effector to determine the position of the manipulator end-effector, as camera is mounted on the tool in eye-in-hand configuration. The paper focuses on determining the pose accurately and to look upon those issues that we encounter in real time. The major contribution of this paper is in two folds: camera pose parameters are easily and accurately recovered from 2D to 3D point correspondence; second is that experiments using real images are conducted, which presents good results.
For over last three decades, numerous automatic brain segmentation techniques in magnetic resonance (MR) images have been proposed. These techniques, however, need to be validated comprehensively. In this study, FS+LDDMM, a recently proposed fully automatic template-based brain segmentation technique, is validated. The validation method uses novel approach in which dependency of FS+LDDMM on initial segmentation parameters is evaluated. These segmentation parameters include choice of template, gross alignment, cropping size and initialization schemes. A database of 46 MR images from young ADHD subjects of an average age of 10.6 years is employed to segment caudate nucleus in subcortical region. The accuracy of the segmentation is computed by comparing FS+LDDMM segmentation with gold standard manual segmentation using metrics, such as, percent volume error, dice coefficient, L1 error and intraclass correlation coefficient (ICC). The FS+LDDMM shows robustness to all these parameters and outperforms FreeSurfer (FS) segmentation. To generalize the performance of FS+LDDMM, however, more experiments need to be conducted for various subcortical objects.
Humans unconsciously use visual cues from lip motion not only in aural conversations but also to communicate subtle emotions. In the past decades lip detection and segmentation has been extensively studied in various fields from speech recognition and human computer interaction to biometrics, but literature comparing visual lip features is almost non-existent. In this paper we present a comparative evaluation of different visual features from lips. Geometric and appearance based features were extracted and their relevance to identifying people was studied by feature selection methods.
Many proposed image watermarking techniques are sensitive to geometric attacks, such as rotation, scaling, translation, or their composites. Even slight geometric distortions can also disable the watermark detector to reliably perform its function. In this paper, we utilize the robust invariant image features and discrete Hahn image moments to design a robust watermarking system that can withstand many geometric-distortions as well as survive a variety of common watermark attacks. Scale-invariant feature transform (SIFT) based bounding boxes and discrete orthogonal Hahn moment-invariants are used to embed watermark information into the selective image patches. Hahn moment-invariants are utilized for watermarking purpose because they are invariant to rotation, scaling, and translation. Watermark detection is performed by synchronizing the SIFT points and then computing the RMS threshold value between the original and the watermarked images. Several tests are performed to check the robustness of the proposed method. Experimental results validate the effectiveness of the scheme as well as prove that the proposed method is robust to several geometric attacks.
Recent work on wavelets applied to images or a video sequence has been exploited for extracting robust illumination invariant features. The paper presents robust background subtraction algorithm to segment motion based video scene in embedded platforms. Every machine or computer vision algorithm to be useful should be able to separate the different background and foreground information (e.g. objects) in the given scene. Therefore, it is essential to the success of any real time algorithm, the scene segmentation invariant to lighting conditions. We designed two main algorithms; Six frames (6-Frames) and Time Interval with Memory (TIME) to segment the video scene robustly based on motion detection in embedded platforms. The former uses the first six frames and the latter samples the frames at regular intervals of time with memory to generate a background reference frame. Our algorithms used bandpass video scene filtering with wavelets for extracting illumination invariant scene features and then combine them efficiently into the background reference frame. Hardware efficient image stabilization capability was added to remove the unwanted motion due to camera movement. The algorithms were tested using three moving bee videos sequences; static background, moving shadow and destabilized. Performance of algorithms was evaluated on the basis of number of frames in which the moving target was detected for each video sequence.
A lot of work has been done in the field of sign language recognition all over the world. The main focus of such work is to make the life of vocally impaired people more comfortable. It also bridges the communication gap between the normal and the abnormal people. The deaf and dumb people need not only learn the standard sign language but the core issue is that they can communicate with the normal people of society. It is also not possible for all the normal people that they learn the sign language to understand whatever is said through gestures. So the communicational gap still stays there even after teaching deaf and dumb people with sign language. In this paper, an approach has been presented in which statistical features are extracted from the hand signs and are then fed to the decision tree for the recognition of the hand gestures. In this research the English alphabet gestures data set has been used and the recognized hand gestures are then represented as both the alphabetical and voice forms. This would help the impaired people to communicate with normal people in the way that they can understand.
This paper presents a robust adaptive moving human detection and recognition method in videos. The human detection method consists of modified moving average background model with supportive secondary model and an adaptive threshold selection model based on Gaussian distribution. The moving average background model is used for background modeling and the background subtraction system is used to provide foreground image through difference image between current image and background model. The adaptive threshold method is used to simultaneously update the system to environment changes. The modified human model consists of five parts with robust features to facilitate human recognition process. For recognition purpose Support Vector Machine has been used as classifier. Experimental results show the effectiveness of proposed system.
Automatic vehicle type recognition (make and model) is very useful in secure access and traffic monitoring applications. It compliments the number plate recognition systems by providing a higher level of security against fraudulent use of number plates in traffic crimes. In this paper we present a simple but powerful probabilistic framework for vehicle type recognition that requires just a single representative car image in the database to recognize any incoming test image exhibiting strong appearance variations, as expected in outdoor image capture e.g. illumination, scale etc. We propose to use a new feature description, local energy based shape histogram 'LESH', in this problem that encodes the underlying shape and is invariant to illumination and other appearance variations such as scale, perspective distortions and color. Our method achieves high accuracy (above 94%) as compared to the state of the art previous approaches on a standard benchmark car dataset. It provides a posterior over possible vehicle type matches which is especially attractive and very useful in practical traffic monitoring and/or surveillance video search (for a specific vehicle type) applications.
Lungs Segmentation from chest CT slices is a precursor for CAD applications. Most of the lungs segmentation methods are scanner dependent. We propose a fully automated machine independent method for segmenting lungs from CT images. The algorithm comprised of three main steps. In the first step, gray level threshold value has been selected by maximizing within class similarity. In the second step, binary mask has been developed using selected gray level threshold value and improved by morphological operations. In the third step, lungs have been segmented utilizing binary mask and original CT slice images. The method has been tested on data set of 25 slices collected from two different sources. Results have been compared with manually delineated lungs on CT images by a radiologist. Mean overlapping fraction, precision, sensitivity/recall, specificity, accuracy and F-measure have been recorded as 0.9929, 0.9962, 0.9966, 0.9997, 0.9995 and 0.9964 respectively.
Owing to dense deployment of light fixtures and multipath-free propagation, visible light localization technology holds potential to overcome the reliability issue of radio localization. However, existing visible light localization systems require customized light hardware, which increases deployment cost and hinders near term adoption. In this paper, we propose LiTell, a simple and robust localization scheme that employs unmodified fluorescent lights (FLs) as location landmarks and commodity smartphones as light sensors. LiTell builds on the key observation that each FL has an inherent characteristic frequency which can serve as a discriminative feature. It incorporates a set of sampling, signal amplification and camera optimization mechanisms, that enable a smartphone to capture the extremely weak and high frequency ( > 80 kHz) features. We have implemented LiTell as a real-time localization and navigation system on Android. Our experiments demonstrate LiTell's high reliability in discriminating different FLs, and its potential to achieve sub-meter location granularity. Our user study in a multi-storey office building, parking lot and grocery store further validates LiTell as an accurate, robust and ready-to-use indoor localization system.
In this work we propose ParkMaster, a low-cost crowdsourcing architecture which exploits machine learning techniques and vision algorithms to evaluate parking availability in cities. While the user is normally driving ParkMaster enables off the shelf smartphones to collect information about the presence of parked vehicles by running image recognition techniques on the phones camera video streaming. The paper describes the design of ParkMaster's architecture and shows the feasibility of deploying such mobile sensor system in nowadays smartphones, in particular focusing on the practicability of running vision algorithms on phones.
Video quality assessment in mobile devices, for instances smart phones and tablets, raises unique challenges such as unavailability of original videos, the limited computation power of mobile devices and inherent characteristics of wireless networks (packet loss and delay). In this paper, we present a metric, Temporal Variation Metric (TVM), to measure the temporal information of videos. Despite its simplicity, it shows a high correlation coefficient of 0.875 to optical flow which captures all motion information in a video. We use the TVM values to derive a reduced-reference temporal quality assessment metric, Temporal Variation Index (TVI), which quantifies the quality degradation incurred in network transmission. Subjective assessments demonstrate that TVI is a very good predictor of users' Quality of Experience (QoE). Its prediction shows a 92.5% of correlation to subjective Mean Opinion Score (MOS) ratings. Through video streaming experiments, we show that TVI can also estimate the network conditions such as packet loss and delay. It depicts an accuracy of almost 95% in extensive tests on 183 video traces.
The author is developing a new interactive documentary project entitled 24-hours.in (www.24-hours.in), exploring new opportunities for participation, collaboration and the potential democratization of documentary production. Utilizing user-generated video captured on mobile phones and available devices, the project is participatory whereby the audience contribute documentary videos, around the theme of 24 hours in a city or location; for example 24hours.in Tampere. Moving beyond the user-generated content model, the project will build up a database of location specific documentary material and aim to create a new system for collaborative documentary production and user-curated content. The project is currently at the proof of concept stage and is being discussed as a work in progress, exploring the impact of mobile phones and the potential that these devices, Web 2.0 and social media may offer for the democratization of documentary production.
We have been witnessing a rapid explosion of video data in richer and broader widely accessed media spaces. Video itself is a very rich medium, including pictures, text and audio that change in time. This richness makes these information spaces very interesting, allowing the communication of huge amounts of information and an excellent platform for creativity to be expressed and explored, but it comes with a challenging complexity to handle. Visualization techniques can help to handle the complexity and to express the richness in these information spaces. This paper presents an interactive environment to visualize and explore a video space with a semantic focus on cultural aspects, and stressing features such as their color dominance, rhythm and movement, at the level of the video space and the individual videos. It allows to capture, experience, and express videos' properties and relations, providing the means to gain new insights into our culture and to influence the expression of its intrinsic aesthetics in creative ways.
3D reconstruction of real world objects is an important content creation tool for the 3D internet. This poster shows how cheap commercial sensors can be used to reconstruct real 3D objects and import those objects within a 3D virtual world. This poster describes the implementation of a 3D content capturing processing chain using Microsoft Kinect. The capturing process includes a method for filtering and segmenting an object from the unwanted background data, a method for registering multiple scans together and a method for creating a solid surface presentation for the captured point cloud. The resulting 3D asset is imported and rendered in RealXtend Tundra 2.
In traditional video, the user is locked to the angle where the camera was pointing to during the capture of the video. With 360? video recording, there are no longer these boundaries, and 360? video capturing devices are becoming more common and affordable to the general public. Hypervideo stretches boundaries even further, allowing to explore the video and to navigate to related information. In this paper, we describe an approach to the design and development of an immersive and interactive interface for the visualization and navigation of 360? hypervideos over the internet. Such videos allow users to pan around to view the contents in different angles and effectively access related information through the hyperlinks. Challenges for presenting this type of hypervideo include: providing users with an appropriate interface capable to explore 360? contents, where the video should change perspective so that the users actually get the feeling of looking around; and providing the appropriate affordances to understand the hypervideo structure and to navigate it effectively in a 360? hypervideo space, even when link opportunities arise in places outside the current viewport.
In this paper we present Synesthetic Video, an interactive video that allows to experience video in cross-sensorial ways, to hear its colors and to influence its visual properties with sound and music, through user interaction or ambient influence. Our main motivations include accessibility, enriching users experiences, stimulating and supporting users creativity, and to learn more about synesthesia and how videos can influence and be influenced by users and the ambient, at the crossroads of art, science and technology.
Many applications use sensors to capture an image of the real world, which is needed for automatic processes. E. g. future driver assistance systems will be based on dynamic information about the car's environment, the car's state and the driver's state. Since there exists no single sensor that can sense the required information, different sensors like radar, video and eye-tracker are used. Typically some provide redundant information about the same real world entity, while others measure different things. Thus, the fusion of information from different sensors is necessary to get a consistent image of the real world. In most sensor fusion systems the sensor configuration is known a priori and the fusion algorithms are adapted for these sensor configurations. Thus, changing a sensor fusion system to enable it to process sensor readings from another sensor configuration is hardly possible or completely impossible. Since in development processes of automotive applications different sensor equipment and environmental requirements exist and change frequently a new approach for adapting sensor fusion systems is necessary. Hence, in this work a framework for sensor fusion systems will be developed that allows a flexible adaptation of fusion mechanisms. Due to real-time requirements of automotive applications and the flexibility of query processing technologies, data stream management technology will be used to develop a flexible framework for multisensor data fusion.
A vis?o possui muitos sensores respons?veis pela capta??o de informa??es que s?o enviadas ao c?rebro. O olhar reflete a sua aten??o, inten??o e interesse. Sendo assim, a detec??o da dire??o do olhar ? uma alternativa promissora para a comunica??o com a m?quina. A aplica??o de t?cnicas para detec??o da dire??o do olhar tem a possibilidade de melhorar significativamente a capacidade de intera??o dos portadores de defici?ncias motoras com os computadores. O objetivo deste trabalho consiste em apresentar um sistema que utiliza t?cnicas de processamento digital de imagens para classificar a dire??o do olhar. Os resultados mostram as simula??es realizadas com as t?cnicas de captura, processamento e classifica??o da dire??o do olhar em uma imagem.
Virtual presenters have a great range of possible applications, like teachers, news presenters and guides of virtual environments, easing the interaction with computers. The animation control of such virtual characters is usually accomplished by an animation script describing all the movements to be performed. Writing a convincing animation script is a demanding and cumbersome task. To ease the animation process, we propose the additional use of a behavior model learned from a real presenter. The article presents the implementation of a 3D virtual news presenter that implicitly follows a behavior model and a script that describes the text to be uttered. The behavior model is given by a set of behavioral rules that represent common non-verbal facial movement patterns displayed by a real presenter whose TV appearances were analyzed