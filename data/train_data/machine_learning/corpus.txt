In a computer game, equipping a bot with a suitable algorithm to locate a human player is difficult. Besides the unpredictable moves made by the player, an unexplored map region poses additional constraints such as new obstacles and pathways that the bot needs to discover quickly. The design criteria of such moving target search (MTS) algorithms would typically need to consider computation efficiency and storage requirements. That is, the bot must appear to be �smart� and �quick� in order to enhance the playability and challenge posed by the game. These criteria, however, pose conflicting requirements. In this article, we study and evaluate the performance and behavior of two novel MTS algorithms, Fuzzy MTS and Abstraction MTS, against existing MTS algorithms in randomly generated mazes of increasing size. Simulations reveal that Fuzzy MTS and Abstraction MTS exhibit competitive performance even with large problem spaces.
We address the problem of mining musical specifications from a training set of songs and using these specifications in a machine improvisation system capable of generating improvisations imitating a given style of music. Our inspiration comes from control improvisation, which combines learning and synthesis from formal specifications. We mine specifications from symbolic musical data with musical and general usage patterns. We use the mined specifications to ensure that an improvised musical sequence satisfies desirable properties given a harmonic context and phrase structure. We present a specification mining strategy based on pattern graphs and apply it to the problem of supervising the improvisation of blues songs. We present an analysis of the mined specifications and compare the results of improvisations generated with and without specifications.
StarCraft is a well-known real-time strategy game developed by Blizzard Entertainment in 1998. One of the characteristics of this game is �fog of war,� which refers to the fact that players cannot see their opponents' regions but only their own unit. This characteristic of the game means that the information required in order to predicting the opponent's strategy is only available through �scouting.� Although the �fog of war� is one of the most important features of the game, it has not been deeply understood in the design of artificial intelligence. In this work, we propose to investigate the effect of the �fog of war� in the prediction of opponent's strategy using machine learning for human players and artificial intelligence (AI) bots. To realize this analysis, we develop a customized replay analyzer that exports the internal game events with/without the fog of war. In the experimental results, we collect replays from various sources: human vs. human, human vs. AI bots, and AI bots vs. AI bots. This systematic analysis with �fog of war� reveals the predictability of the machine-learning algorithms on different conditions and the directions for designing new artificial intelligence for the game.
Musical metacreation (MuMe), also known as musical computational creativity, is a subfield of computational creativity that focuses on endowing machines with the ability to achieve creative musical tasks, such as composition, interpretation, improvisation, accompaniment, mixing, etc. It covers all dimensions of the theory and practice of computational generative music systems, ranging from purely artistic approaches to purely scientific ones, inclusive of discourses relevant to this topic from the humanities. MuMe systems range from purely generative ones to a variety of interactive systems, such as those for computer-assisted composition and computer-assisted sound design. In order to better appreciate the many dimensions of this interdisciplinary domain and see how it overlaps and differs from research in computer music, this introduction provides a general entry point. After defining and introducing the domain, its context, and some of its terminology, we reflect on some challenges and opportunities for the field as a whole.
Many music composition algorithms attempt to compose music in a particular style. The resulting music is often impressive and indistinguishable from the style of the training data, but it tends to lack significant innovation. In an effort to increase innovation in the selection of pitches and note durations, we present a system that discovers musical motifs by coupling machine-learning techniques with an inspirational component. Unlike many generative models, the inspirational component allows the composition process to originate outside of what is learned from the training data. Candidate motifs are extracted from non-musical data such as audio, images, and sleep signals. Machine-learning algorithms select the motifs that most resemble the training data. We find that the inspirational motif discovery process is more efficient than random generation. We also extract motifs from real music scores, identify themes in the piece according to a theme database, and measure the probability of discovering thematic motifs verses non-thematic motifs. We examine the information content of the motifs by comparing the entropy of the discovered motifs, candidate motifs, and training data. We measure innovation by comparing the probability of the training data and the probability of the discovered motifs given the model.
Checking whether one formal language is included in another is important in many verification tasks. In this article, we provide solutions for checking the inclusion of languages given by visibly pushdown automata over both finite and infinite words. Visibly pushdown automata are a richer automaton model than the classical finite-state automata, which allows one, for example, to reason about the nesting of procedure calls in the executions of recursive imperative programs. The presented solutions do not rely on explicit automaton constructions for determinization and complementation. Instead, they are more direct and generalize the so-called Ramsey-based inclusion-checking algorithms, which apply to classical finite-state automata and proved to be effective there to visibly pushdown automata. We also experimentally evaluate these algorithms, demonstrating the virtues of avoiding explicit determinization and complementation constructions.
A data word is a sequence of pairs of a letter from a finite alphabet and an element from an infinite set, where the latter can only be compared for equality. Safety one-way alternating automata with one register on infinite data words are considered, their nonemptiness is shown to be ExpSpace-complete, and their inclusion decidable but not primitive recursive. The same complexity bounds are obtained for satisfiability and refinement, respectively, for the safety fragment of linear temporal logic with freeze quantification. Dropping the safety restriction, adding past temporal operators, or adding one more register, each causes undecidability.
A data tree is an unranked ordered tree whose every node is labeled by a letter from a finite alphabet and an element (�datum�) from an infinite set, where the latter can only be compared for equality. The article considers alternating automata on data trees that can move downward and rightward, and have one register for storing data. The main results are that nonemptiness over finite data trees is decidable but not primitive recursive, and that nonemptiness of safety automata is decidable but not elementary. The proofs use nondeterministic tree automata with faulty counters. Allowing upward moves, leftward moves, or two registers, each causes undecidability. As corollaries, decidability is obtained for two data-sensitive fragments of the XPath query language.
An important step in building up the document database of a full-text retrieval system is to classify each document under one or more classes according to the topical domains that the document discusses. This is commonly referred to as classification. Automatic classification attempts to replace human classifiers by using computers to automate this process. Automatic classification has two major components: (1) the classification scheme which defines the available classes under which a document can be classified and their inter-relationships; and (2) the classification algorithm which defines the rules and procedures for assigning one or more classes defined in the classification scheme to a document.In this paper, we present an automatic classification approach called ACTION. The design goal of ACTION is to achieve the appropriate balance between specificity and exhaustivity, which are important metrics for assessing an automatic classification approach. The key idea of ACTION is a scheme for measuring the significance of each keyword in a given document. The scheme not only takes into account the occurrence frequency of a keyword, but also the logical relationships between the available classes.
There is currently a high demand for information systems that automatically analyze textual data, since many organizations, both private and public, need to process large amounts of such data as part of their daily routine, an activity that cannot be performed by means of human work only. One of the answers to this need is text classification (TC), the task of automatically labelling textual documents from a domain D with thematic categories from a predefined set C. Modern text classification systems have reached high efficiency standards, but cannot always guarantee the labelling accuracy that applications demand. When the level of accuracy that can be obtained is insufficient, one may revert to processes in which classification is performed via a combination of automated activity and human effort. One such process is semi-automated text classification (SATC), which we define as the task of ranking a set D of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of D with the goal of increasing the overall labelling accuracy of D, the expected such increase is maximized. An obvious strategy is to rank D so that the documents that the classifier has labelled with the lowest confidence are top-ranked. In this dissertation we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of validation gain, defined as the improvement in classification efectiveness that would derive by validating a given automatically labelled document. We also propose new effectiveness measures for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially validating a ranked list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method above, and according to the proposed measures, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error. We therefore explore the task of SATC and the potential of our methods, in multiple text classification contexts. This dissertation is, to the best of our knowledge, the first to systematically address the task of semi-automated text classification.
We consider the problem of detecting the formation of a set of wireless sensor nodes based on the pairwise measurements of signal strength corresponding to all transmitter/receiver pairs. We assume that formations take values in a discrete set and develop a composite hypothesis testing approach which uses a Generalized Likelihood Test (GLT) as the decision rule. The GLT distinguishes between a set of probability density function (pdf) families constructed using a custom pdf interpolation technique. The GLT is compared with the simple Likelihood Test (LT). We also adapt one prevalent supervised learning approach, Multiple Support Vector Machines (MSVMs), and compare it with our probabilistic methods. Due to the highly variant measurements from the wireless sensor nodes, and these methods' different adaptability to multiple observations, our analysis and experimental results suggest that GLT is more accurate and suitable for formation detection. The formation detection problem has interesting applications in posture detection with Wireless Body Area Networks (WBANs), which is extremely useful in health monitoring and rehabilitation. Another valuable application we explore concerns autonomous robot systems.
Wireless sensor networks are often deployed in large numbers, over a large geographical region, in order to monitor the phenomena of interest. Sensors used in the sensor networks often suffer from random or systematic errors such as drift and bias. Even if they are calibrated at the time of deployment, they tend to drift as time progresses. Consequently, the progressive manual calibration of such a large-scale sensor network becomes impossible in practice. In this article, we address this challenge by proposing a collaborative framework to automatically detect and correct the drift in order to keep the data collected from these networks reliable. We propose a novel scheme that uses geospatial estimation-based interpolation techniques on measurements from neighboring sensors to collaboratively predict the value of phenomenon being observed. The predicted values are then used iteratively to correct the sensor drift by means of a Kalman filter. Our scheme can be implemented in a centralized as well as distributed manner to detect and correct the drift generated in the sensors. For centralized implementation of our scheme, we compare several kriging- and nonkriging-based geospatial estimation techniques in combination with the Kalman filter, and show the superiority of the kriging-based methods in detecting and correcting the drift. To demonstrate the applicability of our distributed approach on a real world application scenario, we implement our algorithm on a network consisting of Wireless Sensor Network (WSN) hardware. We further evaluate single as well as multiple drifting sensor scenarios to show the effectiveness of our algorithm for detecting and correcting drift. Further, we address the issue of high power usage for data transmission among neighboring nodes leading to low network lifetime for the distributed approach by proposing two power saving schemes. Moreover, we compare our algorithm with a blind calibration scheme in the literature and demonstrate its superiority in detecting both linear and nonlinear drifts.
The ecological sciences have benefited greatly from recent advances in wireless sensor technologies. These technologies allow researchers to deploy networks of automated sensors, which can monitor a landscape at very fine temporal and spatial scales. However, these networks are subject to harsh conditions, which lead to malfunctions in individual sensors and failures in network communications. The resulting data streams often exhibit incorrect data measurements and missing values. Identifying and correcting these is time-consuming and error-prone. We present a method for real-time automated data quality control (QC) that exploits the spatial and temporal correlations in the data to distinguish sensor failures from valid observations. The model adapts to each deployment site by learning a Bayesian network structure that captures spatial relationships between sensors, and it extends the structure to a dynamic Bayesian network to incorporate temporal correlations. This model is able to flag faulty observations and predict the true values of the missing or corrupt readings. The performance of the model is evaluated on data collected by the SensorScope Project. The results show that the spatiotemporal model demonstrates clear advantages over models that include only temporal or only spatial correlations, and that the model is capable of accurately imputing corrupted values.
Dr. Raffaello D'Andrea speaks at length about what it takes to build commercially viable robotic systems, the future of autonomous machines, the role humans will play in this future, and how we can best prepare for it.
Pondering the brain with the help of machine learning expert Andrew Ng and researcher-turned-author-turned-entrepreneur Jeff Hawkins.
Web and semantic technologies will form the foundation for ecosystems of machines that interact with each other and with people as never before.
Babbel's Director of Didactics, Miriam Plieninger, weighs in on how mobile apps are rapidly changing the way we approach language learning.
Quantum computing and machine learning are two technologies that have generated unparalleled amounts of hype among the scientific community and popular press. Both are mysterious, immensely powerful, and on a collision course with each other.
Increasingly, storage vendors are finding it difficult to leverage existing white-box and black-box modeling techniques to build robust system models that can predict system behavior in the emerging dynamic and multi-tenant data centers. White-box models are becoming brittle because the model builders are not able to keep up with the innovations in the storage system stack, and black-box models are becoming brittle because it is increasingly difficult to a priori train the model for the dynamic and multi-tenant data center environment. Thus, there is a need for innovation in system model building area. In this paper we present a machine learning based blackbox modeling algorithm called M-LISP that can predict system behavior in untrained region for these emerging multitenant and dynamic data center environments. We have implemented and analyzed M-LISP in real environments and the initial results look very promising. We also provide a survey of some common machine learning algorithms and how they fare with respect to satisfying the modeling needs of the new data center environments.
In this paper, we address a pattern of diagnosis problems in which each of J entities produces the same K features, yet we are only informed of overall faults from the ensemble. Furthermore, we suspect that only certain entities and certain features are leading to the problem. The task, then, is to reliably identify which entities and which features are at fault. Such problems are particularly prevalent in the world of computer systems, in which a datacenter with hundreds of machines, each with the same performance counters, occasionally produces overall faults. In this paper, we present a means of using a constrained form of bilinear logistic regression for diagnosis in such problems. The bilinear treatment allows us to represent the scenarios with J+K instead of JK parameters, resulting in more easily interpretable results and far fewer false positives compared to treating the parameters independently. We develop statistical tests to determine which features and entities, if any, may be responsible for the labeled faults, and use false discovery rate (FDR) analysis to ensure that our values are meaningful. We show results in comparison to ordinary logistic regression (with L1 regularization) on two scenarios: a synthetic dataset based on a model of faults in a datacenter, and a real problem of finding problematic processes/features based on user-reported hangs.
Event forecasting from social media data streams has many applications. Existing approaches focus on forecasting temporal events (such as elections and sports) but as yet cannot forecast spatiotemporal events such as civil unrest and influenza outbreaks, which are much more challenging. To achieve spatiotemporal event forecasting, spatial features that evolve with time and their underlying correlations need to be considered and characterized. In this article, we propose novel batch and online approaches for spatiotemporal event forecasting in social media such as Twitter. Our models characterize the underlying development of future events by simultaneously modeling the structural contexts and their spatiotemporal burstiness based on different strategies. Both batch and online-based inference algorithms are developed to optimize the model parameters. Utilizing the trained model, the alignment likelihood of tweet sequences is calculated by dynamic programming. Extensive experimental evaluations on two different domains demonstrate the effectiveness of our proposed approach.
This paper demonstrates the viability of a rule-based consultation system as a mechanism for effective resource management through integration of knowledge about users, business problems, and resources. The specific domain within which the expert system will be tested is the Information Center (IC), which deals with end-user computing resources. In the last decade, the information center concept has been proposed repeatedly as an organizational solution to resource management problems related to end user computing. The research hypothesis is that the knowledge and methodologies of IC consultants, as well as institutional policies, can be represented in a knowledge base. The system will then draw conclusions about appropriate software or training solutions based on the interaction of user and resource profiles with the problem definition. The output of the model should be identical to the situation which it is modeling.Currently, a rule-based ES is being developed at the University of Arizona Department of Management Information Systems. This paper will present methodologies for design and implementation of the system. The development approach for the IC environment is unique as compared to expert system development as discussed in the literature. First, the use of prototyping has been stressed for expert system development with the view of using it as a throw-away system. The prototyping of an ES for the IC environment, however, has to take into account the difference between two aspects of a prototype: user interface (dialogue) and system structures (knowledge representation, inferencing techniques). There needs to be iteration of the prototype until the user if comfortable with the dialogue. The prototype will be used to extract more knowledge form the experts as well as for enhancing user dialogue. The system structures will be changed only when the user dialogue has been settled.Second, because of changes in tool availability, resulting in a need for new descriptions of the tools available, the design of an expert system for ICs has to respond dynamically to unique flexibility, portability, and maintenance issues. The ability to transport such a system to similar, but not identical, ICs should be addressed in the design as well.An IC has been described as an organization specifically designed to produce guided services to help end users help themselves. A consultation expert system might be useful in reaching that goal.
Although a number of libraries of patterns have been developed for reuse, there is no mechanism for supporting automated design of object-oriented systems by the intelligent reuse of patterns from such libraries. We describe a web-based system, APSARA, the purpose of which is to create object-oriented designs based upon simple requirements descriptions. The system, developed in Java, implements a pattern retrieval and synthesis methodology that uses natural language processing and automated reasoning heuristics. The system is tested on multiple cases from different domains. The results are reported using metrics defined in the spirit of familiar measures such as recall, precision, coverage and spuriousness. These initial tests suggest that this is a feasible approach for the reuse of patterns in object-oriented design. The testing also reveals specific areas of concern and suggests a number of avenues for extending this research.
Document clustering is crucial to automated document management, especially for the fast-growing volume of textual documents available digitally. Traditional lexicon-based approaches depend on document content analysis and measure overlap of the feature vectors representing different documents, which cannot effectively address word mismatch or ambiguity problems. Alternative query expansion and local context discovery approaches are developed but suffer from limited efficiency and effectiveness, because the large number of expanded terms create noise and increase the dimensionality and complexity of the overall feature space. Several techniques extend lexicon-based analysis by incorporating latent semantic indexing but produce less comprehensible clustering results and questionable performance. We instead propose a concept-based document representation and clustering (CDRC) technique and empirically examine its effectiveness using 433 articles concerning information systems and technology, randomly selected from a popular digital library. Our evaluation includes two widely used benchmark techniques and shows that CDRC outperforms them. Overall, our results reveal that clustering documents at an ontology-based, concept-based level is more effective than techniques using lexicon-based document features and can generate more comprehensible clustering results.
We discuss structural results and learning algorithms for submodular and fractionally subadditive valuation functions. While learning these valuation functions over general distributions turns out to be hard, we present compact approximate representations and efficient learning algorithms for such functions over the uniform distribution.
This extended abstract summarizes the research presented in Dr. Pardoe's recently-completed Ph.D. thesis [Pardoe 2011]. The thesis considers how adaptive trading agents can take advantage of previous experience (real or simulated) in other markets while remaining robust in the face of novel situations in a new market. Its contributions are at the intersection of machine learning and electronic commerce, with particular focus on transfer learning and fully autonomous trading agents.
In this article, we present a new type of classification problem, which we call Comparative Classification Problem (CCP), where we use the term data record to refer to a block of instances. Given a single data record with n instances for n classes, the CCP problem is to map each instance to a unique class. This problem occurs in a wide range of applications where the independent and identically distributed assumption is broken down. The primary difference between CCP and classical classification is that in the latter, the assignment of a translator to one record is independent of the assignment of a translator to a different record. In CCP, however, the assignment of a translator to one record within a block excludes this translator from further assignments to any other record in that block. The interdependency in the data poses challenges for techniques relying on the independent and identically distributed (iid) assumption. In the Pairwise CCP (PWCCP), a pair of records is grouped together. The key difference between PWCCP and classical binary classification problems is that hidden patterns can only be unmasked by comparing the instances as pairs. In this article, we introduce a new algorithm, PWC4.5, which is based on C4.5, to manage PWCCP. We first show that a simple transformation�that we call Gradient-Based Transformation (GBT)�can fix the problem of iid in C4.5. We then evaluate PWC4.5 using two real-world corpora to distinguish between translators on Arabic-English and French-English translations. While the traditional C4.5 failed to distinguish between different translators, GBT demonstrated better performance. Meanwhile, PWC4.5 consistently provided the best results over C4.5 and GBT.
Named entity extraction is a fundamental task for many natural language processing applications on the web. Existing studies rely on annotated training data, which is quite expensive to obtain large datasets, limiting the effectiveness of recognition. In this research, we propose a semisupervised learning approach for web named entity recognition (NER) model construction via automatic labeling and tri-training. The former utilizes structured resources containing known named entities for automatic labeling, while the latter makes use of unlabeled examples to improve the extraction performance. Since this automatically labeled training data may contain noise, a self-testing procedure is used as a follow-up to remove low-confidence annotation and prepare higher-quality training data. Furthermore, we modify tri-training for sequence labeling and derive a proper initialization for large dataset training to improve entity recognition. Finally, we apply this semisupervised learning framework for person name recognition, business organization name recognition, and location name extraction. In the task of Chinese NER, an F-measure of 0.911, 0.849, and 0.845 can be achieved, for person, business organization, and location NER, respectively. The same framework is also applied for English and Japanese business organization name recognition and obtains models with performance of a 0.832 and 0.803 F-measure.
Unsupervised dependency parsing becomes more and more popular in recent years because it does not need expensive annotations, such as treebanks, which are required for supervised and semi-supervised dependency parsing. However, its accuracy is still far below that of supervised dependency parsers, partly due to the fact that their parsing model is insufficient to capture linguistic phenomena underlying texts. The performance for unsupervised dependency parsing can be improved by mining knowledge from the texts and by incorporating it into the model. In this article, syntactic knowledge is acquired from query logs to help estimate better probabilities in dependency models with valence. The proposed method is language independent and obtains an improvement of 4.1% unlabeled accuracy on the Penn Chinese Treebank by utilizing additional dependency relations from the Sogou query logs and Baidu query logs. Morever, experiments show that the proposed model achieves improvements of 8.07% on CoNLL 2007 English using the AOL query logs. We believe query logs are useful sources of syntactic knowledge for many natural language processing (NLP) tasks.
In this paper, we deal with automatic knowledge acquisition from text, specifically the acquisition of causal relations. A causal relation is the relation existing between two events such that one event causes (or enables) the other event, such as �hard rain causes flooding� or �taking a train requires buying a ticket.� In previous work these relations have been classified into several types based on a variety of points of view. In this work, we consider four types of causal relations---cause, effect, precond(ition) and means---mainly based on agents' volitionality, as proposed in the research field of discourse understanding. The idea behind knowledge acquisition is to use resultative connective markers, such as �because,� �but,� and �if� as linguistic cues. However, there is no guarantee that a given connective marker always signals the same type of causal relation. Therefore, we need to create a computational model that is able to classify samples according to the causal relation. To examine how accurately we can automatically acquire causal knowledge, we attempted an experiment using Japanese newspaper articles, focusing on the resultative connective �tame.� By using machine-learning techniques, we achieved 80% recall with over 95% precision for the cause, precond, and means relations, and 30% recall with 90% precision for the effect relation. Furthermore, the classification results suggest that one can expect to acquire over 27,000 instances of causal relations from 1 year of Japanese newspaper articles.
Writer-specific character writing variations such as those of stroke order and stroke number are an important source of variability in the input when handwriting is captured �online� via a stylus and a challenge for robust online recognition of handwritten characters and words. It has been shown by several studies that explicit modeling of character allographs is important for achieving high recognition accuracies in a writer-independent recognition system. While previous approaches have relied on unsupervised clustering at the character or stroke level to find the allographs of a character, in this article we propose the use of constrained clustering using automatically derived domain constraints to find a minimal set of stroke clusters. The allographs identified have been applied to Devanagari character recognition using Hidden Markov Models and Nearest Neighbor classifiers, and the results indicate substantial improvement in recognition accuracy and/or reduction in memory and computation time when compared to alternate modeling techniques.
In this article, we propose a lexicon-free, script-dependent approach to segment online handwritten isolated Tamil words into its constituent symbols. Our proposed segmentation strategy comprises two modules, namely the (1) Dominant Overlap Criterion Segmentation (DOCS) module and (2) Attention Feedback Segmentation (AFS) module. Based on a bounding box overlap criterion in the DOCS module, the input word is first segmented into stroke groups. A stroke group may at times correspond to a part of a valid symbol (over-segmentation) or a merger of valid symbols (under-segmentation). Attention on specific features in the AFS module serve in detecting possibly over-segmented or under-segmented stroke groups. Thereafter, feedbacks from the SVM classifier likelihoods and stroke-group based features are considered in modifying the suspected stroke groups to form valid symbols. The proposed scheme is tested on a set of 10000 isolated handwritten words (containing 53,246 Tamil symbols). The results show that the DOCS module achieves a symbol-level segmentation accuracy of 98.1%, which improves to as high as 99.7% after the AFS strategy. This in turn entails a symbol recognition rate of 83.9% (at the DOCS module) and 88.4% (after the AFS module). The resulting word recognition rates at the DOCS and AFS modules are found to be, 50.9% and 64.9% respectively, without any postprocessing.
Benjamin Kuipers on using commonsense reasoning to make useful conclusions, or, finding gold nuggets in a pan of sand.
This paper aims to visually describe the important concepts of software-based fault tolerance and the relationships thereof using a concept map.
With the advent of microarray technology it has been possible to measure thousands of expression values of genes in a single experiment. Biclustering or simultaneous clustering of both genes and conditions is challenging particularly for the analysis of high-dimensional gene expression data in information retrieval, knowledge discovery, and data mining. The objective here is to find sub-matrices, i.e., maximal subgroups of genes and subgroups of conditions where the genes exhibit highly correlated activities over a range of conditions while maximizing the volume simultaneously. Since these two objectives are mutually conflicting, they become suitable candidates for multi-objective modeling. In this study, we will describe some recent literature on biclustering as well as a multi-objective evolutionary biclustering framework for gene expression data along with the experimental results.
Artificial neural networks could surpass the capabilities of conventional computer-based pattern recognition systems.
A new divisive algorithm for multidimensional data clustering is suggested. Based on the minimization of the sum-of-squared-errors, the proposed method produces much smaller quantization errors than the median-cut and mean-split algorithms. It is also observed that the solutions obtained from our algorithm are close to the local optimal ones derived by the k-means iterative procedure.
Emotion analysis in social media is challenging. While most studies focus on positive and negative sentiments, the differentiation between emotions is more difficult. We investigate the problem as a collection of binary classification tasks on the basis of four opposing emotion pairs provided by Plutchik. We processed the content of messages by three alternative methods: structural and lexical features, latent factors, and natural language processing. The final prediction is suggested by classifiers deriving from the state of the art in machine learning. Results are convincing in the possibility to distinguish the emotions pairs in social media.
Recommendation on signed social rating networks is studied through an innovative approach. Bayesian probabilistic modeling is used to postulate a realistic generative process, wherein user and item interactions are explained by latent factors, whose relevance varies within the underlying network organization into user communities and item groups. Approximate posterior inference captures distrust propagation and drives Gibbs sampling to allow rating and (dis)trust prediction for recommendation along with the unsupervised exploratory analysis of network organization. Comparative experiments reveal the superiority of our approach in rating and link prediction on Epinions and Ciao, besides community quality and recommendation sensitivity to network organization.
Uncertain data streams have been widely generated in many Web applications. The uncertainty in data streams makes anomaly detection from sensor data streams far more challenging. In this article, we present a novel framework that supports anomaly detection in uncertain data streams. The proposed framework adopts the wavelet soft-thresholding method to remove the noises or errors in data streams. Based on the refined data streams, we develop effective period pattern recognition and feature extraction techniques to improve the computational efficiency. We use classification methods for anomaly detection in the corrected data stream. We also empirically show that the proposed approach shows a high accuracy of anomaly detection on several real datasets.
Inter-protocol interference is one of the most critical issues in wireless communication. For example, this becomes extremely problematic in environments where robustness and realtime communication need to be considered, e.g., in industrial automation or health care applications. Recently, possible approaches for interference mitigation have been described in the literature assuming that the interferer is known in advance. We contribute to this line of research with a framework for interferer detection and classification. Essentially, we use a simple IEEE 802.15.4 transceiver as for example used on the TelosB sensor motes to scan the 2.4 GHz ISM band. This band is used by different technologies including Bluetooth, WiFi, and cordless phones. The key challenge is the accurate timing of the scanning of the frequency band. The presented framework supports flexible descriptions of such scan jobs allowing to adapt to the detectors requirements, depending on the interfering protocols.
Machine learning is a pervasive development at the intersection of statistics and computer science. While it can benefit many data-related applications, the technical nature of the research literature and the corresponding algorithms slows down its adoption. Scikit-learn is an open-source software project that aims at making machine learning accessible to all, whether it be in academia or in industry. It benefits from the general-purpose Python language, which is both broadly adopted in the scientific world, and supported by a thriving ecosystem of contributors. Here we give a quick introduction to scikit-learn as well as to machine-learning basics.
The construction of sequential testing procedures from functions of discrete arguments is a common problem in switching theory, software engineering, pattern recognition, and management. The concept of the activity of an argument is introduced, and a theorem is proved which relates it to the expected testing cost of the most general type of decision trees. This result is then extended to trees constructed from relations on finite sets and to decision procedures with cycles. These results are used, in turn, as the basis for a fast heuristic selection rule for constructing testing procedures. Finally, some bounds on the performance of the selection rule are developed.
The performance of the semantic concept detection method depends on, the selection of the low-level visual features used to represent key-frames of a shot and the selection of the feature-fusion method. This paper proposes a set of low-level visual features of considerably smaller size and also proposes novel 'hybrid-fusion' and 'mixed-hybrid-fusion' approaches which are formulated by combining contemporary early and late-fusion strategies. In the proposed hybrid-fusion approach, the features from the same feature group are combined using early-fusion before classifier training; and the concept probability scores from multiple classifiers are merged using late-fusion approach, to get final detection scores. A feature group is defined as the features from the same feature family like color moments. The hybrid-fusion approach is refined and the 'mixed-hybrid-fusion' approach is proposed additionally to further improve the detection rate. Neural Network is used to build classifiers that produce concept probabilities for a test frame. The proposed approaches are evaluated on TRECVID development dataset which contains multi-labeled key-frames. Results show that, the proposed approaches outperform early-fusion and late-fusion approaches by large margins with respect to feature set dimensionality and mean Average Precision (mAP) values.
Discovering patterns from telecommunication usage logs requires methodical procedures yet could reveal hidden information. In this paper, polytomous logistic regression has been applied to facilitate the discovery of these patterns. With an objective of determining characteristics of different categories of subscribers, data mining methods and techniques were used to extract and select data from collated text messages of network users. These were further normalized and cross-validated for model creation. Results disclosed hidden distinct attributes associated with each type of SMS users. These characteristics could facilitate the creation of new schemes of SMS packages as well as bundling of other services, hence a tipoff for improvement of business strategies for maximizing revenue of telecommunication companies.
In traditional logistic regression model, every value of feature has the same weight. In this paper, we propose a new weighting method for logistic regression, which assigns a different weight to each feature value. A gradient approach is used to calculate the optimal weights of feature values.
With the increase of electrical sensors and smart meters installed in distribution networks, the consumption load data of local electricity customers gradually shows its following properties: large scale, big variety, fast generation and low value density. These properties have brought new challenges to load pattern analysis and pattern classification, since traditional methods and technologies cannot be able to meet the current performance requirements of pattern classification on both of the classification accuracy and time consuming. In this paper, facing electricity consumption load data, a novel electricity customer classification method is proposed based on random weighted deep neural learning. In this proposed method, the effective feature information of electricity consumption load data is extracted by training multi-layer auto-associative random weight neural networks with a small size central layer. Then, by combining the well-trained feature information and basic load feature indexes, single-layer neural network is employed to fulfill the electricity customer classification tasks of test samples. Comparative experimental results verified the outstanding performances of our proposed method.
We bring to the fore of the recommender system research community, an inconvenient truth about the current state of understanding how recommender system algorithms and humans influence one another, both computationally and cognitively. Unlike the great variety of supervised machine learning algorithms which traditionally rely on expert input labels and are typically used for decision making by an expert, recommender systems specifically rely on data input from non-expert or casual users and are meant to be used directly by these same non-expert users on an every day basis. Furthermore, the advances in online machine learning, data generation, and predictive model learning have become increasingly interdependent, such that each one feeds on the other in an iterative cycle. Research in psychology suggests that people's choices are (1) contextually dependent, and (2) dependent on interaction history. Thus, while standard methods of training and assessing performance of recommender systems rely on benchmark datasets, we suggest that a critical step in the evolution of recommender systems is the development of benchmark models of human behavior that capture contextual and dynamic aspects of human behavior. It is important to emphasize that even extensive real life user-tests may not be sufficient to make up for this gap in benchmarking validity because user tests are typically done with either a focus on user satisfaction or engagement (clicks, sales, likes, etc) with whatever the recommender algorithm suggests to the user, and thus ignore the human cognitive aspect. We conclude by highlighting the interdisciplinary implications of this endeavor.
Spotify is the world's largest on-demand music streaming company, with over 75 million active listeners choosing what to listen to among tens of millions songs. Discovery and personalization is a key part of the experience and critical to the success of the creator and consumer ecosystem. In this talk, we'll discuss the state of our current discovery approaches, such as the Discover Weekly playlist that has already streamed billions of new discoveries and Fresh Finds, a scalable platform for brand new music that focuses suggestions on the long end of the popularity tail. We'll discuss the technologies at scale necessary to distill the information about music from our listeners and the world at large we collect outside of Spotify -- with the massive amounts of user-item activity data we collect every day to create highly personalized music experiences. Entire teams at Spotify focus on understanding both the creator and listener through collaborative filtering, machine learning, DSP and NLP approaches -- we crawl the web for artist information, scan each note in every one of our millions of songs for acoustic signals, and model users' taste through a cluster analysis and in a latent space based on their historical and real-time listening patterns. The data generated by these analyses have ensured our discovery products are precise and help our users enjoy music and media across our entire catalog. We'll dive deep into the workings of Discover Weekly, our marquee personalized playlist which updates weekly and reached 1 billion streams within the first 10 weeks from its release. The technology behind Discover Weekly is powered by a scalable factor analysis of Spotify's over two billion user-generated playlists matched to each user's current listening behavior. We'll discuss its innovative genesis and the challenges and opportunities the system faces a year after its launch. We'll also discuss Spotify's home page, seen by each of our users, currently undergoing vast efforts around personalization to ensure each listener gets a targeted list of playlists, shows and music to select throughout their day. We'll discuss the various similarity metrics, ranking approaches and user modeling we're working on to increase precision and optimize for our users' happiness.
Regularized matrix factorization models are known to generate high quality rating predictions for recommender systems. One of the major drawbacks of matrix factorization is that once computed, the model is static. For real-world applications dynamic updating a model is one of the most important tasks. Especially when ratings on new users or new items come in, updating the feature matrices is crucial. In this paper, we generalize regularized matrix factorization (RMF) to regularized kernel matrix factorization (RKMF). Kernels provide a flexible method for deriving new matrix factorization methods. Furthermore with kernels nonlinear interactions between feature vectors are possible. We propose a generic method for learning RKMF models. From this method we derive an online-update algorithm for RKMF models that allows to solve the new-user/new-item problem. Our evaluation indicates that our proposed online-update methods are accurate in approximating a full retrain of a RKMF model while the runtime of online-updating is in the range of milliseconds even for huge datasets like Netflix.
Alternating least squares (ALS) is a powerful matrix factorization (MF) algorithm for both explicit and implicit feedback based recommender systems. As shown in many articles, increasing the number of latent factors (denoted by K) boosts the prediction accuracy of MF based recommender systems, including ALS as well. The price of the better accuracy is paid by the increased running time: the running time of the original version of ALS is proportional to K3. Yet, the running time of model building can be important in recommendation systems; if the model cannot keep up with the changing item portfolio and/or user profile, the prediction accuracy can be degraded. In this paper we present novel and fast ALS variants both for the implicit and explicit feedback datasets, which offers better trade-off between running time and accuracy. Due to the significantly lower computational complexity of the algorithm - linear in terms of K - the model being generated under the same amount of time is more accurate, since the faster training enables to build model with more latent factors. We demonstrate the efficiency of our ALS variants on two datasets using two performance measures, RMSE and average relative position (ARP), and show that either a significantly more accurate model can be generated under the same amount of time or a model with similar prediction accuracy can be created faster; for explicit feedback the speed-up factor can be even 5-10.
Implicit feedback is a key source of information for many recommendation and personalization approaches. However, using it typically requires multiple episodes of interaction and roundtrips to a recommendation engine. This adds latency and neglects the opportunity of immediate personalization for a user while the user is navigating recommendations. We propose a novel strategy to address the above problem in a principled manner. The key insight is that as we observe a user's interactions, it reveals much more information about her desires. We exploit this by inferring the within-session user intent on-the-fly based on navigation interactions, since they offer valuable clues into a user's current state of mind. Using navigation patterns and adapting recommendations in real-time creates an opportunity to provide more accurate recommendations. By prefetching a larger amount of content, this can be carried out entirely in the client (such as a browser) without added latency. We define a new Bayesian model with an efficient inference algorithm. We demonstrate significant improvements with this novel approach on a real-world, large-scale dataset from Netflix on the problem of adapting the recommendations on a user's homepage.
We propose a bayesian probabilistic model for explicit preference data. The model introduces a generative process, which takes into account both item selection and rating emission to gather into communities those users who experience the same items and tend to adopt the same rating pattern. Each user is modeled as a random mixture of topics, where each topic is characterized by a distribution modeling the popularity of items within the respective user-community and by a distribution over preference values for those items. The proposed model can be associated with a novel item-relevance ranking criterion, which is based both on item popularity and user's preferences. We show that the proposed model, equipped with the new ranking criterion, outperforms state-of-art approaches in terms of accuracy of the recommendation list provided to users on standard benchmark datasets.
Automated course recommendation can help deliver personalized and effective college advising and degree planning. Nearest neighbor and matrix factorization based collaborative filtering approaches have been applied to student-course grade data to help students select suitable courses. However, the student-course enrollment patterns exhibit grouping structures that are tied to the student and course academic features, which lead to grade data that are not missing at random (NMAR). Existing approaches for dealing with NMAR data, such as Response-aware and context-aware matrix factorization, do not model NMAR data in terms of the user and item features and are not designed with the characteristics of grade data in mind. In this work we investigate how the student and course academic features influence the enrollment patterns and we use these features to define student and course groups at various levels of granularity. We show how these groups can be used to design grade prediction and top-n course ranking models for neighborhood-based user collaborative filtering, matrix factorization and popularity-based ranking approaches. These methods give lower grade prediction error and more accurate top-n course rankings than the other methods that do not take domain knowledge into account.
The need for solving weighted ridge regression (WRR) problems arises in a number of collaborative filtering (CF) algorithms. Often, there is not enough time to calculate the exact solution of the WRR problem, or it is not required. The conjugate gradient (CG) method is a state-of-the-art approach for the approximate solution of WRR problems. In this paper, we investigate some applications of the CG method for new and existing implicit feedback CF models. We demonstrate through experiments on the Netflix dataset that CG can be an efficient tool for training implicit feedback CF models.
We propose a model for learning user preference rankings for the purpose of making product recommendations. The model allows us to learn from pairwise preference statements or from (incomplete) rankings over more than two items. We present two algorithms for performing inference in this model, both with excellent scaling in the number of users and items. The superior predictive performance of the new method is demonstrated on the well-known sushi preference data set. In addition, we show how the model can be used effectively in an active learning setting where we select only a small number of informative items for learning.
At Quora, our mission is to share and grow the world's knowledge. Recommender systems are at the core of this mission: we need to recommend the most important questions to people most likely to write great answers, and recommend the best answers to people interested in reading them. Driven by the above mission statement, we have a variety of interesting and challenging recommendation problems and a large, rich data set that we can work with to build novel solutions for them. In this talk, we will describe several of these recommendation problems and present our approaches solving them.
We propose Meta-Prod2vec, a novel method to compute item similarities for recommendation that leverages existing item metadata. Such scenarios are frequently encountered in applications such as content recommendation, ad targeting and web search. Our method leverages past user interactions with items and their attributes to compute low-dimensional embeddings of items. Specifically, the item metadata is injected into the model as side information to regularize the item embeddings. We show that the new item representations lead to better performance on recommendation tasks on an open music dataset.
Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, field-aware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use.
This turorial offers a rich blend of theory and practice regarding dimensionality reduction methods, to address the information overload problem in recommender systems. This problem affects our everyday experience while searching for knowledge on a topic. Naive Collaborative Filtering cannot deal with challenging issues such as scalability, noise, and sparsity. We can deal with all the aforementioned challenges by applying matrix and tensor decomposition methods. These methods have been proven to be the most accurate (i.e., Netflix prize) and efficient for handling big data. For each method (SVD, SVD++, timeSVD++, HOSVD, CUR, etc.) we will provide a detailed theoretical mathematical background and a step-by-step analysis, by using an integrated toy example, which runs throughout all parts of the tutorial, helping the audience to understand clearly the differences among factorisation methods.
Recommender systems help web users to address information overload. However their performance depends on the number of provided ratings by users. This problem is amplified for a new user because he/she has not provided any rating. To address this problem, active learning methods have been proposed to acquire those ratings from users, that will help most in determining their interests. However, different from the classic active learning, users (the "oracle") are not always able to provide an answer for queries. The easiest way to solve this problem is to ask most popular items, i.e items which have received many ratings from training users. But it is static and presents the same items to all users regardless of the ratings they have provided so far. In this paper we propose a method that improves the most popular selection strategy using the characteristics of matrix factorization. It finds similar users to the new user in the latent space and then selects item which is most popular among the similar users. The experimental results show the proposed method outperforms the most popular method both in terms of error and the number of received ratings.
This paper summarizes RecProfile '16, the first workshop on profiling user preferences for dynamic, online, and real-time recommendations, held in conjunction with RecSys '16, the 10th ACM conference on recommender systems. We describe the main themes arising in the workshop's papers.
Real-life recommender systems often face the daunting task of providing recommendations based only on the clicks of a user session. Methods that rely on user profiles -- such as matrix factorization -- perform very poorly in this setting, thus item-to-item recommendations are used most of the time. However the items typically have rich feature representations such as pictures and text descriptions that can be used to model the sessions. Here we investigate how these features can be exploited in Recurrent Neural Network based session models using deep learning. We show that obvious approaches do not leverage these data sources. We thus introduce a number of parallel RNN (p-RNN) architectures to model sessions based on the clicks and the features (images and text) of the clicked items. We also propose alternative training strategies for p-RNNs that suit them better than standard training. We show that p-RNN architectures with proper training have significant performance improvements over feature-less session models while all session-based models outperform the item-to-item type baseline.
We believe that Deep Learning is one of the next big things in Recommendation Systems technology. The past few years have seen the tremendous success of deep neural networks in a number of complex tasks such as computer vision, natural language processing and speech recognition. Despite this, only little work has been published on Deep Learning methods for Recommender Systems. Notable recent application areas are music recommendation, news recommendation, and session-based recommendation. The aim of the workshop is to encourage the application of Deep Learning techniques in Recommender Systems, to promote research in deep learning methods for Recommender Systems, and to bring together researchers from the Recommender Systems and Deep Learning communities.
In this paper, a new approach to predicting the structure of a social network without any prior knowledge from the social links is proposed. In absence of links among nodes, we assume there are other information resources associated with the nodes which are called node profiles. The task of link prediction and recommendation from text data is to learn similarities between the nodes and then translate pair-wise similarities into social links. In other words, the process is to convert a similarity matrix into an adjacency matrix. In this paper, an alternative approach is proposed. First, hidden topics of node profiles are learned using Latent Dirichlet Allocation. Then, by mapping node-topic and topic-topic relations, a new structure called semi-bipartite graph is generated which is slightly different from regular bipartite graph. Finally, by applying topological metrics such as Katz and short path scores to the new structure, we are able to rank and recommend relevant links to each node. The proposed technique is applied to several co-authorship networks. While most link prediction methods are low precision solutions, the proposed method performs effectively and offers high precision.
Two flavors of the recommendation problem are the explicit and the implicit feedback settings. In the explicit feedback case, users rate items and the user-item preference relationship can be modelled on the basis of the ratings. In the harder but more common implicit feedback case, the system has to infer user preferences from indirect information: presence or absence of events, such as a user viewed an item. One approach for handling implicit feedback is to minimize a ranking objective function instead of the conventional prediction mean squared error. The naive minimization of a ranking objective function is typically expensive. This difficulty is usually overcome by a trade-off: sacrificing the accuracy to some extent for computational efficiency by sampling the objective function. In this paper, we present a computationally effective approach for the direct minimization of a ranking objective function, without sampling. We demonstrate by experiments on the Y!Music and Netflix data sets that the proposed method outperforms other implicit feedback recommenders in many cases in terms of the ErrorRate, ARP and Recall evaluation metrics.
Recommendation Systems are omnipresent on the web nowadays. Most websites today are striving to provide quality recommendations to their customers in order to increase and retain their customers. In this paper, we present our approaches to design a job recommendation system for a career based social networking website - XING. We take a bottom up approach: we start with deeply understanding and exploring the data and gradually build the smaller bits of the system. We also consider traditional approaches of recommendation systems like collaborative filtering and discuss its performance. The best model that we produced is based on Gradient Boosting algorithm. Our experiments show the efficacy of our approaches. This work is based on a challenge organized by ACM RecSys conference 2016. We achieved a final full score of 1,411,119.11 with rank 20 on the official leader board.
Differentially private collaborative filtering is a challenging task, both in terms of accuracy and speed. We present a simple algorithm that is provably differentially private, while offering good performance, using a novel connection of differential privacy to Bayesian posterior sampling via Stochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm lends itself to efficient implementation. By careful systems design and by exploiting the power law behavior of the data to maximize CPU cache bandwidth we are able to generate 1024 dimensional models at a rate of 8.5 million recommendations per second on a single PC.
In this position paper, we take the experimental approach of putting algorithms aside, and reflect on what recommenders would be for people if they were not tied to technology. By looking at some of the shortcomings that current recommenders have fallen into and discussing their limitations from a human point of view, we ask the question: if freed from all limitations, what should, and what could, RecSys be? We then turn to the idea that life itself is the best recommender system, and that people themselves are the query. By looking at how life brings people in contact with options that suit their needs or match their preferences, we hope to shed further light on what current RecSys could be doing better. Finally, we look at the forms that RecSys could take in the future. By formulating our vision beyond the reach of usual considerations and current limitations, including business models, algorithms, data sets, and evaluation methodologies, we attempt to arrive at fresh conclusions that may inspire the next steps taken by the community of researchers working on RecSys.
In item-based collaborative filtering, a critical intermediate step to personalized recommendations is the definition of an item-similarity metric. Existing algorithms compute the item-similarity using the user-to-item ratings (cosine, Pearson, Jaccard, etc.). When computing the similarity between two items A and B many of these algorithms divide the actual number of co-occurring users by some "difficulty" of co-occurrence. We refine this approach by defining item similarity as the ratio of the actual number of co-occurrences to the number of co-occurrences that would be expected if user choices were random. In the final step of our method to compute personalized recommendations we apply the usage history of a user to the item similarity matrix. The well defined probabilistic meaning of our similarities allows us to further improve this final step. We measured the quality of our algorithm on a large real-world data-set. As part of Comcast's efforts to improve its personalized recommendations of movies and TV shows, several top recommender companies were invited to apply their algorithms to one year of Video-on-Demand usage data. Our algorithm tied for first place. This paper includes a MapReduce pseudo code implementation of our algorithm.
Recent research on Recommender Systems, specifically Collaborative Filtering, has focussed on Matrix Factorization (MF) methods, which have been shown to provide good solutions to the cold start problem. However, typically the same settings are used for Matrix factorization regardless of the density of the matrix. In our experiments, we found that for MF, Root Mean Square Error (RMSE) for recommendations increases (i.e. performance drops) for sparse matrices. We propose a Two Stage MF approach so MF is run twice over the whole matrix; the first stage uses MF to generate a small percentage of pseudotransactions that are added to the original matrix to increase its density, and the second stage re-runs MF over this denser matrix to predict the user-item transactions in the testing set. We show using data from Movielens that such methods can improve on the performance of MF for sparse martrices.
Matrix factorization (MF) models and their extensions are standard in modern recommender systems. MF models decompose the observed user-item interaction matrix into user and item latent factors. In this paper, we propose a co-factorization model, CoFactor, which jointly decomposes the user-item interaction matrix and the item-item co-occurrence matrix with shared item latent factors. For each pair of items, the co-occurrence matrix encodes the number of users that have consumed both items. CoFactor is inspired by the recent success of word embedding models (e.g., word2vec) which can be interpreted as factorizing the word co-occurrence matrix. We show that this model significantly improves the performance over MF models on several datasets with little additional computational overhead. We provide qualitative results that explain how CoFactor improves the quality of the inferred factors and characterize the circumstances where it provides the most significant improvements.
This paper describes our solution for the RecSys Challenge 2016. In the challenge, several datasets were provided from a social network for business XING. The goal of the competition was to use these data to predict job postings that a user will interact positively with (click, bookmark or reply). Our solution to this problem includes three different types of models: Factorization Machine, item-based collaborative filtering, and content-based topic model on tags. Thus, we combined collaborative and content-based approaches in our solution. Our best submission, which was a blend of ten models, achieved 7th place in the challenge's final leader-board with a score of 1677 898.52. The approaches presented in this paper are general and scalable. Therefore they can be applied to another problem of this type.
This work focuses on solving the context-aware implicit feedback based recommendation task with factorization and is heavily influenced by the practical considerations. I propose context-aware factorization algorithms that can efficiently work on implicit data. I generalize these algorithms and propose the General Factorization Framework (GFF) in which experimentation with novel preference models is possible. This practically useful, yet neglected feature results in models that are more appropriate for context-aware recommendations than the ones used by the state-of-the-art. I also propose a way to speed up and enhance scalability of the training process, that makes it viable to use the more accurate high factor models with reasonable training times.
The last ten years have seen a tremendous growth in Internet-based online services such as search, advertising, gaming and social networking. Today, it is important to analyze large collections of user interaction data as a first step in building predictive models for these services as well as learn these models in real-time. One of the biggest challenges in this setting is scale: not only does the sheer scale of data necessitate parallel processing but it also necessitates distributed models; with over 900 million active users at Facebook, any user-specific sets of features in a linear or non-linear model yields models of a size bigger than can be stored in a single system. In this talk, I will give a hands-on introduction to one of the most versatile tools for handling large collections of data with distributed probabilistic models: the sum-product algorithm for approximate message passing in factor graphs. I will discuss the application of this algorithm for the specific case of generalized linear models and outline the challenges of both approximate and distributed message passing including an in-depth discussion of expectation propagation and Map-Reduce.
Transcriptions of historical documents are a valuable source for extracting labeled handwriting images that can be used for training recognition systems. In this paper, we introduce the Saint Gall database that includes images as well as the transcription of a Latin manuscript from the 9th century written in Carolingian script. Although the available transcription is of high quality for a human reader, the spelling of the words is not accurate when compared with the handwriting image. Hence, the transcription poses several challenges for alignment regarding, e.g., line breaks, abbreviations, and capitalization. We propose an alignment system based on character Hidden Markov Models that can cope with these challenges and efficiently aligns complete document pages. On the Saint Gall database, we demonstrate that a considerable alignment accuracy can be achieved, even with weakly trained character models.
The peculiar nature in which one or more consonants combine with vowels to produce a compound character in Kannada language results in a huge number of character combinations, running to tens of thousands or more. The aim of the work is therefore, to reduce the number of character combinations by employing a divide and conquer technique. In the first level of the technique, the structural and the dynamic features of online handwritten Kannada characters are exploited to segment the compound Kannada characters into 282 distinct symbols. This reduction in the number of classes overcomes the huge data collection problem and also reduces the computational complexity. In the second level, these 282 symbols are further divided into three distinct sets of stroke groups, thus further reducing the search space for the recognition engine. One or more of these stroke groups can combine to form any of the thousands of Kannada compound characters. Since the focus of this paper is the above strategy, a simple classifier has been used to validate the effectiveness of the proposed scheme in handling the difficult task of recognizing all possible character combinations of Kannada. The features extracted from the segmented stroke groups are mapped to lower dimensional space using PCA. The subspace features of distinct stroke groups are fed to the respective classifiers in an order and the output of these classifiers are combined to output the Unicode of the recognized akshara. The proposed work is an attempt made for the first time in Kannada language which considers all possible combinations of symbols, including Kannada numerals.
Wearable technologies play a central role in human-centered Internet-of-Things applications. Wearables leverage machine learning algorithms to detect events of interest such as physical activities and medical complications. These algorithms, however, need to be retrained upon any changes in configuration of the system, such as addition/ removal of a sensor to/ from the network or displacement/ misplacement/ mis-orientation of the physical sensors on the body. We challenge this retraining model by stimulating the vision of autonomous learning with the goal of eliminating the labor-intensive, time-consuming, and highly expensive process of collecting labeled training data in dynamic environments. We propose an approach for autonomous retraining of the machine learning algorithms in real-time without need for any new labeled training data. We focus on a dynamic setting where new sensors are added to the system and worn on various body locations. We capture the inherent correlation between observations made by a static sensor view for which trained algorithms exist and the new dynamic sensor views for which an algorithm needs to be developed. By applying our real-time dynamic-view autonomous learning approach, we achieve an average accuracy of 81.1% in activity recognition using three experimental datasets. This amount of accuracy represents more than 13.8% improvement in the accuracy due to the automatic labeling of the sensor data in the newly added sensor. This performance is only 11.2% lower than the experimental upper bound where labeled training data are collected with the new sensor.
Leak localization is a major issue faced by water utilities worldwide. Leaks are ideally detected and localized by a network-wide metering infrastructure. However, in many utilities, in-network metering is minimally present at just the inlets of sub-networks called District Metering Area (DMA). We consider the problem of leak localization using data from a single flow meter placed at the inlet of a DMA. We use standard time-series based modeling to detect if a current meter reading is a leak or not, and if so, to estimate the excess flow. Conventional approaches use an a-priori fully calibrated hydraulic model to map the excess flow back to a set of candidate leak locations. However, obtaining an accurate hydraulic model is expensive and hence, beyond the reach of many water utilities. We present an alternate approach that exploits the network structure and static properties in a novel way. Specifically, we extend the use of centrality metrics to infrastructure domains and use these metrics to map from the excess leak flow to the candidate leak location(s). We evaluate our approach on benchmark water utility network topologies as well as on real data obtained from an European water utility. On benchmark topologies, the localization obtained by our method is comparable to that obtained from a complete hydraulic model. On a real-world network, we were able to localize two out of the three leaks whose data we had access to. Of these two cases, we find that the actual leak location was in the candidate set identified by our approach; further, the approach pruned as much as 78% of the DMA locations, indicating a high degree of localization.
Cane-toads (Bufo marinus) are introduced to Australia to negate the insect pests. However, cane-toad population is out of control, and has highly affected native frog species for its great reproductive capacity. Therefore, it is necessary to monitor cane-toads and control their population. Recent advances in acoustic sensors make it possible to monitor cane-toads over long periods. In this paper, we present a novel approach to analyze cane-toad calls for long-term monitoring. Specifically, recorded acoustic data is first transformed into spectrogram using short-time Fourier transform. Then, cane-toad calls are recognized with an oscillation detector. Next, recognized cane-toad calls are annotated for further analysis. Initial analysis of our method demonstrates promising results, with 71.36% true positive rate. With the oscillation detector, cane-toad calling activity is investigated over the whole year.
Mobile sensor networks are a great source of data. By collecting data with mobile sensor nodes from individuals in a user community, e.g. using their smartphones, we can learn global information such as traffic congestion patterns in the city, location of key community facilities, and locations of gathering places. Can we publish and run queries on mobile sensor network databases without disclosing information about individual nodes? Differential privacy is a strong notion of privacy which guarantees that very little will be learned about individual records in the database, no matter what the attackers already know or wish to learn. Still, there is no practical system applying differential privacy algorithms for clustering points on real databases. This paper describes the construction of small coresets for computing k-means clustering of a set of points while preserving differential privacy. As a result, we give the first k-means clustering algorithm that is both differentially private, and has an approximation error that depends sub-linearly on the data's dimension d. Previous results introduced errors that are exponential in d. We implemented this algorithm and used it to create differentially private location data from GPS tracks. Specifically our algorithm allows clustering GPS databases generated from mobile nodes, while letting the user control the introduced noise due to privacy. We provide experimental results for the system and algorithms, and compare them to existing techniques. To the best of our knowledge, this is the first practical system that enables differentially private clustering on real data.
This article describes the implementation of four different machine learning techniques for vehicle classification in a dual ultrasonic/passive infrared traffic flow sensors. Using k-NN, Naive Bayes, SVM and KNN-SVM algorithms, we show that KNN-SVM significantly outperforms other algorithms in terms of classification accuracy. We also show that some of these algorithms could run in real time on the prototype system.
In underwater wireless sensor networks (USWNs), localizing unknown nodes is essential for most applications while is more complex than that of terrestrial WSNs. In this paper, we propose a range-based localization scheme using deep neural network (DNN). Numerical results suggest that the proposed DNN localization algorithm outperforms traditional schemes using least squares support vector machines (LS-SVM) or generalized least squares (GLS) in terms of localization accuracy and efficiency. Moreover, the proposed algorithm requires a small number of anchor nodes, which is plausible for practical applications.
Mobile health (mHealth) leverages the power and ubiquity of mobile and cloud technologies to support patients and clinicians in monitoring and understanding symptoms, side effects and treatment outside the clinical setting; thereby closing the feedback loops of self-care, clinical-care, and personal-evidence-creation. However, to realize this promise, we must develop new data capture, processing and modeling techniques to convert the digital exhaust emitted by mobile phone use into behavioral biomarkers. This calls for a modular layered sensemaking framework in which low level state classifications of raw data (e.g., estimated activity states such as sitting, walking, driving from continuous accelerometer and location traces), are used to derive mid-level semantic features (e.g., total number of ambulatory minutes, number of hours spent out of house), that can then be mapped to particular behavioral biomarkers for specific diseases (e.g., chronic pain, GI disfunction, MS, fatigue, depression, etc). The techniques needed to derive these markers will range from simple functions to machine learning classifiers, and will need to fuse diverse data types, but all will need to cope with noisy, erratic data sources. We are working to build an open architecture and community to speed the rate and robustness of innovation in this space, both academic and commercial (http://openmhealth.org).
Hand washing is an effective countermeasure to the spread of many types of infection. Recently, sensing technology has automated the sampling and study of hand hygiene rates. Surprisingly, many questions about the area are unresolved, motivating further exploration based on wrist-worn commodity sensors (accelerometer and MEMS gyroscope). This paper describes initial work on techniques for measuring the duration of washing events and classifying different scrubbing motions. The work compares different sensor types and their fusion, compares sensing from one wrist to measuring both wrists, and explains results of experiments on a range of hand washing motions in a variety of subject populations, some in clinics of a teaching hospital. Machine learning is used to explore such questions: the paper investigates numerous features extracted from sensor data, looking at sampling rates, windowing, and platform details that affect classification. In training and classification experiments, data collection starts on the wrist, activated by a message from a disinfectant dispenser; data is then transferred by radio to a base station for subsequent reduction, analysis and characterization. Results show that hand hygiene motions can be classified with up to 93% accuracy.
Understanding human drivers' behavior is critical for the self-driving cars, and has been intensively studied in the past decade. We exploit the widely available camera and motion sensor data from car recorders, and propose a hybrid method of recognizing driving events based on the random forest approach. The classification results are analyzed by comparing different features, classifiers and filters. A high accuracy of 98.1% on driving behavior classification is obtained and the robustness is verified on a dataset including 2400 driving events.
We present a novel approach for monitoring beverage intake. Our system is composed of an ultrasonic sensor, an RGB color sensor, and machine learning algorithms. The system not only measures beverage volume but also detects beverage types. The sensor unit is lightweight that can be mounted on the lid of any drinking bottle. Our experimental results demonstrate that the proposed approach achieves more than 97% accuracy in beverage type classification. Furthermore, our regression-based volume measurement has a nominal error of 3%.
Effectively managing the data generated by Large-area Community driven Sensor Networks (LCSNs) is a new and challenging problem. One important step for managing and querying such sensor network data is to create abstractions of the data in the form of models. These models can then be stored, retrieved, and queried as required. In our OpenSense project, we advocate an adaptive model-cover driven strategy towards effectively managing such data. Our strategy is designed considering the fundamental principles of LCSNs. We describe an adaptive approach, called adaptive k-means, and report preliminary results on how it compares with the traditional grid-based approach towards modeling LCSN data. We find that our approach performs better to model the sensed phenomenon in spatial and temporal dimensions. Our results are based on two real datasets.
Blood glucose concentration plays an important role in personal health. Hyperglycemia results in diabetes, leading to health risks such as pancreatic function failure, immunity reduce and ocular fundus diseases [6]. Meanwhile, hypoglycemia also brings complications such as confusion, shakiness, anxiety, and if not treated in time, coma or death [2]. People with diabetes need tight control of their blood glucose concentration to avoid both short-term and long-term physiological complications. In this work, we design BGMonitor, the first personalized smartphone-based non-invasive blood glucose monitoring system that detects abnormal blood glucose events by jointly tracking meal, drugs and insulin intake, physical activity and sleep quality. When BGMonitor detects an abnormal blood glucose event, it reminds the user to double-check by finger pricking or using clinical CGM devices.
A key challenge of HRI is allowing robots to be adaptable, especially as robots are expected to penetrate society at large and to interact in unexpected environments with non-technical users. One way of providing this adaptability is to use Interactive Machine Learning, i.e. having a human supervisor included in the learning process who can steer the action selection and the learning in the desired direction. We ran a study exploring how people use numeric rewards to evaluate a robot's behaviour and guide its learning. From the results we derive a number of challenges when designing learning robots: what kind of input should the human provide? How should the robot communicate its state or its intention? And how can the teaching process by made easier for human supervisors?
Accurate detection of human intention is always challenging for the effective control of assistive and rehabilitation exoskeletons. In this paper, a human arm motion detection system is presented to recognize movements including elbow flexion, elbow extension, pronation and supination. Force sensing resistors (FSR) based sensor bands are developed to monitor the upper arm and forearm muscles activity. The bands are able to read the muscle deformation for different motions. Support Vector Machine (SVM) is implemented to recognize the motions in real time. The results have shown that the sensing method can detect the intended motions with high accuracy.
We improve robotic learning from demonstration (LfD) via an active learning process of interacting with a human expert to establish a semantic structure and labels for a sign language task. This process situates a learned task in a human-accessible conceptual framework, in order to improve skill transfer not only from expert human teacher to robot, but from robot to novice human learner.
This paper describes an architecture for robots interacting with non-expert humans to incrementally acquire domain knowledge. Contextual information is used to generate candidate questions that are ranked using measures of information gain, ambiguity, and human confusion, with the objective of maximizing the potential utility of the response. We report results of preliminary experiments evaluating the architecture in a simulated environment.
We investigate a real robot applicability of our method, general-purpose behavior-learning for high degree-of-freedom robots in varying environments. Our method is based on the learning strategy fusion proposed in [Yamaguchi et al. 2011], and extended theoretically in [Yamaguchi et al. 2013]. This report discusses its applicability to real robot systems, and demonstrates some positive experimental results.
In a human robot interaction scenario, predicting the human motion intention is essential for avoiding inconvenient delays and for a smooth reactivity of the robotic system. In particular, when dealing with hand prosthetic devices, an early estimation of the final hand gesture is crucial for a smooth control of the robotic hand. In this work we develop an electromyographic (EMG) based learning approach that decodes the grasping intention at an early stage of the reaching to grasping motion, i.e before the final grasp/hand preshape takes place. EMG electrodes are used for recording the arm muscles activities and a cyberglove is used to measure the finger joints during the textit{reach and} textit{grasp} motion. Results show that we can correctly classify with $90%$ accuracy for three typical grasps textit{before the onset of the hand pre-shape}. Such an early detection of the grasp intention allows to control a robotic hand simultaneously to the motion of subject's arm, hence generating no delay between the natural arm motion and the artificial hand motion.
Modern robotics applications that involve human-robot interaction require robots to be able to communicate with humans seamlessly and effectively. Natural language provides a flexible and efficient medium through which robots can exchange information with their human partners. Significant advancements have been made in developing robots capable of interpreting free-form instructions, but less attention has been devoted to endowing robots with the ability to generate natural language. We propose a model that enables robots to generate natural language instructions that allow humans to navigate a priori unknown environments. We first decide which information to share with the user according to their preferences, using a policy trained from human demonstrations via inverse reinforcement learning. We then "translate" this information into a natural language instruction using a neural sequence-to-sequence model that learns to generate free-form instructions from natural language corpora. We evaluate our method on a benchmark route instruction dataset and achieve a BLEU score of 72.18% compared to human-generated reference instructions. We additionally conduct navigation experiments with human participants demonstrating that our method generates instructions that people follow as accurately and easily as those produced by humans.
Recent findings in Human-Robot Interaction (HRI) indicate that the adaptation of a robot's behaviors to the human's personality profile makes interaction more engaging, but also that it depends on the task context whether a similar or opposing robot personality is preferred. This late breaking report presents our ongoing work on an approach using Reinforcement Learning and social signals for figuring out and adapting to the human preferences, i.e. desired personality profile. Our scenario involves a "Reeti" robot in the role of a story teller talking about the main characters in the novel "Alice's Adventures in Wonderland" by generating descriptions with varying degree of introversion/extraversion. The learning process is running in real-time during the interaction and allows for simultaneous adaptation without explicitly asking the user about its preferences.
This paper investigates the possibility of recognising individual persons from their walking gait using three-dimensional 'skeleton' data from an inexpensive consumer-level sensor, the Microsoft 'Kinect'. In an experimental pilot study it is shown that the K-means algorithm - as a candidate unsupervised clustering algorithm - is able to cluster gait samples from four persons with a nett accuracy of 43.6%.
Emotions are a fundamental part of everyday life and an important topic in the development of artificial intelligence. We combine a Simultaneous Localization and Mapping algorithm with a model of emotion. The model of emotion is able to generate a mapping from the quantitative figures of the SLAM process to human-like emotions. This enables the robot to communicate its current state towards a human observer using emotional expressions. The paper reports on the design of the model, the result of the affective evaluation during an autonomous path finding process and its comparison to experimental data of a survey.
In this work, we propose methods for automatically generating primitive skills from demonstration of a task. Additionally, we propose methods for improving the existing primitive skills and adding the new primitive skills incrementally and automatically. To validate our proposed methods, we present experimental results of a human-like robot handling three gestures and a task for making coffee.
Facial gesture recognition is one of the main topics in HRI. We have developed a novel algorithm who allows to detect emotional states, like happiness, sadness or emotionless. A humanoid robot is able to detect these states with a ratio of success of 83% and interact in consequence. We use Active Appearance Models (AAMs) to determinate face features and classify the emotions using neural evolution, based on neural networks and differential evolution algorithm.
Personal robots operate in human environments such as homes and offices, co-habiting with people. To effectively train robot algorithms for such scenarios, a large amount of training data containing both people and the environment is required. Collecting such data involves taking a robot into new environments, observing and interacting with people. So far, best practices for robot data collection have been undefined. Fortunately, the human-robot interaction community has conducted field studies whose methodology can serve as a model. In this paper, we draw parallels between field study observation and the data collection process, suggesting that best practices may be transferable. As a use case, we present a robot sensor dataset for training and testing algorithms for person detection in indoor environments.
Gesture recognition is an important aspect of interpersonal social interaction. Developing a similar capacity in a robot will improve human-robot interaction. Various unsupervised clustering methods applied to clustering a set of dynamic human arm gestures are compared. Unsupervised clustering is important in gesture recognition as it imposes no a priori bound on the set of gestures. Results are compared using v-measure, a metric that allows differential weighting between clustering homogeneity and completeness. Experiments show that the best clustering method depends on the desired balance between homogeneity and completeness.
Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.
In the canonical Robot Learning from Demonstration scenario a robot observes performances of a task and then develops an autonomous controller. Current work acknowledges that humans may be suboptimal demonstrators and refines the controller for improved performance. However, there is still an assumption that the demonstrations are successful examples of the task. We here consider the possibility that the human has failed, and propose a model to minimize the possibility of the robot making the same mistakes.
Gender roles influence behavior in social interactions. Thus, real-time gender recognition is essential in Human-Robot Interaction (HRI) for providing timely gender information to improve the experience of HRI. Considering the HRI scenario, a 3D-human-body-shape-based gender recognition is investigated. The 3D information is obtained by processing the depth image from an RGB-D camera. In addition, a machine learning method based on a Support Vector Machine (SVM) was applied. The experimental results showed that our system could achieve real-time accurate gender recognition. It enriched the diversity of existing methods for HRI application.
This paper advocates an approach for learning communicative actions and manual skills in the same framework. We exploit a fundamental relationship between the structure of motor skills, intention, and communication. Communicative actions are acquired using the same learning framework and the same primitive states and actions that the robot uses to construct manual behavior for interacting with other objects in the environment. A prospective behavior algorithm is used to acquire modular policies for conveying intention and goals to nearby human beings and recruiting their assistance. The learning framework and a preliminary case study are presented in which a humanoid robot learns expressive communicative behavior incrementally by discovering the manual affordances of human beings. Results from interactions with 16 people provide support for the hypothesized benefits of this approach. Behavior reuse makes learning from relatively few interactions possible. This approach compliments other efforts in the field by grounding social behavior, and proposes a mechanism for negotiating a communicative vocabulary between humans and robots.
This paper presents a novel method for the computation of potential function using human input for potential based reward shaping. It defines a ranking over state space which is used to define a potential function. Specifically, it seeks multiple, partial to full, rankings of robot's states from a user in a HRI scenario. These rankings are used to learn a ranking model using a learning-to-rank algorithm. The ranking model is used to define a complete ranking of states. From the ranked states, a potential function is computed using a mapping function. For the proof of concept, we compared it with a base-line reinforcement learner in a simulated domain. The empirical results showed that the proposed method clearly outperformed the benchmark.
Adapting to users' intentions is a key requirement for autonomous robots in general, and in care settings in particular. In this paper, a comprehensive long-term study of a mobile robot providing information services to residents, visitors, and staff of a care home is presented, with a focus on adapting to the when and where the robot should be offering its services to best accommodate the users' needs. Rather than providing a fixed schedule, the presented system takes the opportunity of long-term deployment to explore the space of possibilities of interaction while concurrently exploiting the model learned to provide better services. But in order to provide effective services to users in a care home, not only then when and where are relevant, but also the way how the information is provided and accessed. Hence, also the usability of the deployed system is studied specifically, in order to provide a most comprehensive overall assessment of a robotic info-terminal implementation in a care setting. Our results back our hypotheses, (i) that learning a spatio-temporal model of users' intentions improves efficiency and usefulness of the system, and (ii) that the specific information sought after is indeed dependent on the location the info-terminal is offered.
In this paper, we present a human detection system for a domestic robot. A 2D laser scanner based leg detector and a vision based body detector are combined using a grid fusion strategy. This approach has been evaluated on a domestic robot. Furthermore, we propose a methodology to evaluate it in relation to proxemics that could be generalized to other robot's perceptive functions.
We develop the first system to combine task-based and chatbot-style dialogue in a multimodal system for Human-Robot Interaction. We show that Reinforcement Learning is beneficial for training dialogue management (DM) in such systems -- providing a scalable method for training from data and/or simulated users. We first train in simulation, and evaluate the benefits of a combined chat/task policy over systems which can only perform chat or task-based conversation. In a real user evaluation, we then show that a trained combined chat/task multimodal dialogue policy results in longer dialogue interactions than a rule-based approach, suggesting that the learned dialogue policy provides a more engaging mixture of chat and task interaction than a rule-based DM method.
Currently, many studies have been conducted on robot interactions with humans. Object recognition and feature extraction are essential functions for such robots. Discernment behavior is a type of exploratory behavior that supports object feature extraction. We have proposed an active perception model that autonomously learns discernment behaviors. We have shown the effectiveness of our model using a mobile robot simulation. In this study, we applied our model to a real humanoid robot and confirmed that the robot successfully learns exploratory behaviors. We show that the robot can learn suitable exploratory behaviors by online learning applicable to real-world environments.
We present a novel framework for operator assistance in indoor navigation and map building wherein the ground vehicle learns to navigate by imitating the operator commands while training. Our framework reduces the workload on the human operator simplifying the process of human robot interaction. An end to end architecture is presented which takes inputs from camera and LIDAR and outputs the steering angle for the ground vehicle to navigate through an indoor environment. The presented framework includes static obstacle avoidance during navigation and map building. The architecture is made more reliable by an on-line mechanism in which the robot introspects its output and decides whether to rely on its output or transfer vehicle control to a human pilot. The end to end trained framework implicitly learns to avoid obstacles. We show that our framework works under various cases where other frameworks fail.
The use of robots in rehabilitation is an increasingly viable option, given the shortage of well-trained therapists who can address individual patients' needs and priorities. Despite the acknowledged importance of customized therapy for individual patients, the means to realize it has received less research attention. Many approaches rely on rehabilitation robots, such as InMotion [3], where therapy customization is achieved by physically assisting patients when they cannot complete expected exercise movements. Consequently, it is important to accurately detect the patients' unsuccessful efforts to make exercise movements using various signals. An example that utilitzes electromyography signal can be found in Dipietro et al. [1]. These approaches lack of adaptive therapy programs where generic exercise targets do not necessarily address the specific needs/deficit of individual patients nor impose appropriate challenges.
Human-robot collaboration is a potential yet challenging robot development due to the vase diversity of partner human behaviors for the robot to adapt. In this work, we develop a robot learning framework that can learn by data-driven approach, where collaboration data is collected through crowdsourcing of human-robot interaction. We propose the addition of a formal definition incorporating partner's behaviors and set of state features for work conditions related to the collaboration task into the learning policy. A collaborative table setting task experiment scenario was developed with the capability to perform cloud based human-robot interaction for crowdsourcing data gathering. The human-human collaboration experiments were conducted to gather collaboration interaction data to build the case based planning libraries and finally, evaluation experiments were conducted and concluded the effectiveness of the proposed learning approach.
In this paper we explore whether a social robot can learn, in and from a task-oriented interaction with a human user, how to employ different social behaviors to achieve interactional goals under specific situational circumstances. We present a multimodal behavior generation architecture that maps high-level interactional functions and behaviors onto low-level behaviors executable by a robot. While high-level behaviors are selected based on the state of the user as well as the interaction, reinforcement learning is used within each behavior to optimize its local mapping onto lower-level behaviors. The approach is implemented and applied in a scenario in which a social robot (Furhat) assists a human player in solving a Memory game by guiding the attention of the user to target objects. Results of an evaluation study demonstrate that participants are able to solve the Memory faster with the adaptive, assistive robot.
Nowadays, programming by demonstration (PbD) has become an important paradigm for policy learning in roboticsm [3]. The idea of having robots capable of learning from humans through natural communication means is indeed fascinating. As an extension of the traditional PbD learning scheme, where robots only learn by observing a human teacher, our work follows the recently suggested principle of policy refinement and reuse through interactive corrective feedback [1]. However, to be responsive to such feedback, robots must be capable of sensing the world, especially human contact. Our work focuses on the sense of touch. Its integration in robotic applications has many advantages such as: a) safer and more natural interactions with objects and humans, b) improvement and simplification of the control mechanisms for human-robot interaction and object manipulation [2]. Our video reports on two experimental studies conducted with the iCub, a 53 degree of freedom humanoid robot endowed with tactile sensing on its forearms and fingertips. a) In a hand-positioning task, the robot is shown how to bring its hand to the location where an object should be grasped. A wrong placement or a wrong approach to the target is corrected by the teacher though a tactile interface [1]. b) In a reactive grasping task, the robot is taught how to use its fingertip sensors to adapt and maintain its grasp in the face of external perturbations on the grasped object. The results of both our experiments show how tactile sensing can be utilized effectively to learn robust control policies through human coaching, by enabling a) online policy refinement and reuse, and b) rapid adaptation to external perturbations.
In this paper, we present work to construct a robotic tutoring system that can assess student knowledge in real time during an educational interaction. Like a good human teacher, the robot draws on multimodal data sources to infer whether students have mastered language skills. Specifically, the model extends the standard Bayesian Knowledge Tracing algorithm to incorporate an estimate of the student's affective state (whether he/she is confused, bored, engaged, smiling, etc.) in order to predict future educational performance. We propose research to answer two questions: First, does augmenting the model with affective information improve the computational quality of inference? Second, do humans display more prominent affective signals in an interaction with a robot, compared to a screen-based agent? By answering these questions, this work has the potential to provide both algorithmic and human-centered motivations for further development of robotic systems that tightly integrate affect understanding and complex models of inference with interactive, educational robots.
Despite the importance of mutual adaption in human relationships, online learning is not yet used during most successful human-robot interactions. The lack of online learning in HRI to date can be attributed to at least two unsolved challenges: random exploration (a core component of most online-learning algorithms) and the slow convergence rates of previous online-learning algorithms. However, several recently developed online-learning algorithms have been reported to learn at much faster rates than before, which makes them candidates for use in human-robot interactions. In this paper, we explore the ability of these algorithms to learn to interact with people. Via user study, we show that these algorithms alone do not consistently learn to collaborate with human partners. Similarly, we observe that humans fail to consistently collaborate with each other in the absence of explicit communication. However, we demonstrate that one algorithm does learn to effectively collaborate with people when paired with a novel cheap-talk communication system. In addition to this technical achievement, this work highlights the need to address AI and HRI synergistically rather than independently.
We propose a computational model based on inverse-forward model pairs for the simulation and execution of actions. The models are implemented on a humanoid robot and are used to control reaching actions with the arms. In the experimental setup a tool has been attached to the left arm of the robot extending its covered action space. The preliminary investigations carried out aim at studying how the use of tools modifies the body scheme of the robot. The system performs action simulations before the actual executions. For each of the arms, predicted end-effector positions are compared with the desired one and the internal pair presenting the lowest error is selected for action execution. This allows the robot to decide on performing an action either with its hand alone or with the one with the attached tool.
As robots are introduced into human environments for long periods of time, human owners and collaborators will expect them to remember shared events that occur during execution. Beyond naturalness of having memories about recent and longer-term engagements with people, such execution memories can be important in tasks that persist over time by allowing robots to ground their dialog and to refer efficiently to previous events. In this work, we define execution memory as the capability of saving interaction event information and recalling it for later use. We divide the problem into four parts: salience filtering of sensor evidence and saving to short term memory, archiving from short to long term memory and caching from long to short term memory, and recalling memories for use in state inference and policy execution. We then provide examples of how execution memory can be used to enhance user experience with robots.
Robotics is a reasonably mature technology when robots are restricted to operating with well-known and well-engineered environments, e.g. in manufacturing robotics or domestic applications such as vacuum cleaning or lawn mowing. For more diverse tasks and open-ended environments, robotic behaviours are mainly hand-tuned: for most of the one million robots deployed today basically, the environment should be modeled and the robot's skills and strategies programmed by hand. This is a perfectly feasible approach as long as this hand-coding is inexpensive and reliable enough. This will be the case if the environment is well structured and stable and if the robot's tasks are restricted in scope and diversity, with only limited Human-Robot Interaction (HRI). However, if the robot has to face a diversity of tasks and/or a variety of environment, then the robot has to use more complex approaches based on machine learning and planning techniques in order to decide how to act in the environment. Recently, an increasing interest in the research community is how to enable robots to interact and communicate with humans, which is a major step towards integrating robots in our daily life. Within the large domain of HRI, this research area is called Social robotics. Communicating with humans is a complex process, involving several modalities such as speech, facial expressions, gaze, head movements, hand gestures, etc. Social robotics focuses on modeling how humans use these communication channels in order to smoothly interact with each other, and how to endow a robot with similar skills. Modeling such complex multimodal interactions has proven to be a challenge. Machine learning is invading this domain. Recent research performed on interactive data (i.e. challenging the problem of generating co-verbal gestures of one participant given the co-verbal gestures of the other and verbal activities of both participants) showed a great potential for machine learning in learning complex HRI behaviors. However, the behavioural models are task- and situation-specific: if anything changes in the learning problem (the input distribution or shape, the interaction scenario, etc), we need to learn everything again from scratch. This means that policies learnt in one environment might not work -- or work sub-optimally -- in other environments and conditions. A growing interest of research in machine learning is on how to transfer knowledge learned from one task (source task) to a new task (target task). The idea is to capture the similarities between the source and the target tasks, and exploit them in order to learn the target task faster. The shared knowledge can be implicit, like finding latent variables for the input distributions of both tasks. It can also be explicit, like representing robotic knowledge as skills. Our challenge is thus to develop methods to transfer the knowledge model the robot learns on one interactive situation with humans to new tasks/situations -~notably to new interactive tasks~-, in order to enable rapid learning and adaptation to these new tasks.
Motor Babbling has been identified as a self-exploring behaviour adopted by infants and is fundamental for the development of more complex behaviours, self-awareness and social interaction skills. Here, we adopt this paradigm for the learning strategies of a humanoid robot that maps its random arm movements with its head movements, determined by the perception of its own body. Finally, we analyse three random movement strategies and experimentally test on a humanoid robot how they affect the learning speed.
Perceiving when, where and how a robot is touched is an important aspect towards a natural Human-Robot Interaction (HRI). To date, several technologies are used in Social Robotics to determine the area where a touch is performed, in some cases using many sensors. Moreover, most approaches do not tackle the kind of touch performed. In this paper, we introduce a novel technique based on audio analysis, and machine learning techniques. This presents a proof of concept aimed to provide some advantages regarding the state-of-the-art touch technologies for HRI: cost-efficiency since only a few microphones can cover the robot shell completely; robustness as microphones are not affected by electromagnetic interference or by external sounds; and accuracy taking into account the preliminary results.
This paper presents a system in which a robot uses Active Learning (AL) to improve its learning capabilities for pose recognition. We propose a sub-type of Feature Queries, Rank Queries (RQ), in which the user states the relevance of a characteristic of the learning space. In the case of pose learning, these queries refer to the relevance of a single limb for a certain pose. We test the use of RQ with 24 users to learn 3 pointing poses and compare the learning accuracy against a passive learning approach. Our results show that RQ can increase the robot's learning accuracy.
We present our method for learning object categories from the internet using cues obtained through human-robot interaction. Such cues include an object model acquired by observation and the name of the object. Our learning approach emulates the natural learning process of children when they observe their environment, encounter unknown objects and ask adults the name of the object. Using this learning approach, our robot is able to discover objects in a domestic environment by observing when humans naturally move objects as part of their daily activities. Using speech interface,the robot directly asks humans the name of the object by showing an example of the acquired model. The name in text format and the previously learnt model serve as input parameters to retrieve object category images from a search engine, select similar object images, and build a classifier. Preliminary results demonstrate the effectiveness of our learning approach.
As a first step towards human and multiple-UAV interaction, we present a novel method for humans to interact with flying UAVs using locally on-board video cameras. Using machine vision techniques, our approach enables human operators to command and control Parrot drones by giving them directions to move, using simple hand gestures. When a direction to move is given, the robot controller estimates the angle and distance to move with the help of a face score system and the estimated hand direction. This approach offers mobile robots the ability localize with human operators and provides UAVs/UGVs with a better perception of the environment around the human.
This research evaluates reward shaping with unconventional formats of human input. Conventionally, a human teacher is assumed to provide numeric rewards for complete task training. However, there are limitations to this conventional format. Firstly, the continuous demand of numeric rewards is onerous. Secondly, it is limited in extracting useful knowledge from humans. In this research, we have tested three unconventional formats of human input, two to increase social appeal of reward shaping and one to efficiently extraction knowledge from humans. The preliminary results on simulated domains validate the usefulness of these formats in terms of user's comfort and learning performance.
Facial expression recognition is an important topic in the field of human-agent interaction, because facial expression is simple and impressive signal which human can send to others. Though there have been numerous studies on facial image analysis, the performance of expression recognition is still not acceptable due to the diversity of human expression and enormous variations in facial images. In this paper, we try to improve the performance of facial expression recognition by using multi-task learning techniques of neural networks. Through computational experiments on a benchmark database, we show positive possibility of performance improvement using multi-task learning.
In order to make machines able to recognize various patterns, it is important to define an appropriate function for measuring similarities between different objects. Conventional similarity measures are devised mainly for 1D vector data, which may lead to loss of information of 2D matrix data. We cast the calculation of similarity between two matrices as a neural network problem, and design the architecture for learning a similarity measure. We provide experiments on real 2D matrix data in the face recognition and gesture recognition, where we show that the learning of a similarity measure leads to improvements in the performance of the recognition problem. Also we compare the performance of the proposed measure with conventional distance measures for 2D matrix data.
In this paper, we consider a problem of analyzing human behavioral data to predict the human cognitive states and generate corresponding actions of sever-agent. Specifically, we aim at predicting human cognitive states during meal time and generating relevant dining services for the human. For this study, we collect behavioral data using 2 kinds of wearable devices, which are an eye tracker and a watch type EDA device, during meal time. We focus on the characteristics of the behavioral data, which are heterogeneous, noisy and temporal, and suggest a novel machine learning algorithm which can analyze the data integrally. Suggested model has hierarchical structure: the bottom layer combines the multi-modal behavioral data based on causal structure of the data and extracts the feature vector. Using the extracted feature vectors, the upper layer predicts the cognitive states based on temporal correlation between feature vectors. Experimental results show that the suggested model can analyze the behavioral data efficiently and predict the human cognitive states correctly.
Motion understanding and regeneration are two basic aspects of human-agent interaction. One important function of agents is to represent human's activities. For better interaction with human, robot agents should not only do something following human's order, but also be able to understand or even play some actions. Multiple Timescale Recurrent Neural Networks (MTRNN) is believed to be an efficient tool for robots action generation. In our previous work, we extended the concept of MTRNN and developed Supervised MTRNN for motion recognition. In this paper, we use Conditional Restricted Boltzmann Machine (CRBM) to initialize Supervised MTRNN and accelerate the training speed of Supervised MTRNN. Experiment results show that our method can greatly increase the training speed without losing much performance.
The paper presents the model of two variants of a fortriori reasoning applicable in the case of statutory law as well the example of the genuine law case, which has been modeled with use of established methodology. The model of reasoning assumes the existence of "less-more" relation between the analyzed actions, which has been expressed by means of strict partial order and some additional assumptions. The paper also contains the implementation of the analyzed example.
The paper presents a model of teleological interpretation of statutory legal rules as well as an example of the genuine law case, which has been modeled with use of established methodology.
Computational law is an approach to automated legal reasoning focusing on semantically rich laws, regulations, contract terms, and business rules in the context of electronically-mediated actions. Current computational tools for electronic commerce fall short of the demands of business, organizations, and individuals conducting complex transactions over the web. However, the growth of semantic data in the world of electronic commerce and online transactions, coupled with grounded rulesets that explicitly reference that data, provides a setting where applying automated reasoning to law can yield fruitful results, reducing inefficiencies, enabling transactions and empowering individuals with knowledge of how laws affect their behavior.
This paper proposes a knowledge elicitation method based on serious gaming for theory construction about the effects of the law on the behaviours of agents. These games provide input to simulations of business process and product design alternatives. For knowledge representation, we have combined agent role descriptions with a generic task framework. An important thesis of this paper is that, in the interest of quick and simple domain analysis, agent roles, not intelligent agents, should be the focal object of simulation of complex social organizations. At least if getting a grip on social complexity is the purpose of modeling.
This paper describes the development of an expert system demonstration prototype for identifying legal issues within the domain of employee pension plans. Pension plans are submitted by employers to the Internal Revenue Service for approval as to content, and obtain subsequent tax advantages. The prototype is able to identify and analyze legal issues, reach conclusions about a plan's conformity with the issues identified, and provide an explanation of the conclusions reached by the system. Success in the program's ability to identify and explain legal issues was determined by comparing the system's conclusions with those of an expert in the field.
The Israel-Palestinian conflict has been characterized as intractable, inextricable, and the root cause of suffering and misery for many of the people who live in the Middle East. Whilst it would be unwise to believe that the solution to this problem can be provided by negotiation support systems, we believe such systems can provide useful advice and allow disputants to more understand their goals and perform the trade-offs necessary to arrive at acceptable solutions. Given our research on interest based negotiation support systems to provide family mediation advice, we pose the question about the ability of such systems to provide useful advice about the Israel -- Palestinian dispute. We examine the differences between family mediation and international conflict resolution and reflect upon whether results from the former can provide useful advice in the latter.
We describe a system that processes court opinions and retrieves related cases from a citator database, so that new cases can be linked to earlier ones that they impact. The design of the system combines information extraction, information retrieval and machine learning techniques in a novel way. The fully implemented program is capable of performing prior case retrieval at human levels of recall and acceptable levels of precision.
Advanced Persistent Threats (APTs) are a new breed of internet based smart threats, which can go undetected with the existing state of-the-art internet traffic monitoring and protection systems. With the evolution of internet and cloud computing, a new generation of smart APT attacks has also evolved and signature based threat detection systems are proving to be futile and insufficient. One of the essential strategies in detecting APTs is to continuously monitor and analyze various features of a TCP/IP connection, such as the number of transferred packets, the total count of the bytes exchanged, the duration of the TCP/IP connections, and details of the number of packet flows. The current threat detection approaches make extensive use of machine learning algorithms that utilize statistical and behavioral knowledge of the traffic. However, the performance of these algorithms is far from satisfactory in terms of reducing false negatives and false positives simultaneously. Mostly, current algorithms focus on reducing false positives, only. This paper presents a fractal based anomaly classification mechanism, with the goal of reducing both false positives and false negatives, simultaneously. A comparison of the proposed fractal based method with a traditional Euclidean based machine learning algorithm (k-NN) shows that the proposed method significantly outperforms the traditional approach by reducing false positive and false negative rates, simultaneously, while improving the overall classification rates.
Machine learning algorithms have been proven to be vulnerable to a special type of attack in which an active adversary manipulates the training data of the algorithm in order to reach some desired goal. Although this type of attack has been proven in previous work, it has not been examined in the context of a data stream, and no work has been done to study a targeted version of the attack. Furthermore, current literature does not provide any metrics that allow a system to detect these attack while they are happening. In this work, we examine the targeted version of this attack on a Support Vector Machine(SVM) that is learning from a data stream, and examine the impact that this attack has on current metrics that are used to evaluate a models performance. We then propose a new metric for detecting these attacks, and compare its performance against current metrics.
Large scale datacenters are becoming the compute and data platform of large enterprises, but their scale makes them difficult to secure applications running within. We motivate this setting using a real world complex scenario, and propose a data-driven approach to taming this complexity. We discuss several machine learning problems that arise, in particular focusing on inducing so-called whitelist communication policies, from observing masses of communications among networked computing nodes. Briefly, a whitelist policy specifies which machine, or groups of machines, can talk to which. We present some of the challenges and opportunities, such as noisy and incomplete data, non-stationarity, lack of supervision, challenges of evaluation, and describe some of the approaches we have found promising.
Each year, thousands of software vulnerabilities are discovered and reported to the public. Unpatched known vulnerabilities are a significant security risk. It is imperative that software vendors quickly provide patches once vulnerabilities are known and users quickly install those patches as soon as they are available. However, most vulnerabilities are never actually exploited. Since writing, testing, and installing software patches can involve considerable resources, it would be desirable to prioritize the remediation of vulnerabilities that are likely to be exploited. Several published research studies have reported moderate success in applying machine learning techniques to the task of predicting whether a vulnerability will be exploited. These approaches typically use features derived from vulnerability databases (such as the summary text describing the vulnerability) or social media posts that mention the vulnerability by name. However, these prior studies share multiple methodological shortcomings that inflate predictive power of these approaches. We replicate key portions of the prior work, compare their approaches, and show how selection of training and test data critically affect the estimated performance of predictive models. The results of this study point to important methodological considerations that should be taken into account so that results reflect real-world utility.
In this paper, we compare the effectiveness of Hidden Markov Models (HMMs) with that of Profile Hidden Markov Models (PHMMs), where both are trained on sequences of API calls. We compare our results to static analysis using HMMs trained on sequences of opcodes, and show that dynamic analysis achieves significantly stronger results in many cases. Furthermore, in comparing our two dynamic analysis approaches, we find that using PHMMs consistently outperforms our technique based on HMMs.
Modern detection systems use sensor outputs available in the deployment environment to probabilistically identify attacks. These systems are trained on past or synthetic feature vectors to create a model of anomalous or normal behavior. Thereafter, run-time collected sensor outputs are compared to the model to identify attacks (or the lack of attack). While this approach to detection has been proven to be effective in many environments, it is limited to training on only features that can be reliably collected at detection time. Hence, they fail to leverage the often vast amount of ancillary information available from past forensic analysis and post-mortem data. In short, detection systems do not train (and thus do not learn from) features that are unavailable or too costly to collect at run-time. Recent work proposed an alternate model construction approach that integrates forensic "privilege" information---features reliably available at training time, but not at run-time---to improve accuracy and resilience of detection systems. In this paper, we further evaluate two of proposed techniques to model training with privileged information: knowledge transfer, and model influence. We explore the cultivation of privileged features, the efficiency of those processes and their influence on the detection accuracy. We observe that the improved integration of privileged features makes the resulting detection models more accurate. Our evaluation shows that use of privileged information leads to up to 8.2% relative decrease in detection error for fast-flux bot detection over a system with no privileged information, and 5.5% for malware classification.
Compromised smart meters reporting false power consumption data in Advanced Metering Infrastructure (AMI) may have drastic consequences on a smart grid's operations. Most existing works only deal with electricity theft from customers. However, several other types of data falsification attacks are possible, when meters are compromised by organized rivals. In this paper, we first propose a taxonomy of possible data falsification strategies such as additive, deductive, camouflage and conflict, in AMI micro-grids. Then, we devise a statistical anomaly detection technique to identify the incidence of proposed attack types, by studying their impact on the observed data. Subsequently, a trust model based on Kullback-Leibler divergence is proposed to identify compromised smart meters for additive and deductive attacks. The resultant detection rates and false alarms are minimized through a robust aggregate measure that is calculated based on the detected attack type and successfully discriminating legitimate changes from malicious ones. For conflict and camouflage attacks, a generalized linear model and Weibull function based kernel trick is used over the trust score to facilitate more accurate classification. Using real data sets collected from AMI, we investigate several trade-offs that occur between attacker's revenue and costs, as well as the margin of false data and fraction of compromised nodes. Experimental results show that our model has a high true positive detection rate, while the average false alarm rate is just 8%, for most practical attack strategies, without depending on the expensive hardware based monitoring.
With more than two million applications, Android marketplaces require automatic and scalable methods to efficiently vet apps for the absence of malicious threats. Recent techniques have successfully relied on the extraction of lightweight syntactic features suitable for machine learning classification, but despite their promising results, the very nature of such features suggest they would unlikely--on their own--be suitable for detecting obfuscated Android malware. To address this challenge, we propose DroidSieve, an Android malware classifier based on static analysis that is fast, accurate, and resilient to obfuscation. For a given app, DroidSieve first decides whether the app is malicious and, if so, classifies it as belonging to a family of related malware. DroidSieve exploits obfuscation-invariant features and artifacts introduced by obfuscation mechanisms used in malware. At the same time, these purely static features are designed for processing at scale and can be extracted quickly. For malware detection, we achieve up to 99.82% accuracy with zero false positives; for family identification of obfuscated malware, we achieve 99.26% accuracy at a fraction of the computational cost of state-of-the-art techniques.
In this paper, case study on fingerprints of twins has been carried out on an original database of twenty eight twins to understand the difficulties in discrimination. Database of 28 twins is created by collecting fingerprints from different parts of Maharashtra state, India by using optical fingerprint sensor with ultra-precise 500 DPI resolution. After creating the database, a fingerprint impression of each twin is studied in detail. The study is mainly conducted for intra class classification and salient minutiae identification. These fingerprints are compared and studied. This paper is discusses discrimination issues of them. Mat lab based software is developed for this purpose and tested for identification of twins. Mat lab based software is quiet capable of recognizing the images from database and has a % accuracy of 67.5 with False Acceptance Rate (FAR) and False Rejection Rate (FAR)of 11.5 % and 32.5 respectively. Results show that the twins having same class of fingerprints can be discriminated only at minutiae level.
Data mining techniques have been widely used in various applications. However, the misuse of these techniques may lead to the disclosure of sensitive information. Large repositories of data contain sensitive information that must be protected against unauthorized access. The protection of the confidentiality of this information has been a long-term goal for the database security research community and for the government statistical agencies. Recent advances in data mining and machine learning algorithms have increased the disclosure risks that one may encounter when releasing data to outside parties. In this paper, we present approach that modifies few transactions in transaction database to decrease support or confidence of sensitive rules. Since the correlation among rules can make it impossible to achieve this goal, we suggest method to hide the sensitive rules with the reduced number of modified entries. The method presented in this paper hides all the selected sensitive rules, limit the side effects i. e. lost rules from the original database and spurious rules (new rules) generated and measures the count of all the constraints for minimum support threshold (MST) and minimum confidence threshold (MCT) value sets.
Whenever, user has to deal with lots of files present in the system, managing and storing these files systematically is very important in order to make it easier to access them. Here comes the role of our FS Filter Driver which helps user to classify these files and check the intrusions in these files and store them in separate directories according to their file extensions. It also classifies the files which are downloaded from the Internet. Thus Driver eases the programmer's job of handling a lot of files by classifying them systematically.
Evolutionary algorithms (EAs) are always be a good choice to solve multi-objective problems (MLPs). EA use ranking schemes to solve MLP's. Most of ranking schemes use only objective values to rank the solution where as some use the condition in which the problem arises to rank the solutions. In this paper we have proposed one ranking method which use situation as well as objective values to rank the solutions. This method is currently being tested on standard single and multi objective problems, the early results are encouraging.
In this paper, a simple and efficient symbolic text classification is presented. We propose a new method of representing documents based on clustering of term frequency vectors. For each class of documents we propose to create multiple clusters to preserve the intraclass variations. Term frequency vectors of each cluster are used to form a symbolic representation by the use of interval valued features. Subsequently, a new feature selection method based on a new dissimilarity measure is also presented. The new feature selection method reduces the features in the representation phase for effective text classification. It keeps the best features for effective representation and simultaneously reduces the time taken to classify a given document. To corroborate the efficacy of the proposed model we conducted experimentation on various datasets. Experimental results reveal that the proposed method gives better results when compared to the state of the art techniques. In addition, as the method is based on a simple matching scheme, it requires a negligible time.
Traditionally reasoning systems have been implemented using symbolic methods of artificial intelligence. Connectionist methods of implementing reasoning systems form an alternative paradigm. Among the connectionist reasoning systems two types of representational methods can be used. They are i) localist and ii) distributed representational methods. In the literature, some localist methods for reasoning were used in connectionist systems. Since those systems used localist representations, advantages of distributed representations are not obtainable by them. In this paper, we describe the design and implementation of a connectionist knowledge based system which integrates a connectionist predicate logic reasoning system and a connectionist semantic network. The system uses distributed coarse-coded representations. The connectionist predicate logic system supports both simple rules as well as a complex rule having multiple conjunctions. Distributed representations have advantages of increased fault tolerance, graceful degradation of performance; neural plausibility, cognitive modeling and parallel distributed processing. The system besides showing above features allows the communication between these two connectionist systems and makes it possible to access the information of attributes and corresponding values from the connectionist semantic network for the entities used in the connectionist predicate logic system.
Image classification plays an important part in the fields of Remote sensing, Image analysis and Pattern recognition. Image classification can be done using conventional methods. But conventional methods lead to misclassification due to strictly convex boundaries. Textural features are included for better classification but are inconvenient for conventional methods. The proposed system uses textural feature based image classification using neural network. Textural features are extracted using Gray level co-occurrence matrix and artificial neural network is developed for the classification of images into different classes. Neural network is trained by supervised learning using standard back propagation algorithm for the classification of images.
This paper presents a part of research work to develop a method for automatic summarization of sets of research paper abstracts of desired subject in a specific area that may be retrieved by a digital library system or search engine in response to a user query. Many digital libraries or online services provide research papers published in journals, conferences or workshops for reference purpose to the user. Research papers contain a wealth of high quality information by specifying research objectives, research methods, evaluation of research objectives, research results and concluding remarks. However, a research paper abstract is relatively long and browsing too many of such research paper abstracts results in information overload. Therefore, it would be helpful to summarize a set of research paper abstracts to assist users in grasping the main ideas on a desired topic in a specific area.
Named Entity Recognition (NER) system has two sub-tasks, first is identification and second is classification. In first NER identifies words in texts which represent proper names like location, person-name, organization, date, time etc. and in second it classifies them in to predefined categories.
The Naso Frontal Angle (NFA) measurement is made to identify the Down syndrome in screening second trimester fetus and presented in this paper. The Mean shift analysis and canny operators are utilized for segmenting the frontal bone and the nasal bone region and the exact NFA has been estimated. It is observed from the results that the measured NFA have an average of 114.19��0.134 and 115.86��0.273 in the 13th week and 19th week of gestation respectively.
This paper presents an effective application of expert system for analysis Cardiac disorder like arrhythmia. An electrocardiogram (ECG) is a bioelectrical signal which records the heart's electrical activity verses time. It is an important diagnostic tool for assessing heart functions. The technique used is feature extraction and neural network for signal analysis. The processed signal source came from the MIT-BIH arrhythmia database which was developed for research in cardiac electro-physiology. Research in the field of mathematical science and physics is concentrating more on mathematical analysis of system comprising multiple elements that interact in complex ways. These factors gave birth to a major research trend aimed at clarifying the structure and operating principle inherent in the information processing system of human being and other animals and constructing an information processing device based on these structures and operating principle. The term "Neurocomputing" is used to refer to the information engineering aspect of this research. Artificial Neural Network (ANN) has been widely accepted tool for complex decision making problems. This is a powerful tool for medical diseases decision-making. ANN has ability to take its own decision based on its trained data. This paper concludes with a discussion in analysis cardiac disorder and future usage of artificial neural networks in the area of biomedical.
In this paper we present a method to cluster documents taking into account short contextual information within the document, further which is weighted by importance and used as input to self organizing map. A knowledge network is a knowledge map which clusters similar knowledge sources into knowledge domain. Knowledge sources taken as input and created as a conceptual map of the domain space. Like sources are clustered and are placed together on the map. This work tries to evaluate the value of Kohonen self organizing map i.e. topology-preserving map for use as an interactive textual knowledge mapping tool for categorizing set of textual knowledge sources. This work develops a method for clustering a set of documents related to foundry industry. Based on textual similarity, collection of huge documents is organized into clusters. The work provides an interactive and user-friendly system for foundry industry where collection of documents is mapped into clusters based on the context of the text. To achieve the mapping of knowledge sources to clusters, a neural network approach for clustering the documents is used. This is based on Kohenon's self-organizing maps (SOM) i.e. topology-preserving map which is an unsupervised competitive method of learning.
The basic aim of biometric identification system is to discriminate automatically between subjects in a reliable and dependable way, according to specific-target application. The randomness of iris pattern makes it one of the most reliable biometric traits. The personal identification approaches using mutialgorithmic are more promising now a day. In this paper a multialgoritmic approach for feature extraction using, a new block-sum method, which results in a compact and efficient feature vector, Haar transform and multiresolution feature extraction techniques is used. The experimental results show that this technique gives most promising results as compared to the existing approaches.
E-learning can play a very important role in facilitating teachers to share their teaching material, tools and experiences with others through the medium of Internet and Web Technologies. This may further be extended in assessing the level of understanding of the students using the conventional method of question paper based examination. There are several practical difficulties in setting up a good question paper. We address this issues in the context of a distributed question bank (DQB) created by different experts in related fields. This DQB can then be used to automatically or manually generate a question paper when needed. This paper focuses on one of the critical aspects in generating a question paper from the question bank i.e. finding the types of similarities among the questions. This is essential to ensure that the questions taken are not pertaining to overlapping concepts leading to redundancy. In this paper we explore the possibility of using the semantic web technologies in general and ontology in particular in addressing the issues in question similarity.
Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. Unsupervised learning (clustering) deals with which have not been pre classified in any way and so do not have a class attribute associated with them. The scope of applying clustering algorithm is to discover useful but unknown classes of items. Unsupervised learning is an approach of learning where instances are automatically placed into meaningful groups based on their similarity. This paper addresses fundamental concepts of unsupervised learning while it serveys recent clustering algorithm and their complexities.
An enhanced Computer Aided Clinical Decision Making System using Multiple classifier systems (MCSs) based on the combination of a set of different classifiers for classifying the breast tumor as malignant and benign has been developed and presented in this paper. The Multilayer Back Propagation Neural Network (MBPN), Radial-Basis-Function Neural Network (RBFNN), Asymmetrical Support Vector Machine (ASVM) and combined classifier with major voting method, behaviour-knowledge space method have been used to classify the tumor. The multiple features with optimal feature selection and combined classifier with behaviour-knowledge space method is found to have the accuracy 99.97%. The performance of the proposed clinical decision support system has been estimated and found that this hybrid system will provide valuable information to the physicians in clinical pathology.
In microarray gene expression data, monitor gene in different tissues & where each experiment with additional response variable such as a cancer type. Although the number of measure genes is in the thousand, it is assume that only law marker components of gene subset determine the type of a tissue. This paper proposes a new clustering method based on Minimal Spanning Tree for finding such groups of genes. By applying the proposed algorithm to gene expression data, meaning full clusters of gene are discovered. Significant genes are the subtend from each cluster & they contain useful information for sample classification. Thus a small pool of subtend genes can be used to build classification with high classification rode. The performance of this method is presented based on the predictive accuracy of K-NN rule, on colon cancer & leukemia data set.
In this paper an analysis of resonant frequency of electrically thin and thick rectangular patch antenna analysis is done using emotional back propagation network by providing inputs, which are length of the patch, width of the patch, height of the patch and relative permittivity. The analysis can be used in reducing a significant bottleneck like low computational speed in the electromagnetic method of microstrip analysis, with good accuracy. The authors are of the opinion that this method of using emotional BPN would save more time than the methods proposed till now which provide the accuracy of this level.
Knowledge can be captured and made available to both machines and humans by an ontology. Ontology can be served as a structured knowledge representation scheme, capable of assisting the construction of a personalized learning path. This paper describes the processes of conceptualization and specification, or building of, an ontology. The domain for which the ontology has been constructed is software risk identification. The required concepts, the semantic description of the concepts and the interrelationship among the concepts along with all other ontological components have been collected from various literatures and experience of the people from software industry. From which, a taxonomy has been constructed by using the property 'isA' and the design architecture for the required ontology has also been sketched out manually with nearly four different types of properties. In order to reduce implementation efforts, the Prot�g� platform, a scalable and integrated framework for ontological engineering, has been used to construct the ontology. The constructed ontology has been represented in owl format, which makes it more machine understandable. Then the semantic representation of the knowledge has been made using the OWL document generator, which automatically generates a set of documents from the ontology. In order to understand the knowledge in more detailed way again the ontology has been visualized using ontoviz tool.
In a vehicle license plate extraction system, plate region detection is the key step before the final recognition. This paper presents a license plate detection algorithm from complex background based on gradient analysis and prolonged haar wavelet transform. First the license plate region is approximately detected using gradient analysis and top hat transformation of horizontal projection by choosing appropriate threshold. Accurate detection is obtained using multi resolution feature of candidates by using prolonged haar wavelet transformation. Due to the limitation of haar wavelet, we use expanded version of it to get better location of license plate without using morphological operations. Finally, accurate vertical position of license plate is detected and license plate is extracted from image. The test images taken from various scenes were employed, including diverse angles, different lightening conditions. The experiments shows that proposed method can quickly and correctly detect the region of license plate.
In this paper, we describe a model for reasoning using forward chaining for predicate logic rules and facts with coarse-coded distributed representations for instantiated predicates in a connectionist frame work. Distributed representations are known to give advantages of good generalization, error correction and graceful degradation of performance under noise conditions. The system supports usage of complex rules which involve multiple conjunctions and disjunctions. The system supports parallel and independent execution of predicate logic rule chains in a connectionist environment. The system solves the variable binding problem in a new way using coarse-coded distributed representations of instantiated predicates. Its performance with regard to generalization on unseen inputs and its ability to exhibit fault tolerance under noise conditions is studied and has been found to give good results.
Our faces are complex objects with features that can vary over time. However, we humans have a natural ability to recognize faces and identify persons in a glance. Unfortunately, this natural ability does not exist in machines, so we need to create intelligent autonomous machines to simulate recognition system artificially. We have implemented intelligent machine for face recognition. A discriminative and robust feature---kernel enhanced informative Genetic Algorithm based Gabor feature is proposed in this paper for face recognition. We have used Genetic Algorithm approach for selection of parameter of Gabor filter. We proposed optimization function for selection of parameters of Gabor in this paper. Experiments have been conducted on face and non face images from the ORL database. The method was tested on several images from ORL database and experimental results were great.
In a Bluetooth network, transmission rate is unpredictable due to interferences by other wireless devices or general Bluetooth channel noises and presents long delay and excessive data loss, due to variations in bit rate. It is therefore almost impossible to transmit MPEG VBR video over a Bluetooth channel, without data loss, excessive time delay or image quality degradation. This paper presents an integrated Neuro-Fuzzy scheme and a Rule-Based-Fuzzy scheme applications to Moving Picture Expert Group video transmission in Bluetooth. In this work, a traffic-shaping buffer is introduced before the Host Controller Interface (HCI) of the Bluetooth protocol stack. The computer simulation results show that the application of the proposed scheme reduces excessive time delay and data loss at the HCI, as compared with a conventional video transmission in Bluetooth.
This paper presents a new approach for obtaining patterns for identification of power quality (PQ) disturbances present in electrical power systems with the use of continuous wavelet transform (CWT). A new difference coefficient matrix (DCM) is proposed, which is calculated from the difference of the CWT coefficients of the pure sinusoidal signal and the PQ disturbance signal. Then, the scale wise sums of coefficients of all the rows of DCM give unique feature matrix (UFM). This paper shows that the UFM posses unique features that can be used to generate the unique patterns of various PQ disturbances. The algorithms of the proposed approach are given together with its implementation on various cases of PQ disturbances namely sag, interruption, swell, transient, harmonics, and flicker with different magnitudes of each of the disturbances. The results show that unique pattern is obtained for each PQ disturbance irrespective of its magnitude, which can be treated as signature of the respective PQ disturbance.
Magnetic Resonance Imaging (MRI) is a non-invasive technique for assessing abnormalities of tissue composition. The objective of the paper is to develop an expert system which can diagnose Brain Tumor with the highest accuracy using artificial neural network. After extracting features from MRI data using wavelet packets, we use artificial neural networks to determine the abnormal and normal spectra. The advantage of wavelet packets is that it gives richest analysis when compared with the wavelet transforms there by adding advantages to the performance of the system. Two cancer detection Techniques have been discussed in this paper based on which the conclusion is drawn. Error Back Propagation Training Learning rule is used to train the neural network system.
Iris recognition is a method of biometric authentication which uses pattern recognition techniques. Biometrics refers to the automatic recognition of individuals based on their physiological and behavioral characteristics [1]. A behavioral characteristic is more a reflection of an individual's psychological makeup like signature; speech patterns etc. whereas a physiological characteristic is relatively stable physical characteristic like face, fingerprints, gait, palm print and iris patterns etc. variation in physical characteristics is smaller than a behavioral characteristic. In this paper, we investigate a novel method for iris recognition using one dimensional Discrete Sine Transform (DST) as a means of feature extraction for later classification. The DST of a series of averaged patches are taken from normalized iris images and a small subset of coefficients is used to form feature vectors. Classification is carried out using neural network. The feature extraction capabilities of the DST are optimized on the two largest publicly available iris image data sets, 2,156 images of 308 eyes from the CASIA database and 2,955 images of 150 eyes from the Bath database.
In this paper, we analyze and compare the performance of four different community detection algorithms, each following a different approach. The performance of the algorithms is compared on a variety of benchmark graphs with known community structure. Experiments reveal the strengths and weaknesses of the involved algorithms and demonstrate the necessity to devise local and efficient community detection techniques that perform well under a variety of changing conditions.
Post-stroke patients usually undergo therapy that focuses on general gross-motor movements such as walking, balancing, involving lower limbs; or general arm movements, leaving them with fine-dexterity and eye-hand coordination problems at their chronic stage. In this paper, we discuss the possibility to come up with assessment metrics for eye-hand coordination therapy, using a mapping between a robotic haptic device to a virtual environment and a training algorithm based on Complex Valued Neural Networks that will determine how close a determined movement pattern is in relationship with that traced by a healthy individual. Most of the current robotic systems' therapy relies on the patient's performance on standardized clinical tests such as the functional independence measure (FIM), motor power score, and the upper limb subsection of the Fugl-Meyer (FM) scales. These systems don't have other standardized metrics for assessment purposes. There is a need to establish a more intelligent and tailored therapy that could be implemented for patients to use at home in between therapy sessions, or in the long term. This therapy should be based on performance data gathered by the robotic/computer system that will provide an assessment procedure with improved objectivity and precision. This paper presents a preliminary design and simulation results of virtual environment tasks that interface with a haptic robotic device, as well as the training of a complex valued based neural network using patterns traced by healthy individuals. The idea is to use this trained algorithm, to identify in the future, how close the patients' fine motor movements are to the healthy subject's fine coordination movements; and determine a form of metrics that can be used for assessment and future therapy decisions.
Therapeutic Neurofeedback (NFB) using real-time electroencephalography (EEG) data works by reinforcing desired brainwave patterns. Although EEG is a well-established diagnostic tool and EEG-NFB shows great promise for enhancing cognitive performance and treating neurological disorders, proof of its efficacy has been limited. Here we characterize a novel Self-Calibrating Protocol (SCP) method coupled to five standard machine learning algorithms to classify brain states corresponding to the experience of "pain" or "no pain". Our results indicate that commercially available, wearable EEG sensors provide sufficient data fidelity to robustly differentiate the two "perceptually opposite" brain states. Crucially, use of SCP allows us for the first time to bypass the pitfalls associated with trying to force an individual's brain wave patterns to match "normed" target patterns obtained over population averages. These are necessary steps towards personalized NFB therapies and bespoke Brain-Computer Interfaces and brain training suitable to a wide variety of individual needs.
This paper presents an automated methodology for extracting the spatiotemporal activity model of a person using a wireless sensor network deployed inside a home. The sensor network is modeled as a source of spatiotemporal symbols whose output is triggered by the monitored person's motion over space and time. Using this stream of symbols, we formulate the problem of human activity modeling as a spatiotemporal pattern-matching problem on top of the sequence of symbolic information the sensor network produces and solve it using an exhaustive search algorithm. The effectiveness of the proposed methodology is demonstrated on a real 30-day dataset extracted from an ongoing deployment of a sensor network inside a home monitoring an elder. Our algorithm examines the person's data over these 30 days and automatically extracts the person's daily pattern.
Low birth weight is heavily correlated with health issues. Very low birth weight (VLBW) infants, with a birth weight below 1500 g, are particularly at risk, and often subject to multiple developmental problems. The Neonatal Intensive Care Unit (NICU) at Helsinki University Central Hospital has been collecting patient data in a database since 1999. We studied data collected from 2059 VLBW infants admitted between 1999 and 2013. Our aim was to study the variance of oxygen saturation measurements and compliance with guidelines as an example of using statistical means to assess quality of care from vital trend measurements. As an example of quality control, we have studied the discrepancy between automatic measurements and manual readings taken from the same sensor output.
This work assesses the performance of the Gradient Descent and Exponentiated Gradient online learning algorithms on the Wikipedia Page Traffic Statistics Dataset. The two algorithms are trained to predict future Wikipedia page traffic during a 7-month period. Predictions are a weighted combination of feature attributes, which are various measurements of page traffic change. The algorithms improve their predictions as they learn patterns from the data sequence by updating a weight distribution over the features. Cumulative loss for predictions during the 7-month sequence is used as an evaluation metric. The cumulative loss measurements demonstrate that the Exponentiated Gradient algorithm predicts future Wikipedia page traffic more accurately Gradient Descent does with the computed features. The cumulative loss measurements also suggest that the Exponentiated Gradient algorithm becomes less confused than the Gradient Descent algorithm becomes when irrelevant features are present among relevant features.
Aim of this paper is to investigate how a series of different factors are coexisting in shipping accidents. We analyzed 355 shipping accident reports from the European Maritime Safety Agency (EMSA), which are publicly available from the official EMSA website. For this purpose we used the K-means clustering method with 15 a priori defined clusters. Our results indicated that human factors often coexist with parameters related to the condition of the ship and other external factors (i.e. bad weather). Our investigation aims to contribute to the better understanding of underlying factors so that more targeted staff training, manning and shipping maintenance measures can be taken to prevent future events.
Physical exercising is an essential part of any rehabilitation plan. The subject must be committed to a daily exercising routine, as well as to a frequent contact with the therapist. Rehabilitation plans can be quite expensive and time-consuming. On the other hand, tele-rehabilitation systems can be really helpful and efficient for both subjects and therapists. In this paper, we present ReAdapt, an adaptive module for a tele-rehabilitation system that takes into consideration the progress and performance of the exercising utilizing multisensing data and adjusts the session difficulty resulting to a personalized session. Multimodal data such as speech, facial expressions and body motion are being collected during the exercising and feed the system to decide on the exercise and session difficulty. We formulate the problem as a Markov Decision Process and apply a Reinforcement Learning algorithm to train and evaluate the system on simulated data.
This paper presents a diary system which reduces the time to keep and maintain a diary by assisting the user with automatic suggestions for possible events and activities. The system uses a smartphone app reading the internal sensors to detect the current situation. It interrupts the user when unsure in prediction of the current situation. No external hardware is used to assure a broad audience of later endusers. The overall goal is to create a multimedia enriched digital personal diary. A study has been conducted where 16 participants collected data over two weeks from the built-in sensors of an ordinary smartphone. The data has been annotated by the users with labels of daily activities. We applied different classification algorithms to test the feasibility to detect these activity labels from the collected sensor data only. The experiments show that a perfect classification of activities of daily living is not possible but a system like this is usable to assist the journaling by suggestions and semi-automatic logging. The digital diary keeping process should take less time and is more versatile than an old-fashioned handwritten diary. A diary like this can be used to revive past events and help people with dementia to remember the past.
Object recognition serves obvious purposes in assisted living environments, where robotic devices can be used as companions to assist humans in need. The recent introduction of vision based sensors, which are able to extract depth sensing information about the environment, in addition to the traditional RGB video, presents new opportunities and challenges for more accurate object recognition. The current work, presents an object recognition approach that uses RGB-D point cloud data and a novel feature extraction methodology, in combination with well-known supervised learning algorithms, to achieve accurate, real-time recognition of a large number of objects. In our experiments, we use a dataset of household objects organized into 51 categories, and evaluate the recognition accuracy and time efficiency of a set of different supervised learning methods.
This paper presents a promising approach to enhance multi-sensor based activity recognition in smart homes. The research is originated in the domain of Active and Assisted Living which mainly is about supporting older people to master their daily life activities. The paper proposes (a) a windowing technique which can be used for online sensor streaming and (b) a set of different statistical spatio-temporal features to recognize activities in real time. In order to check the overall performance, this approach was tested using the CASAS dataset. The results proved a high performance based on different evaluation metrics despite a large number of classes.
Obesity is a worldwide epidemic and is an underlying cause for most major chronic diseases. In majority of cases the underlying cause for obesity is a greatly skewed imbalance between the food intake and the number of calories burnt by the patient. One of the first steps in managing obesity is the correct recording of food and fluids that are ingested in the body. Traditional methods like food diaries have generally produced grossly inaccurate results. In order to automate the process of capturing ingestion, a method for detecting, analyzing, and recording sounds related to ingestion is being developed. In this paper, preliminary swallow sound analysis is presented with the intention of implementing automated ingestion detection as part of an obesity and overweight management system. Three basic algorithmic approaches are discussed as well as filtering options. More complex methods for analysis are explored as well, which include nonlinear analysis and the use of Self Organizing Maps (SOM).
In this paper, we present an Adaptive Multimodal Dialogue System for Depressive and Anxiety Disorders Screening (DADS). The system interacts with the user through verbal and non-verbal communication to elicit the information needed to make referrals and recommendations for depressive and anxiety disorders while encouraging the user and keeping them calm. We designed the problem using interconnected Markov Decision Processes using sub-goals to deal with the large state space. We present the problem formulation and the experimental procedure for the training data collection and the system training following the methodology of Wizard-of-Oz experiments.
Advances in electroencephalography (EEG) sensors and embedded signal processing modules are driving interest in wearable EEG devices. EEG based Neurofeedback (NFB) works by reinforcing desired brainwave patterns and shows great promise for enhancing performance and treating mental disorders. Yet, both clinical and at-home efficacies remain low. We propose a novel Self-Calibrating Protocol (SCP) which can robustly classify brain states when coupled with five standard machine learning algorithms. Our results indicate that commercially available, wearable EEG sensors provide sufficient data fidelity at fast enough rates to differentiate between arbitrary, user-defined states in real time. We conclude that SCP allows for the first time to completely bypass the pitfalls of using "normed" neurophysiological states and leads off to individualized therapeutics and bespoke performance enhancement applications.
Activity recognition is the most challenging stage of technological assistance which offers automatic support, when needed, to elderly and disabled people such as Alzheimer's patients living in smart homes. Many approaches and techniques were proposed for activity recognition while other technological assistance stages were barely explored. In this paper, after presenting our activity recognition approach and explaining how the artificial agent will use it to decide when to intervene offering help, we use time series forecasting in order to better choose the intervention time.
A major challenge in today's world is the Big Data problem, which manifests itself in Web and Mobile domains as rapidly changing and heterogeneous data streams. A data-mining system must be able to cope with the influx of changing data in a continual manner. This calls for Lifelong Machine Learning, which in contrast to the traditional one-shot learning, should be able to identify the learning tasks at hand and adapt to the learning problems in a sustainable manner. A foundation for lifelong machine learning is transfer learning, whereby knowledge gained in a related but different domain may be transferred to benefit learning for a current task. To make effective transfer learning, it is important to maintain a continual and sustainable channel in the life time of a user in which the data are annotated. In this talk, I outline the lifelong machine learning situations, give several examples of transfer learning and applications for lifelong machine learning, and discuss cases of successful extraction of data annotations to meet the Big Data challenge.
Many social network applications face the following problem: given a network G=(V,E) with labels on a small subset O \subset V of nodes and an optional set of features on nodes and edges, predict the labels of the remaining nodes. Much research has gone into designing learning models and inference algorithms for accurate predictions in this setting. However, a core hurdle to any prediction effort is that for many nodes there is insufficient evidence for inferring a label. We propose that instead of focusing on the impossible task of providing high accuracy over all nodes, we should focus on selectively making the few node predictions which will be correct with high probability. Any selective prediction strategy will require that the scores attached to node predictions be well-calibrated. Our evaluations show that existing prediction algorithms are poorly calibrated. We propose a new method of training a graphical model using a conditional likelihood objective that provides better calibration than the existing joint likelihood objective. We augment it with a decoupled confidence model created using a novel unbiased training process. Empirical evaluation on two large social networks show that we are able to select a large number of predictions with accuracy as high as 95%, even when the best overall accuracy is only 40%.
Transfer learning focuses on the learning scenarios when the test data from target domains and the training data from source domains are drawn from similar but different data distribution with respect to the raw features. Some recent studies argued that the high-level concepts (e.g. word clusters) can help model the data distribution difference, and thus are more appropriate for classification. Specifically, these methods assume that all the data domains have the same set of shared concepts, which are used as the bridge for knowledge transfer. However, besides these shared concepts each domain may have its own distinct concepts. To address this point, we propose a general transfer learning framework based on non-negative matrix tri-factorization which allows to explore both shared and distinct concepts among all the domains simultaneously. Since this model provides more flexibility in fitting the data it may lead to better classification accuracy. To solve the proposed optimization problem we develop an iterative algorithm and also theoretically analyze its convergence. Finally, extensive experiments show the significant superiority of our model over the baseline methods. In particular, we show that our method works much better in the more challenging tasks when distinct concepts may exist.
Extreme multi-label classification refers to supervised multi-label learning involving hundreds of thousands or even millions of labels. Datasets in extreme classification exhibit fit to power-law distribution, i.e. a large fraction of labels have very few positive instances in the data distribution. Most state-of-the-art approaches for extreme multi-label classification attempt to capture correlation among labels by embedding the label matrix to a low-dimensional linear sub-space. However, in the presence of power-law distributed extremely large and diverse label spaces, structural assumptions such as low rank can be easily violated. In this work, we present DiSMEC, which is a large-scale distributed framework for learning one-versus-rest linear classifiers coupled with explicit capacity control to control model size. Unlike most state-of-the-art methods, DiSMEC does not make any low rank assumptions on the label matrix. Using double layer of parallelization, DiSMEC can learn classifiers for datasets consisting hundreds of thousands labels within few hours. The explicit capacity control mechanism filters out spurious parameters which keep the model compact in size, without losing prediction accuracy. We conduct extensive empirical evaluation on publicly available real-world datasets consisting upto 670,000 labels. We compare DiSMEC with recent state-of-the-art approaches, including - SLEEC which is a leading approach for learning sparse local embeddings, and FastXML which is a tree-based approach optimizing ranking based loss function. On some of the datasets, DiSMEC can significantly boost prediction accuracies - 10% better compared to SLECC and 15% better compared to FastXML, in absolute terms.
The SDA workshop at WSDM 2015 is the fifth International Workshop on Scalable Data Analytics, following the previous four workshops of SDA respectively held at IEEE Big Data 2013, PAKDD 2014, IEEE Big Data 2014, and IEEE ICDM 2014. This series of workshops aims to provide professionals, researchers, and technologists with a single forum where they can discuss and share the state-of-the-art theories and applications of scalable data analytics technologies. In particular, in the era of information explosion, the scientific, biomedical, and engineering research communities are undergoing a profound transformation where discoveries and innovations increasingly rely on massive amounts of data. The characteristics of volume, velocity, variety and veracity originated in the massive big data then bring challenges to current data analytics techniques. The focus of the fifth SDA is to discuss how we can scale up data analytics techniques for modeling and analyzing big data from various domains.
Online groups, including chat groups and forums, are becoming important avenues for gathering and exchanging information ranging from troubleshooting devices, to sharing experiences, to finding medical information and advice. Thus, issues about the health and stability of these groups are of particular interest to both industry and academia. In this paper we conduct a large scale study with the objectives of first, characterizing essential aspects of the interactions between the participants of such groups and second, characterizing how the nature of these interactions relate to the health of the groups. Specifically, we concentrate on Twitter Discussion Groups (TDGs), self-organized groups that meet on Twitter by agreeing on a hashtag, date and time. These groups have repeated, real-time meetings and are a rising phenomenon on Twitter. We examine the interactions in these groups in terms of the social equality and mobility of the exchange of attention between participants, according to the @mention convention on Twitter. We estimate the health of a group by measuring the retention rate of participants and the change in the number of meetings over time. We find that social equality and mobility are correlated, and that equality and mobility are related to a group's health. In fact, equality and mobility are as predictive of a group's health as some prior characteristics used to predict health of other online groups. Our findings are based on studying 100 thousand sessions of over two thousand discussion groups over the period of June 2012 to June 2013. These finding are not only relevant to stakeholders interested in maintaining these groups, but to researchers and academics interested in understanding the behavior of participants in online discussions. We also find the parallel with findings on the relationship between economic mobility and equality and health indicators in real-world nations striking and thought-provoking.
Given a database with missing or uncertain content, our goal is to correct and fill the database by extracting specific information from a large corpus such as the Web, and to do so under resource limitations. We formulate the information gathering task as a series of choices among alternative, resource-consuming actions and use reinforcement learning to select the best action at each time step. We use temporal difference q-learning method to train the function that selects these actions, and compare it to an online, error-driven algorithm called SampleRank. We present a system that finds information such as email, job title and department affiliation for the faculty at our university, and show that the learning-based approach accomplishes this task efficiently under a limited action budget. Our evaluations show that we can obtain 92.4% of the final F1, by only using 14.3% of all possible actions.
Computational advertising refers to finding the most relevant ads matching a particular context on the web. The core problem attacked in computational advertising CA is of the match making between the ads and the context. My research work aims at leveraging various user interaction, ad and advertiser related information and contextual information for improving the relevance, ranking and targeting of ads. The research work focuses on the identification of various factors that contribute in retrieving and ranking the most relevant set of ads that match best with the context. Specifically, information associated with the user, publisher and advertiser is leveraged for this purpose.
Many factorization models like matrix or tensor factorization have been proposed for the important application of recommender systems. The success of such factorization models depends largely on the choice of good values for the regularization parameters. Without a careful selection they result in poor prediction quality as they either underfit or overfit the data. Regularization values are typically determined by an expensive search that requires learning the model parameters several times: once for each tuple of candidate values for the regularization parameters. In this paper, we present a new method that adapts the regularization automatically while training the model parameters. To achieve this, we optimize simultaneously for two criteria: (1) as usual the model parameters for the regularized objective and (2) the regularization of future parameter updates for the best predictive quality on a validation set. We develop this for the generic model class of Factorization Machines which subsumes a wide variety of factorization models. We show empirically, that the advantages of our adaptive regularization method compared to expensive hyperparameter search do not come to the price of worse predictive quality. In total with our method, learning regularization parameters is as easy as learning model parameters and thus there is no need for any time-consuming search of regularization values because they are found on-the-fly. This makes our method highly attractive for practical use.
Software applications often use classification models to trigger specialized experiences for users. Search engines, for example, use query classifiers to trigger specialized "instant answer" experiences where information satisfying the user query is shown directly on the result page, and email applications use classification models to automatically move messages to a spam folder. When such applications have acceptable default (i.e., non-specialized) behavior, users are often more sensitive to failures in model precision than failures in model recall. In this paper, we consider model-selection algorithms for these precision-constrained scenarios. We develop adaptive model-selection algorithms to identify, using as few samples as possible, the best classifier from among a set of (precision) qualifying classifiers. We provide statistical correctness and sample complexity guarantees for our algorithms. We show with an empirical validation that our algorithms work well in practice.
This paper proposes a novel method for transductive classification on heterogeneous information networks composed of multiple types of vertices. Such networks naturally represent many real-world Web data such as DBLP data (author, paper, and conference). Given a network where some vertices are labeled, the classifier aims to predict labels for the remaining vertices by propagating the labels to the entire network. In the label propagation process, many studies reduce the importance of edges connecting to a high-degree vertex. The assumption is unsatisfactory when reliability of a label of a vertex cannot be implied from its degree. On the basis of our intuition that edges bridging across communities are less trustworthy, we adapt edge betweenness to imply the importance of edges. Since directly applying the conventional edge betweenness is inefficient on heterogeneous networks, we propose two additional refinements. First, the centrality utilizes the fact that networks contain multiple types of vertices. Second, the centrality ignores flows originating from endpoints of considering edges. The experimental results on real-world datasets show our proposed method is more effective than a state-of-the-art method, GNetMine. On average, our method yields 92.79 � 1.25% accuracy on a DBLP network even if only 1.92% of vertices are labeled. Our simple weighting scheme results in more than 5 percentage points increase in accuracy compared with GNetMine.
In recent years, deep learning has been a very hot topic in the machine learning community. It has brought break-through results in image classification and speech recognition. Most recently, researchers have also got many promising results in natural language processing using deep learning techniques. As machine learning techniques are widely used in the Web search and data mining applications, many researchers and practitioners are studying the possibility of applying the recently-developed deep learning techniques into these applications. Some of them have made very promising progress, and thus it is a good time to hold a workshop to discuss and share the problems and progress in using deep learning techniques to improve Web search and data mining tasks.
In this article we consider the problem of mapping a noisy estimate of a user's current location to a semantically meaningful point of interest, such as a home, restaurant, or store. Despite the poor accuracy of GPS on current mobile devices and the relatively high density of places in urban areas, it is possible to predict a user's location with considerable precision by explicitly modeling both places and users and by combining a variety of signals about a user's current context. Places are often simply modeled as a single latitude and longitude when in fact they are complex entities existing in both space and time and shaped by the millions of people that interact with them. Similarly, models of users reveal complex but predictable patterns of mobility that can be exploited for this task. We propose a novel spatial search algorithm that infers a user's location by combining aggregate signals mined from billions of foursquare check-ins with real-time contextual information. We evaluate a variety of techniques and demonstrate that machine learning algorithms for ranking and spatiotemporal models of places and users offer significant improvement over common methods for location search based on distance and popularity.