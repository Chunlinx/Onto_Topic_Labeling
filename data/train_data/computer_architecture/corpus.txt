Modern massively multiplayer online games (MMOGs) allow hundreds of thousands of players to interact with a large, dynamic virtual world. Implementing a scalable MMOG service is challenging because the system is subject to high workload variability, but nevertheless must always operate under very strict quality of service (QoS) requirements. Traditionally, MMOG services are implemented as large dedicated IT infrastructures with aggressive over-provisioning of resources in order to cope with the worst-case workload scenario. In this article we address the problem of building a large-scale, multitier MMOG service using resources provided by a Cloud computing infrastructure. The Cloud paradigm allows customers to request as many resources as they need using a pay-as-you-go model. We harness this paradigm by proposing a dynamic provisioning algorithm, which can resize the resource pool of a MMOG service to adapt to workload variability and maintain a response time below a given threshold. We use a queuing network performance model to quickly estimate the system response time for different configurations. The performance model is used within a greedy algorithm to compute the minimum number of servers to be allocated on each tier in order to satisfy the system response time constraint. Numerical experiments are used to validate the effectiveness of the proposed approach.
This article focuses on the introduction of control, authoring, and composition in human-computer music improvisation through the description of a guided music generation model and a reactive architecture, both implemented in the software ImproteK. This interactive music system is used with expert improvisers in work sessions and performances of idiomatic and pulsed music and more broadly in situations of structured or composed improvisation. The article deals with the integration of temporal specifications in the music generation process by means of a fixed or dynamic �scenario� and addresses the issue of the dialectic between reactivity and planning in interactive music improvisation. It covers the different levels involved in machine improvisation: the integration of anticipation relative to a predefined structure in a guided generation process at a symbolic level, an architecture combining this anticipation with reactivity using mixed static/dynamic scheduling techniques, and an audio rendering module performing live re-injection of captured material in synchrony with a non-metronomic beat. Finally, it sketches a framework to compose improvisation sessions at the scenario level, extending the initial musical scope of the system. All of these points are illustrated by videos of performances or work sessions with musicians.
The MPEG-4 standards define a technique for 3D facial and body model animations (FAPS/BAPS respectively), as seen in animation systems such as Greta. The way this technique works is in contrast to the set of animation techniques currently used within modern games technologies and applications, which utilize more advanced, expressive animation systems such as Skeletal, Morph Target, and Inverse Kinematics. This article describes an object-oriented, Java-based framework for the integration and transformation of MPEG4 standards-compliant animation streams known as Charisma. Charisma is designed for use with modern games animation systems; this article illustrates the application of this framework on top of our Java/OpenGL-based games engine framework known as Homura.
In massively multiplayer online games (MMOGs) there is a great demand for high bandwidth connections with irregular access patterns. Such irregular demand is because players, who can vary from a few hundred to several tens of thousands, often occupy the virtual environment of the game in different ways with varying densities. Hence there is a great need for decentralized architectures with multiple servers that employ load-balancing algorithms to manage regions of the virtual environment. In such systems, each player only connects to the server that manages the region where the player's avatar is located, whereas each server is responsible for mediating the interaction between all pairs of players connected to it. Devising the proper load-balancing algorithm so that it takes spatial and variable occupations into account is a challenging problem which requires adaptive (and possibly dynamic) partitioning of the virtual environment. In this work, we propose the use of a kd-tree for partitioning the game environment into regions, and dynamically adjust the resulting subdivisions based on the distribution of avatars in the virtual environment. We compared our algorithm to competing approaches found in the literature and demonstrated that our algorithm performed better in most aspects we analyzed.
This article presents a peer-to-peer overlay for massively multiplayer online games with a focus on fast-paced action. More than other genres, action games like first-person shooters employ fast and dynamic game mechanics. In multiplayer environments, these properties have to be reflected by the underlying network structure. At the same time, the system should be able to support a huge amount of users in order to deliver a massive experience to the participating players. The capacity of current client/server systems limits the number of players in a game, preventing the desired massive experience. To provide both a scalable and a responsive system, we use a fully distributed peer-to-peer network with a dynamic connection scheme. By exploiting local interests in the virtual world, our system supports a huge number of users. Therefore, an area-of-interest mechanism is applied to the connection scheme. Users do not connect to all participating users, but they only establish connections to other users they are interested in. These neighbors are determined by the user's perception of the virtual world. Instead of using a purely distance-based approach, our system uses a more flexible neighbor-based approach that supports the use of multiple metrics to determine the set of interesting nodes for each user. A second kind of connection�so-called NetConnectors�utilizes the players' distribution in the virtual world to ensure overlay consistency. For the dissemination of messages, we use a publish/subscribe mechanism. This prevents inconsistencies introduced by unidirectional neighborhood relations that can occur with sender-oriented models. Further, the publish/subscribe mechanism models the users' interests more accurately. In addition to the regular sending mechanism, we implemented a Geocast algorithm that allows information distribution to arbitrary regions of the virtual world. While regular messages are always addressed to specific users, Geocasts cover certain geographical regions. Thus, Geocasts can be used to disseminate messages to all users that are located in the addressed region. Simulations show that our design performs well in terms of scalability. By keeping the amount of connections per user nearly constant, users do not get overloaded with too many connections. This also applies for crowded regions where the user density is much higher compared to an evenly populated virtual world. Another important aspect of fast-paced multiplayer games is the users' motion behavior. Different movement strategies are evaluated for their impact on network load and connection dynamics.
Digital continuous media (CM) are now well established as an integral part of many applications. With highdefinition (HD) displays becoming increasingly common and large network bandwidth available, high-quality video streaming has become feasible, and novel, innovative applications possible. However, the majority of existing systems for HD-quality streaming are based on offline content and use elaborate buffering techniques that introduce long latencies. Therefore, these solutions are ill-equipped for interactive real-time applications. Also, due to the massive amount of data required to transmit such streams, simultaneously achieving low latency and keeping the bandwidth low are contradictory requirements. Our HYDRA project (Highperformance Data Recording Architecture) focuses on the acquisition, transmission, storage, and rendering of high-resolution media such as H-quality video and multiple channels of audio. HYDRA consists of multiple components to achieve its overall functionality. Here we elaborate on the live-streaming capabilities of HYDRA that enable media streaming across an IP-based network with commodity equipment.
The semantics of most logics of time and probability is given via a probability distribution over threads, where a thread is a structure specifying what will be true at different points in time (in the future). When assessing the probabilities of statements such as �Event a will occur within 5 units of time of event b,� there are many different semantics possible, even when assessing the truth of this statement within a single thread. We introduce the syntax of annotated probabilistic temporal (APT) logic programs and axiomatically introduce the key notion of a frequency function (for the first time) to capture different types of intrathread reasoning, and then provide a semantics for intrathread and interthread reasoning in APT logic programs parameterized by such frequency functions. We develop a comprehensive set of complexity results for consistency checking and entailment in APT logic programs, together with sound and complete algorithms to check consistency and entailment. The basic algorithms use linear programming, but we then show how to substantially and correctly reduce the sizes of these linear programs to yield better computational properties. We describe a real world application we are developing using APT logic programs.
An unprecedented rate of growth in the number of vehicles has resulted in acute road congestion problems worldwide, especially in many developing countries. In this article, we present Road-RFSense, a practical RF sensing--based road traffic estimation system for developing regions. Our first contribution is a new mechanism to sense road occupancy, based on variation in RF link characteristics, when line of sight between a transmitter-receiver pair is obstructed. We design algorithms to classify traffic states into two classes, free-flow versus congested, at timescales of 20 seconds with greater than 90% accuracy. We also present a traffic queue length measurement system, where a network of RF sensors can correlate the traffic state classification decisions of individual sensors and detect traffic queue length in real time. Deployment of our system on a Mumbai road gives correct estimates, validated against 9 hours of image-based ground truth. Our third contribution is a large-scale data-driven study, in collaboration with city traffic authorities, to answer questions regarding road-specific classification model training. Finally, we explore multilevel classification into seven different traffic states using a larger set of RF-based features and careful choice of classification algorithms.
Over recent years, the continuous interest in wireless sensor networks (WSNs) has led to the appearance of new modeling methods and simulation environments for WSN applications. A broad variety of different simulation tools have been designed to explore and validate WSN systems before actual implementation and real-world deployment. These tools address different design aspects and offer various simulation abstractions to represent and model real-world behavior. In this article, we present a comprehensive comparative study of mainstream open-source simulation tools for WSNs. Two benchmark applications are designed to evaluate the frameworks with respect to the simulation runtime performance, network throughput, communication medium modeling, packet reception rate, network latency, and power consumption estimation accuracy. Such metrics are also evaluated against measurements on physical prototypes. Our experiments show that the tools produce equivalent results from a functional point of view and capacity to model communication phenomena, while the ability to model details of the execution platform significantly impacts the runtime simulation performance and the power estimation accuracy. The benchmark applications are also made available in the public domain for further studies.
GPS is a commonly used and convenient technology for determining absolute position in outdoor environments, but its high power consumption leads to rapid battery depletion in mobile devices. An obvious solution is to duty cycle the GPS module, which prolongs the device lifetime at the cost of increased position uncertainty while the GPS is off. This article addresses the trade-off between energy consumption and localization performance in a mobile sensor network application. The focus is on augmenting GPS location with more energy-efficient location sensors to bound position estimate uncertainty while GPS is off. Empirical GPS and radio contact data from a large-scale animal tracking deployment is used to model node mobility, radio performance, and GPS. Because GPS takes a considerable, and variable, time after powering up before it delivers a good position measurement, we model the GPS behavior through empirical measurements of two GPS modules. These models are then used to explore duty cycling strategies for maintaining position uncertainty within specified bounds. We then explore the benefits of using short-range radio contact logging alongside GPS as an energy-inexpensive means of lowering uncertainty while the GPS is off, and we propose strategies that use RSSI ranging and GPS back-offs to further reduce energy consumption. Results show that our combined strategies can cut node energy consumption by one third while still meeting application-specific positioning criteria.
The increasing adoption of wireless sensor network technology in a variety of applications, from agricultural to volcanic monitoring, has demonstrated their ability to gather data with unprecedented sensing capabilities and deliver it to a remote user. However, a key issue remains how to maintain these sensor network deployments over increasingly prolonged deployments. In this article, we present the challenges that were faced in maintaining continual operation of an automated wildlife monitoring system over a one-year period. This system analyzed the social colocation patterns of European badgers (Meles meles) residing in a dense woodland environment using a hybrid RFID-WSN approach. We describe the stages of the evolutionary development, from implementation, deployment, and testing, to various iterations of software optimization, followed by hardware enhancements, which in turn triggered the need for further software optimization. We highlight the main lessons learned: the need to factor in the maintenance costs while designing the system; to consider carefully software and hardware interactions; the importance of rapid prototyping for initial deployment (this was key to our success); and the need for continuous interaction with domain scientists which allows for unexpected optimizations.
In this article, we describe a neighbour disjoint multipath (NDM) scheme that is shown to be more resilient amidst node or link failures compared to the two well-known node disjoint and edge disjoint multipath techniques. A centralised NDM was first conceptualised in our initial published work utilising the spatial diversity among multiple paths to ensure robustness against localised poor channel quality or node failures. Here, we further introduce a distributed version of our NDM algorithm adapting to the low-power and lossy network (LLN) characteristics. We implement our distributed NDM algorithm in Contiki OS on top of LOADng�a lightweight On-demand Ad hoc Distance Vector Routing protocol. We compare this implementation's performance with a standard IPv6 Routing Protocol for Low power and Lossy Networks (RPL), and also with basic LOADng, running in the Cooja simulator. Standard performance metrics such as packet delivery ratio, end-to-end latency, overhead and average routing table size are identified for the comparison. The results and observations are provided considering a few different application traffic patterns, which serve to quantify the improvements in robustness arising from NDM. The results are confirmed by experiments using a public sensor network testbed with over 100 nodes.
Position information plays a pivotal role in wireless sensor network (WSN) applications and protocol/algorithm design. In recent years, range-free localization algorithms have drawn much research attention due to their low cost and applicability to large-scale WSNs. However, the application of range-free localization algorithms is restricted because of their dramatic accuracy degradation in practical anisotropic WSNs, which is mainly caused by large error of distance estimation. Distance estimation in the existing range-free algorithms usually relies on a unified per hop length (PHL) metric between nodes. But the PHL between different nodes might be greatly different in anisotropic WSNs, resulting in large error in distance estimation. We find that, although the PHL between different nodes might be greatly different, it exhibits significant locality; that is, nearby nodes share a similar PHL to anchors that know their positions in advance. Based on the locality of the PHL, a novel distance estimation approach is proposed in this article. Theoretical analyses show that the error of distance estimation in the proposed approach is only one-fourth of that in the state-of-the-art pattern-driven scheme (PDS). An anchor selection algorithm is also devised to further improve localization accuracy by mitigating the negative effects from the anchors that are poorly distributed in geometry. By combining the locality-based distance estimation and the anchor selection, a range-free localization algorithm named <underline>S</underline>elective <underline>M</underline>ultilateration (SM) is proposed. Simulation results demonstrate that SM achieves localization accuracy higher than 0.3r, where r is the communication radius of nodes. Compared to the state-of-the-art solution, SM improves the distance estimation accuracy by up to 57% and improves localization accuracy by up to 52% consequently.
Existing mechanisms for querying wireless sensor networks leak client interests to the servers performing the queries. The leaks are not only in terms of specific regions of interest but also of client access patterns. In this article we introduce the problem of preserving the privacy of clients querying a wireless sensor network owned by untrusted organizations. We first propose an efficient protocol, SPYC, that ensures full client privacy in settings where the servers providing access to the network are honest-but-curious and whose collaboration does not extend beyond well-defined administrative purposes. Furthermore, we study the same query privacy problem in a setting where servers exhibit malicious behavior or where powerful external attackers have access to sensor network traffic information. In this setting we propose two metrics for quantifying the privacy achieved by a client's query sequence. We then extend SPYC with a suite of practical algorithms, then analyze the privacy and efficiency levels they provide. Our TOSSIM simulations show that the proposed extensions are communication efficient while significantly improving client privacy levels.
This article is concerned with a new distributed state estimation problem for a class of dynamical systems in sensor networks. The target plant is described by a set of differential equations disturbed by a Brownian motion and randomly occurring nonlinearities (RONs) subject to time delays. The RONs are investigated here to reflect network-induced randomly occurring regulation of the delayed states on the current ones. Through available measurement output transmitted from the sensors, a distributed state estimator is designed to estimate the states of the target system, where each sensor can communicate with the neighboring sensors according to the given topology by means of a directed graph. The state estimation is carried out in a distributed way and is therefore applicable to online application. By resorting to the Lyapunov functional combined with stochastic analysis techniques, several delay-dependent criteria are established that not only ensure the estimation error to be globally asymptotically stable in the mean square, but also guarantee the existence of the desired estimator gains that can then be explicitly expressed when certain matrix inequalities are solved. A numerical example is given to verify the designed distributed state estimators.
A problem is introduced in which a moving body (robot, human, animal, vehicle, and so on) travels among obstacles and binary detection beams that connect between obstacles or barriers. Each beam can be viewed as a virtual sensor that may have many possible alternative implementations. The task is to determine the possible body paths based only on sensor observations that each simply report that a beam crossing occurred. This is a basic filtering problem encountered in many settings, under a variety of sensing modalities. Filtering methods are presented that reconstruct the set of possible paths at three levels of resolution: (1) the possible sequences of regions (bounded by beams and obstacles) visited, (2) equivalence classes of homo-topic paths, and (3) the possible numbers of times the path winds around obstacles. In the simplest case, all beams are disjoint, distinguishable, and directed. More complex cases are then considered, allowing for any amount of beams overlapping, indistinguishability, and lack of directional information. The method was implemented in simulation. An inexpensive, low-energy, easily deployable architecture was also created which implements the beam model and validates the methods of the article with experiments.
We present a formalisation of a category of schemes that we refer to as broadcast-enhanced key predistribution schemes (BEKPSs). These schemes are suitable for networks with access to a trusted base station and an authenticated broadcast channel. We demonstrate that the access to these extra resources allows for the creation of BEKPSs with advantages over key predistribution schemes such as flexibility and more efficient revocation. There are many possible ways to implement BEKPSs, and we propose a framework for describing and analysing them. In their paper �From Key Predistribution to Key Redistribution,� Cicho? et al. [2010] propose a scheme for �redistributing� keys to a wireless sensor network using a broadcast channel after an initial key predistribution. We classify this as a BEKPS and analyse it in that context. We provide simpler proofs of some results from their paper, give a precise analysis of the resilience of their scheme, and discuss possible modifications. We then study two scenarios where BEKPSs may be particularly desirable and propose a suitable family of BEKPSs for each case. We demonstrate that they are practical and efficient to implement, and our analysis shows their effectiveness in achieving suitable trade-offs between the conflicting priorities in resource-constrained networks.
A sensor network is a distributed system where sensor nodes autonomously collect local data and collaborate to solve global problems. Recent work has shown that sensor functionality varies with node temperature. Extreme temperatures can decrease node/network lifetime by leading to premature hardware failure and reducing battery capacity. Furthermore, high temperatures can increase sensor measurement noise and disrupt communication between overheated sensor nodes, thereby interfering with their ability to contribute valuable information to collaborative tasks. In the past, sensor networks only consisted of low-end devices with limited power, computational capabilities, and available bandwidth. Such devices would only experience high temperatures in harsh environments. However, sensor networks are now envisioned for applications that require higher-end devices, such as smart cameras, smart phones, and laptops. The power dissipated by such devices is much larger than low-end sensors and can create thermal emergencies in sensor hardware even in calm environments. In this article, we present unique management opportunities for distributed estimation tasks in sensor networks consisting of high-end devices prone to thermal issues. We attempt to balance both thermal- and performance-related constraints by examining trade-offs between sensor sampling rate, number of sensors, node temperature, and state estimation error. Initially, we devise a scheduling algorithm which can achieve a desired real-time performance constraint while maintaining a thermal limit on temperature assuming identical nodes in the network. Then, we extend the concept to a network consisting of heterogeneous sensor nodes. Analytical results and simulation experiments are done for state estimation with a Kalman filter for simplicity, but our main contributions should easily extend to any form of estimation with measurable error. Results show that our policies can successfully balance the trade-offs between thermal- and performance-related constraints. Note that our analyses, schemes, and results are less applicable to low-end sensors whose operation does not cause high node temperature. This work is most suited for high-performance sensors and upper-tier sensors which experience greater workloads.
Data centers have become a critical computing infrastructure in the era of cloud computing. Temperature monitoring and forecasting are essential for preventing server shutdowns because of overheating and improving a data center�s energy efficiency. This article presents a novel cyber-physical approach for temperature forecasting in data centers, one that integrates Computational Fluid Dynamics (CFD) modeling, in situ wireless sensing, and real-time data-driven prediction. To ensure forecasting fidelity, we leverage the realistic physical thermodynamic models of CFD to generate transient temperature distribution and calibrate it using sensor feedback. Both simulated temperature distribution and sensor measurements are then used to train a real-time prediction algorithm. As a result, our approach reduces not only the computational complexity of online temperature modeling and prediction, but also the number of deployed sensors, which enables a portable, noninvasive thermal monitoring solution that does not rely on the infrastructure of a monitored data center. We extensively evaluated the proposed system on a rack of 15 servers and a testbed of five racks and 229 servers in a small-scale production data center. Our results show that our system can predict the temperature evolution of servers with highly dynamic workloads at an average error of 0.52?C, within a duration up to 10 minutes. Moreover, our approach can reduce the required number of sensors by 67% while maintaining desirable prediction fidelity.
We address the energy-efficient data redistribution problem in data-intensive sensor networks (DISNs). In a DISN, a large volume of data gets generated, which is first stored in the network and is later collected for further analysis when the next uploading opportunity arises. The key concern in DISNs is to be able to redistribute the data from data-generating nodes into the network under limited storage and energy constraints at the sensor nodes. We formulate the data redistribution problem where the objective is to minimize the total energy consumption during this process while guaranteeing full utilization of the distributed storage capacity in the DISNs. We show that the problem is APX-hard for arbitrary data sizes; therefore, a polynomial time approximation algorithm is unlikely. For unit data sizes, we show that the problem is equivalent to the minimum cost flow problem, which can be solved optimally. However, the optimal solution's centralized nature makes it unsuitable for large-scale distributed sensor networks. Thus, we design a distributed algorithm for the data redistribution problem which performs very close to the optimal, and compare its performance with various intuitive heuristics. The distributed algorithm relies on potential function-based computations, incurs limited message and computational overhead at both the sensor nodes and data generator nodes, and is easily implementable in a distributed manner. We analytically study the convergence and performance of the proposed algorithm and demonstrate its near-optimal performance and scalability under various network scenarios. In addition, we implement the distributed algorithm in TinyOS, evaluate it using TOSSIM simulator, and show that it outperforms EnviroStore, the only existing scheme for data redistribution in sensor networks, in both solution quality and message overhead. Finally, we extend the proposed algorithm to avoid disproportionate energy consumption at different sensor nodes without compromising the solution quality.
Most existing work on coverage, connectivity, and geographic forwarding considers a two-dimensional (2D) space, where the sensors are deployed in a 2D field. However, there are several cases where the 2D assumption is not valid for the design of those types of wireless sensor networks (WSNs), such as underwater sensor deployment and sensors deployed on the trees of different heights in a forest. In this article, we investigate the problem of k-coverage in three-dimensional (3D) WSNs, where each point in a 3D field is covered by at least k sensors simultaneously. Moreover, it is commonly assumed in most of the work on the problem of geographic forwarding in WSNs that all the sensors are always on (or active) during the network operational lifetime, and, particularly, during data forwarding. However, this type of design is neither practical nor efficient for the sensors whose energy is crucial and limited. Therefore, we consider geographic forwarding in 3D duty-cycled k-covered WSNs, where the sensors can switch between on and off states (i.e., duty-cycled sensors) to save energy. First, we provide a rigorous analysis of the k-coverage problem in 3D WSNs using Helly's Theorem and the Reuleaux tetrahedron model, and compute the sensor spatial density to k-cover a 3D field. Second, based on this analysis, we compute a lower bound and an upper bound on the number of overlapping Reuleaux tetrahedra that are necessary to fill a 3D convex shape, such as the sensing sphere of a sensor. Third, using these results, we present a localized (i.e., based on local information of one-hop neighbors), pseudo-distributed (i.e., not fully distributed) protocol to achieve k-coverage of a 3D field with a reduced number of active sensors, while ensuring connectivity between them. Fourth, we discuss our composite geographic forwarding protocol for 3D duty-cycled k-covered WSNs using a combination of deterministic and opportunistic schemes to forward sensed data towards the sink. We will study the problem of 3D space filling (or space covering) in the context of the above-mentioned problems in 3D WSNs. Fifth, we relax two widely used assumptions, namely sensor homogeneity and sensing range convexity, to generalize our k-coverage protocol in 3D space. Last, we show several simulation results of our framework for joint k-coverage and composite geographic forwarding in 3D duty-cycled WSNs, called 3D-kCov-ComFor. We found a close-to-perfect match between our theoretical and simulation results.
Idle listening is a major source of energy waste in wireless sensor networks. It can be reduced through Low-Power Listening (LPL) techniques in which a node is allowed to sleep for a significant amount of time. In contrast to conventional fixed sleep time policies, we introduce a novel dynamic sleep time control approach that further reduces control packet energy waste by utilizing known data traffic statistics. We propose two distinct approaches to dynamically compute the sleep time, depending on the objectives and constraints of the network. The first approach provides a dynamic sleep time policy that guarantees a specified average delay at the sender node resulting from packets waiting for the end of a sleep interval at the receiver. The second approach determines the optimal policy that minimizes total energy consumed. In the case where data traffic statistics are unknown, we propose an adaptive learning algorithm to estimate them online and develop corresponding sleep time computation algorithms. Simulation results are included to illustrate the use of dynamic sleep time control and to demonstrate how it dominates fixed sleep time methods. An implementation of our approach on a commercial sensor node supports the computational feasibility of the proposed approach.
Wireless Sensor Networks (WSNs) are embracing an increasing number of real-time applications subject to strict delay constraints. Utilizing the methodology of potential field in physics, in this article we effectively address the challenges of real-time routing in WSNs. In particular, based on a virtual composite potential field, we propose the Potential-based Real-Time Routing (PRTR) protocol that supports real-time routing using multipath transmission. PRTR minimizes delay for real-time traffic and alleviates possible congestions simultaneously. Since the delay bounds of real-time flows are extremely important, the end-to-end delay bound for a single flow is derived based on the Network Calculus theory. The simulation results show that PRTR minimizes the end-to-end delay for real-time routing, and also guarantees a tight bound on the delay.
A novel energy-balanced task-scheduling method is proposed that extends the lifespan of wireless sensor networks (WSNs) for collaborative target tracking using an unscented Kalman filter (UKF) algorithm. It is shown that the tracking accuracy is approximately proportional to the number of active sensor nodes participating in collaborative tracking. Excessive sensor nodes thus may be put to sleep mode to conserve energy provided there are a sufficient number of active sensor nodes. It is then shown that the lifespan of a WSN is dictated by the distribution of residue energy of sensor nodes. Specifically, we have shown that an energy-balanced WSN is likely to maximize its lifespan. As such, at each step of the tracking task, the head node must judiciously select active nodes from all sensors within the sensing range to minimize residue energy variations (energy balanced) while achieving desired tracking accuracy. This is formulated as a subset selection problem, which is shown to have a complexity that is NP-hard. Several energy-balanced scheduling for tracking (EBaST) heuristic algorithms are proposed to solve this problem with polynomial execution complexities. Extensive simulations have been conducted to compare EBaST against some state-of-the-art scheduling algorithms. It is observed that EBaST is more capable of significantly extending the WSNs lifespan than competing algorithms while delivering comparable or better tracking accuracy.
Smart camera networks have recently emerged as a new class of sensor network infrastructure that is capable of supporting high-power in-network signal processing and enabling a wide range of applications. In this article, we provide an exposition of our efforts to build a low-bandwidth wireless camera network platform, called CITRIC, and its applications in smart camera networks. The platform integrates a camera, a microphone, a frequency-scalable (up to 624 MHz) CPU, 16 MB FLASH, and 64 MB RAM onto a single device. The device then connects with a standard sensor network mote to form a wireless camera mote. With reasonably low power consumption and extensive algorithmic libraries running on a decent operating system that is easy to program, CITRIC is ideal for research and applications in distributed image and video processing. Its capabilities of in-network image processing also reduce communication requirements, which has been high in other existing camera networks with centralized processing. Furthermore, the mote easily integrates with other low-bandwidth sensor networks via the IEEE 802.15.4 protocol. To justify the utility of CITRIC, we present several representative applications. In particular, concrete research results will be demonstrated in two areas, namely, distributed coverage hole identification and distributed object recognition.
This study presents several extensions to our previous work on the PipeProbe system, a mobile sensor system for identifying the spatial topology of hidden water pipelines (i.e., non-moldable pipes such as copper and PVC) behind walls or under floors [Lai et al. 2010]. The PipeProbe system works by dropping a tiny wireless sensor capsule into the source of a water pipeline. As the PipeProbe capsule traverses the pipelines, it gathers and transmits pressure and angular velocity readings. Through spatiotemporal analysis of these sensor readings, the proposed algorithm locates all turning points in the pipelines and maps their 3D spatial topology. This study expands upon previous research by developing new sensing techniques that identify variable-diameter pipes and differentiate 90-degree pipe turns from 45-degree pipe bends.
Wireless reprogramming of sensor nodes is a critical requirement in long-lived wireless sensor networks (WSNs) addressing several concerns, such as fixing bugs, upgrading the operating system and applications, and adapting applications behavior according to the physical environment. In such resource-poor platforms, the ability to efficiently delimit and reconfigure the necessary portion of sensor software�instead of updating the full binary image�is of vital importance. However, most existing approaches in this field have not been adopted widely to date due to the extensive use of WSN resources or lack of generality. In this article, we therefore consider WSN programming models and runtime reconfiguration models as two interrelated factors and we present an integrated approach for addressing efficient reprogramming in WSNs. The middleware solution we propose, <scp<RemoWare</scp<, is characterized by mitigating the cost of post-deployment software updates on sensor nodes via the notion of in situ reconfigurability and providing a component-based programming abstraction in order to facilitate the development of dynamic WSN applications. Our evaluation results show that <scp<RemoWare</scp< imposes a very low energy overhead in code distribution and component reconfiguration and consumes approximately 6% of the total code memory on a <scp<TelosB</scp< sensor platform.
In order to prolong the lifetime of a wireless sensor network (WSN) devoted to monitoring an area of interest, a useful means is to exploit network redundancy, activating only the sensors that are strictly necessary for coverage and making them work with the minimum necessary sensing radius. In this article, we introduce the first algorithm that reduces sensor coverage redundancy through joint Sensor Activation and sensing Radius Adaptation (SARA) in general application scenarios comprising two classes of devices: sensors with variable sensing radius and sensors with fixed sensing radius. This device heterogeneity is explicitly addressed by modeling the coverage problem through Voronoi-Laguerre diagrams that, differently from Voronoi diagrams, allow for correctly identifying each sensor coverage region depending on the sensor current radius and the radii of its neighboring nodes. SARA executes quickly with guaranteed termination and, given the currently available nodes, it always guarantees maximum coverage. By means of extensive simulations, we show that SARA obtains remarkable improvements with respect to previous solutions, ensuring, in networks with heterogeneous nodes, longer network lifetime and wider coverage.
Terra is a system for programming wireless sensor network (WSN) applications. It combines the use of configurable virtual machines with a reactive scripting language that can be statically analyzed to avoid unbounded execution and memory conflicts. This approach allows the flexibility of remotely uploading code on motes to be combined with a set of guarantees for the programmer. The choice of the specific set of components in a virtual machine configuration defines the abstraction level seen by the application script. We describe a specific component library built for Terra, which we designed taking into account the functionality commonly needed in WSN applications�typically for sense and control. We also discuss the programming environment resulting from the combination of a statically analyzable scripting language with this library of components. Finally, we evaluate Terra by measuring its overhead in a basic application and discussing its use and cost in a typical monitoring WSN scenario.
Building a micro-solar power system is challenging because it must address long-term system behavior under highly variable solar energy and consider a large design space. We develop a practical theory of micro-solar power systems that is materialized in a simulation suite that models component and system behavior over a long time scale and in an external environment that depends on time, location, weather, and local variations. This simulation provides sufficient accuracy to guide specific design choices in a large design space. Unlike the many macro-solar calculators, this design tool models detailed behavior of milliwatt systems in the worst conditions, rather than typical behavior of kilowatt systems in the best conditions. Our simulation suite is validated with a concrete design of micro-solar power systems, the HydroWatch node. With our simulation suite, micro-solar power systems can be designed in a systematic fashion. Putting the model and empirical vehicle together, the design choices in each component of a micro-solar power system are studied to reach a deployable candidate. The deployment is evaluated by analyzing the effects of different solar profiles across the network. The analysis from the deployment can be used to refine the next system-design iteration.
Coverage is one of the fundamental concepts in the design of wireless sensor networks (WSNs) in the sense that the monitoring quality of a phenomenon depends on the quality of service provided by the sensors in terms of how well a field of interest is covered. It enables the sensors to detect any event that may occur in the field, thus, meeting the application-specific requirements. Several applications require k-coverage, where each point in the field is covered by at least k sensors, which helps increase data availability to ensure better data reliability. Achieving k-coverage of a field of interest becomes a more challenging issue in sparsely deployed WSNs. Though the problem of coverage in WSNs has been well studied in the literature, only little research efforts have been devoted to the case of sparsely deployed WSNs. Thus, in this article, we investigate the problem of k-coverage in sparse WSNs using static and mobile sensors, which do not necessarily have the same communication range, sensing range, and energy supply. Precisely, we propose an optimized, generalized framework for k-coverage in sparsely deployed WSNs, called k-SCHEMES, which exploits sensor heterogeneity and mobility. First, we characterize k-coverage using heterogeneous sensors based on Helly's Theorem. Second, we introduce our energy-efficient four-tier architecture to achieve mobile k-coverage of a region of interest in a field. Third, on top of this architecture, we suggest two data-gathering protocols, called direct data-gathering and forwarding chain-based data-gathering, using the concept of mobile proxy sink. We found that the second data-gathering protocol outperforms the first one. For energy-efficient forwarding, we compute the minimum transmission distance between any pair of consecutive mobile proxy sinks forming the forwarding chain as well as the corresponding optimum number of mobile proxy sinks in this chain. We corroborate our analysis with several simulation results.
Monitoring aquatic environment is of great interest to the ecosystem, marine life, and human health. This article presents the design and implementation of Samba�an aquatic surveillance robot that integrates an off-the-shelf Android smartphone and a robotic fish to monitor harmful aquatic processes such as oil spills and harmful algal blooms. Using the built-in camera of the smartphone, Samba can detect spatially dispersed aquatic processes in dynamic and complex environment. To reduce the excessive false alarms caused by the nonwater area (e.g., trees on the shore), Samba segments the captured images and performs target detection in the identified water area only. However, a major challenge in the design of Samba is the high energy consumption resulted from continuous image segmentation. We propose a novel approach that leverages the power-efficient inertial sensors on smartphones to assist image processing. In particular, based on the learned mapping models between inertial and visual features, Samba uses real-time inertial sensor readings to estimate the visual features that guide image segmentation, significantly reducing the energy consumption and computation overhead. Samba also features a set of lightweight and robust computer vision algorithms, which detect harmful aquatic processes based on their distinctive color features. Last, Samba employs a feedback-based rotation control algorithm to adapt to spatiotemporal development of the target aquatic process. We have implemented a Samba prototype and evaluated it through extensive field experiments, lab experiments, and trace-driven simulations. The results show that Samba can achieve a 94% detection rate, a 5% false alarm rate, and a lifetime up to nearly 2 months.
Wearable sensing systems have recently enabled a variety of medical monitoring and diagnostic applications in wireless health. The need for multiple sensors and constant monitoring leads these systems to be power hungry and expensive with short operating lifetimes. We introduce a novel methodology that takes advantage of contextual and semantic properties in human behavior to enable efficient design and optimization of such systems from the data and information point of view. This, in turn, directly influences the wireless communication and local processing power consumption. We exploit intrinsic space and temporal correlations between sensor data while considering both user and system contextual behavior. Our goal is to select a small subset of sensors that accurately capture and/or predict all possible signals of a fully instrumented wearable sensing system. Our approach leverages novel modeling, partitioning, and behavioral optimization, which consists of signal characterization, segmentation and time shifting, mutual signal prediction, and a simultaneous minimization composed of subset sensor selection and opportunistic sampling. We demonstrate the effectiveness of the technique on an insole instrumented with 99 pressure sensors placed in each shoe, which cover the bottom of the entire foot, resulting in energy reduction of 72% to 97% for error rates of 5% to 17.5%.
Recent advances in Microelectronic Mechanical Systems (MEMS) and wireless communication technologies have fostered the rapid development of networked embedded systems like wireless sensor networks. System software for these self-organizing systems often needs to be updated for a variety of reasons. We present a holistic software update (i.e., reprogramming) system called R3 for networked embedded systems. R3 has two salient features. First, the binary differencing algorithm within R3 (R3diff) ensures an optimal result in terms of the delta size under a configurable cost measure. Second, the similarity preserving method within R3 (R3sim) optimizes the binary code format for achieving a large similarity with a small metadata overhead. Overall, R3 achieves the smallest delta size compared with other software update approaches such as Stream, Rsync, RMTD, Zephyr, Hermes, and R2 (e.g., 50%--99% reduction compared to Stream and about 20%--40% reduction compared to R2). R3�s implementation on TelosB/TinyOS is lightweight and efficient. We release our code at http://code.google.com/p/r3-dongw.
This article considers a problem of periodically estimating energy consumption breakdowns for main appliances inside building using a single power meter and the knowledge of the ON/OFF states of individual appliances. In the first part of this article, we formulate the problem as a constrained convex optimization problem with tunable parameters. Then we propose an online algorithm that adaptively determines the optimization parameters to robustly estimate the breakdown information. The proposed solution is evaluated by experiment using a scaled-down proof-of-concept prototype with real measurements. In the second part, we provide detailed analysis to understand the performance of our proposed algorithm. We first develop a stochastic model to describe evolution of appliances� ON/OFF states using continuous-time Markov chain. Then we derive analytical bounds of estimation error and the probability of a rank-deficient binary matrix. Those analytical bounds are verified by extensive simulations. Finally, we study the effect of collinearity of binary data matrix on estimation performance. Simulation results suggest that our algorithm is robust against the collinearity of binary dataset.
Recent advances in smartphone processing power have opened the possibilities for them to act as the processing component of software-defined radios (SDRs). For low-power sensor network systems using various communication protocols, this means that smartphones, when equipped with an SDR, can be their system management end-devices, (potentially) without the need for external communication modules. Nevertheless, the high processor and energy usage overhead of SDRs remains a major technical barrier that blocks the practical adoption of smartphone-based SDRs. In this work, we show that implementation flexibility at the software can relax this overhead. Specifically, we show, using an implementation of the low-power listening (LPL) Medium Access Control (MAC), that software improvements have the potential to significantly reduce the operational overhead of SDRs. Moreover, we show that implementing packet reception filters can help further reduce the performance overhead without sacrificing application-level message exchange qualities. Empirical results with a smartphone-based SDR suggest that by combining LPL with packet reception filters, the processing and energy overhead can be reduced by two to three orders of magnitude. We not only see this as a chance to practically realize smartphones as a wireless sensing system controller but also believe that the experiences with practical smartphone-based SDRs can provide guidelines for future wireless protocol and low-power radio designs that are suitable for mobile computing environments.
In wireless sensor networks, a critical system service is the localization service that determines the locations of geographically distributed sensor nodes. The raw data used by this service are the distance measurements between neighboring nodes and the position knowledge of anchor nodes. However, these raw data may contain outliers that strongly deviate from their true values, which include both the outlier distances and the outlier anchors. These outliers can severely degrade the accuracy of the localization service. Therefore, we need a robust localization algorithm that can reject these outliers. Previous studies in this field mainly focus on enhancing multilateration with outlier rejection ability, since multilateration is a primitive operation used by localization service. But patch merging, a powerful operation for increasing the percentage of localizable nodes in sparse networks, is almost neglected. We thus propose a robust patch merging operation that can reject outliers for both multilateration and patch merging. Based on this operation, we further propose a robust network localization algorithm called RobustLoc. This algorithm makes two major contributions. (1) RobustLoc can achieve a high percentage of localizable nodes in both dense and sparse networks. In contrast, previous methods based on robust multilateration almost always fail in sparse networks with average degrees between 5 and 7. Our experiments show that RobustLoc can localize about 90% of nodes in a sparse network with 5.5 degrees. (2) As far as we know, RobustLoc is the first to uncover the differences between outlier distances and outlier anchors. Our simulations show that RobustLoc can reject colluding outlier anchors reliably in both convex and concave networks.
Networking protocols for multihop wireless sensor networks (WSNs) are required to simultaneously minimize resource usage as well as optimize performance metrics such as latency and reliability. This article explores the energy-latency-reliability tradeoff for broadcast in WSNs by presenting a new protocol called PBBF. Essentially, for a given reliability level, energy and latency are found to be inversely related and our study quantifies this relationship at the reliability boundary. Therefore, PBBF offers an application designer considerable flexibility in the choice of desired operation points. Furthermore, we propose an extension to dynamically adjust the PBBF parameters to minimize the input required from the designer.
Energy efficiency is one of the key objectives in data gathering in wireless sensor networks (WSNs). Recent research on energy-efficient data gathering in WSNs has explored the use of Compressive Sensing (CS) to parsimoniously represent the data. However, the performance of CS-based data gathering methods has been limited since the approaches failed to take advantage of judicious network configurations and effective CS-based data aggregation procedures. In this article, a novel Hierarchical Data Aggregation method using Compressive Sensing (HDACS) is presented, which combines a hierarchical network configuration with CS. Our key idea is to set multiple compression thresholds adaptively based on cluster sizes at different levels of the data aggregation tree to optimize the amount of data transmitted. The advantages of the proposed model in terms of the total amount of data transmitted and data compression ratio are analytically verified. Moreover, we formulate a new energy model by factoring in both processor and radio energy consumption into the cost, especially the computation cost incurred in relatively complex algorithms. We also show that communication cost remains dominant in data aggregation in the practical applications of large-scale networks. We use both the real-world data and synthetic datasets to test CS-based data aggregation schemes on the SIDnet-SWANS simulation platform. The simulation results demonstrate that the proposed HDACS model guarantees accurate signal recovery performance. It also provides substantial energy savings compared with existing methods.
Wireless sensor network applications, such as those for natural disaster warning, vehicular traffic monitoring, and surveillance, have stringent accuracy requirements for detecting or classifying events and demand long system lifetimes. Through quantitative study, we show that existing event detection approaches are challenged to explore the sensing capability of a deployed system and choose the right sensors to meet user-specified accuracy. Event detection systems are also challenged to provide a generic system that efficiently adapts to environmental dynamics and works easily with a range of applications, machine learning approaches, and sensor modalities. Consequently, we propose Watchdog, a modality-agnostic event detection framework that clusters the right sensors to meet user-specified detection accuracy during runtime while significantly reducing energy consumption. Watchdog can use different machine learning techniques to learn the sensing capability of a heterogeneous sensor deployment and meet accuracy requirements. To address environmental dynamics and ensure energy savings, Watchdog wakes up and puts to sleep sensors as needed to meet user-specified accuracy. Through evaluation with real vehicle detection trace data and a building traffic monitoring testbed of IRIS motes, we demonstrate the superior performance of Watchdog over existing solutions in terms of meeting user-specified detection accuracy, energy savings, and environmental adaptability.
Wireless sensor networks (WSNs) are characterized by localized interactions, that is, protocols are often based on message exchanges within a node�s direct radio range. We recognize that for these protocols to work effectively, nodes must have consistent information about their shared neighborhoods. Different types of faults, however, can affect this information, severely impacting a protocol�s performance. We factor this problem out of existing WSN protocols and argue that a notion of neighborhood view consistency (NVC) can be embedded within existing designs to improve their performance. To this end, we study the problem from both a theoretical and a system perspective. We prove that the problem cannot be solved in an asynchronous system using any of Chandra and Toueg�s failure detectors. Because of this, we introduce a new software device called pseudocrash failure detector (PCD), study its properties, and identify necessary and sufficient conditions for solving NVC with PCDs. We prove that, in the presence of transient faults, NVC is impossible to solve with any PCDs, thus define two weaker specifications of the problem. We develop a global algorithm that satisfies both specifications in the presence of unidirectional links, and a localized algorithm that solves the weakest specification in networks of bidirectional links. We implement the latter atop two different WSN operating systems, integrate our implementations with four different WSN protocols, and run extensive micro-benchmarks and full-stack experiments on a real 90-node WSN testbed. Our results show that the performance significantly improves for NVC-equipped protocols; for example, the Collection Tree Protocol (CTP) halves energy consumption with higher data delivery.
Dr. Raffaello D'Andrea speaks at length about what it takes to build commercially viable robotic systems, the future of autonomous machines, the role humans will play in this future, and how we can best prepare for it.
New health care systems that integrate wearable sensors, personal devices, and servers promise to fundamentally change the way health care services are delivered and used.
At Italy's oldest technical university, students learn about IoT concepts and technologies by building end-to-end prototypical systems.
In an ad hoc wireless network where wired infrastructures are not feasible, energy and bandwidth conservation are the two key elements presenting challenges to researchers. Limited bandwidth makes a network easily congested by the control signals of the routing protocol. Routing schemes developed for wired networks seldom consider restrictions of this type. Instead, they assume that the network is mostly stable and that the overhead for routing messages is negligible. Considering these differences between wired and wireless network, it is necessary to develop a wireless routing protocol that limits congestion in the network [1, 5, 8, 9, 10, 11].This paper proposes minor modifications to the existing Ad hoc On Demand Vector (AODV) routing protocol (RFC 3561) in order to restrict congestion in networks during a particular type of Denial of Service (DoS) attack. In addition to this, it incurs absolutely no additional overhead [4]. We describe the DoS attack caused due to Route Request (RREQ) flooding and its implications on existing AODV-driven Mobile Ad hoc Networks (MANET) [2, 14]. To combat this DoS attack, a proactive scheme [12] is proposed. We present an illustration to describe the implications of RREQ flooding on pure AODV and the modified AODV protocols. To quantify the effectiveness of the proposed scheme, we simulated a DoS [6] attack in a mobile environment and study the performance results.
Oceans cover a majority of our planet and are currently lacking in regards to exploration and technological innovations. One technology that can help enable more aquatic applications is underwater acoustic networks (UANs). This article discusses the current status of UANs, the new applications that can be provided, and the challenges faced by this technology.
In this paper, a secure environment for electronic commerce is introduced. The environment is formed via a synthesis of biometrics consumer authentication with a security token. Such a token is a smart card containing cryptographic keys and a cryptographic microprocessor for data encryption. The keys are used to further authenticate the possessor of the card as the actual owner and also to facilitate secure electronic financial transactions. New technologies like these bring benefits to society by enhancing the standard of living, however, numerous challenges are introduced [1].Biometrics is a Greek composite word stemming from the synthesis of bio and metric, meaning life measurement. In this context, the science of biometrics is concerned with the accurate measurement of unique biological characteristics of an individual in order to securely identify them to a computer or other electronic system. Biological characteristics measured usually include fingerprints, voice patterns, retinal and iris scans, face patterns, and even the chemical composition of an individual's DNA [9].
If we let machines put us out of work, it will be because of a failure of imagination and the will to make a better future!
The ultimate goal of the Internet of Things and wearable revolution is to gift every person with their own magic genie, who will understand all of their needs and desires and thereby enrich the world around them.
The fusion of next generation sensors and advanced information systems, combined with advances in unmanned aircraft systems that have emerged through aerospace engineering technologies, will contribute to the challenge of feeding our future world in a sustainable manner. Without these advances, the world may find itself short of food and perhaps on the brink of global conflict.
Unmanned aircraft systems. Aerial robots. Drones. Regardless of the name, this new technology is being developed to revolutionize the sampling and understanding of complex atmospheric phenomena.
Exploiting parallelism may require developers to think differently about how their programs are written.
Which computational problems can be solved in polynomial-time and which cannot? Though seemingly technical, this question has wide-ranging implications and brings us to the heart of both theoretical computer science and modern physics.
Special purpose quantum computers---realized with current technology---have the potential to revolutionize physics, chemistry, and materials science.
Quantum computing and machine learning are two technologies that have generated unparalleled amounts of hype among the scientific community and popular press. Both are mysterious, immensely powerful, and on a collision course with each other.
The use of unmanned aerial drones will revolutionize news reporting, but many issues need to be resolved before things can really take off.
We already know algorithms can make our lives and our work more efficient, but how can we go beyond that to create trustworthy, fair, and enjoyable workplaces in which workers can find meaning and continuously learn?
A cyber-physical systems perspective on the design of vehicular networking solutions for safer and greener transportation.
Crabster CR200 is a giant crab robot with six legs and 30 powerful joints developed at the Korea Research Institute of Ships and Ocean Engineering. The robot can help explore ancient shipwrecks in areas of harsh tidal currents and turbid water, where traditional underwater vehicles have trouble operating.
One of the major challenges in today's computing world is energy management in portable devices and servers. Power management is essential to increase battery life. High end server systems use large clusters of machines that consume enormous amount of power. Past research has devised both software and hardware techniques to memory energy management but has overlooked the performance of applications in such environments. The result is that some of these techniques slowed down an application by 835%. In this paper, we look at software techniques for memory energy management without compromising on performance. The paper conceives of a new approach called BOS - Ballooning in the OS inspired from the VMware ESX server. The BOS approach consists of a kernel daemon which continuously monitors the accesses to memory chips and disk I/O. Based on the profiled information, the BOS daemon decides about powering down/up chips. Powering down is emulated within the kernel using mechanisms such as page migration and invisible buddy. Results indicate that chips with more allocated pages may not always be the most frequently accessed ones. A study has been done analyzing the effect of decreased memory size on disk activity and based on the study, a threshold based policy is proposed which is found to settle in the operating point for a simple applicaton. A single page migration incurs a cost of approximately 13?s and is one of the bottlenecks in the BOS approach.
Almost all hardware platforms to date have been homogeneous with one or more identical processors managed by the operating system (OS). However, recently, it has been recognized that power constraints and the need for domain-specific high performance computing may lead architects towards building heterogeneous architectures and platforms in the near future. In this paper, we consider the three types of heterogeneous core architectures: (a) Virtual asymmetric cores: multiple processors that have identical core micro-architectures and ISA but each running at a different frequency point or perhaps having a different cache size, (b) Physically asymmetric cores: heterogeneous cores, each with a fundamentally different microarchitecture (in-order vs. out-of-order for instance) running at similar or different frequencies, with identical ISA and (c) Hybrid cores: multiple cores, where some cores have tightly-coupled hardware accelerators or special functional units. We show case studies that highlight why existing OS and hardware interaction in such heterogeneous architectures is inefficient and causes loss in application performance, throughput efficiency and lack of quality of service. We then discuss hardware and software support needed to address these challenges in heterogeneous platforms and establish efficient heterogeneous environments for platforms in the next decade. In particular, we will outline a monitoring and prediction framework for heterogeneity along with software support to take advantage of this information. Based on measurements on real platforms, we will show that these proposed techniques can provide significant advantage in terms of performance and power efficiency in heterogeneous platforms.
Compression has been used in numerous ways for many years, but recently two factors have combined in a way to push compression to the forefront of distributed systems. First, the disparity between processor speeds and I/O rates is ever-increasing, making it possible to perform compression in software to a much greater extent than was previously feasible. Second, the growth of new applications demanding enormous data rates, such as digital video and audio, makes hardware compression increasingly desirable: I discuss the importance of compression in various environments and describe how compression may be used not only to reduce the demand for disk space, disk bandwidth, and network bandwidth, but also to appear to extend physical memory.
In this work we present the results of a project devoted to provide programming facilities to develop hard real-time software. We have used MINIX operating system as a tool for our experience. We allow the programmer to define timing constraints for the tasks, letting to the Operating System the work of running these tasks in a timely fashion. In this way, we can improve productivity, security and costs in the system development cycle.
The Fourth Workshop on Hot Topics in Software Upgrades (HotSWUp 2012) was held on June 3, 2012 in Zurich, Switzerland. The workshop was co-located with ICSE 2012. The goal of HotSWUp is to identify, through interdisciplinary collaboration, cutting-edge research ideas for implementing software upgrades. The workshop combined presentations of peer-reviewed research papers with a keynote speech on how empirical software engineering can help reduce update-induced failures. The audience included researchers and practitioners from academia and industry. In addition to the technical presentations, the program allowed ample time for discussions, which were driven by debate questions provided in advance by the presenters. HotSWUp provides a premier forum for discussing problems that are often considered niche topics in the established research communities. For example, the technical discussions at HotSWUp'12 covered dynamic software updates, package management tools, using model-checking and verification to verify updates, empirical software engineering and repository mining, and highlighted many synergies among these and other topics.
This paper outlines the design objectives and research goals for HARTOS, a distributed real-time operating system being developed at The University of Michigan. This effort is part of a larger research project to design and implement an experimental distributed real-time system called the Hexagonal Architecture for Real-Time Systems (HARTS). An important feature of HARTS is the use of an intelligent network processor to handle many of the functions relating to communications. The paper focuses on the communications aspects of the operating system and the control software kernel of the network processor. The preliminary version of the kernel provides good support for inter-process communication and distributed control. Its performance has been measured and analyzed and found to be comparable to that of other message passing systems like the V system.
Many-core chips are changing the way high-performance computing systems are built and programmed. As it is becoming increasingly difficult to maintain cache coherence across many cores, manufacturers are exploring designs that do not feature any cache coherence between cores. Communications on such chips are naturally implemented using message passing, which makes them resemble clusters, but with an important difference. Special hardware can be provided that supports very fast on-chip communications, reducing latency and increasing bandwidth. We present one such chip, the Single-Chip Cloud Computer (SCC). This is an experimental processor, created by Intel Labs. We describe two communication libraries available on SCC: RCCE and Rckmb. RCCE is a light-weight, minimal library for writing message passing parallel applications. Rckmb provides the data link layer for running network services such as TCP/IP. Both utilize SCC's non-cache-coherent shared memory for transferring data between cores without needing to go off-chip. In this paper we describe the design and implementation of RCCE and Rckmb. To compare their performance, we consider simple benchmarks run with RCCE, and MPI over TCP/IP.
Interprocess Communications play an important role in system performance in the throughput as well as in its reliability. McQuillan and Walden describe such design considerations as buffering and pipelining, error control, multiplexing and addressing, and flow control in interprocess communications. Danthine and Bremer proposed a model for representing protocols which can be used to verify the definition of the protocol and evaluate its system performance. Opderbeck presents measurement esults on performance of control procedures of the ARPA Network, which reveals that efficiency of the current protocol can be as low as 20% of line efficiency. While using a new host-to-host protocol an average of 30-40% efficiency can be achieved. In order to reduce the overhead in interprocess communications, much thought must be given on the efficient implementation and use of protocols, rather than only on their feasibility. Much more work is needed to be done in this area.
As usual, the SIGOPS workshop provided a great platform for interesting discussion. Among other things, a controversy arose around the usefulness of causal ordering in a distributed system. In this paper, I explain causality in non-technical terms, and enumerate some of the most prevalent misconceptions that surrounded causality. Next I present some important examples where causal delivery is a necessary and sufficient ordering of events.
This issue of the Operating Systems Review focuses on self-organizing systems; that is, systems whose functionality emerges through the autonomous activities of their components. Self-organization is a powerful paradigm for structuring systems ranging from stand-alone operating systems all the way up to planetary-scale distributed systems. Unlike the previously dominant organizational paradigms, self-organization has the potential to be much more robust because the individual components can autonomously compensate for the failures of other components, to scale to much larger systems because they avoid centralized components that pose bottlenecks, and to provide much better performance because they draw on the resources of a great many components distributed in many different places.This issue of the OSR brings together various papers on the application of the self-organizing design paradigm to various contexts. The vast majority of the papers in this issue relate to peer-to-peer (P2P) systems, a newly emerging class of distributed systems in which self-organization has played a foundational role. Peer-to-peer systems can be characterized by the ability of clients to also function as servers. This ability to shift traditional service functionality from dedicated servers (which provide resources) to a collection of self-organizing nodes that would otherwise serve solely as consumers has enabled a new and broad class of systems with unprecedented performance, robustness, and scale. There has been a huge outpouring of research on self-organizing peer-to-peer systems, whose applications include content delivery, naming systems, messaging, publish-subscribe, data repositories, just to name a few. While a recent panel at SOSP asked the question of whether P2P is "still useless," the overwhelming evidence at the panel was that self-organization techniques are now a standard part of the distributed system designer's tool chest, and have enabled our community, with the aid of PlanetLab, to tackle and address some of the outstanding problems in Internet-scale systems. Planetary scale systems are no longer just a theoretical endeavor; practical self-organizing systems seem to be here to stay.The papers in this issue cover foundations of self-organizing systems, such as elementary building blocks, such as clouds, for managing large scale networks, as well as basic techniques for reconciling quality of service with P2P, and for bridging the gap between centralized and peer-to-peer systems. A primary concern in any self-organizing system is whether the participants have incentives to autonomously follow a protocol, so some of the papers examine mechanism design and incentive-compatibility. Finally, a few of the papers examine self-organization within the context of a single, stand-alone operating system, examining, for instance, self-repair in componentized OSes. I hope you will enjoy reading this issue.
Twelve years have passed since VMware engineers first virtualized the x86 architecture. This technological breakthrough kicked off a transformation of an entire industry, and virtualization is now (once again) a thriving business with a wide range of solutions being deployed, developed and proposed. But at the base of it all, the fundamental quest is still the same: running virtual machines as well as we possibly can on top of a virtual machine monitor. We review how the x86 architecture was originally virtualized in the days of the Pentium II (1998), and follow the evolution of the virtual machine monitor forward through the introduction of virtual SMP, 64 bit (x64), and hardware support for virtualization to finish with a contemporary challenge, nested virtualization.
In this paper I refine the concept of interrupt to define a more structured mechanism with straightforward semantics. Some commonly implemented interrupt mechanisms are reviewed; their semantics are deduced and analyzed. I show how a different understanding of interrupts resolves the deficiencies of existing interrupt mechanisms and provides the programmer with powerful tools for addressing real-time and multiprocessing problems. On this basis a recommendation is made for architectural and operating system support of the improved interrupt mechanism.
This paper gives a short overview of the architecture of the distributed real-time system MARS (MAintainable Real-Time System) and describes the design and implementation of its operating system. The main purpose of the MARS kernel is to achieve a timely execution of hard real-time tasks and to provide an efficient communication mechanism suitable for distributed real-time systems.
A thread can make itself exclusive by disabling execution of all other threads running in the same address space. This brute force synchronization mechanism is useful in cases otherwise requiring numerous semaphore or mutex operations or when synchronizytion has to be added to existing software. Based on the L3 experiences, the paper describes how to implement exclusiveness by locking the relevant region of a task's address space. This optimistic method scales better than suspending and later reactivating all threads explicitly.
Cloud computing is currently the most important trend in the Information and Communication Technology (ICT) industry, and it has still not fully realized its potential. Reasons for its popularity are the opportunities to rapidly allocate vast amounts of computing resources and the fact that resources are accounted per use. While cloud computing was initiated by major industry players, academia has rapidly caught up; currently we see a vast number of cloud computing-related research efforts. However, since industry pushes development and many research aspects of cloud computing demand for large compute resources and real workloads, pure academic efforts have difficulties to address the most important issues and to have a major impact. On the other hand, academia usually tends to explore disruptive ideas that would not be addressed by industry alone. This paper summarizes the approaches and methods of five EU-funded research projects that focus on cloud computing in general and address important issues such as security, dependability, and interoperability. These aspects have received limited attention by the industry so far. The key to success of these large joint efforts is the close collaboration between partners from academia and industry spread all over Europe. The specific projects are Cloud-TM, Contrail, mOASIC, TClouds and VISION Cloud. Besides presenting the individual projects and their key contributions, we provide a perspective on future ICT research in Europe.
There are many compelling reasons to use a shared, public testbed such as GENI, Emulab, or PlanetLab to conduct experiments in computer science and networking. These testbeds support creating experiments with a large and diverse set of resources. Moreover these testbeds are constructed to inherently support the repeatability of experiments as required for scientifically sound research. Finally, the artifacts needed for a researcher to repeat their own experiment can be shared so that others can readily repeat the experiment in the same environment. However using a shared, public testbed is different from conducting experiments on resources either owned by the experimenter or someone the experimenter knows. Experiments on shared, public testbeds are more likely to use large topologies, use scarce resources, and need to be tolerant to outages and maintenances in the testbed. In addition, experimenters may not have access to low-level debugging information. This paper describes a methodology for new experimenters to write and deploy repeatable and sharable experiments which deal with these challenges by: having a clear plan; automating the execution and analysis of an experiment by following best practices from software engineering and system administration; and building scalable experiments. In addition, the paper describes a case study run on the GENI testbed which illustrates the methodology described.
Pointer tainting is a form of Dynamic Information Flow Tracking used primarily to prevent software security attacks such as buffer overflows. Researchers have also applied pointer tainting to malware and virus analysis. A recent paper by Slowinska and Bos has criticized pointer tainting as a security mechanism, arguing that it is has serious, inherent false positive and false negative defects. We present a rebuttal that addresses the confusion due to the two uses of pointer tainting in security literature. We clarify that many of the arguments against pointer tainting apply only to its use as a malware and virus analysis platform, but do not apply to the application of pointer tainting to memory corruption protection. Hence, we argue that pointer tainting remains a useful and promising technique for robust protection against memory corruption attacks.
Performance is a central requirement to the wide-spread adoption of virtualization. To deliver on the promise of simplifying IT via virtualization, the virtualization platform must provide excellent performance with minimal effort. Virtualization performance encompasses several different dimensions. An application running in a virtual machine must perform on-par with the same application natively. Multiple virtual machines running on the same host must scale well and share resources effectively. In this paper we will describe how virtualization performance at all of these levels has progressed with advances in software and hardware. We then discuss some of the challenges and opportunities that lie ahead as we move into the era of cloud computing
Cloud computing carries the promise of providing powerful new models and abstractions that could transform the way IT services are delivered today. In order to establish the readiness of clouds to deliver meaningful enterprise-class IT services, we identify three key issues that ought to be addressed as first priority from the perspective of potential cloud users: how to deploy large-scale distributed services, how to deliver high availability services, and how to perform problem resolution on the cloud. We analyze multiple sources of publicly available data to establish cloud user expectations and compare against the current state of cloud offerings, with a focus on contrasting the different requirements from two classes of users -- the individual and the enterprise. Through this process, our initial findings indicate that while clouds are ready to support usage scenarios for individual users, there are still rich areas of future research to be explored to enable clouds to support large distributed applications such as those found in enterprise.
Regplicated data management systems adopt the 1-copy serializability criteria for processing transactions. In order to achieve this goal, many approaches, rely on obtaining votes from other sites, for processing update requests. In this paper, a technique for generation of precedence graphs for each transaction execution is proposed. The transaction data flow graph approach is a fully distributed approach. The proposed technique, is free from deadlocks, and avoids resubmission of transactions.
Datacenters demand big memory servers for big data. For blade servers, which disaggregate memory across multiple blades, we derive technology and architectural models to estimate communication delay and energy. These models permit new case studies in refusal scheduling to mitigate NUMA and improve the energy efficiency of data movement. Preliminary results show that our model helps researchers coordinate NUMA mitigation and queueing dynamics. We find that judiciously permitting NUMA reduces queueing time, benefiting throughput, latency and energy efficiency for datacenter workloads like Spark. These findings highlight blade servers' strengths and opportunities when building distributed shared memory machines for data analytics.
Modeling parallel algorithms at the architecture level enables exploring side-effects of the weakly ordered nature of modern processors. Formal verification of such models with model-checking can ensure that algorithm guarantees will hold even in the presence of the most aggressive compiler and processor optimizations. This paper proposes a virtual architecture to model the effects of such optimizations. It first presents the OoOmem framework to model out-of-order memory accesses. It then presents the OoOisched framework to model the effects of out-of-order instruction scheduling. These two frameworks are explained and tested using weaklyordered memory interaction scenarios known to be affected by weak ordering. Then, modeling of user-level RCU (Read- Copy Update) synchronization algorithms is presented. It uses the virtual architecture proposed to verify that the RCU guarantees are indeed respected.
We describe the design of, and experience with, Query, a monitoring system that supports the Akamai EdgePlatform. Query is a foundation of Akamai's approach to administering its distributed computing platform, allowing administrators, operations staff, developers, customers, and automated systems near real-time access to data about activity in Akamai's network. Users extract information regarding the current state of the network via a SQL-like interface. Versions of Query have been deployed since the inception of Akamai's platform, and it has scaled to support a distributed platform of 60,000+ servers, collecting over 200 gigabytes of data and answering over 30,000 queries approximately every 2 minutes.
Virtualization drives higher resource utilization and makes provisioning new systems very easy and cheap. This combination has led to an ever-increasing number of virtual machines: the largest data centers will likely have more than 100K in few years, and many deployments will span multiple data centers. Virtual machines are also getting increasingly more capable, consisting of more vCPUs, more memory, and higher-bandwidth virtual I/O devices with a variety of capabilities like bandwidth throttling and traffic mirroring To reduce the work for IT administrators managing these environments, VMware and other companies provide several monitoring, automation, and policy-driven tools. These tools require a lot of information about various aspects of each VM and other objects in the system, such as physical hosts, storage infrastructure, and networking. To support these tools and the hundreds of simultaneous users who manage the environment, the management software needs to provide secure access to the data in real-time with some degree of consistency and backwardcompatibility, and very high availability under a variety of failures and planned maintenance. Such software must satisfy a continuum of designs: it must perform well at large-scale to accommodate the largest datacenters, but it must also accommodate smaller deployments by limiting its resource consumption and overhead according to demand. The need for high-performance, robust management tools that scale from a few hosts to cloud-scale poses interesting challenges for the management software. This paper presents some of the techniques we have employed to address these challenges
Cloud computing provides us with general purpose storage and server hosting platforms at a reasonable price. We explore the possibility of tapping these resources for the purpose of hosting source code repositories for individual projects as well as entire open source communities. An analysis of storage costs is presented, and a complete hosting solution is built and evaluated as a proof-of-concept.
The rise of ad-hoc data-intensive computing has led to the development of data-parallel programming systems such as Map/Reduce and Hadoop, which achieve scalability by tightly coupling storage and computation. This can be limiting when the ratio of computation to storage is not known in advance, or changes over time. In this work, we examine decoupling storage and computation in Hadoop through SuperDataNodes, which are servers that contain an order of magnitude more disks than traditional Hadoop nodes. We found that SuperDataNodes are not only capable of supporting workloads with high storage-to-processing workloads, but in some cases can outperform traditional Hadoop deployments through better management of a large centralized pool of disks.
This paper presents the motivation for using object-oriented technique to construct a message-based model for real-time operating systems. It introduces the components of the model and its running mechanism. This model chooses the preempt schedule policy and encapsulates priority with message transmitted between objects. The model is priority-driven and can avoid priority inversion.
In the design of real-time systems, tasks are often assigned priorities. Preemptive priority driven schedulers are used to schedule tasks to meet the timing requirements. Priority inversion is the term used to describe the situation when a higher priority task's execution is delayed by lower priority tasks. Priority inversion can occur when there is contention for resources among tasks of different priorities. The duration of priority inversion could be long enough to cause tasks to miss their deadlines. Priority inversion cannot be completely eliminated. However, it is important to identify sources of priority inversion and minimize the duration of priority inversion. IN the paper we present a comprehensive review of the problem of and solutions to unbounded priority inversion.
The UNIX&reg; operating system, developed by AT&amp;T Bell Laboratories, has become a standard operating system gaining rapid acceptance because of its superior flexibility, portability, and a number of support tools to increase programmer productivity. However, UNIX was originally designed for multitasking and time-sharing, and therefore conventional UNIX does not have an adequate response time and data throughput needed to support real-time applications.Many attempts have been made to adapt the UNIX kernel to provide a real-time environment. MODCOMP has developed REAL/IX operating system, which is a fully preemptive, low latency UNIX kernel. This paper discusses real-time performance of REAL/IX and compares it to MASSCOMP RTU operating system.
The uncontrolled use of the cache hierarchy in a multicore processor by real-time tasks may impact their worst-case execution times. Several operating system techniques have been recently proposed to deal with caches in a multiprocessor in order to improve predictability, such as cache partitioning, cache locking, and real-time scheduling. However, the contention caused by the cache coherence protocol and its implication for real-time tasks is still an open problem. In this paper, we present the design and evaluation of a real-time operating system for cache-coherent multicore architectures. The real-time operating system infrastructure includes real-time schedulers, cache partitioning, and cache coherence contention detection through hardware performance counters. We evaluate the real-time operating system in terms of run-time overhead, schedulability of realtime tasks, cache partitioning performance, and hardware performance counters usability. Our results indicate that: (i) a real-time operating system designed from scratch reduces the run-time overhead, and thus improves the realtime schedulability, when compared to a patched operating system; (ii) cache partitioning reduces the contention in the shared cache and provides safe real-time bounds; and (iii) hardware performance counters can detect when real-time tasks interfere with each other at the shared cache level. Scheduling, cache partitioning, and hardware performance counters together are a step-forward to provide real-time bounds in cache-coherent architectures.
In order to be economically feasible and to offer high levels of availability and performance, large scale distributed systems depend on the automation of repair services. While there has been considerable work on mechanisms for such automated services, a framework for evaluating and optimizing the policies governing such mechanisms has been lacking. In this paper we propose one such framework and report on our initial experience in applying the framework to analyze and optimize the operation a geo-distributed cloud storage system at Microsoft.
We continue to be concerned with interprocess communications systems (such as those described in references 1, 2, and 3 and called �thin-wire� communications systems in reference 4) which are suitable for communication between processes that are not co-located in the same operating system but rather reside in different operating systems on different computers connected by a computer communications network. Further, the systems with which we are concerned are assumed to communicate using addressed messages (e.g., reference 5) which are multiplexed onto the logical communications channel between the source process and the destination process, rather than using such traditional methods as shared memory (an impossibility for distributed communicating processes) or dedicated physical communications channels between pairs of processes desiring to communicate (which is considered to be prohibitively expensive).
Next generation real-time systems will require greater flexibility and predictability than is commonly found in today's systems. These future systems include the space station, integrated vision/robotics/AI systems, collections of humans/robots coordinating to achieve common objectives (usually in hazardous environments such as undersea exploration or chemical plants), and various command and control applications. The Spring kernel is a research oriented kernel designed to form the basis of a flexible, hard real-time operating system for such applications. Our approach challenges several basic assumptions upon which most current real-time operating systems are built and subsequently advocates a new paradigm based on the notion of predictability and a method for on-line dynamic guarantees of deadlines. The Spring kernel is being implemented on a network of (68020 based) multiprocessors called SpringNet.
Castro and Liskov proposed in 1999 a successful solution for byzantine fault-tolerant replication, named PBFT, which overcame performance drawbacks of earlier byzantine faulttolerant replication protocols. Other proposals extended PBFT with further optimizations, improving PBFT performance in certain conditions. One of the key optimizations of PBFT-based protocols is the use a request batching mechanism. If the target distributed system is dynamic, that is, if its underlying characteristics change dynamically, such as workload, channel QoS, network topology, etc., the configuration of the request batching mechanism must follow the dynamics of the system or it may not yield the desired performance improvement. This paper addresses this challenge by proposing an innovative solution to the dynamic configuration of request batching parameters inspired on feedback control theory. In order to evaluate its efficiency, the proposed solution is simulated in various scenarios and compared with the original version used in the PBFT-family protocols.
Finding an efficient configuration for cluster-based multi-tier Internet services is often a difficult task. Moreover, even a good configuration could become obsolete, depending on workload evolution. In this paper, we address both problems by dynamically calculating an optimal configuration for multi-tier Internet services and applying this configuration to the managed application. Our approach is based on two main components. A model of the underlying application, and a controller using this model to find the optimal configuration according current environment and performance objectives. We evaluate the model accuracy and the controller efficiency. Experiments show that our solution improves resource consumption, and may lead to significant energy savings, besides matching the performance objectives even with a dynamic workload.
Portability is an important attribute of real-time operating systems because their target hardware environments routinely vary from special purpose processors to parallel machines to distributed execution environments. In this paper, we address the issue of operating systems portability by development of a real-time threads package based on the Mach cthreads interface. Real-time threads have been implemented on standard Unix platforms and on a 32-node BBN Butterfly multiprocessor. In contrast to cthreads, the schedulability of real-time threads may be determined dynamically at the time of threads creation. In addition, any scheduling guarantees made at thread creation time are maintained when threads communicate or cooperate using the package's other primitives.
Cloud providers are auctioning their excess capacity using dynamically priced virtual instances. These spot instances provide significant savings compared to on-demand or fixed price instances. The users willing to use these resources are asked to provide a maximum bid price per hour, and the cloud provider runs the instances as long as the market price is below the user's bid price. By using such resources, the users are exposed explicitly to failures, and need to adapt their applications to provide some level of fault tolerance. In this paper, we expose the effect of bidding in the case of virtual HPC clusters composed of spot instances. We describe the interesting effect of uniform versus non-uniform bidding in terms of both the failure rate and the failure model. We propose an initial attempt to deal with the problem of predicting the runtime of a parallel application under various bidding strategies and various system parameters. We describe the relationship between bidding strategies and programming models, and we build a preliminary optimization model that uses real price traces from Amazon Web Services as inputs, as well as instrumented values related to the processing and network capacities of cluster instances on the EC2 services. Our results show preliminary insights into the relationship between non-uniform bidding and application scaling strategies.
Secure, fault-tolerant distributed systems are difficult to build, to validate, and to operate. Conservative design for such systems dictates that their security and fault tolerance depend on a very small number of assumptions taken on faith; such assumptions are typically called the "trusted computing base" (TCB) of a system. However, a rich trade-off exists between larger TCBs and more secure, more faulttolerant, or more efficient systems. In our recent work, we have explored this trade-off by defining "small," generic trusted primitives--for example, an attested, monotonically sequenced FIFO buffer of a few hundred machine words guaranteed to hold appended words until eviction and showing how such primitives can improve the performance, fault tolerance, and security of systems using them. In this article, we review our efforts in generating simple trusted primitives such as an attested circular buffer (called Attested Appendonly Memory), and an attested human activity detector. We describe the benefits of using these primitives to increase the fault-tolerance of replicated systems and archival storage, and to improve the security of email SPAM and click-fraud prevention systems. Finally, we share some lessons we have learned from this endeavor.
The integration of mobile/portable computers within existing static networks introduces a new set of issues to distributed computations. A mobile host can connect to the network from different locations at different times; distributed algorithms therefore, cannot rely on the assumption that a participant maintains a fixed and universally known location in the network at all times. Mobile hosts communicate with the rest of the network via a wireless broadcast medium and portable computers such as laptops and palmtops, often operate in a "disconnected" or "doze" mode to conserve battery power. These unique physical features of a mobile computing environment need to be considered as well, while designing distributed algorithms for such systems.
This article presents the design goals and architecture for a unified execution model (UEM) for cloud computing and clusters. The UEM combines interfaces for logical provisioning and distributed command execution with integrated mechanisms for establishing and maintaining communication, synchronization, and control. In this paper, the UEM architecture is described, and an existing application which could benefit from its facilities is used to illustrate its value.
Enterprise and scientific data sets double every year, forcing similar growths in storage size and power consumption. As a consequence, current system architectures used to build data warehouses are about to hit a power consumption wall. In this paper we propose an alternative architecture comprising large number of so-called Amdahl blades that combine energy-efficient CPUs with solid state disks to increase sequential read I/O throughput by an order of magnitude while keeping power consumption constant. We also show that while keeping the total cost of ownership constant, Amdahl blades offer five times the throughput of a state-of-theart computing cluster for data-intensive applications. Finally, using the scaling laws originally postulated by Amdahl, we show that systems for data-intensive computing must maintain a balance between low power consumption and per-server throughput to optimize performance perWatt.
Gossip-based protocols are commonly used for diffusing information in large-scale distributed applications. GO (Gossip Objects) is a per-node gossip platform that we developed in support of this class of protocols. GO allows nodes to join multiple gossip groups without losing the appealing fixed bandwidth guarantee of gossip protocols, and the platform also optimizes latency in a principled manner. Our algorithm is based on the observations that multiple rumors can often be squeezed into a single IP packet, and that indirect routing of rumors can speed up delivery. We formalize these observations and develop a theoretical analysis of this algorithm. We have also implemented GO, and studied the effectiveness of the algorithm by comparing it to the more standard random dissemination gossip strategy.
Identifying errors in early design phases leads to a decrease in the repairing cost compared to the situation in which such problems are discovered only in advanced design phases. This work is a first step toward an automatic verification approach for embedded and real-time systems' high-level specifications, such as UML models. This paper presents a model-driven framework to simulate system's behavior already in early design phases, prior to the implementation phase. More specifically, the mentioned framework simulates the behavior specified within UML models, generating a trace of executed actions for the selected behaviors. The achieved results show that early simulation of UML models is practicable, opening room for its usage in different CASE tools for early verification and validation of embedded and real-time systems.
Many factors have contributed to the birth and continued growth of mobile computing, including recent advances in hardware and communications technology. With this new paradigm however come new challenges in computer operating systems development. These challenges include heretofore relatively unusual items such as frequent network disconnections, communications bandwidth limitations, resource restrictions, and power limitations. It is the last of these challenges that we shall explore in this paper---that is the question of what techniques can be employed in mobile computer operating systems that can reduce the power consumption of today's mobile computing devices.
Virtual private servers and application checkpoint and restart are two advanced operating system features which place different but related requirements on the way kernel-provided resources are accessed by userspace. In Linux, kernel resources, such as process IDs and SYSV shared messages, have traditionally been identified using global tables. Since 2005, these tables have gradually been transformed into per-process namespaces in order to support both resource availability on application restart and virtual private server functionality. Due to inherent differences in the resources themselves, the semantics of namespace cloning differ for many of the resources. This paper describes the existing and proposed namespaces as well as their uses.
This session dealt with distributed systems with proven �existence theorems�; their communications mechanisms must be a compromise between idealism and workability. During the session Vic Lesser raised the question whether complex systems are necessarily hierarchical. This is the contention of the General System Theory school, out of which many books have appeared, the most accessible being Beyond Reductionism edited by A. Koestler and J.R. Smythies (Beacon Press, Boston, 1975). Another discussion concerns the question of simultaneity in large systems.
While application performance and power-efficiency are both important, application correctness is even more important. In other words, if the application is misbehaving, it is little consolation that it is doing so quickly or power-efficiently. In the Log-Based Architectures (LBA) project, we are focusing on a challenging source of application misbehavior: software bugs, including obscure bugs that only cause problems during security attacks. To help detect and fix software bugs, we have been exploring techniques for accelerating dynamic program monitoring tools, which we call "lifeguards". Lifeguards are typically written today using dynamic binary instrumentation frameworks such as Valgrind or Pin. Due to the overheads of binary instrumentation, lifeguards that require instructiongrain information typically experience 30X-100X slowdowns, and hence it is only practical to use them during explicit debug cycles. The goal in the LBA project is to reduce these overheads to the point where lifeguards can run continuously on deployed code. To accomplish this, we propose hardware mechanisms to create a dynamic log of instruction-level events in the monitored application and stream this information to one or more software lifeguards running on separate cores on the same multicore processor. In this paper, we highlight techniques and features of LBA that reduce the slowdown to just 2%--51% for sequential programs and 28%--51% for parallel programs.
Energy efficiency is one of the major challenges in big datacenters. To facilitate processing of large data sets in a distributed fashion, the MapReduce programming model is employed in these datacenters. Hadoop is an open-source implementation of MapReduce which contains a distributed file system. Hadoop Distributed File System provides a data block replication scheme to preserve reliability and data availability. The distribution of the data block replicas over the nodes is performed randomly by meeting some constraints (e.g., preventing storage of two replicas of a data block on a single node). This study makes use of flexibility in the data block placement policy to increase energy efficiency in datacenters. Furthermore, inspired by Zaharia et al.'s delay scheduling algorithm, a scheduling algorithm is introduced, which takes into account energy efficiency in addition to fairness and data locality properties. Computer simulations of the proposed method suggest its superiority over Hadoop's standard settings.
One of the main challenges to harness the potential of Cloud computing is the design of programming models that simplify the development of large-scale parallel applications and that allow ordinary programmers to take full advantage of the computing power and the storage provided by the Cloud, both of which made available, on demand, in a pay-only-forwhat-you-use pricing model. In this paper, we discuss the use of the Transactional Memory programming model in the context of the cloud computing paradigm, which we refer to as Cloud-TM. We identify where existing Distributed Transactional Memory platforms still fail to meet the requirements of the cloud and of its users, and we point several open research problems whose solution we deem as essential to materialize the Cloud-TM vision.
This article reports the results of an investigation of organizational assimilation of an emerging information technology�database machines. Results of database machine adoption and implementation are reported, with emphasis on practical implications for managing information technology diffusion. The importance of top management support and champions who push for adoption and implementation are indicated. Other factors discussed include technological awareness, vendor involvement,and training, among others. Built on innovation diffusion and IS implementation studies, a model of organizational-level adoption and implementation underpins the interpretation of the results.
Research in the larger management discipline is increasingly turning towards practice as an important unit of analysis to study organizational form and function. Reflecting this practice turn, information systems research is specifically interested in what people think they do when confronted with technology, what they actually do in the process of using technology, and what using technology does to them and the organizations they are a part of. In the wake of this development, sociomateriality has been proposed as a concept to study these organizational phenomena. The concept of sociomateriality, however, is not without its difficulties. Much debate has ensued on the ontological assumptions the practice turn and the concept of sociomateriality should be based on, and many scholars using the concept report how difficult it is to empirically investigate the intertwining or entanglement of the social and the material in practice. In this paper, we revisit Parsons' and Shils' (1951) general theory of action to address some of these difficulties and develop a framework for researching the social and the material in practice. Particularly, we propose that Parsons' and Shils' orientations of action help to better understand and conceptualize what individuals do before acting, what role technology plays, and how individuals and technology shape and are being shaped by each other in action across time. Through ten methodological principles, we demonstrate that such a framework can improve conceptualization and guide the selection and design of research methods in order to generate the data needed for sociomaterial theorizing.
Mobile technology has mobilized the human interaction in all dimensions by supporting mobile collaboration. As collaboration is key to learning in today's educational environment, mobile technology has tremendous potential in supporting and improving education and its delivery. Given that mobile technology for education is a new phenomenon that is gaining popularity, the values of using mobile technology to support education need to be further researched and better understood. In this research, we used the Value-Focused Thinking approach to interview students and instructors to identify the values of education that are enabled by mobile technology. These values are represented in the form of a means-ends objective network that not only captures the values of education facilitated by mobile technology but also depicts the relationships between these values. The means-ends objective network derived from this research can serve as a conceptual foundation for future studies and provide useful guidelines to practitioners for implementation of mobile technology in education.
With advances in the areas of telecommunications, computing and miniaturization of computers, the use of mobile technology is becoming prevalent within organizations. Consequently, a shift towards a nomadic computing environment, capable of supporting workers anywhere and anytime, is commonly observed. While many of the issues associated with such environments are technological in nature, this paper focuses on the social aspect of the shift to a nomadic computing environment, and examines its impact of employees' ability to effectively collaborate with one another. Studying changes at the individual level, we argue that an increase in workers' social mobility, brought upon by the move to a nomadic computing environment, is likely to have a negative effect on their social capital. Social capital has been shown to positively impact collaboration in various settings, including the workplace. We further argue that the above negative effect is contingent upon the type of mobile technology used by nomadic workers. The paper concludes with suggestions for model extensions and avenues for future research.
Online merchants use personalization technologies to gain knowledge of an individual customer and then generate preference-matched web content for the customer. Among the various types of personalization technologies, this research focuses on personalization engines that generate preference-matched content based on a customer's prior transactions. Extant research in this area has focused on how to maximize knowledge mined from transaction logs to generate content that is highly similar to the customer's past revealed preferences. However, it remains an empirical question as to whether the content closely matched with previous transactions is most likely to influence choice behavior. In this study, we postulate that the content closely matched with previous transactions may not be the most influential in biasing a customer. In the consideration and choice process, an individual's personality traits play a pivotal role in moderating the effect of personalized content. Drawing on research in marketing, we examine three key personality traits: need for cognition; variety seeking, and need for uniqueness; and explore their effects on choice behavior in the context of transaction-driven personalization. Research hypotheses are tested with over 2,000 pre-selected subjects in an online experiment based on a ringtone download website. We find that individual personality traits moderate content consideration and choice. Theoretical and practical implications of the findings are discussed.
This paper reports the results of a field study conducted to obtain managers perceptions of their Management Information Systems (MIS). The basic study involved responses from over 100 MIS/DP managers, users, and other business professionals. The research sampled the perspectives of both MIS and non-MIS personnel and found no great disparities in either the favorable quality of services provided or in the ranking of tasks contributing to system productivity.
The failure rate of Customer Relationship Management (CRM) implementations is estimated to be greater than 65%. Lowering the failure rate and supporting the success of information systems (IS) are the ultimate goals of IS practitioners and researchers. However, most previous research in the area has focused on identifying factors such as critical success factors (CSFs) that are correlated with IS success. There has been little research on how IS implementation may lead to successful results. The state of knowledge in IS implementation may be likened to cooking with a list of ingredients but not the recipe. Drawing on process theory, this study examines the process of IS implementation by explaining how factors of IS implementation influence each other and how interactions among them produce results. Based on one successful case and two unsuccessful cases, we develop the process model of IS implementation, by which the process of IS implementation and the dynamics of IS success can be explained. The proposed model facilitates an understanding of how repeating patterns of IS failure can be reversed, and could serve to guide new IS implementation projects.
This article reports on a case study of three firms which examined the organizational-level measures and process-level measures that were used to identify the effects of IT-enabled BPR (Business Process Redesign) projects. Firms in three distinct industry sectors provided the context for document review and semi-structured interview studies to identify those measures used to determine whether the firm's project was considered successful and how the firm had made the determination. A theoretical framework is proposed and provisionally confirmed demonstrating the link between IT-payoffs and BPR payoffs and the creation of intermediate assets that were identifiable and measurable. Lessons learned and opportunities for future research are presented.
Cloud Computing is the fastest growing technology in the IT industry. It helps in providing services and resources with the help of internet. Resources are always provided by the Cloud Service Provider. Resources may be servers, storage, applications and networks. Keeping Resources in the cloud environment can be helpful in saving infrastructure cost and time for the user. Transferring the entire information of the enterprise onto the cloud contains lots of security issues and threats. This paper focuses on the various concepts related to cloud computing, its various business and service models and its entities along with several issues and challenges related to it.
The adaptive management of complex applications deployed across multiple heterogeneous PaaS platforms is one of the problems that have emerged with the cloud revolution. The recently started EU research project SeaClouds aims at providing seamless adaptive multi-cloud management of complex applications by supporting the distribution, monitoring and migration of application modules over multiple heterogeneous PaaS platforms. We present the context, motivations and objectives of SeaClouds, its relation with other cloud initiatives, and its initial architecture.
This paper develops a metaphor that expresses what is so difficult about software production. The purpose of the metaphor is to be applicable to real-world software production situations, especially on larger or more complex projects, where decisions are made in ways that neglect some of the aspects of software production that make it so difficult, especially that seeing how to solve a problem can be immensely difficult to do from the beginning and as a result one sometimes has to abandon a whole approach and start again. The use of the metaphor is justified with reference to related literature and by application to certain characteristic situations.
Competition among software providers creates enormous pressure on design and development teams to improve application performance. However, increased performance leads to systems whose behaviour is harder to predict. This in turn makes software harder to manage, or self-manage in the case of autonomic software. In this paper we elaborate on this problem, first in generic terms, and then taking memory-usage monitoring in a Java Virtual Machine as a specific example. We motivate the need for more research in developing monitoring techniques that can cope with the complexity of modern software systems. We finally present our own efforts in this direction.
An extremely time-consuming task of producing an embedded real-time system is the final analysis and fine-tuning of the system's timing. Existing CASE tools focus on the software specification and design of embedded systems. They provide little, if any, support after the software has been implemented. Even if the developer uses a CASE tool to design their system, it likely does not meet the timing specifications on the first try. This happens because the CASE tool's software design and real-time analysis is based only on estimated data and idealized models. The tools do not take into account practical concerns such as operatin system overhead, interrupt handling, limitations of the programming language or processor, inaccuracies in estimating worst-case execution time of each process, and software errors introduced at the implementation phase by the programmers.Performance monitoring tools allow developers to obtain raw data from the underlying embedded system in real-time. These tools provide most, if not all, of the data needed to pinpoint the problem. Such data, however, is not provided in a symbolic fashion, and thus could be very difficult to understand. The monitors only show what happened during run-time, without correlating those results to the original specifications. Performance monitors also do not perform any analysis on the data that is collected. As a result, there is no means to easily differentiate between parts of the execution that are "normal" versus those parts thathave difficult-to-detect timing errors. Only an expert's eye can quickly spot the differences.We are investigating tools that can help embedded system designers analyze, debug, and fine-tune the timing characteristics of their embedded implementations. Such a tool can have a major impact, by allowing designers whose expertise is in an area other than real-time system analysis, such as communications, controls, or hardware design, to use the tool and obtain valuable information on how to fix their code that is not performing according to specifications.
Building real-time applications can be one of the most difficult jobs facing today's software engineers. With all the talk about Web services and Web based applications we sometimes forget that there is a large community of practice engaged in the construction of hard real-time systems. Real-time software engineering has been around for several decades. Typical real-time applications can be found in the areas of communications, avionics, process control, and other specialized applications such as signaling and switching systems.
The focus of this work is the analysis and performance evaluation of different weighted-average voting algorithms, such as standard weighted-average, flexible weighted-average and static and dynamic rule-based fuzzy weighted-average voter algorithms. We evaluate the safety performance of six existing voting algorithms and propose a modified dynamic-bandwidth-based fuzzy-voting algorithm with soft-dynamic threshold value. Experimental results show that our novel voting algorithm shows higher safety than the other weighted-average voting algorithms.
This is a report from a one-day second international workshop on "Information Systems in Distributed Environments" (ISDE), which was organized in conjunction with the OnTheMove Federated Conferences & Workshops (OTM 2010) at Hersonissos-Crete, Greece, on October 26, 2010. The main focus of this workshop was to provide a venue for the discussion of challenges related to the development, operation, and maintenance of distributed information systems, and their creation in the context of global development projects. Further dissemination of research results will lead to an improvement of distributed information system development and deployment across the glob.
The dynamic behavior of distributed systems requires that their performance characteristics be determined rigorously, preferably in the early stages of software engineering process. Evaluation of the performance at the end of software development leads to increase in the cost of design change. To compare design alternatives or to identify system bottlenecks, quantitative system analysis must be carried out from the early stages of the software development life cycle. In this paper we describe a process model, Hybrid Performance Prediction Process Model that allows modeling and evaluating distributed systems with the explicit goal of assessing performance of the software system during feasibility study. The use case performance engineering approach proposed in this paper exploits use case model and provides flexibility to integrate the software performance prediction process with software engineering process. We use an e-parking application to demonstrate various elements in our framework. The performance metrics are obtained and analyzed by considering two software architectures. Sensitivity analysis on the behavior of resources is carried out. This analysis helps to determine the capacity of the execution environment to obtain the defined performance objectives.
High-availability clusters are groups of servers that provide a reliable framework for applications to achieve a minimum downtime and quick recovery time without any human intervention and yet are completely opaque to the users. Almost all industries are continuously pursuing the goal to minimize their critical application downtimes by using various techniques such as fault tolerance, redundancy, mirroring and clustering. During downtime, applications become unavailable to end users, which can lead to financial, reputation and regulatory business impacts. High-availability clusters provide a mechanism to migrate complete applications or services from one server to another seamlessly without any human intervention at the time of failure of any critical component or the complete server. Hence, an application would start on a healthy server, without end users realizing this failover. In this work, key features and aspects of two cluster products, 'Symantec Veritas Cluster Suite' and 'Red Hat Cluster' were compared against each other based on various parameters. A simulated environment was created to perform a comprehensive analysis of performances of both products. In this work, measurement of average failover time was taken and compared as the key reliability and serviceability attribute. Thus, based on this experimental work, it is concluded that in a controlled test environment running a simple web-server application, Red Hat cluster gives a better failover performance as compared to Veritas Cluster Suite. However, in a large-scale environment, if we consider factors like operating system compatibility, supported applications, compatibility with volume managers, hardware compatibility, reliability, fault tolerance, predictive failure, self-healing etc., then the Veritas Cluster Suite comes out as the winner.
The success of e-commerce companies in a Confucian cultural context takes more than advanced IT and process design that have proven successful in Western countries. The example of eBay�s failure in China indicates that earning the trust of Chinese consumers is essential to success, yet the process of building that trust requires something different from that in the Western culture. This article attempts to build a theoretical model to explore the relationship between the Confucian culture and online trust. We introduce two new constructs, namely process flexibility and perceived control, as particularly important factors in online trust formation in the Chinese cultural context. A survey was conducted to test the proposed theoretical model. This study offers a new explanation for online trust formation in the Confucian context. The findings of this article can provide guidance for companies hoping to successfully navigate the Chinese online market in the future.
Ageing population is at the center of the looming healthcare crisis in most parts of the developed and developing world. Australia, like most of the western world, is bracing up for the looming ageing population crisis, spiraling healthcare costs, and expected serious shortage of healthcare workers. Assistive service and companion (social) robots are being seen as one of the ways for supporting aged care facilities to meet this challenge and improve the quality of care of older people including mental and physical health outcomes, as well as to support healthcare workers in personalizing care. In this article, the authors report on the design and implementation of first-ever field trials of Matilda, a human-like assistive communication (service and companion) robot for improving the emotional well-being of older people in three residential care facilities in Australia involving 70 participants. The research makes several unique contributions including Matilda�s ability to break technology barriers, positively engage older people in group and one-to-one activities, making these older people productive and useful, helping them become resilient and cope better through personalization of care, and finally providing them sensory enrichment through Matilda�s multimodal communication capabilities.
Bounding the price of stability of undirected network design games with fair cost allocation is a challenging open problem in the Algorithmic Game Theory research agenda. Even though the generalization of such games in directed networks is well understood in terms of the price of stability (it is exactly Hn, the n-th harmonic number, for games with n players), far less is known for network design games in undirected networks. The upper bound carries over to this case as well, while the best known lower bound is 2:245. For more restricted but interesting variants of such games, such as broadcast and multicast games, sublogarithmic upper bounds are known, while the best known lower bounds are 1:818 and 1:862, respectively. In this letter, we discuss a recent breakthrough in this field of research: an O(1) upper bound on the price of stability for undirected broadcast games.
A consideration of the state of computational grids with respect to standards, current uses, and a road map for commercial benefit beyond their common applications
In this interview conducted by Ubiquity editor Walter Tichy, Prof. Thomas Fahringer of the Institute of Computer Science, University of Innsbruck (Austria) discusses the difficulty in predicting the performance of parallel programs, and the subsequent popularity of auto-tuning to automate program optimization.
Business people on the go need portability and mobility in their computing environments. However, such mobile environments often suffer from transient faults. This article discusses how to manage faults in order to get better availability for small wireless devices.
Chips with multiple processors, called multicore chips, have caused a resurgence of interest in parallel computing. Multicores are now available in servers, PCs, laptops, embedded systems, and mobile devices. Because multiprocessors could be mass-produced for the same cost as uniprocessors, parallel programming is no longer reserved for a small elite of programmers such as operating system developers, database system designers, and supercomputer users. Thanks to multicore chips, everyone's computer is a parallel machine. Parallel computing has become ubiquitous. In this symposium, seven authors examine what it means for computing to enter the parallel age.
Virtualization has become a hot topic. Cloud computing is the latest and most prominent application of this time-honored idea, which is almost as old as the computing field itself. The term "cloud" seems to have originated with someone's drawing of the Internet as a puffy cloud hiding many servers and connections. A user can receive a service from the cloud without ever knowing which machine (or machines) rendered the service, where it was located, or how many redundant copies of its data there are. One of the big concerns about the cloud is that it may assign many computational processes to one machine, thereby making that machine a bottleneck and giving poor response time. Faouzi Kamoun addresses this concern head on, and assures us that in most cases the virtualization used in the cloud and elsewhere improves performance. He also addresses a misconception made prominent in a Dilbert cartoon, when the boss said he wanted to virtualize the servers to save electricity.
The computer code for Mehta and Patel's (1983) network algorithm for Fisher's exact test on unordered r�c contingency tables is provided. The code is written in double precision FORTRAN 77. This code provides the fastest currently available method for executing Fisher's exact test, and is shown to be orders of magnitude superior to any other available algorithm. Many important details of data structures and implementation that have contributed crucially to the success of the network algorithm are recorded here.
The idea of the economical method is applied for generating samples from any discrete distribution. In the resulting procedure, the expected number of uniformly distributed random numbers is less than in the alias method (practically 1). A refinement gives a version where in limit just one uniformly distributed number is required at the expense of some storage space.
This article addresses the execution cost of arithmetic operations with a focus on fuzzy arithmetic. Thanks to an appropriate representation format for fuzzy intervals, we show that it is possible to halve the number of operations and divide by 2 to 8 the memory requirements compared to conventional solutions. In addition, we demonstrate the benefit of some hardware features encountered in today�s accelerators (GPU) such as static rounding, memory usage, instruction-level parallelism (ILP), and thread-level parallelism (TLP). We then describe a library of fuzzy arithmetic operations written in CUDA and C++. The library is evaluated against traditional approaches using compute-bound and memory-bound benchmarks on Nvidia GPUs, with an observed performance gain of 2 to 20.
Distributed administration of network repositories demands a low-overhead procedure for cooperating repositories around the world to ensure they hold identical contents. Netlib has adopted some refinements on the widespread scheme of anonymous ftp and s-R. Checksum files and two small C programs give an easily maintained system that copes with communication breakdowns and subtle changes in repository contents. The packaging of these C programs inside a shell pipeline provides an explicit command stream that can readily be checked before execution. Protecting files, keeping logs, and so forth become effortless and reliable. The same tools, applied on a smaller scale, allow more people to participate in the editorial work of maintaining a high-quality repository, by eliminating the need for directly manipulating files at remote sites.
Price of anarchy is an oft-used worst-case measure of the inefficiency of noncooperative decentralized architectures. For a noncooperative load-balancing game with two classes of servers and for a finite or infinite number of dispatchers, we show that the price of anarchy is an overly pessimistic measure that does not reflect the performance obtained in most instances of the problem. We explicitly characterize the worst-case traffic conditions for the efficiency of noncooperative load-balancing schemes and show that, contrary to a common belief, the worst inefficiency is in general not achieved in heavy traffic.
The high cost of provisioning resources to meet peak application demands has led to the widespread adoption of pay-as-you-go cloud computing services to handle workload fluctuations. Some enterprises with existing IT infrastructure employ a hybrid cloud model where the enterprise uses its own private resources for the majority of its computing, but then �bursts� into the cloud when local resources are insufficient. However, current commercial tools rely heavily on the system administrator�s knowledge to answer key questions such as when a cloud burst is needed and which applications must be moved to the cloud. In this article, we describe Seagull, a system designed to facilitate cloud bursting by determining which applications should be transitioned into the cloud and automating the movement process at the proper time. Seagull optimizes the bursting of applications using an optimization algorithm as well as a more efficient but approximate greedy heuristic. Seagull also optimizes the overhead of deploying applications into the cloud using an intelligent precopying mechanism that proactively replicates virtualized applications, lowering the bursting time from hours to minutes. Our evaluation shows over 100% improvement compared to na�ve solutions but produces more expensive solutions compared to ILP. However, the scalability of our greedy algorithm is dramatically better as the number of VMs increase. Our evaluation illustrates scenarios where our prototype can reduce cloud costs by more than 45% when bursting to the cloud, and that the incremental cost added by precopying applications is offset by a burst time reduction of nearly 95%.
Internet of Things (IoT) devices are usually considered external application dependencies that only provide data or process and execute simple instructions. The recent emergence of IoT devices with embedded execution environments allows practitioners to deploy and execute custom application logic directly on the device. This approach fundamentally changes the overall process of designing, developing, deploying, and managing IoT systems. However, these devices exhibit significant differences in available execution environments, processing, and storage capabilities. To accommodate this diversity, a structured approach is needed to uniformly and transparently deploy application components onto a large number of heterogeneous devices. This is especially important in the context of large-scale IoT systems, such as in the smart city domain. In this article, we present LEONORE, an infrastructure toolset that provides elastic provisioning of application components on resource-constrained and heterogeneous edge devices in large-scale IoT deployments. LEONORE supports push-based as well as pull-based deployments. To improve scalability and reduce generated network traffic between cloud and edge infrastructure, we present a distributed provisioning approach that deploys LEONORE local nodes within the deployment infrastructure close to the actual edge devices. We show that our solution is able to elastically provision large numbers of devices using a testbed based on a real-world industry scenario.
Cloud applications can benefit from the on-demand capacity of cloud infrastructures, which offer computing and data resources with diverse capabilities, pricing, and quality models. However, state-of-the-art tools mainly enable the user to specify �if-then-else� policies concerning resource usage and size, resulting in a cumbersome specification process that lacks expressiveness for enabling the control of complex multilevel elasticity requirements. In this article, first we propose SYBL, a novel language for specifying elasticity requirements at multiple levels of abstraction. Second, we design and develop the rSYBL framework for controlling cloud services at multiple levels of abstractions. To enforce user-specified requirements, we develop a multilevel elasticity control mechanism enhanced with conflict resolution. rSYBL supports different cloud providers and is highly extensible, allowing service providers or developers to define their own connectors to the desired infrastructures or tools. We validate it through experiments with two distinct services, evaluating rSYBL over two distinct cloud infrastructures, and showing the importance of multilevel elasticity control.
With the proliferation of mobile devices and sensors, complex event proceesing (CEP) is becoming increasingly important to scalably detect situations in real time. Current CEP systems are not capable of dealing efficiently with highly dynamic mobile consumers whose interests change with their location. We introduce the distributed mobile CEP (MCEP) system which automatically adapts the processing of events according to a consumer's location. MCEP significantly reduces latency, network utilization, and processing overhead by providing on-demand and opportunistic adaptation algorithms to dynamically assign event streams and computing resources to operators of the MCEP system.
The Domain Name System (DNS) provides a critical service for the Internet -- mapping of user-friendly domain names to their respective IP addresses. Yet, there is no standard set of metrics quantifying the Quality of Domain Name Service (QoDNS), let alone a thorough evaluation of it. This article attempts to fill this gap from the perspective of a DNS proxy/cache, which is the bridge between clients and authoritative servers. We present an analytical model of DNS proxy operations that offers insights into the design trade-offs of DNS infrastructure and the selection of critical DNS parameters. Due to the critical role DNS proxies play in QoDNS, they are the focus of attacks including cache poisoning attack. We extend the analytical model to study DNS cache poisoning attacks and their impact on QoDNS metrics. This analytical study prompts us to present Domain Name Cross-Referencing (DoX), a peer-to-peer systems for DNS proxies to cooperatively defend cache poisoning attacks. Based on QoDNS, we compare DoX with the cryptography-based DNS Security Extension (DNSSEC) to understand their relative merits.
Recent years have witnessed the emergence and rapid development of collaborative Web-based applications exemplified by Web-based office productivity applications. One major challenge in building these applications is maintaining data consistency while meeting the requirements of fast local response, total work preservation, unconstrained interaction, and customizable collaboration mode. These requirements are important in determining users� experiences in interaction and collaboration, and in meeting users� diverse needs under complex and dynamic collaboration and networking environments; but none of existing solutions is able to meet all of them. In this article, we present a data consistency maintenance solution capable of meeting these requirements for collaborative Web-based applications. Major technical contributions include an efficient sequence-based operation transformation control algorithm based on the concept of contextualization, an operation broadcast protocol for supporting a variety of collaboration modes, an operation replaying algorithm for ensuring fast local response and efficient remote operation replay, and a set of communication protocols for managing the integrity of collaborative Web-based sessions. The proposed solution has been implemented in a prototype collaborative Web-based editor WRACE and the correctness of the solution is formally verified in the article.
BitTorrent (BT) plays an important role in Internet content distribution. Because public BTs suffer from the free-rider problem, Darknets are becoming increasingly popular, which use Sharing Ratio Enforcement to increase their efficiency. We crawled and traced 17 Darknets from September 2009 to February 2011, and obtained datasets about over 5 million torrents. We conducted a broad range of measurements, including traffic, sites, torrents, and users activities. We found that some of the features of Darknets are noticeably different from public BTs. The results of our study reflect both macroscopic and microscopic aspects of the overall ecosystem of BitTorrent Darknets.
Collaborative peer-to-peer (P2P), grid, and cloud computing rely on resource discovery (RD) solutions to aggregate groups of multi-attribute, dynamic, and distributed resources. However, specific characteristics of real-world resources and queries, and their impact on P2P-based RD, are largely unknown. We analyze the characteristics of resources and queries using data from four real-world systems. These characteristics are then used to qualitatively and quantitatively evaluate the fundamental design choices for P2P-based multi-attribute RD. The datasets exhibit several noteworthy features that affect the performance. For example, compared to uniform queries, real-world queries are relatively easier to resolve using unstructured, superpeer, and single-attribute-dominated query-based structured P2P solutions, as queries mostly specify only a small subset of the available attributes and large ranges of attribute values. However, all the solutions are prone to significant load balancing issues, as the resources and queries are highly skewed and correlated. The implications of our findings for improving RD solutions are also discussed.
In this article we present a distributed system that stores name-to-address bindings and provides name resolution to a network of computers. This name system consists of a network of name services that are individually self-configuring and self-administering. The name service consists of an agent program that works in conjunction with the current implementation of the Domain Name System (DNS) program. The DNS agent program automatically configures the Berkeley Internet Name Domain (BIND) process during start-up and dynamically reconfigures and administers the BIND process based on the changing state of the network. The proposed name system offers high scalability and fault-tolerance capabilities and communicates using standard Internet protocols.
Despite the prognosed use of event correlation techniques for monitoring critical complex infrastructures or dealing with disasters in the physical world, little work exists on making event correlation systems themselves tolerant to failure. Existing systems either provide no guarantees on event deliveries, do not support multicast and thus provide no guarantees across individual processes, or then rely on centralized components or strong assumptions on the infrastructure. The FAIDECS system attempts to reconcile strong guarantees with practical performance in the presence of process crash failures. To that end, the FAIDECS system uses an overlay network with specific guarantees aligned with its proposed correlation language and guarantees. However, the language proposed lacks expressivity, and the system itself supports only very specific rigid semantics, incapable of supporting even fundamental features like sliding windows. After providing a comprehensive overview of the FAIDECS model and system, this article bridges the gap between strong guarantees and more established correlation languages and systems in several steps. First, we propose alternative semantics for several modules of the FAIDECS matching engine and revisit guarantees. Second, we pinpoint which guarantees are contradicted by which combinations of semantic options. Third, we investigate four correlation languages�StreamSQL, EQL, CEL, and TESLA�showing which semantic options their respective features correspond to in our model, and thus, ultimately, which guarantees of FAIDECS are maintained by which language features.
Support for distributed application management in large-scale networked environments remains in its early stages. Although a number of solutions exist for subtasks of application deployment, monitoring, and maintenance in distributed environments, few tools provide a unified framework for application management. Many of the existing tools address the management needs of a single type of application or service that runs in a specific environment, and these tools are not adaptable enough to be used for other applications or platforms. To this end, we present the design and implementation of Plush, a fully configurable application management infrastructure designed to meet the general requirements of several different classes of distributed applications. Plush allows developers to specifically define the flow of control needed by their computations using application building blocks. Through an extensible resource management interface, Plush supports execution in a variety of environments, including both live deployment platforms and emulated clusters. Plush also uses relaxed synchronization primitives for improving fault tolerance and liveness in failure-prone environments. To gain an understanding of how Plush manages different classes of distributed applications, we take a closer look at specific applications and evaluate how Plush provides support for each.
This paper provides a way to think formally about the aggregation processes that take place in networks where individual actors (whether sensors, robots, or people) possess data whose value may decay over time. The various actors use data to make decisions: the larger the value, the better (i.e. more informed) the decision. At every moment, individual actors have the choice of making a decision or else to defer the decision to a later time. However, the longer they wait, the lower the value of the data they hold. To counter-balance the effect of time discounting, we define an algebraic operation that we call aggregation, whereby two or more actors integrate their data in the hope of increasing its value. Our main contribution is a formal look at the value of time-discounted information and at the algebra of its aggregation. We allow aggregation of time-discounted information to proceed in an arbitrary, not necessarily pairwise, manner. Our model relates aggregation decisions to the ensuing value of information and suggests natural thresholding strategies for the aggregation of the information collected by sets of network actors. Extensive simulations have confirmed the accuracy of our theoretical predictions.
The design of Ripple-2, a wireless in-situ soil moisture sensing system is presented in this paper. The main objective of such system is to collect high fidelity and fine grained data both spatially and temporally compared to radar remote sensing, which is the more traditional way of capturing soil moisture, and to use the former to validate and calibrate the latter. To do so, the in-site sensor network must cover a sufficiently large area, on the order of at least a few square kilometers. At the same time, cost constraints (both in deployment and in maintenance) puts a limit on the total number of sensor nodes, resulting in a very sparse (on average) network. The main challenge in designing the system lies in achieving reliability and energy efficiency in such a sparse network. For instance, in our pilot deployment, a 200mx400m area is covered by 22 nodes (average inter-node distance > 50m). Traditional WSN technology typically calls for many more nodes to be deployed in such an area. Ripple-2 is introduced as a non-traditional WSN architecture where (1) the network is physically and logically segmented into isolated clusters, (2) a regular node (or end device, ED) only communicates with the cluster head (CH) of its segment, and (3) the ED-CH communication is distinct from the CH-sink (or CH-Data Server) and both links can use virtually any kind of point-to-point wireless technology. We use both simulated and empirical results to demonstrate the effectiveness of Ripple-2; it proves to be ideal for low duty-cycle data collection applications due to its exceptional small network overhead (typically smaller than 1%) and its robustness to the size of the network.
One factor that has limited the use of Bluetooth as a networking technology for publicly accessible mobile services is the way in which it handles Device Discovery. Establishing a Bluetooth connection between two devices that have not seen each other before is slow and, from a usability perspective, often awkward. In this paper we present the implementation of an end-to-end Bluetooth-based mobile service framework designed specifically to address this issue. Rather than using the standard Bluetooth Device Discovery model to detect nearby mobile services, our system relies on machine-readable visual tags for out-of-band device and service selection. Our work is motivated by the recent proliferation of cameraphones and PDAs with built-in cameras. We have implemented the described framework completely for Nokia Series 60 cameraphones and demonstrated that our tag-based connection-establishment technique (i) offers order of magnitude time improvements over the standard Bluetooth Device Discovery model; and (ii) is significantly easier to use in a variety of realistic scenarios. Our implementation is available for free download.
To operate in dynamic and potentially unknown environments a mobile client must first discover the local services that match its requirements, and then interact with these services to obtain the application functionality. However, high levels of heterogeneity characterize mobile environments; that is, contrasting discovery protocols including SLP, UPnP and Jini, and different styles of service interaction paradigms e.g. Remote Procedure Call, Publish-Subscribe and agent based solutions. Therefore given this type of heterogeneity, utilizing single discovery and interaction systems is not optimal as the client will only be able to use the services available to that particular platform. Hence, in this paper we present an adaptive middleware solution to this problem. ReMMoC is a Web-Services based reflective middleware that allows mobile clients to be developed independently of both discovery and interaction mechanisms. We describe the architecture, which dynamically reconfigures to match the current service environment. Finally, we investigate the incurred performance overhead such dynamic behaviour brings to the discovery and interaction process.
Vehicular networking has significant potential to enable diverse range of applications, including safety and convenience. As the number of vehicles and applications using the specially designated wireless spectrum grow, one can expect substantial increase in the bandwidth requirements. In this paper and demonstration, we advocate that dynamic spectrum access techniques facilitating the utilization of white spaces for vehicles will be the first step towards solving the expected spectrum shortage. In the demonstration, we will introduce field tests of a multi-hop inter-vehicle communication system with distributed and autonomous TV white space channel selection, performed in Japan from January to February 2012. Furthermore, in addition to the field test videos, we will show an indoor emulation of the entire system used during the field tests in the vehicles.
Context awareness enables applications to adapt themselves to their computing environment in order to better suit the needs of the user and the tasks. This paper describes a general middleware infrastructure for context collection and dissemination, realized as a Context Service. By way of two example applications, this paper also illustrates how context information provided by our context service can be exploited to enhance the user experience. These two applications are built upon the abstraction provided by the Context Service and thus help validate the design of this service. The first application, a Notification Dispatcher, uses context to route messages to the most appropriate communication device for a recipient. The second application, a context-aware content distribution system, uses context to predict users' access to web content, and uses these predictions to pre-process and pre-distribute content in order to reduce the access latency.
This article is part of a continuing series to discuss ongoing activities within the mobile ad hoc networking (manet) working group (WG) of the IETF. This article provides a brief overview of recent events.
In this paper, we describe a novel context based storage system that use context to manage user data and make it available to him based on his situation. First, we examine several existing systems that use context with documents. Subsequently, a new storage system is presented that uses context to aid in the capture of and access to documents in mobile environment. We describe file browser and calendar applications that we have developed within our mobile computing infrastructure utilizing the features of context based storage. Novel features of our system include the access rights mechanism for data and support for group activity.
We propose a non-persistent localization system using a self-organizing reference grid of autonomous robot systems. The key idea is to continuously maintain accurate relative positions between the robots using an enhanced mass spring relaxation model. The robots estimate distances between neighboring systems using an ultrasonic system, measuring both the time of flight based distance and the angle between the systems. The algorithm then adapts the local position of the robot in the grid according to its neighbors. We developed a mass spring relaxation model allowing to maintain a completely self-organizing reference grid. In mass spring, newly arriving nodes can introduce oscillations and self-localization might fail or take a long time to converge. Therefore, we first use the available grid to localize the arriving system with reference to the grid before including the robot as a new reference point. Misplaced nodes are detected and corrected by our enhancements. In turn, the grid is able to provide accurate localization services, e.g. for flying robots.
The most intuitive memory model for shared-memory multi-threaded programming is sequential consistency (SC), but it disallows the use of many compiler and hardware optimizations and thus affects performance. Data-race-free (DRF) models, such as the C++11 memory model, guarantee SC execution for data-race-free programs. But these models provide no guarantee at all for racy programs, compromising the safety and debuggability of such programs. To address the safety issue, the Java memory model, which is also based on the DRF model, provides a weak semantics for racy executions. However, this semantics is subtle and complex, making it difficult for programmers to reason about their programs and for compiler writers to ensure the correctness of compiler optimizations. We present the drfx memory model, which is simple for programmers to understand and use while still supporting many common optimizations. We introduce a memory model (MM) exception that can be signaled to halt execution. If a program executes without throwing this exception, then drfx guarantees that the execution is SC. If a program throws an MM exception during an execution, then drfx guarantees that the program has a data race. We observe that SC violations can be detected in hardware through a lightweight form of conflict detection. Furthermore, our model safely allows aggressive compiler and hardware optimizations within compiler-designated program regions. We formalize our memory model, prove several properties of this model, describe a compiler and hardware design suitable for drfx, and evaluate the performance overhead due to our compiler and hardware requirements.
An unsound claim can misdirect a field, encouraging the pursuit of unworthy ideas and the abandonment of promising ideas. An inadequate description of a claim can make it difficult to reason about the claim, for example, to determine whether the claim is sound. Many practitioners will acknowledge the threat of unsound claims or inadequate descriptions of claims to their field. We believe that this situation is exacerbated, and even encouraged, by the lack of a systematic approach to exploring, exposing, and addressing the source of unsound claims and poor exposition. This article proposes a framework that identifies three sins of reasoning that lead to unsound claims and two sins of exposition that lead to poorly described claims and evaluations. Sins of exposition obfuscate the objective of determining whether or not a claim is sound, while sins of reasoning lead directly to unsound claims. Our framework provides practitioners with a principled way of critiquing the integrity of their own work and the work of others. We hope that this will help individuals conduct better science and encourage a cultural shift in our research community to identify and promulgate sound claims.
Dynamic software updating (DSU) systems facilitate software updates to running programs, thereby permitting developers to add features and fix bugs without downtime. This article introduces Kitsune, a DSU system for C. Kitsune�s design has three notable features. First, Kitsune updates the whole program, rather than individual functions, using a mechanism that places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune makes the important aspects of updating explicit in the program text, making the program�s semantics easy to understand while minimizing programmer effort. Finally, the programmer can write simple specifications to direct Kitsune to generate code that traverses and transforms old-version state for use by new code; such state transformation is often necessary and is significantly more difficult in prior DSU systems. We have used Kitsune to update six popular, open-source, single- and multithreaded programs and find that few program changes are required to use Kitsune, that it incurs essentially no performance overhead, and that update times are fast.
We investigate the problem of evaluating Fortran 90-style array expressions on massively parallel distributed-memory machines. On such a machine, an elementwise operation can be performed in constant time for arrays whose corresponding elements are in the same processor. If the arrays are not aligned in this manner, the cost of aligning them is part of the cost of evaluating the expression tree. The choice of where to perform the operation then affects this cost.We describe the communication cost of the parallel machine theoretically as a metric space; we model the alignment problem as that of finding a minimum-cost embedding of the expression tree into this space. We present algorithms based on dynamic programming that solve the embedding problem optimally for several communication cost metrics: multidimensional grids and rings, hypercubes, fat-trees, and the discrete metric. We also extend our approach to handle operations that change the shape of the arrays.
This paper presents the material needed for exposing the reader to the advantages of using Prolog as a language for describing succinctly most of the algorithms needed in prototyping and implementing compilers or producing tools that facilitate this task. The available published material on the subject describes one particular approach in implementing compilers using Prolog. It consists of coupling actions to recursive descent parsers to produce syntax-trees which are subsequently utilized in guiding the generation of assembly language code. Although this remains a worthwhile approach, there is a host of possibilities for Prolog usage in compiler construction. The primary aim of this paper is to demonstrate the use of Prolog in parsing and compiling. A second, but equally important, goal of this paper is to show that Prolog is a labor-saving tool in prototyping and implementing many non-numerical algorithms which arise in compiling, and whose description using Prolog is not available in the literature. The paper discusses the use of unification and nondeterminism in compiler writing as well as means to bypass these (costly) features when they are deemed unnecessary. Topics covered include bottom-up and top-down parsers, syntax-directed translation, grammar properties, parser generation, code generation, and optimizations. Newly proposed features that are useful in compiler construction are also discussed. A knowledge of Prolog is assumed.
In this article we present a compiler-based technique to help develop correct real-time systems. The domain we consider is that of multiprogrammed real-time applications, in which periodic tasks control physical systems via interacting with external sensors and actuators. While a system is up and running, these operations must be performed as specified�otherwise the system may fail. Correctness depends not only on each program individually, but also on the time-multiplexed behavior of all of the programs running together. Errors due to overloaded resources are exposed very late in a development process, and often at runtime. They are usually remedied by human-intensive activities such as instrumentation, measurement, code tuning and redesign. We describe a static alternative to this process, which relies on well-accepted technologies from optimizing compilers and fixed-priority scheduling. Specifically, when a set of tasks are found to be overloaded, a scheduling analyzer determines candidate tasks to be transformed via program slicing. The slicing engine decomposes each of the selected tasks into two fragments: one that is �time critical� and the other �unobservable.� The unobservable part is then spliced to the end of the time-critical code, with the external semantics being maintained. The benefit is that the scheduler may postpone the unobservable code beyond its original deadline, which can enhance overall schedulability. While the optimization is completely local, the improvement is realized globally, for the entire task set.
As shared-memory multiprocessor systems become widely available, there is an increasing need for tools to simplify the task of developing parallel programs. This paper describes one such tool, the automatic parallelization system in the Stanford SUIF compiler. This article represents a culmination of a several-year research effort aimed at making parallelizing compilers significantly more effective. We have developed a system that performs full interprocedural parallelization analyses, including array privatization analysis, array reduction recognition, and a suite of scalar data-flow analyses including symbolic analysis. These analyses collaborate in an integrated fashion to exploit coarse-grain parallel loops, computationally intensive loops that can execute on multiple processors independently with no cross-processor synchronization or communication. The system has successfully parallelized large interprocedural loops over a thousand lines of code completely automatically from sequential applications.This article provides a comprehensive description of the analyses in the SUIF system. We also present extensive empirical results on four benchmark suites, showing the contribution of individual analysis techniques both in executing more of the computation in parallel, and in increasing the granularity of the parallel computations. These results demonstrate the importance of interprocedural array data-flow analysis, array privatization and array reduction recognition; a third of the programs spend more than 50&percent; of their execution time in computations that are parallelized with these techniques. Overall, these results indicate that automatic parallelization can be effective on sequential scientific computations, but only if the compiler incorporates all of these analyses.
A new technique for computer-to-computer communication is presented that can increase the performance of distributed systems. This technique, called remote evaluation, lets one computer send another computer a request in the form of a program. A computer that receives such a request executes the program in the request and returns the results to the sending computer. Remote evaluation provides a new degree of flexibility in the design of distributed systems. In present distributed systems that use remote procedure calls, server computers are designed to offer a fixed set of services. In a system that uses remote evaluation, server computers are more properly viewed as programmable processors. One consequence of this flexibility is that remote evaluation can reduce the amount of communication that is required to accomplish a given task. In this paper we discuss the semantics of remote evaluation and its effect on distributed system design. We also summarize our experience with a prototype implementation.
Hand-held robotic surgical instruments are developed to acquire the maneuvered hand motion of the surgeon and then provide a control signal for real-time compensation of the physiological tremor in three-dimensional (3-D) space. For active tremor compensation, accurate modeling and estimation of physiological tremor is essential. The current modeling techniques that models tremor in 3D space consider the motion in three-axes [x, y, and z axes) as three separate one-dimensional signals and then perform modeling separately. Recently, it has been shown that for physiological tremor motion there exists cross dimensional coupling and it improves the modeling accuracy. Motivated by this, a quaternion variant for extreme learning machines is developed for accurate 3D modeling of tremor. The developed method is validated with real tremor data and the obtained results highlighted the suitability of this method for accurate tremor modeling in 3D space.
Indoor localization system based on received signal strength (RSS) often operate under non-line-of-sight (NLOS) conditions that can cause ranging errors. To identify non-line-of-sight status and line-of-sight (LOS) status and improve the accuracy of indoor localization, a D-S evidence theory based NLOS identification algorithm was proposed. The algorithm is divided into two parts. In the first part, respectively extract a variety of RSS features in NLOS and LOS status and choose the appropriate filter for data processing; in the second part, RSS features fused by D-S evidence theory are used to identify NLOS and LOS status. In an actual test environment, the algorithm was applied to fingerprinting localization, then analyzed the advantages of fusing RSS features and the influence of RSS features selection on experimental results. Finally, compared with several commonly used NLOS and LOS identification algorithm, it shows the proposed algorithm can deal with the indoor localization environment with obstacles with a high localization accuracy and good stability.
Pilot real time decision has a significant influence on the aircraft battlefield survivability. The ability of datalink is transmitting and sharing information which will provide support to pilot to make real time decision. In this paper, an algorithm is proposed to build a pilot real time decision model considering the performance of datalink. Firstly, an active deception jamming model is built which is used between the aircraft and the radar guided missile. Secondly, a Link16 model is built by OPENT and the throughput of two different slot assignment schemes is obtained by the simulation. Thirdly, a pilot real time decision model is built based on the influence diagram. The example shows that: the throughput will increase as the repetition rate increases which will lead that the pilot has a low level mental pressure and will obtain an optimal decision and the aircraft also has a high battlefield survivability.
Recommender systems predict user preferences based on a range of available information. For systems in which users generate streams of content (e.g., blogs, periodically-updated newsfeeds), users may rate the produced content that they read, and be given accurate predictions about future content they are most likely to prefer. We design a distributed mechanism for predicting user ratings that avoids the disclosure of information to a centralized authority or an untrusted third party: users disclose the rating they give to certain content only to the user that produced this content. We demonstrate how rating prediction in this context can be formulated as a matrix factorization problem. Using this intuition, we propose a distributed gradient descent algorithm for its solution that abides with the above restriction on how information is exchanged between users. We formally analyse the convergence properties of this algorithm, showing that it reduces a weighted root mean square error of the accuracy of predictions. Although our algorithm may be used many different ways, we evaluate it on the Neflix data set and prediction problem as a benchmark. In addition to the improved privacy properties that stem from its distributed nature, our algorithm is competitive with current centralized solutions. Finally, we demonstrate the algorithm's fast convergence in practice by conducting an online experiment with a prototype user-generated content exchange system implemented as a Facebook application.
PiMiair.org is a participatory indoor air quality data sharing project we launched in January 2014. Over 200 PiMi air boxes, a low-cost indoor air quality monitor, were given out to volunteer users across China. The PiMi air boxes measure the approximate indoor particulate matter concentration, and the ambient temperate and humidity. When a user accesses the PiMi air box for his personal air quality data on his smartphone, the data is relayed to the backend PiMi cloud server for analysis. Accumulating large amount of indoor air quality data under different circumstances, the PiMi cloud server is able to use statistical learning methodologies to detect point of interests (POIs) in the data series, and asks users to label their activities or events at the POIs. Together with the user-reported physicality information on the indoor environments, PiMiair.org is able to quantitatively evaluate the impacts of the environment physicality and human behaviors on the indoor air quality, and mine the knowledges on how to alleviate indoor air pollution. We believe that by sharing these knowledge among the community, healthier breathing environments could be nurtured for the well-being of the public.
The continuous fluctuation of electric network frequency (ENF) presents a fingerprint indicative of time, which we call natural timestamp. This paper studies the time accuracy of these natural timestamps obtained from powerline electromagnetic radiation (EMR), which is mainly excited by powerline voltage oscillations at the rate of the ENF. However, since the EMR signal is often weak and noisy, extracting the ENF is challenging, especially on resource-limited sensor platforms. We design an efficient EMR conditioning algorithm and evaluate the time accuracy of EMR natural timestamps on two representative classes of IoT platforms - a high-end single-board computer with a customized EMR antenna and a low-end mote with a normal conductor wire acting as EMR antenna. Extensive measurements at five sites in a city, which are away from each other for up to 24 km, show that the high-end and low-end nodes achieve median time errors of about 50 ms and 150 ms, respectively. To demonstrate the use of the EMR natural timestamps, we discuss two applications, namely time recovery and runtime clock verification.
As the compensation and extension of static wireless sensor nodes, wearable helmets can build regional early warning network of personnel security. In this paper, a wearable helmet is presented towards early warning of leaking toxic gas in large-scale petrochemical plants for protecting the lives and safety of workers better.
Sensor data acquired from multiple sensors simultaneously is featuring increasingly in our evermore pervasive world. Buildings can be made smarter and more efficient, spaces more responsive to users. A fundamental building block towards smart spaces is the ability to understand who is present in a certain area. A ubiquitous way of detecting this is to exploit the unique vocal features as people interact with one another. As an example, consider audio features sampled during a meeting, yielding a noisy set of possible voiceprints. With a number of meetings and knowledge of participation (e.g. through a calendar or MAC address), can we learn to associate a specific identity with a particular voiceprint? Obviously enrolling users into a biometric database is time-consuming and not robust to vocal deviations over time. To address this problem, the standard approach is to perform a clustering step (e.g. of audio data) followed by a data association step, when identity-rich sensor data is available. In this paper we show that this approach is not robust to noise in either type of sensor stream; to tackle this issue we propose a novel algorithm that jointly optimises the clustering and association process yielding up to three times higher identification precision than approaches that execute these steps sequentially. We demonstrate the performance benefits of our approach in two case studies, one with acoustic and MAC datasets that we collected from meetings in a non-residential building, and another from an online dataset from recorded radio interviews.
A mobile phone, as a pervasive device, has great potential in human wellness monitoring. In this demo, we first present the design and implementation of our hardware - SEPTIMU. SEPTIMU consists of a small baseboard and a pair of tiny sensor boards embedded inside conventional earphones. The baseboard provides power conversion and data communication through the normal audio jack interface. The embedded sensor board is 1�1cm2 and integrates 3-axis accelerometer, gyroscope, thermometer, photodiode and microphone. Secondly, we evaluate SEPTIMU using a mobile application that continuously monitors body posture and provides feedback to the user.
Pyroelectric Infra-Red (PIR) sensors are used in many applications including security. PIRs detect the presence of humans and animals from the radiation of their body heat. This could be used to trigger events, e.g., opening doors, recording video, etc. PIRs are used widely because of their low power consumption. Hitherto, PIR sensors were used for binary event generation -- human/animal present or not-present. At the same time simple binary output hinders the use of PIR sensors in a wide variety of sophisticated applications. In the literature, we find limited characterization of analog output from PIR sensors that could provide much more information. We built a simple array of PIR sensors and packaged them in a tower. We used two sets of four PIR sensors and tapped their analog signals after amplification. Our major contribution is the characterization of analog signals from the PIR sensors. We describe many interesting aspects obtained from the analog signals, which have not been explored until now. We also show their correspondence with the range, speed and size of the moving object. Using the characterization of PIR sensors analog data as well as simple binary decisions from these PIR sensors, we: (i) classify moving object with high precision; and (ii) localize the moving object. The major incentives are low operating power compared to WSNs. We achieve 30 cm accuracy in 80% of the times, when ranging up to 5 m. Over multiple experiments for different persons in the range 1--10 m, we show that the error probability for localization is 0.08 at moderate distances (around 5--6 m). Our work will help in designing better detection and application triggers using PIR sensors in the near future. We believe that this work will open up new avenues in the development of new applications with PIR sensors.
Abstract--Manufacturers announce on a regular basis the availability of novel tiny devices, most of them featuring network interfaces: the Internet of Things (IoT) is already here -- from the hardware perspective. On the software side however, embedded platforms available so far made it uneasy for developers to build apps that run across heterogeneous IoT hardware. Linux does not scale down to small, energy-constrained devices, while alternatives such as Contiki yield a steep learning curve and lengthy development life-cycles because they rule out standard programming and debugging tools. RIOT is a new open source software platform bridging this gap. RIOT allows just about any programmer to develop IoT application with zero learning curve. This is achieved by allowing standard C and C++ application programming with multi-threading, using well-known debugging tools (gdb, Valgrind, profilers etc.), while requiring only a minimum of 1.5 kB of RAM. RIOT also provides built-in energy efficiency and real-time capabilities. These characteristics make this platform attractive in several contexts, including teaching in the field of the Internet of Things, and experimental research in the domain of sensor networks and the IoT.
This paper focuses on using wearable equipment to make workers participate in constructing monitoring maps in large-scale petrochemical plants, which collaborates with static sensor nodes in given areas. Furthermore, this study provides a sensing pattern with less cost and higher flexibility, which effectively contribute to collective effort with static sensor nodes. Several open research issues are discussed.
This paper presents a smart badge system for managing museum visitors and improving their viewing experience. The badge uses a LoRa wireless interface for two-way long-distance communication with tour guides and the museum's control center. The control center can collect information such as the visitors' viewing p, location, and status of activity level, enabling it to wirelessly send suggestions of exhibits and visiting paths to visitors, avoiding the overcrowding of visitors in one location. Meanwhile, visitors can communicate with the control center to update their locations and ask simple questions via the badge. In this paper, we introduce the system architecture and the prototype smart badge that we used to evaluate the performance of this system.
Radio frequency based device-free passive (DfP) localization techniques have shown great potentials in localizing individual human subjects, without requiring them to carry any radio devices. In this study, we extend the DfP technique to count and localize multiple subjects in indoor environments. To address the impact of multipath on indoor radio signals, we adopt a fingerprinting based approach to infer subject locations from observed signal strengths through profiling the environment. When multiple subjects are present, our objective is to use the profiling data collected by a single subject to count and localize multiple subjects without any extra effort. In order to address the non-linearity of the impact of multiple subjects, we propose a successive cancellation based algorithm to iteratively determine the number of subjects. We model indoor human trajectories as a state transition process, exploit indoor human mobility constraints and integrate all information into a conditional random field (CRF) to simultaneously localize multiple subjects. As a result, we call the proposed algorithm SCPL -- sequential counting, parallel localizing. We test SCPL with two different indoor settings, one with size 150 m2 and the other 400 m2. In each setting, we have four different subjects, walking around in the deployed areas, sometimes with overlapping trajectories. Through extensive experimental results, we show that SCPL can count the present subjects with 86% counting percentage when their trajectories are not completely overlapping. Our localization algorithms are also highly accurate, with an average localization error distance of 1.3 m.
We present SiCILIA, a hardware platform that exploits the physical and environmental variables of an individual's thermal environment to infer the amount of clothing insulation and thermal sensation without intervention or feedback. In this demo, we present the inference algorithms that build upon theories of body heat transfer. Using a single Micro-controller (an Arduino Uno), a dual-axis motor, an IR sensor, and an Ultrasonic Range Finder (URF) we will demonstrate SiCILIA's capability of inferring an individual's clothing insulation level correctly and effectively.
We introduce the Concurrent Activity Recognizer (CAR) - an efficient deep learning structure that recognizes complex concurrent teamwork activities from multimodal data. We implemented the system in a challenging medical setting, where it recognizes 35 different activities using Kinect depth video and data from passive RFID tags on 25 types of medical objects. Our preliminary results showed our system achieved an 84% average accuracy with 0.20 F1-Score.
Road lighting management, i.e., finding a faulty street lamp or street lamps blocked by overgrown trees for example, can improve road illumination and reduce traffic accidents. Based on previous work [1], which explores the use of illumination maps to detect faulty lamps, we present a road lighting management system for detecting significant changes in road lighting automatically. This system is characterized by eliminating any need to modify conventional street lamps. Therefore, the cost is extremely low compared to other approaches, e.g., Wireless sensor network for street lamp monitoring. We design a special embedded system, the Hitchhiker, for installation on fixed-route vehicles, such as shuttle buses, to collect illumination readings along the vehicle's route. All data from the Hitchhiker is uploaded to the Google App Engine (GAE) server for data storage, analysis, and user query; this allows identification of possible locations of faulty street lamps on the web-based management interface. We designed and implemented a prototype of the Hitchhiker and GAE server to demonstrate the practicability of this system; we are working toward enabling this design for automatic faulty street lamp detection in the near future.
This demo presents the world's smallest wireless motion-sensing platform based on Bluetooth 4.0 Low Energy (BLE) Technology. It is merely 8 x 8 mm2 in area but is complete with a user-programmable microcontroller and integrated RF, a digital triaxial accelerometer with programmable threshold detection, and a real-time clock and calendar chip, and a magnetic sensor/switch. This system has been used in a number of applications, including a proximity tag, a pedometer, an air mouse with gesture recognition, and a BLE-to-IR remote controller.
Precision ball screw assembly (hereafter called "ball screw"), as shown in Fig. 1, is a mechanical wear out part that widely used in CNC (computer numerical control) machine tools to control the movement of processing targets and spindles. Up until now, there has been no simple way to directly measure ball screw for knowing the state of wear quantitatively. An indirect approach is logging all the signals (vibration, temperature, and preload change) during the operation of ball screw, and to use them to construct the wear model for estimating its remaining lifetime. To achieve this goal, we proposed a cloud-based logging system in this study that emphasizes (1) logging all the signals during operation in a ball screw's whole lifetime, and transferring to the data server without data loss; and (2) saving all the data into the cloud data storage of the ball screw's manufacturer. The data collected from many ball screws can be used to analyze and construct the wear model of ball screw, allowing the manufacturer to understand the state of wear and send a warning to the tool machine's owner before excessive wear.
In this paper, we are interested in the 3D through-wall imaging of a completely unknown area, using WiFi RSSI and Unmanned Aerial Vehicles (UAVs) that move outside of the area of interest to collect WiFi measurements. It is challenging to estimate a volume represented by an extremely high number of voxels with a small number of measurements. Yet many applications are time-critical and/or limited on resources, precluding extensive measurement collection. In this paper, we then propose an approach based on Markov random field modeling, loopy belief propagation, and sparse signal processing for 3D imaging based on wireless power measurements. Furthermore, we show how to design efficient aerial routes that are informative for 3D imaging. Finally, we design and implement a complete experimental testbed and show high-quality 3D robotic through-wall imaging of unknown areas with less than 4% of measurements.
Following Baryshnikov-Coffman-Kwak [1], we use network cyclic cellular automata to generate a decentralized protocol, with only a small fraction of sensors awake. The work here shows that waves of awake-state nodes turn corners and automatically solve pusuit/evasion-type problems without centralized coordination.
There is a growing number of examples that use the microphones in phone for various acoustic processing tasks as mobile phones become increasingly computationally powerful. However, there is no general physiological acoustic anomaly detection service on smartphones. To this end, we propose a physiological acoustic anomaly detection service which contains classifiers that can be used to detect irregularity and anomalies in lung sounds and notifies the user. We also present and discuss on some preliminary results.
Topology management schemes conserve energy in wireless ad hoc networks by identifying redundant nodes that may turn off their radios or other components while maintaining connectivity. We present Naps, a randomized topology management scheme that does not rely on geographic location information, provides exibility in the target density of waking nodes, and sends only a periodic heartbeat message between waking neighbors; thus it is implementable even on modest hardware. We formally analyze the connectivity of the waking graphs produced by Naps, showing that these graphs have nearly complete connectivity even at relatively low densities. We examine simulation results for a wide range of initial deployment densities and for heterogeneous and mobile deployments.
The continuous fluctuation of electric network frequency (ENF) presents a fingerprint indicative of time, which we call natural timestamp. This live demo demonstrates the accuracy of the natural timestamps obtained by four wired voltage sensors and four wireless electromagnetic radiation (EMR) sensors that are geographically distributed in Singapore. The voltage sensors and the EMR sensors capture the minute fluctuations of the length of each voltage cycle and the average ENF over every 50 voltage cycles, respectively. The evaluation in our prior studies [1, 3] has shown that the natural timestamps recorded by the voltage sensors and the EMR sensors give sub-millisecond and sub-second average time errors, respectively. This demo will also show their time errors.
The monitoring of electrical and water fixtures in the home is being applied for a variety of "smart home" applications, such as recognizing activities of daily living (ADLs) or conserving energy or water usage. Fixture monitoring techniques generally fall into two categories: fixture recognition and fixture disaggregation. However, existing techniques require users to explicitly identify each individual fixture, either by placing a sensor on it or by manually creating training data for it. In this paper, we present a new fixture discovery system that automatically infers the existence of electrical and water fixtures in the home. We call the system FixtureFinder. The basic idea is to use data fusion between the smart meters and other sensors or infrastructure already in the home, such as the home security or automation system, and to find repeating patterns in the fused data stream. To evaluate FixtureFinder, we deployed the system into 4 different homes for 7-10 days of data collection. Our results show that FixtureFinder is able to identify and differentiate major light and water fixtures in less than 10 days, including multiple copies of light bulbs and sinks that have identical power/water profiles.
Truck weight data is used in many areas of transportation such as weight enforcement and pavement condition assessment. This paper describes a wireless sensor network (WSN) that estimates the weight of moving vehicles from pavement vibrations caused by vehicular motion. The WSN consists of: acceleration sensors that report pavement vibration; vehicle detection sensors that report a vehicle's arrival and departure times; and an access point (AP) that synchronizes all the sensors and records the sensor data. The paper also describes a novel algorithm that estimates a vehicle's weight from pavement vibration and vehicle detection data, and calculates pavement deflection in the process. A prototype of the system has been deployed near a conventional Weigh-In-Motion (WIM) system on I-80 W in Pinole, CA. Weights of 52 trucks at different speeds and loads were estimated by the system under different pavement temperatures and varying environmental conditions, adding to the challenges the system must overcome. The error in load estimates was less than 10% for gross weight and 15% for individual axle weights. Different states have different requirements for WIM but the system described here outperformed the nearby conventional WIM, and meets commonly used standards in United States. The system also opens up exciting new opportunities for WSNs in pavement engineering and intelligent transportation.
In this paper, we present Intercom, a simulator framework that provides separate components to address the interdependent aspects of IoT systems, such as sensing, physical interaction, wireless communication, and computation. We initially evaluate a scalable sensing and communication model, which simulates wireless signal strength measurements with an average error of 6.1dBm.
We present the results, experiences and lessons learned from comparing a diverse set of technical approaches to indoor localization during the 2014 Microsoft Indoor Localization Competition. 22 different solutions to indoor localization from different teams around the world were put to test in the same unfamiliar space over the course of 2 days, allowing us to directly compare the accuracy and overhead of various technologies. In this paper, we provide a detailed analysis of the evaluation study's results, discuss the current state-of-the-art in indoor localization, and highlight the areas that, based on our experience from organizing this event, need to be improved to enable the adoption of indoor location services.
An ultra-small, low-power sensor-net module called ZN1 is developed. ZN1 integrates MCU, ZigBee RF into an ultra-small 15 -- 15-mm module with an ultra-low standby current less than 1 ?A. The and highly reliable operation for more than 15 years of the ZN1 enable low-cost practical application-specific sensor nodes for HVAC, structure monitoring, office management, security, medicine, and healthcare. We describe architecture and technologies for developing the ZN1 module. Moreover, we demonstrate two applications that we developed simultaneously: food sanitation and home healthcare. We also show results of the ZN1 module evaluation and discuss reliability of the ZN1.
Acoustic source localization based on time difference of arrival (TDOA) measurements from spatially separated sensors is an important problem in wireless sensor networks (WSNs). While extensive research works have been performed on algorithm development, limited attention has been paid in how to form the sensor pairs. In the literature, most of the works adopt a centralized sensor pairing strategy, where only one common sensor node is chosen as the reference. However, due to the multi-hop nature of WSNs, it is well known that this kind of centralized signal processing method is power consuming since raw measurement data is involved in the transmissions. To reduce the requirements for both network bandwidth and power consumptions, we propose an in-network sensor pairing method to collect the TDOA measurements while guaranteeing the quality of source localization. The solution involves finding a minimal sized dominating set (MSDS) for a graph of the muti-hop network. It has been proved that in-network sensor pairing can result in the same Cramer-Rao-Bound (CRB) as the centralized one but at a far more less communication cost. Furthermore, the structure of the proposed in-network sensor pairing coincides with the decentralized source localization, which is an important application of our method.
Steerable surveillance cameras offer a unique opportunity to support multiple vision applications simultaneously. However, state-of-art camera systems do not support this as they are often limited to one application per camera. We believe that we should break the one-to-one binding between the steerable camera and the application. By doing this we can quickly move the camera to a new view needed to support a different vision application. When done well, the scheduling algorithm can support a larger number of applications over an existing network of surveillance cameras. With this in mind we developed Panoptes, a technique that virtualizes a camera view and presents a different fixed view to different applications. A scheduler uses camera controls to move the camera appropriately providing the expected view for each application in a timely manner, minimizing the impact on application performance. Experiments with a live camera setup demonstrate that Panoptes can support multiple applications, capturing up to 80% more events of interest in a wide scene, compared to a fixed view camera.
In this paper we will describe a positioning system based on synchronized IR light. Each node will be assigned a timeslot where they will send out an IR light. There is an IR camera that is also synchronized to the timeslots that will detect the position of each node and the ID that corresponds to the timeslot. To synchronize the clock of all nodes an IR flashlight is sent out that is detected by a photodiode on the nodes. The demo will show live video stream from a network camera where the ID and position of each node in view will be overlaid in real-time in the video.
Solutions to outdoor air pollution require societal changes; however, we focus on indoor home air quality to allow for individual control over the breathing environment. We present AirFeed: a real time air quality monitoring system that provides measurements on particulate matter, temperature, and humidity. Interactions with users based on data analysis and user/sensor feedback form distinguishable patterns between several types of activities. We can better inform the user how daily habits affect living environments. Several deployments are actively collecting data for future data analysis and improved pattern recognition.
We present here a first practical energy distribution architecture that allows us to decouple energy supply from sensing activities in WSN. Such a separation of responsibilities enables us to utilize abundant energy sources distant from the sensing location, allowing unrestricted lifetime and resolving unequal energy consumption in WSN. We demonstrate energy transfer for practical decoupling using low-cost and -footprint, laser ?-power beaming that powers current WSN platforms at 100m of range. We design and implement LAMP: a tiered architecture to manage energy supply to both mesh and clustered WSN deployments using an energy distribution protocol. We evaluate our system to show that, for an additional cost of $29 per mote, LAMP can support perpetual mesh functionality for up to 40 sensors or 120 nodes in clustered operation.