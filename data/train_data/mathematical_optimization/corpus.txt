Visual sensor networks (VSNs) are becoming increasingly popular in a number of application domains. A distinguishing characteristic of VSNs is to self-configure to minimize the need for operator control and to improve scalability. One of the areas of self-configuration is camera coverage control that is, how should cameras adjust their field-of-views to cover maximum targets? This is an NP-hard problem. We show that the existing heuristics have a number of weaknesses that influence both coverage and overhead. Therefore, we first propose a computationally efficient centralized heuristic that provides near-optimal coverage for small-scale networks. However, it requires significant communication and computation overhead, making it unsuitable for large-scale networks. Thus, we develop a distributed algorithm that outperforms the existing distributed algorithm with lower communication overhead, at the cost of coverage accuracy. We show that the proposed heuristics guarantee to cover at least half of the targets covered by the optimal solution. Finally, to gain benefits of both centralized and distributed algorithms, we propose a hierarchical algorithm where cameras are decomposed into neighborhoods that coordinate their coverage using an elected local coordinator. We observe that the hierarchical algorithm provides scalable near-optimal coverage with networking cost significantly less than that of centralized and distributed solutions.
We present a robust, dynamic scheme for the automatic self-deployment and relocation of mobile sensor nodes (e.g., unmanned ground vehicles, robots) around areas where phenomena take place. Our scheme aims (i) to sense environmental contextual parameters and accurately capture the spatiotemporal evolution of a certain phenomenon (e.g., fire, air contamination) and (ii) to fully automate the deployment process by letting nodes relocate, self-organize (and self-reorganize), and optimally cover the focus area. Our intention is to �opportunistically� modify the previous placement of nodes to attain high-quality phenomenon monitoring. The required intelligence is fully distributed within the mobile sensor network so the deployment algorithm is executed incrementally by different nodes. The presented algorithm adopts the Particle Swarm Optimization technique, which yields very promising results as reported in the article (performance assessment). Our findings show that the proposed algorithm captures a certain phenomenon with very high accuracy while maintaining the networkwide energy expenditure at low levels. Random occurrences of similar phenomena put stress upon the algorithm which manages to react promptly and efficiently manage the available sensing resources in the broader setting.
Self-sustainability is a crucial step for modern sensor networks. Here, we offer an original and comprehensive framework for autonomous sensor networks powered by renewable energy sources. We decompose our design into two nested optimization steps: the inner step characterizes the optimal network operating point subject to an average energy consumption constraint, while the outer step provides online energy management policies that make the system energetically self-sufficient in the presence of unpredictable and intermittent energy sources. Our framework sheds new light into the design of pragmatic schemes for the control of energy-harvesting sensor networks and permits to gauge the impact of key sensor network parameters, such as the battery capacity, the harvester size, the information transmission rate, and the radio duty cycle. We analyze the robustness of the obtained energy management policies in the cases where the nodes have differing energy inflow statistics and where topology changes may occur, devising effective heuristics. Our energy management policies are finally evaluated considering real solar radiation traces, validating them against state-of-the-art solutions, and describing the impact of relevant design choices in terms of achievable network throughput and battery-level dynamics.
Acoustic source localization has many important applications. Convex relaxation provides a viable approach of obtaining good estimates very efficiently. There are two popular convex relaxation methods using either semi-definite programming (SDP) or second-order cone programming (SOCP). However, the performances of the methods have not been studied properly in the literature and there is no comparison in terms of accuracy and performance. The aims of this article are twofold. First of all, we study and compare several convex relaxation methods. We demonstrate, by numerical examples, that most of the convex relaxation methods cannot localize the source exactly, even in the performance limit when the time difference of arrival (TDOA) information is exact. In addressing this problem, we propose a novel mixed SDP-SOCP relaxation model and study the characteristics of the optimal solutions and its localizable region. Furthermore, an error correction scheme for the proposed SDP-SOCP model is developed so that exact localization can be achieved in the performance limit. Experimental data have been collected in a room with two different array configurations to demonstrate our proposed approach.
We present a method to simplify the implementation of a kernel supporting a high level concurrent language on a bare multiprocessor system.User and system level languages (with as much common syntax as possible) are defined. The system language consists of those constructs which do not require concurrency support, plus other facilities which make it well suited to write the kernel supporting the user language. Such constructs must provide the equivalent of a supervisor call interface within a high level language.All machine dependencies for both the system and user languages have been encapsulated in the code generation phase of the compilation process. Our method reduces the retargeting problem of a kernel to the orientability problem of the code generator.Finally our experiences in using such methodology are discussed.
In this article, we present an algorithmic system for determining the proper correspondence between place markers and their labels in historical maps. We assume that the locations of place markers (usually pictographs) and labels (pieces of text) have already been determined -- either algorithmically or by hand -- and we want to match the labels to the markers. This time-consuming step in the digitization process of historical maps is nontrivial even for humans but provides valuable metadata (e.g., when subsequently georeferencing the map). To speed up this process, we model the problem in terms of combinatorial optimization, solve that problem efficiently, and show how user interaction can be used to improve the quality of the results. We also consider a version of the model where we are given label fragments and additionally have to decide which fragments go together. We show that this problem is NP-hard. However, we give a polynomial-time algorithm for a restricted version of this fragment assignment problem. We have implemented the algorithm for the main problem and tested it on a manually extracted ground truth for eight historical maps with a combined total of more than 12,800 markers and labels. On average, the algorithm correctly matches 96% of the labels and is robust against noisy input. It furthermore performs a sensitivity analysis and in this way computes a measure of confidence for each of the matches. We use this as the basis for an interactive system where the user�s effort is directed to checking those parts of the map where the algorithm is unsure; any corrections the user makes are propagated by the algorithm. We discuss a prototype of this system and statistically confirm that it successfully locates those areas on the map where the algorithm needs help.
In this article, the expectation maximization (EM) algorithm is applied for modeling the throughput of emergency departments via available time-series data. The dynamics of emergency department throughput is developed and evaluated, for the first time, as a stochastic dynamic model that consists of the noisy measurement and first-order autoregressive (AR) stochastic dynamic process. By using the EM algorithm, the model parameters, the actual throughput, as well as the noise intensity, can be identified simultaneously. Four real-world time series collected from an emergency department in West London are employed to demonstrate the effectiveness of the introduced algorithm. Several quantitative indices are proposed to evaluate the inferred models. The simulation shows that the identified model fits the data very well.
A Stackelberg game is played between a leader and a follower. The leader first chooses an action, and then the follower plays his best response, and the goal of the leader is to pick the action that will maximize his payoff given the follower's best response. Stackelberg games capture, for example, the following interaction between a retailer and a buyer. The retailer chooses the prices of the goods he produces, and then the buyer chooses to buy a utility-maximizing bundle of goods. The goal of the retailer here is to set prices to maximize his profit---his revenue minus the production cost of the purchased bundle. It is quite natural that the retailer in this example would not know the buyer's utility function. However, he does have access to revealed preference feedback---he can set prices, and then observe the purchased bundle and his own profit. We give algorithms for efficiently solving, in terms of both computational and query complexity, a broad class of Stackelberg games in which the follower's utility function is unknown, using only "revealed preference" access to it. This class includes the profit maximization problem, as well as the optimal tolling problem in nonatomic congestion games, when the latency functions are unknown. Surprisingly, we are able to solve these problems even though the corresponding maximization problems are not concave in the leader's actions.
We identify and address algorithmic and game-theoretic issues arising from welfare maximization in the well-studied two-stage stochastic optimization framework. In contrast, prior work in algorithmic mechanism design has focused almost exclusively on optimization problems without uncertainty. We show both positive results, by demonstrating a mechanism that implements the social welfare maximizer in sequential ex post equilibrium, and also negative results, by showing the impossibility of dominant-strategy implementation. In this letter, we describe the relationship between mechanism design and stochastic optimization, and highlight our key technical results. An extended abstract will appear in WINE 2007, and a journal version is under preparation.
Ant Colony Optimization and Swarm Optimization are the classical areas of researches in the field of Computer Science. Computer Scientists are trying to map the Biological and Natural Solution with the Artificial one, since last 2 decades. Finally, Genetic Algorithm (GA), Genetic Programming (GP) and Artificial Neural Network (ANN), ACO[13], etc were derived from the Biological, Genetical and Natural characteristics of the living organisms.We have developed a population based stochastic optimization technique inspired by social behavior of Mosquito (Female). This research will open a new side of 'Particle Swarm Optimization', in future.
This article develops an affine-scaling method for linear programming in standard primal form. Its descent search directions are formulated in terms of the null-space of the linear programming matrix, which, in turn, is defined by a suitable basis matrix. We describe some basic properties of the method and an experimental implementation that employs a periodic basis change strategy in conjunction with inexact computation of the search direction by an iterative method, specifically, the conjugate-gradient method with diagonal preconditioning. The result of a numerical study on a number of nontrivial problems representative of problems that arise in practice are reported and discussed.A key advantage of the primal null-space affine-scaling method is its compatibility with the primal simplex method. This is considered in the concluding section, along with implications for the development of a more practical implementation.
This article presents BiqCrunch, an exact solver for binary quadratic optimization problems. BiqCrunch is a branch-and-bound method that uses an original, efficient semidefinite-optimization-based bounding procedure. It has been successfully tested on a variety of well-known combinatorial optimization problems, such as Max-Cut, Max-k-Cluster, and Max-Independent-Set. The code is publicly available online; a web interface and many conversion tools are also provided.
Understanding the behavior of Runge-Kutta codes when stability considerations restrict the stepsize provides useful information for stiffness detection and other implementation details. Analysis of equilibrium states on test problems is presented which provides predictions and insights into this behavior. The implications for global error are also discussed.
The most widely used ordering scheme to reduce fills and operations in sparse matrix computation is the minimum-degree algorithm. The notion of multiple elimination is introduced here as a modification to the conventional scheme. The motivation is discussed using the k-by-k grid model problem. Experimental results indicate that the modified version retains the fill-reducing property of (and is often better than) the original ordering algorithm and yet requires less computer time. The reduction in ordering time is problem dependent, and for some problems the modified algorithm can run a few times faster than existing implementations of the minimum-degree algorithm. The use of external degree in the algorithm is also introduced.
We present four basic Fortran subroutines for nondifferentiable optimization with simple bounds and general linear constraints. Subroutine PMIN, intended for minimax optimization, is based on a sequential quadratic programming variable metric algorithm. Subroutines PBUN and PNEW, intended for general nonsmooth problems, are based on bundle-type methods. Subroutine PVAR is based on special nonsmooth variable metric methods. Besides the description of methods and codes, we propose computational experiments which demonstrate the efficiency of this approach.
DSDP implements the dual-scaling algorithm for semidefinite programming. The source code for this interior-point algorithm, written entirely in ANSI C, is freely available under an open source license. The solver can be used as a subroutine library, as a function within the Matlab environment, or as an executable that reads and writes to data files. Initiated in 1997, DSDP has developed into an efficient and robust general-purpose solver for semidefinite programming. Its features include a convergence proof with polynomially bounded worst-case complexity, primal and dual feasible solutions when they exist, certificates of infeasibility when solutions do not exist, initial points that can be feasible or infeasible, relatively low memory requirements for an interior-point method, sparse and low-rank data structures, extensibility that allows applications to customize the solver and improve its performance, a subroutine library that enables it to be linked to larger applications, scalable performance for large problems on parallel architectures, and a well-documented interface and examples of its use. The package has been used in many applications and tested for efficiency, robustness, and ease of use.
We propose FORTRAN subroutines for approximately solving the feedback vertex and arc set problems on directed graphs using a Greedy Randomized Adaptive Search Procedure (GRASP). Implementation and usage of the package is outlined and computational experiments are reported illustrating solution quality as a function of running time.
In processing networks, ordinary network constraints are supplemented by proportional flow restrictions on arcs entering or leaving some nodes. This paper describes a new primal partitioning algorithm for solving pure processing networks using a working basis of variable dimension. In testing against MPSX/370 on a class of randomly generated problems, a FORTRAN implementation of this algorithm was found to be an order-of-magnitude faster. Besides indicating the use of our methods in stand-alone fashion, the computational results also demonstrate the desirability of using these methods as a high-level module in a mathematical programming system.
A procedure for generating non-differentiable, continuously differentiable, and twice continuously differentiable classes of test functions for multiextremal multidimensional box-constrained global optimization is presented. Each test class consists of 100 functions. Test functions are generated by defining a convex quadratic function systematically distorted by polynomials in order to introduce local minima. To determine a class, the user defines the following parameters: (i) problem dimension, (ii) number of local minima, (iii) value of the global minimum, (iv) radius of the attraction region of the global minimizer, (v) distance from the global minimizer to the vertex of the quadratic function. Then, all other necessary parameters are generated randomly for all 100 functions of the class. Full information about each test function including locations and values of all local minima is supplied to the user. Partial derivatives are also generated where possible.
This remark describes an improvement and a correction to Algorithm 778. It is shown that the performance of the algorithm can be improved significantly by making a relatively simple modification to the subspace minimization phase. The correction concerns an error caused by the use of routine dpmeps to estimate machine precision.
A new global optimization algorithm for functions of many continuous variables is presented, derived from the basic Simulated annealing method. Our main contribution lies in dealing with high-dimensionality minimization problems, which are often difficult to solve by all known minimization methods with or without gradient. In this article we take a special interest in the variables discretization issue. We also develop and implement several complementary stopping criteria. The original Metropolis iterative random search, which takes place in a Euclidean space Rn, is replaced by another similar exploration, performed within a succession of Euclidean spaces Rp, with p <<n. This Enhanced Simulated Annealing (ESA) algorithm was validated first on classical highly multimodal functions of 2 to 100 variables. We obtained significant reductions in the number of function evaluations compared to six other global optimization algorithms, selected according to previously published computational results for the same set of test functions. In most cases, ESA was able to closely approximate known global optima. The reduced ESA computational cost helped us to refine further the obtained global results, through the use of some local search. We have used this new minimizing procedure to solve complex circuit design problems, for which the objective function evaluation can be exceedingly costly.
Methods are known for the exact computation of the solution of integer systems of linear equations AX = B with a nonsingular coefficient matrix A by congruence techniques. These methods are now generalized for systems with an arbitrary integer coefficient matrix A. To make congruence techniques applicable, a common denominator of all elements of the solution X = A+B must be computed. This is achieved by defining the natural denominator CODE of A+ and describing it by some formulas. Methods for the exact computation of additional results (consistency, null space, solution of at most R nonzero elements), a recursive test to save computing time, and a comparison with some results from the literature are presented.
SFSDP is a Matlab package for solving sensor network localization (SNL) problems. These types of problems arise in monitoring and controlling applications using wireless sensor networks. SFSDP implements the semidefinite programming (SDP) relaxation proposed in Kim et al. [2009] for sensor network localization problems, as a sparse version of the full semidefinite programming relaxation (FSDP) by Biswas and Ye [2004]. To improve the efficiency of FSDP, SFSDP exploits the aggregated and correlative sparsity of a sensor network localization problem. As a result, SFSDP can handle much larger problems than other software as well as three-dimensional anchor-free problems. SFSDP analyzes the input data of a sensor network localization problem, solves the problem, and displays the computed locations of sensors. SFSDP also includes the features of generating test problems for numerical experiments.
This article proposes a competitive divide-and-conquer algorithm for solving large-scale black-box optimization problems for which there are thousands of decision variables and the algebraic models of the problems are unavailable. We focus on problems that are partially additively separable, since this type of problem can be further decomposed into a number of smaller independent subproblems. The proposed algorithm addresses two important issues in solving large-scale black-box optimization: (1) the identification of the independent subproblems without explicitly knowing the formula of the objective function and (2) the optimization of the identified black-box subproblems. First, a Global Differential Grouping (GDG) method is proposed to identify the independent subproblems. Then, a variant of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is adopted to solve the subproblems resulting from its rotation invariance property. GDG and CMA-ES work together under the cooperative co-evolution framework. The resultant algorithm, named CC-GDG-CMAES, is then evaluated on the CEC�2010 large-scale global optimization (LSGO) benchmark functions, which have a thousand decision variables and black-box objective functions. The experimental results show that, on most test functions evaluated in this study, GDG manages to obtain an ideal partition of the index set of the decision variables, and CC-GDG-CMAES outperforms the state-of-the-art results. Moreover, the competitive performance of the well-known CMA-ES is extended from low-dimensional to high-dimensional black-box problems.
An algorithm is presented for finding a root of a real function. The algorithm combines bisection with second and third order methods using derivatives estimated from objective function values. Globaql convergence is ensured and the number of function evaluations is bounded by four times the number needed by bisection. Numerical comparisons with existing algorithms indicate the superiority of the new algorithm in all classes of problems.
SparsePOP is a Matlab implementation of the sparse semidefinite programming (SDP) relaxation method for approximating a global optimal solution of a polynomial optimization problem (POP) proposed by Waki et al. [2006]. The sparse SDP relaxation exploits a sparse structure of polynomials in POPs when applying �a hierarchy of LMI relaxations of increasing dimensions� Lasserre [2006]. The efficiency of SparsePOP to approximate optimal solutions of POPs is thus increased, and larger-scale POPs can be handled.
A method is presented for the generation of test problems for global optimization algorithms. Given a bounded polyhedron in R and a vertex, the method constructs nonconvex quadratic functions (concave or indefinite) whose global minimum is attained at the selected vertex. The construction requires only the use of linear programming and linear systems of equations.
It has recently been shown that cancellation errors in a quasi-Newton method can increase without bound as the method converges. A simple test is presented to determine when cancellation errors could lead to significant contamination of the approximating matrix.
Dantzig--Wolfe Decomposition is recognized as a powerful, algorithmic tool for solving linear programs of block-angular form. While use of the approach has been reported in a wide variety of domains, there has not been a general implementation of Dantzig--Wolfe decomposition available. This article describes an open-source implementation of the algorithm. It is general in the sense that any properly decomposed linear program can be provided to the software for solving. While the original description of the algorithm was motivated by its reduced memory usage, modern computers can also take advantage of the algorithm's inherent parallelism. This implementation is parallel and built upon the POSIX threads (pthreads) library. Some computational results are provided to motivate use of such parallel solvers, as this implementation outperforms state-of-the-art commercial solvers in terms of wall-clock runtime by an order of magnitude or more on several problem instances.
Two modifications of Bartels and Conn's algorithm for solving linearly constrained discrete l1 problems are described. The modifications are designed to improve performance of the algorithm under conditions of degeneracy.
Object-oriented programming is a relatively new tool in the development of optimization software. The code extensibility and the rapid algorithm prototyping capability enabled by this programming paradigm promise to enhance the reliability, utility, and ease of use of optimization software. While the use of object-oriented programming is growing, there are still few examples of general purpose codes written in this manner, and a common approach is far from obvious. This paper describes OPT++, a C++ class library for nonlinear optimization. The design is predicated on the concept of distinguishing between an algorithm-independent class hierarchy for nonlinear optimization problems and a class hierarchy for nonlinear optimization methods that is based on common algorithmic traits. The interface is designed for ease of use while being general enough so that new optimization algorithms can be added easily to the existing framework. A number of nonlinear optimization algorithms have been implemented in OPT++ and are accessible through this interface. Furthermore, example applications demonstrate the simplicity of the interface as well as the advantages of a common interface in comparing multiple algorithms.
In this paper we present a new binary-programming formulation for the Steiner problem in graphs (SPG), which is well known to be NP-hard. We use this formulation to generate test problems with known optimal solutions. The technique uses the KKT optimality conditions on the corresponding quadratically constrained optimization problem.
A SemiDefinite Programming (SDP) problem is one of the most central problems in mathematical optimization. SDP provides an effective computation framework for many research fields. Some applications, however, require solving a large-scale SDP whose size exceeds the capacity of a single processor both in terms of computation time and available memory. SDPARA (SemiDefinite Programming Algorithm paRAllel package) [Yamashita et al. 2003b] was designed to solve such large-scale SDPs. Its parallel performance is outstanding for general SDPs in most cases. However, the parallel implementation is less successful for some sparse SDPs obtained from applications such as Polynomial Optimization Problems (POPs) or Sensor Network Localization (SNL) problems, since this version of SDPARA cannot directly handle sparse Schur Complement Matrices (SCMs). In this article we improve SDPARA by focusing on the sparsity of the SCM and we propose a new parallel implementation using the formula-cost-based distribution along with a replacement of the dense Cholesky factorization. We verify numerically that these features are key to solving SDPs with sparse SCMs more quickly on parallel computing systems. The performance is further enhanced by multithreading and the new SDPARA attains considerable scalability in general. It also finds solutions for extremely large-scale SDPs arising from POPs which cannot be obtained by other solvers.
Roundoff errors cannot be avoided when implementing numerical programs with finite precision. The ability to reason about rounding is especially important if one wants to explore a range of potential representations, for instance, for FPGAs or custom hardware implementations. This problem becomes challenging when the program does not employ solely linear operations as non-linearities are inherent to many interesting computational problems in real-world applications. Existing solutions to reasoning possibly lead to either inaccurate bounds or high analysis time in the presence of nonlinear correlations between variables. Furthermore, while it is easy to implement a straightforward method such as interval arithmetic, sophisticated techniques are less straightforward to implement in a formal setting. Thus there is a need for methods that output certificates that can be formally validated inside a proof assistant. We present a framework to provide upper bounds on absolute roundoff errors of floating-point nonlinear programs. This framework is based on optimization techniques employing semidefinite programming and sums of squares certificates, which can be checked inside the Coq theorem prover to provide formal roundoff error bounds for polynomial programs. Our tool covers a wide range of nonlinear programs, including polynomials and transcendental operations as well as conditional statements. We illustrate the efficiency and precision of this tool on non-trivial programs coming from biology, optimization, and space control. Our tool produces more accurate error bounds for 23% of all programs and yields better performance in 66% of all programs.
L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables. It is intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems. L-BFGS-B can also be used for unconstrained problems and in this case performs similarly to its predessor, algorithm L-BFGS (Harwell routine VA15). The algorithm is implemented in Fortran 77.
A stochastic algorithm is presented for finding the global optimum of a function of n variables subject to general constraints. The algorithm is intended for moderate values of n, but it can accommodate objective and constraint functions that are discontinuous and can take advantage of parallel processors. The performance of this algorithm is compared to that of the Nelder-Mead Simplex algorithm and a Simulated Annealing algorithm on a variety of nonlinear functions. In addition, one-, two-, four-, and eight-processor versions of the algorithm are compared using 64 of the nonlinear problems with constraints collected by Hock and Schittkowski. In general, the algorithm is more robust than the Simplex algorithm, but computationally more expensive. The algorithm appears to be as robust as the Simulated Annealing algorithm, but computationally cheaper. Issues discussed include algorithm speed and robustness, applicability to both computer and mathematical models, and parallel efficiency.
A truncated Newton minimization package, TNPACK, was described in ACM Transactions on Mathematical Software 14, 1 (Mar. 1992), pp.46�111. Modifications to enhance performance, especially for large-scale minimization of molecular potential functions, are described here. They involve three program segments of TNPACK: negative curvature test, modified Cholesky factorization, and line-search stopping rule.
This article considers the problem of minimizing a convex, separable quadratic function subject to a knapsack constraint and a box constraint. An algorithm called NAPHEAP has been developed to solve this problem. The algorithm solves the Karush-Kuhn-Tucker system using a starting guess to the optimal Lagrange multiplier and updating the guess monotonically in the direction of the solution. The starting guess is computed using the variable fixing method or is supplied by the user. A key innovation in our algorithm is the implementation of a heap data structure for storing the break points of the dual function and computing the solution of the dual problem. Also, a new version of the variable fixing algorithm is developed that is convergent even when the objective Hessian is not strictly positive definite. The hybrid algorithm NAPHEAP that uses a Newton-type method (variable fixing method, secant method, or Newton's method) to bracket a root, followed by a heap-based monotone break point search, can be faster than a Newton-type method by itself, as demonstrated in the numerical experiments.
The choice of cloud providers whose offers best fit the requirements of a particular application is a complex issue due to the heterogeneity of the services in terms of resources, costs, technology, and service levels that providers ensure. This article investigates the effectiveness of multiobjective genetic algorithms to resolve a multicloud brokering problem. Experimental results provide clear evidence about how such a solution improves the choice made manually by users returning in real time optimal alternatives. It also investigates how the optimality depends on different genetic algorithms and parameters, problem type, and time constraints.
Needs and studies on service robot are growing with rapid market extension. Generating human-like motions on robot is one research area of human-robot interaction. There are several studies about motion generation and humanoid design. However, kinematic analysis of humanoid design for service tasks should also performed. This paper focuses on motor alignment of humanoid shoulder. After we measure human greeting gestures with 3D capture camera and predefine workspace, simulation is performed to find motor alignment on humanoid shoulder to minimize total required motor power. Result shows that there are specific alignment for designing shoulder to perform greeting tasks on small motor powers.
The decentralized transshipment problem is a two-stage decision making problem where the companies first choose their individual production levels in anticipation of random demands and after demand realizations they pool residuals via transshipment. The coordination will be achieved if at optimality all the decision variables, i.e. production levels and transshipment patterns, in the decentralized system are the same as those of centralized system. In this paper, we study the coordination via transshipment prices. We propose a procedure for deriving the transshipment prices based on the coordinating allocation rule introduced by Anupindi et al. [1]. With the transshipment prices being set, the companies are free to match their residuals based on their individual preferences. We draw upon the concept of pair-wise stability to capture the dynamics of corresponding matching process. As the main result of this paper, we show that with the derived transshipment prices, the optimum transshipment patterns are always pair-wise stable, i.e. there are no pairs of companies that can be jointly better off by unilaterally deviating from the optimum transshipment patterns.
In this paper, a method for sparse signal recovery, based on iteratively reweighted least square approach using fewer measurements has been studied. Simulation results demonstrating the performance of the algorithm for different SNRs ranges, number of measurements, degree of sparsity, and various values of degree of normalization p have been presented. Results show that improvement is achieved by incorporating regularization strategy.
This paper analyzes the role of dictionary selection in Sparse Representation-based Classification (SRC). While SRC introduces interesting results in the field of classification, its performance is highly limited by the number of training samples to form the classification matrix. Different studies addressed this issue by using a more compact representation of the training data in order to achieve higher classification speed and accuracy. Representative selection methods which are analyzed in this paper include Metaface dictionary learning, Fisher Discriminative Dictionary Learning (FDDL), Sparse Modeling Representative Selection (SMRS), and random selection of the training samples. The first two methods build their own dictionaries via an optimization process while the other two methods select the representatives directly from the original training samples. These methods, along with the original method which uses all training samples to form the classification matrix, were examined on two face datasets and one digit dataset. The role of feature extraction was also studied using two dimensionality reduction methods, down-sampling and random projection. The results show that the FDDL method leads to the best classification accuracy followed by the SMRS method as the second best. On the other hand, the SMRS method requires a much smaller learning time which makes it more appropriate for dynamic situations where the dictionary is regularly updated with new samples. The accuracy of the Metaface dictionary learning method was specifically less than the other two methods. As expected, using all the training samples as the dictionary resulted in the best recognition rates in all the datasets but the classification times for this approach were far larger than the required time using any of the three dictionary learning methods.
Search engine ad auctions typically have a significant fraction of advertisers who are budget constrained, i.e., if allowed to participate in every auction that they bid on, they would spend more than their budget. This yields an important problem: selecting the ad auctions which these advertisers participate, in order to optimize different system objectives such as the return on investment for advertisers, and the quality of ads shown to users. We present a system and algorithms for optimizing budget constrained spend. The system is designed be deployed in a large search engine, with hundreds of thousands of advertisers, millions of searches per hour, and with the query stream being only partially predictable. We have validated the system design by implementing it in the Google ads serving system and running experiments on live traffic. We have also compared our algorithm to previous work that casts this problem as a large linear programming problem limited to popular queries, and show that our algorithms yield substantially better results.
Most online social networks provide a mechanism for users to broadcast messages to their personalized network through actions like shares, likes and tweets. Receiving positive feedback from the network such as likes, comments and retweets in response to such actions can provide a strong incentive for users to broadcast more often in the future. We call such feedback by the network, that influences a user to perform certain desirable future actions, social incentives. For example, after a user shares an article to her social network, receiving positive feedback such as a ''like'' from a friend can potentially encourage her to continue sharing more regularly. Typically, for every user's visit to an online social network site, good messages need to be ranked and selected by a recommender system from a large set of candidate messages (broadcasted by the user's network). In this paper, we propose a novel recommendation problem: How should we recommend messages to users to incentivize neighbors in their personal network to perform desirable actions in the future with high likelihood, without significantly hurting overall engagement for the entire system? For instance, messages could be content shared by neighbors. The goal in this case would be to encourage more content shares in the future. We call this problem social incentive optimization and study an instance of it for LinkedIn's news feed. We observe that a user who receives positive social feedback from neighbors has a higher likelihood of broadcasting more frequently. Using this observation, we develop a novel recommendation framework that incentivize users to broadcast more often, without significantly hurting overall feed engagement. We demonstrate the effectiveness of our approach through causal analysis on retrospective data and online A/B experiments.
This paper tackles the energy consumption minimization in transmit-only Wireless Sensor Networks and castes it as a convex optimization problem that is solved efficiently by an SOCP solver. The problem focuses on finding the optimal placement of cluster head nodes within the deployment field that minimizes the energy depletion while meeting network's coverage and connectivity requirements. Simulation results showed that reduction in energy consumption in a range of (16-24)% can be achieved by employing the optimization-based approach as compared to other standard placement methods.
Orthogonal Frequency Division Multiplexing (OFDM) has recently emerged as a promising technology. A network using OFDM based Spectrum-sliced Elastic Optical Path (SLICE) has a higher spectrum efficiency, due to the fine granularity of subcarrier frequencies used. To minimize the utilized spectrum in SLICE networks, the routing and spectrum allocation problem (RSA) has to be efficiently solved. We have solved the RSA problem using a Mixed Integer Linear Programming (MILP) formulation and have compared our approaches with another recent formulation.
We describe a tool-supported performance exploration approach in which we use genetic algorithms to find a potential user behavioural pattern that maximizes the resource utilization of the system under test. This work is built upon our previous work in which we generate load from workload models that describe the expected behaviour of the users. In this paper, we evolve a given probabilistic workload model (specified as a Markov Chain Model) by optimizing the probability distribution of the edges in the model and generating different solutions. During the evolution, the solutions are ranked according to their fitness values. The solutions with the highest fitness are chosen as parent solutions for generating offsprings. At the end of an experiment, we select the best solution among all the generations. We validate our approach by generating load from both the original and the best solution model, and by comparing the resource utilization they create on the system under test.
Multi-objective evolutionary algorithms are efficient in solving problems with two or three objectives. However, recent studies have shown that they face many difficulties when tackling problems involving a larger number of objectives and their behaviors become similar to a random walk in the search space since most individuals become non-dominated with each others. Motivated by the interesting results of decomposition-based approaches and preference-based ones, we propose in this paper a new decomposition-based dominance relation called TSD-dominance (Targeted Search Directions based dominance) to deal with many-objective optimization problems. Our dominance relation has the ability to create a strict partial order on the set of Pareto-equivalent solutions using a set of well-distributed reference points, thereby producing a finer grained ranking of solutions. The TSD-dominance is subsequently used to substitute the Pareto dominance in NSGA-II. The new obtained MOEA, called TSD-NSGA-II has been statistically demonstrated to provide competitive and better results when compared with three recently proposed decomposition-based algorithms on commonly used benchmark problems involving up to twenty objectives.
Genetic Programming (GP) is presented as an approach for the automatic generation of high quality mutation strategies for the Differential Evolution (DE) algorithm. To evaluate the generated mutation strategies, some well known continuous global optimization problems were selected. Also, a multi-layer perceptron neural network was trained for regression and classification tasks. Statistical analysis of the results showed quite satisfactory performance of the discovered mutation strategy (dubbed ADAM), presenting competitive or better results than classical mutation strategies. The contribution of this work is to show that GP is a potential tool for exploring new ideas in metaheuristics development, allowing for an automatic generation of mutation strategies that can present fast performance and high quality solutions.
This paper presents an optimisation algorithm designed to perform in-situ automatic fitting of cochlear implants.All patients are different, which means that cochlear parametrisation is a difficult and long task, with results ranging from perfect blind speech recognition to patients who cannot make anything out of their implant and just turn it off.The proposed method combines evolutionary algorithms and medical expertise to achieve autonomous interactive fitting through a Personal Digital Assistant (PDA).
We propose in this paper a new approach called ?-LGP (?-Linear Genetic Programming), a variation of the well-know LGP (Linear Genetic Programming) algorithm. Starting with an LGP based only on effective macro and micromutations, the ?-LGP proposed in this work consists in extending the way in which the individuals are chosen for reproduction. In this model, a constant number (?) of a particular kind of mutation is applied to each individual, thus exploring its neighbouring fitness regions, and might be replaced by one of its children according to different criteria. Several configurations were tested in the benchmark problem known as Santa Fe Ant Trail. Results obtained show a very significant improvement by using this proposed variation. For the Ant Trail problem, ?-LGP outperformed not only LGP, but also several state-of-the-art methods.
This paper presents a distributed software architecture that allows the cooperation among research institutions in the field of Combinatorial Optimization --- DEVOpT: Distributed Evolutionary Optimization Centers. It has as main aims to share existing algorithms for optimization problems, to allow the easy testing of these algorithms with existing instances, to provide fast and better ways to design new algorithms, and to share computational power among the cooperating institutions. This is achieved respecting the autonomy and heterogeneity of the cooperating institutions. The distributed architecture is discussed here and also a case study of a Parallel Memetic Algorithm to solve the Asymmetric Traveling Salesman Problem (ATSP) running on this environment is analyzed.
Satisfiability problems with preferences enrich the expressive power of the Boolean Satisfiability problem (SAT) and facilitate the representation of qualitative/quantitative preferences on literals/formulas, defining an optimization problem. In some cases, it is not strictly necessary to compute an optimal solution, but it is enough to compute a sub-optimal solution of high quality and, possibly, provide a lower bound on the probability of finding an optimal solution. The 1/e - rule is the optimal stopping rule for the secretary problem that guarantees an optimal solution with probability at least 1/e can be found. In this paper: we show how to apply the 1/e-rule for solving satisfiability problems with preferences; we show that its theoretical success rate of about 37% is greater than 90% on random benchmarks; and, we show that the performance of the 1/e-rule on structured benchmarks is sometimes many orders-of-magnitude worse than that of complete search-based algorithms, and we explain the reasons why. We propose an algorithm based on the idea underlying the 1/e-rule, which needs the generation of just two solutions: the experimental evaluation shows that the average success rate of the proposed algorithm is a good approximation of the theoretical one of the 1/e-rule, since it is about 50.92% on 1956 structured problems and 48.33% on 2400 randomly generated instances with 200 variables.
We consider the long-standing problem of the automatic generation of regular expressions for text extraction, based solely on examples of the desired behavior. We investigate several active learning approaches in which the user annotates only one desired extraction and then merely answers extraction queries generated by the system. The resulting framework is attractive because it is the system, not the user, which digs out the data in search of the samples most suitable to the specific learning task. We tailor our proposals to a state-of-the-art learner based on Genetic Programming and we assess them experimentally on a number of challenging tasks of realistic complexity. The results indicate that active learning is indeed a viable framework in this application domain and may thus significantly decrease the amount of costly annotation effort required.
Distributed data stream processing allows to optimize resource consumption. A query's operators can be executed by several systems. The placement of filter or aggregate operators near the data source omits unnecessary data transfer. The operator placement decision is a complex problem. In certain scenarios the goal is not only overall minimization of e.g. resource consumption but an evenly distributed load. We propose an operator fission algorithm, that works on the basis of an initial operator placement. The algorithm selects certain operators from the set of operators that allow fission for parallel execution by multiple systems. Load is thus divided between processors in a more fine-grained way, resulting in lower maximum load and lower load variance. We present and evaluate three different variants of the algorithm to allow tuning the trade-off between optimization time and result quality.
Optimal resource utilization is one of the biggest challenges for executing tasks within the cloud. The resource provider is responsible for providing the resources by creating virtual machines for executing task over a cloud. To utilize the resources optimally, the resource provider has to take care of the process of allocating resources to Virtual Machine Manager (VMM). In this paper, an efficient way to utilize the resources, within the cloud, has been proposed considering remaining resources should be maximum at a single machine but not distributed. As a framework to virtual resource mapping, a Simple Genetic Algorithm is applied to solve the heuristic of allocating problem. We may also use conversion of multiple parameters into single equivalent parameter so that number of inputs and comparisons will be reduced.
This material is based on work in progress. Imagine that we had a piece of matter that can change its physical properties like shape, density, conductivity, or color in a programmable fashion based on either user input or autonomous sensing. This is the vision behind what is commonly known as programmable matter. Programmable matter is the subject of many recent novel distributed computing proposals --- ranging from DNA tiles, shape-changing molecules, and synthetic cells, to reconfigurable modular robotics --- each pursuing solutions for specific application scenarios with their own, special capabilities and constraints.
We consider the problem of finding efficient trees to send information from k sources to a single sink in a network where information can be aggregated at intermediate nodes in the tree. Specifically, we assume that if information from j sources is traveling over a link, the total information that needs to be transmitted is f(j). One natural and important (though not necessarily comprehensive) class of functions is those which are concave, non-decreasing, and satisfy f(0) = 0. Our goal is to find a tree which is a good approximation simultaneously to the optimum trees for all such functions. This problem is motivated by aggregation in sensor networks, as well as by buy-at-bulk network design.We present a randomized tree construction algorithm that guarantees E[maxfCf/C*(f)] 1 + log k, where Cf is a random variable denoting the cost of the tree for function f and C* (f) is the cost of the optimum tree for function f. To the best of our knowledge, this is the first result regarding simultaneous optimization for concave costs. We also show how to derandomize this result to obtain a deterministic algorithm that guarantees maxf/C*(f) = O(log k). Both these results are much stronger than merely obtaining a guarantee on maxfE[Cf/C* (f)]. A guarantee on maxfE[Cf/C* (f)] can be obtained using existing techniques, but this does not capture simultaneous optimization since no one tree is guaranteed to be a good approximation for all f simultaneously.While our analysis is quite involved, the algorithm itself is very simple and may well find practical use. We also hope that our techniques will prove useful for other problems where one needs simultaneous optimization for concave costs.
Quadratic Assignment is a basic problem in combinatorial optimization, which generalizes several other problems such as Traveling Salesman, Linear Arrangement, Dense k Subgraph, and Clustering with given sizes. The input to the Quadratic Assignment Problem consists of two n x n symmetric non-negative matrices W = (wi, j) and D = (di, j). Given matrices W, D, and a permutation ?: [n] ? [n], the objective function is [EQUATION]. In this paper, we study the Maximum Quadratic Assignment Problem, where the goal is to find a permutation ? that maximizes Q(?). We give an �(?n) approximation algorithm, which is the first non-trivial approximation guarantee for this problem. The above guarantee also holds when the matrices W, D are asymmetric. An indication of the hardness of Maximum Quadratic Assignment is that it contains as a special case, the Dense k Subgraph problem, for which the best known approximation ratio ? n1/3 (Feige et al. [8]). When one of the matrices W, D satisfies triangle inequality, we obtain a [EQUATION] approximation algorithm. This improves over the previously bestknown approximation guarantee of 4 (Arkin et al. [4]) for this special case of Maximum Quadratic Assignment. The performance guarantee for Maximum Quadratic Assignment with triangle inequality can be proved relative to an optimal solution of a natural linear programming relaxation, that has been used earlier in Branch-and-Bound approaches (see eg. Adams and Johnson [1]). It can also be shown that this LP has an integrality gap of [EQUATION] for general Maximum Quadratic Assignment.
In this paper, we consider the restless bandit problem, which is one of the most well-studied generalizations of the celebrated stochastic multi-armed bandit problem in decision theory. In its ultimate generality, the restless bandit problem is known to be PSPACE-Hard to approximate to any non-trivial factor, and little progress has been made on this problem despite its significance in modeling activity allocation under uncertainty. We make progress on this problem by showing that for an interesting and general subclass that we term Monotone bandits, a surprisingly simple and intuitive greedy policy yields a factor 2 approximation. Such greedy policies are termed index policies, and are popular due to their simplicity and their optimality for the stochastic multi-armed bandit problem. The Monotone bandit problem strictly generalizes the stochastic multi-armed bandit problem, and naturally models multi-project scheduling where the state of a project becomes increasingly uncertain when the project is not scheduled. We develop several novel techniques in the design and analysis of the index policy. Our algorithm proceeds by introducing a novel "balance" constraint to the dual of a well-known LP relaxation to the restless bandit problem. This is followed by a structural characterization of the optimal solution by using both the exact primal as well as dual complementary slackness conditions. This yields an interpretation of the dual variables as potential functions from which we derive the index policy and the associated analysis.
We introduce a problem called sequential trial optimization, a generalization of the well studied set cover problem with a new objective function. We give a simple algorithm that achieves a constant factor approximation to this problem. Sequential trial optimization naturally arises in heterogenous search environments such as peer to peer networks.
How far can a stack of n identical blocks be made to hang over the edge of a table? The question dates back to at least the middle of the 19th century and the answer to it was widely believed to be of order log n. However, at SODA'06, Paterson and Zwick constructed n-block stacks with overhangs of order n1/3. Here we complete the solution to the overhang problem, and answer Paterson and Zwick's primary open question, by showing that order n1/3 is best possible. At the heart of the argument is a lemma (possibily of independent interest) showing that order d3 non-adaptive coinflips are needed to propel a discrete random walk on the number line to distance d. We note that our result is not a mainstream algorithmic result, yet it is about the solution to a discrete optimization problem. Moreover, it illusrates how methods founded in theoretical computer science can be aplied to a problem that has puzzled some mathematicians and physicists for more than 150 years.
The online multi-armed bandit problem and its generalizations are repeated decision making problems, where the goal is to select one of several possible decisions in every round, and incur a cost associated with the decision, in such a way that the total cost incurred over all iterations is close to the cost of the best fixed decision in hindsight. The difference in these costs is known as the regret of the algorithm. The term bandit refers to the setting where one only obtains the cost of the decision used in a given iteration and no other information. Perhaps the most general form of this problem is the non-stochastic bandit linear optimization problem, where the set of decisions is a convex set in some Euclidean space, and the cost functions are linear. Only recently an efficient algorithm attaining � (?T) regret was discovered in this setting. In this paper we propose a new algorithm for the bandit linear optimization problem which obtains a regret bound of � (?Q), where Q is the total variation in the cost functions. This regret bound, previously conjectured to hold in the full information case, shows that it is possible to incur much less regret in a slowly changing environment even in the bandit setting. Our algorithm is efficient and applies several new ideas to bandit optimization such as reservoir sampling.
The quest for a PTAS for Nash equilibrium in a two-player game seeks to circumvent the PPAD-completeness of an (exact) Nash equilibrium by finding an approximate equilibrium, and has emerged as a major open question in Algorithmic Game Theory. A closely related problem is that of finding an equilibrium maximizing a certain objective, such as the social welfare. This optimization problem was shown to be NP-hard by Gilboa and Zemel [Games and Economic Behavior 1989]. However, this NP-hardness is unlikely to extend to finding an approximate equilibrium, since the latter admits a quasi-polynomial time algorithm, as proved by Lipton, Markakis and Mehta [Proc. of 4th EC, 2003]. We show that this optimization problem, namely, finding in a two-player game an approximate equilibrium achieving large social welfare is unlikely to have a polynomial time algorithm. One interpretation of our results is that the quest for a PTAS for Nash equilibrium should not extend to a PTAS for finding the best Nash equilibrium, which stands in contrast to certain algorithmic techniques used so far (e.g. sampling and enumeration). Technically, our result is a reduction from a notoriously difficult problem in modern Combinatorics, of finding a planted (but hidden) clique in a random graph G(n, 1/2). Our reduction starts from an instance with planted clique size k = O(log n). For comparison, the currently known algorithms due to Alon, Krivelevich and Sudakov [Random Struct. & Algorithms, 1998], and Krauthgamer and Feige [Random Struct. & Algorithms, 2000], are effective for a much larger clique size k = ?(?n).
Integer programs defined by two equations with two free integer variables and nonnegative continuous variables have three types of nontrivial facets: split, triangle or quadrilateral inequalities. In this paper, we compare the strength of these three families of inequalities. In particular we study how well each family approximates the integer hull. We show that, in a well defined sense, triangle inequalities provide a good approximation of the integer hull. The same statement holds for quadrilateral inequalities. On the other hand, the approximation produced by split inequalities may be arbitrarily bad.
Motivated by applications in combinatorial optimization, we initiate a study of the extent to which the global properties of a metric space (especially, embeddability in l1 with low distortion) are determined by the properties of small subspaces. We note connections to similar issues studied already in Ramsey theory, complexity theory (especially PCPs), and property testing. We prove both upper bounds and lower bounds on the distortion of embedding locally constrained metrics into various target spaces.
In most of the known polynomially solvable cases of the symmetric travelling salesman problem (TSP) which result from restrictions on the underlying distance matrices, the restrictions have the form of so-called four-point conditions (the inequalities involve four cities). In this paper we treat all possible (symmetric) four-point conditions and investigate whether the corresponding TSP can be solved in polynomial time. As a by-product of our classification we obtain new families of exponential neighborhoods for the TSP which can be searched in polynomial time and for which conditions on the distance matrix can be formulated so that the search for an optimal TSP solution can be restricted to these exponential neighborhoods.
Bisubmodular functions are a natural "directed", or "signed", extension of submodular functions with several applications. Recently Fujishige and Iwata showed how to extend the Iwata, Fleischer, and Fujishige (IFF) algorithm for submodular function minimization (SFM) to bisubmodular function minimization (BSFM). However, they were able to extend only the weakly polynomial version of IFF to BSFM. Here we investigate the difficulty that prevented them from also extending the strongly polynomial version of IFF to BSFM, and we show a way around the difficulty. This new method gives the first combinatorial strongly polynomial algorithm for BSFM. This further leads to extending Iwata's fully combinatorial version of IFF to BSFM.
We show that any linear program (LP) in n nonnegative variables and m equality constraints defines in a natural way a unique sink orientation of the n-dimensional cube. From the sink of the cube, we can either read off an optimal solution to the LP, or we obtain certificates for infeasibility or unboundedness.This reduction complements the implicit local neighborhoods induced by the vertex-edge structure of the feasible region with an explicit neighborhood structure that allows random access to all 2n candidate solutions. Using the currently best sink-finding algorithm for general unique sink orientations, we obtain the fastest deterministic LP algorithm in the RAM model, for the central case n = 2m.
We consider "online bandit geometric optimization," a problem of iterated decision making in a largely unknown and constantly changing environment. The goal is to minimize "regret," defined as the difference between the actual loss of an online decision-making procedure and that of the best single decision in hindsight. "Geometric optimization" refers to a generalization of the well-known multi-armed bandit problem, in which the decision space is some bounded subset of Rd, the adversary is restricted to linear loss functions, and regret bounds should depend on the dimensionality d, rather than the total number of possible decisions. "Bandit" refers to the setting in which the algorithm is only told its loss on each round, rather than the entire loss function.McMahan and Blum [10] presented the best known algorithm in this setting, and proved that its expected additive regret is O(poly(d)T3/4). We simplify and improve their analysis of this algorithm to obtain regret O(poly(d)T2/3).We also prove that, for a large class of full-information online optimization problems, the optimal regret against an adaptive adversary is the same as against a non-adaptive adversary.
We consider the problem of minimizing weighted flow time on a single machine in the preemptive setting. Our main result is an O(log W) competitive online algorithm where the maximum to the minimum ratio of weights is W. More generally our algorithm achieves a competitive ratio of k if there are k weight classes. This gives the first O(1)-competitive algorithm for constant k. No O(1) competitive algorithm was known previously even for the special case of k = 2. These results settle a question posed by Chekuri et al [5] about the existence of a "truly" online algorithm with a non-trivial competitive ratio. We also give a "semi-online" algorithm with competitive ratio O(log n + log P), where P is ratio of the maximum to minimum job size. Our second result deals with the non-clairvoyant setting where the job sizes are unknown (but the weight of the jobs are known). We consider the resource augmentation model, and give a non-clairvoyant online algorithm, which if allowed a (1 + ?) speed-up, is (1 + l/?) competitive against an optimal offline, clairvoyant algorithm.
In the last few years, researchers have evaluated the performance of e-government portals in order to identify best practices and understand some of the factors that influence the quality of the information and services they provide to citizens. Most of these evaluations consider only the results or outputs, but ignore the inputs in terms of capabilities and resources that governments have available for these efforts. This paper argues that using data envelopment analysis (DEA) could help to better understand how efficient are governments in their use of certain inputs to produce high quality e-government portals. DEA is applied to calculate an efficiency score based on some portal characteristics (outputs) such as information, interaction, transaction, integration, and participation, and some organizational, institutional and contextual factors (inputs) such as government capacity, potential demand, and operation cost. The state government portals in Mexico are used for the empirical analysis. Our results indicate that there are some states that are never in the first places in terms of quality, but they have very few resources and capabilities and therefore, they are highly efficient.
In this short paper, we describe the production data approach to data curation. We argue that by treating data in a similar fashion to how we build production software, that data will be more readily accessible and available for broad re-use. We should be treating data as an ongoing process. This includes considering third-party contributions; planning for cyclical releases; bug fixes, tracking, and versioning; and issuing licensing and citation information with each release.
Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce (i) faster SVMs where training errors are no longer support vectors, and (ii) much faster Transductive SVMs.
We describe and analyze two stochastic methods for l1 regularized loss minimization problems, such as the Lasso. The first method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature/example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.
To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alternative to standard stochastic search methods. It maintains a multinormal distribution on the set of solution candidates. The Natural Gradient is used to update the distribution's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information matrix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness evaluations. The algorithm yields competitive results on a number of benchmarks.
In a principal-agent problem, a principal seeks to motivate an agent to take a certain action beneficial to the principal, while spending as little as possible on the reward. This is complicated by the fact that the principal does not know the agent's utility function (or type). We study the online setting where at each round, the principal encounters a new agent, and the principal sets the rewards anew. At the end of each round, the principal only finds out the action that the agent took, but not his type. The principal must learn how to set the rewards optimally. We show that this setting generalizes the setting of selling a digital good online.We study and experimentally compare three main approaches to this problem. First, we show how to apply a standard bandit algorithm to this setting. Second, for the case where the distribution of agent types is fixed (but unknown to the principal), we introduce a new gradient ascent algorithm. Third, for the case where the distribution of agents' types is fixed, and the principal has a prior belief (distribution) over a limited class of type distributions, we study a Bayesian approach.
Sutton, Szepesv�ri and Maei (2009) recently introduced the first temporal-difference learning algorithm compatible with both linear function approximation and off-policy training, and whose complexity scales only linearly in the size of the function approximator. Although their gradient temporal difference (GTD) algorithm converges reliably, it can be very slow compared to conventional linear TD (on on-policy problems where TD is convergent), calling into question its practical utility. In this paper we introduce two new related algorithms with better convergence rates. The first algorithm, GTD2, is derived and proved convergent just as GTD was, but uses a different objective function and converges significantly faster (but still not as fast as conventional TD). The second new algorithm, linear TD with gradient correction, or TDC, uses the same update rule as conventional TD except for an additional term which is initially zero. In our experiments on small test problems and in a Computer Go application with a million features, the learning rate of this algorithm was comparable to that of conventional TD. This algorithm appears to extend linear TD to off-policy learning with no penalty in performance while only doubling computational requirements.
Quadratic program relaxations are proposed as an alternative to linear program relaxations and tree reweighted belief propagation for the metric labeling or MAP estimation problem. An additional convex relaxation of the quadratic approximation is shown to have additive approximation guarantees that apply even when the graph weights have mixed sign or do not come from a metric. The approximations are extended in a manner that allows tight variational relaxations of the MAP problem, although they generally involve non-convex optimization. Experiments carried out on synthetic data show that the quadratic approximations can be more accurate and computationally efficient than the linear programming and propagation based alternatives.
Handling of objects through robot hands is a complex problem in most applications. This makes grasp planning very important and thereby it becomes necessary to formulate efficient schemes for automated grasp synthesis. A method for fast synthesis of grasp configurations is proposed in this work through the use of Particle Swarm Optimization (PSO). The algorithm uses a feasible grasp as the input to improve the grasp quality. The largest ball criterion is used as a measure of the grasp quality. Surface tessellated objects are used as test cases for the implementation and the optimization has been conducted for frictional and non-frictional instances.
Last decade has seen a growing interest of robotics and control community towards understanding the dynamics and synthesis of control law for unmanned aerial vehicles (UAVs). Many control theoretic challenges are captured by UAV systems due to their inherent nonlinear dynamics and the underactuation characteristics. Quadrotor or quadcopter systems belongs to such UAV category having vertical take off and landing (VTOL) capabilities, which makes them ideally suitable for defense and surveillance applications. In this paper, we present a model predictive control (MPC) strategy for stabilization and trajectory tracking of a quadrotor UAV having bounded thrust. The major advantage of MPC over other control schemes is its constraint handling capacity. This feature makes it suitable for dealing with systems having realistic state and input thrust constraints. We illustrate the effectiveness of MPC technique to track a given trajectory while respecting the bound on control input of a quadrotor UAV.
The paper presents a nonlinear model predictive control (NMPC) strategy for stabilization and trajectory tracking control of planar vertical Take-off and landing (PVTOL) aircraft. PVTOL system is considered as a benchmark for investigating dynamics and control related issues for unmanned aerial vehicles (UAVs). Control problem of such systems is made challenging due to their under-actuated nature and nonlinear dynamics. We propose a nonlinear MPC methodology to effectively deal with such nonlinearities. Also, the proposed control strategy takes into account state (roll angle) as well as input thrust constraints. Validation of control law is performed through MATLAB simulations for various operating conditions and coupling scenarios.
Conventional laparoscopic surgical tools penetrate a patient body through a fixed point called trocar point. Surgeons maintain this pivoting point on the patient body with their coordinated hand movements. Mechanism used for achieving this kinematic constraint is known as Remote Centre of Motion (RCM) mechanism and is widely used in surgical robots. Active RCM mechanism will enable positioning the constraint point virtually anywhere within the reach of the linkage. In this work, we present an optimisation strategy for an active RCM to minimise the extracorporeal workspace. The link lengths of the active RCM mechanism have been optimised and the dexterity of the tool has been analysed for various trocar positions. Kinematic analysis of the RCM mechanism, optimisation strategy, and the results of the dexterity analysis are presented.
Ad blockers are a formidable threat to the vitality of the online advertising eco-system. Understanding their prevalence and impact is challenging due to the massive scale and diversity of the eco-system. In this paper, we utilize unique data gathering assets to assess the prevalence and impact of ad blockers from an Internet-wide perspective. Our study is based on (i) a 2 million person world-wide user panel that provides ground truth for ad blocker installations and (ii) telemetry from large number of publisher web pages and ads served to publishers. We describe a novel method for assessing the prevalence of ad blocker installations that is based on Mixture Proportion Estimation. We apply this method to nearly 2 trillion web transactions collected over the period of 1 month (February 2016), to derive ad blocker prevalence estimates for desktop systems in diverse geographic areas and for diverse demographic groups. Next, using deployment estimates we consider the impact of ad blockers on users and on publisher sites. Specifically, we report on the reduction of ads shown to users with ad blockers installed and show that even though a user may have an ad blocker installed, they are still exposed to a significant number of ads. We also characterize the impact of ad blockers across different categories of publisher sites including those that may be participating in whitelisting.
We present an approach for optimizing programs that uncovers additional opportunities for optimization of a statement by predicating the statement. In this paper predication algorithms for achieving partial dead code elimination (PDE) are presented. The process of predication embeds a statement in a control flow structure such that the statement is executed only if the execution follows a path along which the value computed by the statement is live. The control flow restructuring performed to achieve predication is expressed through slicing transformations. This approach achieves PDE that is not realizable by existing algorithms. We prove that our algorithm never increases the operation count along any path, and that for acyclic code all partially dead statements are eliminated. The slicing transformation that achieves predication introduces into the program additional conditional branches. These branches are eliminated in a branch deletion step based upon code duplication. We also show how PDE can be used by acyclic schedulers for VLIW processors to reduce critical path lengths along frequently executed paths.
Interprocedural dataflow information enables link-time and post-link-time optimizers to perform analyses and code transformations that are not possible in a traditional compiler. This paper describes the interprocedural dataflow analysis techniques used by Spike, a post-linktime optimizer for Alpha/NT executables. Spike uses dataflow analysis to summarize the register definitions, uses, and kills that occur external to each routine, allowing Spike to perform a variety of optimizations that require interprocedural dataflow information. Because Spike is designed to optimize large PC applications, the time required to perform interprocedural dataflow analysis could potentially be unacceptably long, limiting Spike's effectiveness and applicability. To decrease dataflow analysis time, Spike uses a compact representation of a program's intraprocedural and interprocedural control flow that efficiently summarizes the register definitions and uses that occur in the program. Experimental results are presented for the SPEC95 integer benchmarks and eight large PC applications. The results show that the compact representation allows Spike to compute interprocedural dataflow information in less than 2 seconds for each of the SPEC95 integer benchmarks. Even for the largest PC application containing over 1.7 million instructions in 340 thousand basic blocks, interprocedural dataflow analysis requires just 12 seconds.
A simulated annealing ant colony algorithm based on backfire method(BASAC) is presented to tackle the quadratic assignment problem (QAP). The simulated annealing algorithm is combined with the ant colony algorithm. The backfire method which has a small temperature attenuation coefficient is used in the temperature control process. After each round of searching, the solution set got by the colony is treated as the candidate set. Base on the simulated annealing method, the solution in the candidate set is chosen to join the update set with possibility which determined by the temperature. The update set is used to update the trail information matrix. And also the current best solution is used to enhance the tail information. The pheromone trails matrix is reset when the algorithm is in the stagnant state. The computer experiments demonstrate this algorithm has high calculation stability and converging speed.
Differential Evolution (DE) is well-known as a simple and efficient evolutionary algorithm for global optimization problems. However, the mutation strategies used in DE greatly affect its performance. Although many mutation operators have been proposed in DE, for each operator there are some types of optimization problems that cannot be solved efficiently. In this paper, we propose a novel DE using a mixed mutation strategy (MMSDE), which integrate four different mutation operators, may be able to overcome the shortcomings of a pure strategy. In order to verify the performance of MMSDE, we test it on 8 famous benchmark functions. The simulation results show that MMSDE performs equally well or better than classical DE, modified DE (MoDE) and trigonometric mutation DE (TDE) on all of the test problems.
This paper builds on SASSY, a system for automatically generating SOA software architectures that optimize a given utility function of multiple QoS metrics. In SASSY, SOA software systems are automatically re-architected when services fail or degrade. Optimizing both architecture and service provider selection presents a pair of nested NP-hard problems. Here we adapt hill-climbing, beam search, simulated annealing, and evolutionary programming to both architecture optimization and service provider selection. Each of these techniques has several parameters that influence their efficiency. We introduce in this paper a meta-controller that automates the run-time selection of heuristic search techniques and their parameters. We examine two different meta-controller implementations that each use online learning. The first implementation identifies the best heuristic search combination from various prepared combinations. The second implementation analyzes the current self-architecting problem (e.g. changes in performance metrics, service degradations/failures) and looks for similar, previously encountered re-architecting problems to find an effective heuristic search combination for the current problem. A large set of experiments demonstrates the effectiveness of the first meta-controller implementation and indicates opportunities for improving the second meta-controller implementation.
Increased use of solar energy is forcing energy companies to devise new time of use (ToU) tariff scheme to counter revenue losses. Designing ToU tariff scheme is a complex multi-stage problem. The adoption of smart meters and availability of high performance multicore systems has opened up newer and better ways of tariff design. The design of ToU tariff schemes typically involves identifying various demand periods which is accomplished by analyzing the intraday consumption patterns across various geographies. The optimal tariff parameters for all the demand periods is then computed by solving a constrained optimization problem. In this paper, we present a present multi dimensional grid search approach to compute the optimal tariff parameters for a ToU scheme. The grid search method was then efficiently implemented on Nvidia GPUs with dynamic parallelism. The annual energy consumption data processing for nearly 0.3 million consumers and computation of consumption pattern and demand periods was carried out using MPI based parallel processing on an Intel Haswell system.
Static multi-issue machines, such as traditional Very Long Instructional Word (VLIW) architectures, move complexity from the hardware to the compiler. This is motivated by the ability to support high degrees of instruction-level parallelism without requiring complicated scheduling logic in the processor hardware. The simpler-control hardware results in reduced area and power consumption, but leads to a challenge of engineering a compiler with good code-generation quality. Transport triggered architectures (TTA), and other so-called exposed datapath architectures, take the compiler-oriented philosophy even further by pushing more details of the datapath under software control. The main benefit of this is the reduced register file pressure, with a drawback of adding even more complexity to the compiler side. In this article, we propose an Integer Linear Programming (ILP)-based instruction scheduling model for TTAs. The model describes the architecture characteristics, the particular processor resource constraints, and the operation dependencies of the scheduled program. The model is validated and measured by compiling application kernels to various TTAs with a different number of datapath components and connectivity. In the best case, the cycle count is reduced to 52% when compared to a heuristic scheduler. In addition to producing shorter schedules, the number of register accesses in the compiled programs is generally notably less than those with the heuristic scheduler; in the best case, the ILP scheduler reduced the number of register file reads to 33% of the heuristic results and register file writes to 18%. On the other hand, as expected, the ILP-based scheduler uses distinctly more time to produce a schedule than the heuristic scheduler, but the compilation time is within tolerable limits for production-code generation.
In this article, we consider how to automatically create pleasing photo collages created by placing a set of images on a limited canvas area. The task is formulated as an optimization problem. Differently from existing state-of-the-art approaches, we here exploit subjective experiments to model and learn pleasantness from user preferences. To this end, we design an experimental framework for the identification of the criteria that need to be taken into account to generate a pleasing photo collage. Five different thematic photo datasets are used to create collages using state-of-the-art criteria. A first subjective experiment where several subjects evaluated the collages, emphasizes that different criteria are involved in the subjective definition of pleasantness. We then identify new global and local criteria and design algorithms to quantify them. The relative importance of these criteria are automatically learned by exploiting the user preferences, and new collages are generated. To validate our framework, we performed several psycho-visual experiments involving different users. The results shows that the proposed framework allows to learn a novel computational model which effectively encodes an inter-user definition of pleasantness. The learned definition of pleasantness generalizes well to new photo datasets of different themes and sizes not used in the learning. Moreover, compared with two state-of-the-art approaches, the collages created using our framework are preferred by the majority of the users.
One of the most widely used biomimicry algorithms is the Particle Swarm Optimization (PSO). Since its introduction in 1995, it has caught the attention of both researchers and academicians as a way of solving various optimization problems, such as in the fields of engineering and medicine, to computer image processing and mission critical operations. PSO has been widely applied in the field of swarm robotics, however, the trend of creating a new variant PSO for each swarm robotic project is alarming. We investigate the basic properties of PSO algorithms relevant to the implementation of swarm robotics and characterize the limitations that promote this trend to manifest. Experiments were conducted to investigate the convergence properties of three PSO variants (original PSO, SPSO and APSO) and the global optimum and local optimal of these PSO algorithms were determined. We were able to validate the existence of premature convergence in these PSO variants by comparing 16 functions implemented alongside the PSO variant. This highlighted the fundamental flaws in most variant PSOs, and signifies the importance of developing a more generalized PSO algorithm to support the implementation of swarm robotics. This is critical in curbing the influx of custom PSO and theoretically addresses the fundamental flaws of the existing PSO algorithm.
With recent advances in consumer electronics and the increasingly urgent need for public security, camera networks have evolved from their early role of providing simple and static monitoring to current complex systems capable of obtaining extensive video information for intelligent processing, such as target localization, identification, and tracking. In all cases, it is of vital importance that the optimal camera configuration (i.e., optimal location, orientation, etc.) is determined before cameras are deployed as a suboptimal placement solution will adversely affect intelligent video surveillance and video analytic algorithms. The optimal configuration may also provide substantial savings on the total number of cameras required to achieve the same level of utility. In this article, we examine most, if not all, of the recent approaches (post 2000) addressing camera placement in a structured manner. We believe that our work can serve as a first point of entry for readers wishing to start researching into this area or engineers who need to design a camera system in practice. To this end, we attempt to provide a complete study of relevant formulation strategies and brief introductions to most commonly used optimization techniques by researchers in this field. We hope our work to be inspirational to spark new ideas in the field.
Data clustering is a popular unsupervised data mining tool that is used for partitioning a given dataset into homogeneous groups based on some similarity/dissimilarity metric. Traditional clustering algorithms often make prior assumptions about the cluster structure and adopt a corresponding suitable objective function that is optimized either through classical techniques or metaheuristic approaches. These algorithms are known to perform poorly when the cluster assumptions do not hold in the data. Multiobjective clustering, in which multiple objective functions are simultaneously optimized, has emerged as an attractive and robust alternative in such situations. In particular, application of multiobjective evolutionary algorithms for clustering has become popular in the past decade because of their population-based nature. Here, we provide a comprehensive and critical survey of the multitude of multiobjective evolutionary clustering techniques existing in the literature. The techniques are classified according to the encoding strategies adopted, objective functions, evolutionary operators, strategy for maintaining nondominated solutions, and the method of selection of the final solution. The pros and cons of the different approaches are mentioned. Finally, we have discussed some real-life applications of multiobjective clustering in the domains of image segmentation, bioinformatics, web mining, and so forth.
A smart power grid transforms the traditional electric grid into a user-centric, intelligent power network. The cost-saving potential of smart homes is an excellent motivating factor to involve users in smart grid operations. To that end, this survey explores the contemporary cost-saving strategies for smart grids from the users� perspective. The study shows that optimization methods are the most popular cost-saving techniques reported in the literature. These methods are used to plan scheduling and power utilization schemes of household appliances, energy storages, renewables, and other energy generation devices. The survey shows that trading energy among neighborhoods is one of the effective methods for cost optimization. It also identifies the prediction methods that are used to forecast energy price, generation, and consumption profiles, which are required to optimize energy cost in advance. The contributions of this article are threefold. First, it discusses the computational methods reported in the literature with their significance and limitations. Second, it identifies the components and their characteristics that may reduce energy cost. Finally, it proposes a unified cost optimization framework and addresses the challenges that may influence the overall residential energy cost optimization problem in smart grids.
This paper gives an algorithm for solving linear programming problems. For a problem with n constraints and d variables, the algorithm requires an expectedOd2n+lognOdd/2+O1+Od4nlogn arithmetic operations, asn??. The constant factors do not depend on d. Also, an algorithm is given for integer linear programming. Let 4 bound the number of bits required to specify the rational numbers defining an input constraint or the objective function vector. Let n and d be as before. Then, the algorithm requires expected O2ddn+8ddnl nnlnn +dod4 lnn operations on numbers with do14 bits, as n?? , where the constant factors do not depend on d or4to other convex programming problems. For example, an algorithm for finding the smallest sphere enclosing a set of n points in Edhas the same time bound.
An approximate but fairly rapid method for solving integer linear programming problems is presented, which utilizes, in part, some of the philosophy of �direct search� methods. The method is divided into phases which can be applied in order and has the desirable characteristic that a best feasible solution is always available. Numerical results are presented for a number of test problems. Some possible extensions and improvements are also presented.
We study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure.
The optimization problem for linear functions on finite languages is studied, and an (almost) complete characterization of those functions for which a primal and a dual greedy algorithm work well with respect to a canonically associated linear programming problem is given. The discussion in this paper is within the framework of ordered languages, and the characterization uses the notion of rank feasibility of a weighting with respect to an ordered language. This yields a common generalization of a sufficient condition, obtained recently by Korte and Lov�sz for greedoids, and the greedy algorithm for ordered sets in Faigel's paper [6]. Ordered greedoids are considered the appropriate generalization of greedoids, and the connection is established between ordered languages, polygreedoids, and Coxeteroids. Answering a question of Bj�rner, the author shows in particular that a polygreedoid is a Coxeteroid if and only if it is derived from an integral polymatroid.
The ordinal regression problem is an extension to the standard multiple regression problem in terms of assuming only ordinal properties for the dependent variable (rank order of preferred brands in a product class, academic ranks for students in a class, etc.) while retaining the interval scale assumption for independent (or predictor) variables. The linear programming formulation for obtaining the regression weights for ordinal regression, developed in an earlier paper, is outlined and computational improvements and alternatives which utilize the special structure of this linear program are developed and compared for their computational efficiency and storage requirements. A procedure which solves the dual of the original linear programming formulation by the dual simplex method with upper bounded variables, in addition to utilizing the special structure of the constraint matrix from the point of view of storage and computation, performs the best in terms of both computational efficiency and storage requirements. Using this special procedure, problems with 100 observations and 4 independent variables take less than 1/2 minute, on an average, on the IBM 360/67. Results also show that the linear programming solution procedure for ordinal regression is valid � the correlation coefficient between �true� and predicted values for the dependent variable was greater than .9 for most of the problems tested.
An algorithm is proposed for the bounded variable pure integer programming problem which treats general integer variables directly in an implicit enumeration procedure closely related to that advanced by Balas and Geoffrion for binary programming problems. Means of obtaining near optimum solutions through a slight modification of the algorithm are discussed. Techniques which use bounds on variables to improve algorithmic efficiency are developed and examined computationally. Further computational results indicate that direct treatment of general integer variables is significantly more effective than binary expansion.
We prove super-polynomial lower bounds on the size of linear programming relaxations for approximation versions of constraint satisfaction problems. We show that for these problems, polynomial-sized linear programs are no more powerful than programs arising from a constant number of rounds of the Sherali--Adams hierarchy. In particular, any polynomial-sized linear program for Max Cut has an integrality gap of � and any such linear program for Max 3-Sat has an integrality gap of ?.
In propositional logic, several problems, such as satisfiability, MAX SAT and logical inference, can be formulated as integer programs. In this paper, we consider sets of clauses for which the corresponding integer programs can be solved as linear programs. We prove that balanced sets of clauses have this property.
Methods are described and results presented for greatly reducing the computation time for long narrow problems of the transportation problem of linear programming. The code builds on known methods with two principal innovations: a substantial reduction in the size of the tree representation of shipments, and a set of methods for calculating improved starting solutions.
It has been observed by many people that a striking number of quite diverse mathematical problems can be formulated as problems in integer programming, that is, linear programming problems in which some or all of the variables are required to assume integral values. This fact is rendered quite interesting by recent research on such problems, notably by R. E. Gomory [2, 3], which gives promise of yielding efficient computational techniques for their solution. The present paper provides yet another example of the versatility of integer programming as a mathematical modeling device by representing a generalization of the well-known �Travelling Salesman Problem� in integer programming terms. The authors have developed several such models, of which the one presented here is the most efficient in terms of generality, number of variables, and number of constraints. This model is due to the second author [4] and was presented briefly at the Symposium on Combinatorial Problems held at Princeton University, April 1960, sponsored by SIAM and IBM. The problem treated is: (1) A salesman is required to visit each of n cities, indexed by 1, ��� , n. He leaves from a �base city� indexed by 0, visits each of the n other cities exactly once, and returns to city 0. During his travels he must return to 0 exactly t times, including his final return (here t may be allowed to vary), and he must visit no more than p cities in one tour. (By a tour we mean a succession of visits to cities without stopping at city 0.) It is required to find such an itinerary which minimizes the total distance traveled by the salesman. Note that if t is fixed, then for the problem to have a solution we must have tp ? n. For t = 1, p ? n, we have the standard traveling salesman problem. Let dij (i ? j = 0, 1, ��� , n) be the distance covered in traveling from city i to city j. The following integer programming problem will be shown to be equivalent to (1): (2) Minimize the linear form ?0?i?j?n? dijxij over the set determined by the relations ?ni=0i?j xij = 1 (j = 1, ��� , n) ?nj=0j?i xij = 1 (i = 1, ��� , n) ui - uj + pxij ? p - 1 (1 ? i ? j ? n) where the xij are non-negative integers and the ui (i = 1, �, n) are arbitrary real numbers. (We shall see that it is permissible to restrict the ui to be non-negative integers as well.) If t is fixed it is necessary to add the additional relation: ?nu=1 xi0 = t Note that the constraints require that xij = 0 or 1, so that a natural correspondence between these two problems exists if the xij are interpreted as follows: The salesman proceeds from city i to city j if and only if xij = 1. Under this correspondence the form to be minimized in (2) is the total distance to be traveled by the salesman in (1), so the burden of proof is to show that the two feasible sets correspond; i.e., a feasible solution to (2) has xij which do define a legitimate itinerary in (1), and, conversely a legitimate itinerary in (1) defines xij, which, together with appropriate ui, satisfy the constraints of (2). Consider a feasible solution to (2). The number of returns to city 0 is given by ?ni=1 xi0. The constraints of the form ? xij = 1, all xij non-negative integers, represent the conditions that each city (other than zero) is visited exactly once. The ui play a role similar to node potentials in a network and the inequalities involving them serve to eliminate tours that do not begin and end at city 0 and tours that visit more than p cities. Consider any xr0r1 = 1 (r1 ? 0). There exists a unique r2 such that xr1r2 = 1. Unless r2 = 0, there is a unique r3 with xr2r3 = 1. We proceed in this fashion until some rj = 0. This must happen since the alternative is that at some point we reach an rk = rj, j + 1 < k. Since none of the r's are zero we have uri - uri + 1 + pxriri + 1 ? p - 1 or uri - uri + 1 ? - 1. Summing from i = j to k - 1, we have urj - urk = 0 ? j + 1 - k, which is a contradiction. Thus all tours include city 0. It remains to observe that no tours is of length greater than p. Suppose such a tour exists, x0r1 , xr1r2 , ��� , xrprp+1 = 1 with all ri ? 0. Then, as before, ur1 - urp+1 ? - p or urp+1 - ur1 ? p. But we have urp+1 - ur1 + pxrp+1r1 ? p - 1 or urp+1 - ur1 ? p (1 - xrp+1r1) - 1 ? p - 1, which is a contradiction. Conversely, if the xij correspond to a legitimate itinerary, it is clear that the ui can be adjusted so that ui = j if city i is the jth city visited in the tour which includes city i, for we then have ui - uj = - 1 if xij = 1, and always ui - uj ? p - 1. The above integer program involves n2 + n constraints (if t is not fixed) in n2 + 2n variables. Since the inequality form of constraint is fundamental for integer programming calculations, one may eliminate 2n variables, say the xi0 and x0j, by means of the equation constraints and produce an equivalent problem with n2 + n inequalities and n2 variables. The currently known integer programming procedures are sufficiently regular in their behavior to cast doubt on the heuristic value of machine experiments with our model. However, it seems appropriate to report the results of the five machine experiments we have conducted so far. The solution procedure used was the all-integer algorithm of R. E. Gomory [3] without the ranking procedure he describes. The first three experiments were simple model verification tests on a four-city standard traveling salesman problem with distance matrix [ 20 23 4 30 7 27 25 5 25 3 21 26 ] The first experiment was with a model, now obsolete, using roughly twice as many constraints and variables as the current model (for this problem, 28 constraints in 21 variables). The machine was halted after 4000 pivot steps had failed to produce a solution. The second experiment used the earlier model with the xi0 and x0j eliminated, resulting in a 28-constraint, 15-variable problem. Here the machine produced the optimal solution in 41 pivot steps. The third experiment used the current formulation with the xi0 and x0j eliminated, yielding 13 constraints and 9 variables. The optimal solution was reached in 7 pivot steps. The fourth and fifth experiments were used on a standard ten-city problem, due to Barachet, solved by Dantzig, Johnson and Fulkerson [1]. The current formulation was used, yielding 91 constraints in 81 variables. The fifth problem differed from the fourth only in that the ordering of the rows was altered to attempt to introduce more favorable pivot choices. In each case the machine was stopped after over 250 pivot steps had failed to produce the solution. In each case the last 100 pivot steps had failed to change the value of the objective function. It seems hopeful that more efficient integer programming procedures now under development will yield a satisfactory algorithmic solution to the traveling salesman problem, when applied to this model. In any case, the model serves to illustrate how problems of this sort may be succinctly formulated in integer programming terms.
This paper presents a branch and bound method for solving problems in which the objective function is quadratic, the constraints are linear, and some or all variables are required to be integer. The algorithm is obtained by grafting an inverse-basis version of Beale's method onto the Land-Doig procedure. The code has been tested on a computer, and computational results with various strategies of branching are reported.
The subset sum problem is to decide whether or not the 0-l integer programming problem &Sgr;ni=l aixi = M, ?I, xI = 0 or 1, has a solution, where the ai and M are given positive integers. This problem is NP-complete, and the difficulty of solving it is the basis of public-key cryptosystems of knapsack type. An algorithm is proposed that searches for a solution when given an instance of the subset sum problem. This algorithm always halts in polynomial time but does not always find a solution when one exists. It converts the problem to one of finding a particular short vector v in a lattice, and then uses a lattice basis reduction algorithm due to A. K. Lenstra, H. W. Lenstra, Jr., and L. Lovasz to attempt to find v. The performance of the proposed algorithm is analyzed. Let the density d of a subset sum problem be defined by d = n/log2(maxi ai). Then for �almost all� problems of density d < 0.645, the vector v we searched for is the shortest nonzero vector in the lattice. For �almost all� problems of density d < 1/n, it is proved that the lattice basis reduction algorithm locates v. Extensive computational tests of the algorithm suggest that it works for densities d < dc(n), where dc(n) is a cutoff value that is substantially larger than 1/n. This method gives a polynomial time attack on knapsack public-key cryptosystems that can be expected to break them if they transmit information at rates below dc(n), as n ? ?.
For P-complete problems such as traveling salesperson, cycle covers, 0-1 integer programming, multicommodity network flows, quadratic assignment, etc., it is shown that the approximation problem is also P-complete. In contrast with these results, a linear time approximation algorithm for the clustering problem is presented.
Computational experience with a modified linear programming method for the inequality or equality set covering problem (i.e. minimize cx subject to Ex ? e or Ex = e, xi = 0 or 1, where E is a zero-one matrix, e is a column of ones, and c is a nonnegative integral row) is presented. The zero-one composition of the constraint matrix and the right-hand side of ones suggested an algorithm in which dual simplex iterations are performed whenever unit pivots are available and Gomory all integer cuts are adjoined when they are not. Applications to enumerative and heuristic schemes are also discussed.
A new fitness-based individual migration operator after the model of a migration phenomenon in a natural ecosystem is presented. The relative fitness variations are evaluated for each individual and those are used to determine whether an individual migrate to a new solution candidate which is far from its original position. This migration guarantees the search diversity by the uniformly dispersed individuals on a search space at the initial phase of evolution and fast convergence by the migrated individuals as the evolution progresses. The usefulness of the migration operator has been tested using evolutionary programming (EP) which adopts the operator. The performance of the proposed EP has been compared with those of other well-known EP algorithms through benchmark problems on continuous function optimization.
Let f : Rs ? R be a real-valued function, and let x = (x1,...,xs)T ? Rs be partitioned into t subsets of non-overlapping variables as x = (X1,...,Xt)T, with Xi ? Rpi for i = 1,...,t, ?i=1tpi = s. Alternating optimization (AO) is an iterative procedure for minimizing f(x) = f(X1, X2,..., Xt) jointly over all variables by alternating restricted minimizations over the individual subsets of variables X1,...., Xt. Alternating optimization has been (more or less) studied and used in a wide variety of areas. Here a self-contained and general convergence theory is presented that is applicable to all partitionings of x. Under reasonable assumptions, the general AO approach is shown to be locally, q-linearly convergent, and to also exhibit a type of global convergence.
The efficiency of optimization in fuzzy c-means clustering is investigated. Numerous, powerful, general-purpose simultaneous optimization (SO) methods, and hybrid methods combining these and the most widely used alternating optimization (AO) method, are extensively tested for speed comparison. AO is clearly the best and simplest of the methods we tested when used on data sets of small or moderate sizes, especially those containing well-separated clusters. This justifies the extremely wide use of AO. On large-scale problems, some methods we tested are significantly faster than AO.
A recent paper [14] has considered the possibility of combining interior point strategy with steepest descent method when solving convex programming problems, in such a way that the convergence property of the interior point method remains valid but many iterations do not request the solution of a system of equations. Motivated by this general idea, the paper [14] proposed a hybrid algorithm which combines a primal-dual potential reduction algorithm with the use of the steepest descent direction of the potential function. The O(?n|ln ?|) complexity of the potential reduction algorithm remains valid but the overall computational cost can be reduced. In this paper, we discuss the relation between this method and general projected descent direction methods, and compare it with a projected steepest descent direction method for solving complex programming problems.
Genetic Algorithms (GA) are innovative search algorithms based on natural phenomena whose main advantages lie in solving mostly high complex problems. This paper provides how a conventional GA can effectively solve the Product Mix and Material Match problem. Some novel ideas in chromosome representation and evaluation are also addressed. This GA approach produces good results with fast convergence speed at the shop floor level, which is verified and validated via real world applications. The product mix and material match approach may help the manager to control the production of electronics manufacturing to meet the customer's demand.
In this paper we present a new algorithm - called the quartic method - for one-dimensional optimization. The quartic method is the third and final member of a family of algorithms called the Taylor Approximation Methods which includes Newton's method and Euler's method. Like its two distinguished relatives, the new method is also expected to be very efficient in practice. We present preliminary numerical results comparing the quartic method with both Newton's method and other fourth order algorithms. The numerical results suggest that the new method is significantly faster than Newton's method (and other fourth order algorithms) both in terms of the number of iterations and the actual running time. Theoretical considerations and preliminary numerical results suggest that the quartic method could emerge as a serious candidate for practical use in the future.
In this paper, optimal control for stochastic linear singular system with indefinite control cost and cross term in the cost functional is obtained using neural networks. The goal is to provide optimal control with reduced calculus effort by comparing the solutions of the matrix Riccati differential equation (MRDE) obtained from well known traditional Runge Kutta (RK) method and nontraditional neural network method. To obtain the optimal control, the solution of MRDE is computed by feed forward neural network (FFNN). Accuracy of the solution of the neural network approach to the problem is qualitatively better. The advantage of the proposed approach is that, once the network is trained, it allows instantaneous evaluation of solution at any desired number of points spending negligible computing time and memory. The computation time of the proposed method is shorter than the traditional RK method. An illustrative numerical example is presented for the proposed method.
This article focuses on the introduction of control, authoring, and composition in human-computer music improvisation through the description of a guided music generation model and a reactive architecture, both implemented in the software ImproteK. This interactive music system is used with expert improvisers in work sessions and performances of idiomatic and pulsed music and more broadly in situations of structured or composed improvisation. The article deals with the integration of temporal specifications in the music generation process by means of a fixed or dynamic �scenario� and addresses the issue of the dialectic between reactivity and planning in interactive music improvisation. It covers the different levels involved in machine improvisation: the integration of anticipation relative to a predefined structure in a guided generation process at a symbolic level, an architecture combining this anticipation with reactivity using mixed static/dynamic scheduling techniques, and an audio rendering module performing live re-injection of captured material in synchrony with a non-metronomic beat. Finally, it sketches a framework to compose improvisation sessions at the scenario level, extending the initial musical scope of the system. All of these points are illustrated by videos of performances or work sessions with musicians.
In 2003, Atserias and Dalmau resolved a major open question about the resolution proof system by establishing that the space complexity of a Conjunctive Normal Form (CNF) formula is always an upper bound on the width needed to refute the formula. Their proof is beautiful but uses a nonconstructive argument based on Ehrenfeucht-Fra�ss� games. We give an alternative, more explicit, proof that works by simple syntactic manipulations of resolution refutations. As a by-product, we develop a �black-box� technique for proving space lower bounds via a �static� complexity measure that works against any resolution refutation�previous techniques have been inherently adaptive. We conclude by showing that the related question for polynomial calculus (i.e., whether space is an upper bound on degree) seems unlikely to be resolvable by similar methods.
We present a simple proof of the bounded-depth Frege proof lower bounds of Pitassi et al. [1993] and Kraj�?ek et al. [1995] for the pigeonhole principle. Our method uses the interpretation of proofs as two player games given by Pudl�k and Buss. Our lower bound is conceptually simpler than previous ones, and relies on tools and intuition that are well known in the context of computational complexity. This makes the lower bound of Pitassi et al. [1993] and Kraj�?ek et al. [1995] accessible to the general computational complexity audience. We hope this new view will open new directions for research in proof complexity.
Probabilistic abstract argumentation combines Dung�s abstract argumentation framework with probability theory in order to model uncertainty in argumentation. In this setting, we address the fundamental problem of computing the probability that a set of arguments is an extension according to a given semantics. We focus on the most popular semantics (i.e., admissible, stable, complete, grounded, preferred, ideal-set, ideal, stage, and semistable) and show the following dichotomy result: computing the probability that a set of arguments is an extension is either FP or FP#P-complete depending on the semantics adopted. Our polynomial-time results are particularly interesting, as they hold for some semantics for which no polynomial-time technique was known so far.
There are methods to turn short refutations in polynomial calculus (Pc) and polynomial calculus with resolution (Pcr) into refutations of low degree. Bonet and Galesi [1999, 2003] asked if such size-degree tradeoffs for Pc [Clegg et al. 1996; Impagliazzo et al. 1999] and Pcr [Alekhnovich et al. 2004] are optimal. We answer this question by showing a polynomial encoding of the graph ordering principle on m variables which requires Pc and Pcr refutations of degree ?(&sqrt; m). Tradeoff optimality follows from our result and from the short refutations of the graph ordering principle in Bonet and Galesi [1999, 2001]. We then introduce the algebraic proof system Pcrk which combines together polynomial calculus and k-DNF resolution (Resk). We show a size hierarchy theorem for Pcrk: Pcrk is exponentially separated from Pcrk+1. This follows from the previous degree lower bound and from techniques developed for Resk. Finally we show that random formulas in conjunctive normal form (3-CNF) are hard to refute in Pcrk.
The semantics of most logics of time and probability is given via a probability distribution over threads, where a thread is a structure specifying what will be true at different points in time (in the future). When assessing the probabilities of statements such as �Event a will occur within 5 units of time of event b,� there are many different semantics possible, even when assessing the truth of this statement within a single thread. We introduce the syntax of annotated probabilistic temporal (APT) logic programs and axiomatically introduce the key notion of a frequency function (for the first time) to capture different types of intrathread reasoning, and then provide a semantics for intrathread and interthread reasoning in APT logic programs parameterized by such frequency functions. We develop a comprehensive set of complexity results for consistency checking and entailment in APT logic programs, together with sound and complete algorithms to check consistency and entailment. The basic algorithms use linear programming, but we then show how to substantially and correctly reduce the sizes of these linear programs to yield better computational properties. We describe a real world application we are developing using APT logic programs.
We study the computational complexity of Boolean constraint satisfaction problems with cardinality constraint. A Galois connection between clones and coclones has received a lot of attention in the context of complexity considerations for constraint satisfaction problems. This connection does not seem to help when considering constraint satisfaction problems that support in addition a cardinality constraint. We prove that a similar Galois connection, involving a weaker closure operator and partial polymorphisms, can be applied to such problems. Thus, we establish dichotomies for the decision as well as for the counting problems in Schaefer's framework.
In a seminal paper from 1985, Sistla and Clarke showed that the model-checking problem for Linear Temporal Logic (LTL) is either NP-complete or PSPACE-complete, depending on the set of temporal operators used. If in contrast, the set of propositional operators is restricted, the complexity may decrease. This article systematically studies the model-checking problem for LTL formulae over restricted sets of propositional and temporal operators. For almost all combinations of temporal and propositional operators, we determine whether the model-checking problem is tractable (in PTIME) or intractable (NP-hard). We then focus on the tractable cases, showing that they all are NL-complete or even logspace solvable. This leads to a surprising gap in complexity between tractable and intractable cases. It is worth noting that our analysis covers an infinite set of problems, since there are infinitely many sets of propositional operators.
We introduce a new tractable temporal constraint language, which strictly contains the Ord-Horn language of B�rkert and Nebel and the class of AND/OR precedence constraints. The algorithm we present for this language decides whether a given set of constraints is consistent in time that is quadratic in the input size. We also prove that (unlike Ord-Horn) the constraint satisfaction problem of this language cannot be solved by Datalog or by establishing local consistency.
We study the (non-uniform) quantified constraint satisfaction problem QCSP(H) as H ranges over semicomplete digraphs. We obtain a complexity-theoretic trichotomy: QCSP(H) is either in P, is NP-complete, or is Pspace-complete. The largest part of our work is the algebraic classification of precisely which semicomplete digraphs enjoy only essentially unary polymorphisms, which is combinatorially interesting in its own right.
In recent research on nonmonotonic logic programming, repeatedly strong equivalence of logic programs P and Q has been considered, which holds if the programs P?R and Q?R have the same answer sets for any other program R. This property strengthens the equivalence of P and Q with respect to answer sets (which is the particular case for R=?), and has its applications in program optimization, verification, and modular logic programming. In this article, we consider more liberal notions of strong equivalence, in which the actual form of R may be syntactically restricted. On the one hand, we consider uniform equivalence where R is a set of facts, rather than a set of rules. This notion, which is well-known in the area of deductive databases, is particularly useful for assessing whether programs P and Q are equivalent as components of a logic program which is modularly structured. On the other hand, we consider relativized notions of equivalence where R ranges over rules over a fixed alphabet, and thus generalize our results to relativized notions of strong and uniform equivalence. For all these notions, we consider disjunctive logic programs in the propositional (ground) case as well as some restricted classes, providing semantical characterizations and analyzing the computational complexity. Our results, which naturally extend to answer set semantics for programs with strong negation, complement the results on strong equivalence of logic programs and pave the way for optimizations in answer set solvers as a tool for input-based problem solving.
We systematically investigate the complexity of model checking the existential positive fragment of first-order logic. In particular, for a set of existential positive sentences, we consider model checking where the sentence is restricted to fall into the set; a natural question is then to classify which sentence sets are tractable and which are intractable. With respect to fixed-parameter tractability, we give a general theorem that reduces this classification question to the corresponding question for primitive positive logic, for a variety of representations of structures. This general theorem allows us to deduce that an existential positive sentence set having bounded arity is fixed-parameter tractable if and only if each sentence is equivalent to one in bounded-variable logic. We then use the lens of classical complexity to study these fixed-parameter tractable sentence sets. We show that such a set can be NP-complete, and consider the length needed by a translation from sentences in such a set to bounded-variable logic; we prove superpolynomial lower bounds on this length using the theory of compilability, obtaining an interesting type of formula size lower bound. Overall, the tools, concepts, and results of this article set the stage for the future consideration of the complexity of model checking on more expressive logics.
Let A and B be strings of common length n. Define LLCS(A, B) to be the length of the longest common subsequence of A and B. Hunt and Szymanski presented an algorithm for finding LLCS(A, B) with time complexity O((r + n)logn), where r is the number of elements in the set {(i, j)|A[i] = B[j]}. In the worst case the algorithm has running time of O(n2logn). We present an improvement to this algorithm which changes the time complexity to O(r + n(LLCS(A, B) + logn)). Some experimental results show dramatic improvements for large n.
We survey several algorithms for searching a string in a piece of text. We include theoretical and empirical results, as well as the actual code of each algorithm. An extensive bibliography is also included.
A decimal notation satisfies many simple mathematical properties, and it is a useful tool in the analysis of trees. A practical method is presented, that compresses the decimal codes while maintaining the fast determination of relations(e.g., ancestor, descendant, brother, etc.). A special node, called a kernel node, including many common subcodes of the other codes, is defined, and a compact data structure is presented using the kernel nodes. Let n(m) be the number of the total(kernel) nodes. It is theoretically proved that encoding a decimal code is a constant time, that the worst-case time complexity of compressing the decimal codes is O(n+ m2), and that the size of the data structure is proportional to m. From the experimental results of some hierarchical semantic primitives for natural language processing, it is shown that the ratio m/n becomes an extremely small value, ranging from 0.047 to 0.13.
Large networks of cameras have been increasingly employed to capture dynamic events for tasks such as surveillance and training. When using active cameras to capture events distributed throughout a large area, human control becomes impractical and unreliable. This has led to the development of automated approaches for online camera control. We introduce a new automated camera control approach that consists of a stochastic performance metric and a constrained optimization method. The metric quantifies the uncertainty in the state of multiple points on each target. It uses state-space methods with stochastic models of target dynamics and camera measurements. It can account for occlusions, accommodate requirements specific to the algorithms used to process the images, and incorporate other factors that can affect their results. The optimization explores the space of camera configurations over time under constraints associated with the cameras, the predicted target trajectories, and the image processing algorithms. The approach can be applied to conventional surveillance tasks (e.g., tracking or face recognition), as well as tasks employing more complex computer vision methods (e.g., markerless motion capture or 3D reconstruction).
One of the most fundamental tasks of wireless sensor networks is to provide coverage of the deployment region. We study the coverage of a line interval with a set of wireless sensors with adjustable coverage ranges. Each coverage range of a sensor is an interval centered at that sensor whose length is decided by the power the sensor chooses. The objective is to find a range assignment with the minimum cost. There are two variants of the optimization problem. In the discrete variant, each sensor can only choose from a finite set of powers, whereas in the continuous variant, each sensor can choose power from a given interval. For the discrete variant of the problem, a polynomial-time exact algorithm is designed. For the continuous variant of the problem, NP-hardness of the problem is proved and followed by an ILP formulation. Then, constant-approximation algorithms are designed when the cost for all sensors is proportional to r? for some constant ? ? 1, where r is the covering radius corresponding to the chosen power. Specifically, if ? = 1, we give a 1.25-approximation algorithm and a fully polynomial-time approximation scheme; if ? > 1, we give a 2-approximation algorithm. We also show that the approximation analyses are tight.
Visual sensor networks (VSNs) are becoming increasingly popular in a number of application domains. A distinguishing characteristic of VSNs is to self-configure to minimize the need for operator control and to improve scalability. One of the areas of self-configuration is camera coverage control that is, how should cameras adjust their field-of-views to cover maximum targets? This is an NP-hard problem. We show that the existing heuristics have a number of weaknesses that influence both coverage and overhead. Therefore, we first propose a computationally efficient centralized heuristic that provides near-optimal coverage for small-scale networks. However, it requires significant communication and computation overhead, making it unsuitable for large-scale networks. Thus, we develop a distributed algorithm that outperforms the existing distributed algorithm with lower communication overhead, at the cost of coverage accuracy. We show that the proposed heuristics guarantee to cover at least half of the targets covered by the optimal solution. Finally, to gain benefits of both centralized and distributed algorithms, we propose a hierarchical algorithm where cameras are decomposed into neighborhoods that coordinate their coverage using an elected local coordinator. We observe that the hierarchical algorithm provides scalable near-optimal coverage with networking cost significantly less than that of centralized and distributed solutions.
We present a robust, dynamic scheme for the automatic self-deployment and relocation of mobile sensor nodes (e.g., unmanned ground vehicles, robots) around areas where phenomena take place. Our scheme aims (i) to sense environmental contextual parameters and accurately capture the spatiotemporal evolution of a certain phenomenon (e.g., fire, air contamination) and (ii) to fully automate the deployment process by letting nodes relocate, self-organize (and self-reorganize), and optimally cover the focus area. Our intention is to �opportunistically� modify the previous placement of nodes to attain high-quality phenomenon monitoring. The required intelligence is fully distributed within the mobile sensor network so the deployment algorithm is executed incrementally by different nodes. The presented algorithm adopts the Particle Swarm Optimization technique, which yields very promising results as reported in the article (performance assessment). Our findings show that the proposed algorithm captures a certain phenomenon with very high accuracy while maintaining the networkwide energy expenditure at low levels. Random occurrences of similar phenomena put stress upon the algorithm which manages to react promptly and efficiently manage the available sensing resources in the broader setting.
Self-sustainability is a crucial step for modern sensor networks. Here, we offer an original and comprehensive framework for autonomous sensor networks powered by renewable energy sources. We decompose our design into two nested optimization steps: the inner step characterizes the optimal network operating point subject to an average energy consumption constraint, while the outer step provides online energy management policies that make the system energetically self-sufficient in the presence of unpredictable and intermittent energy sources. Our framework sheds new light into the design of pragmatic schemes for the control of energy-harvesting sensor networks and permits to gauge the impact of key sensor network parameters, such as the battery capacity, the harvester size, the information transmission rate, and the radio duty cycle. We analyze the robustness of the obtained energy management policies in the cases where the nodes have differing energy inflow statistics and where topology changes may occur, devising effective heuristics. Our energy management policies are finally evaluated considering real solar radiation traces, validating them against state-of-the-art solutions, and describing the impact of relevant design choices in terms of achievable network throughput and battery-level dynamics.
Acoustic source localization has many important applications. Convex relaxation provides a viable approach of obtaining good estimates very efficiently. There are two popular convex relaxation methods using either semi-definite programming (SDP) or second-order cone programming (SOCP). However, the performances of the methods have not been studied properly in the literature and there is no comparison in terms of accuracy and performance. The aims of this article are twofold. First of all, we study and compare several convex relaxation methods. We demonstrate, by numerical examples, that most of the convex relaxation methods cannot localize the source exactly, even in the performance limit when the time difference of arrival (TDOA) information is exact. In addressing this problem, we propose a novel mixed SDP-SOCP relaxation model and study the characteristics of the optimal solutions and its localizable region. Furthermore, an error correction scheme for the proposed SDP-SOCP model is developed so that exact localization can be achieved in the performance limit. Experimental data have been collected in a room with two different array configurations to demonstrate our proposed approach.
Wireless sensor networks can often be viewed in terms of a uniform deployment of a large number of nodes in a region of Euclidean space. Following deployment, the nodes self-organize into a mesh topology with a key aspect being self-localization. Having obtained a mesh topology in a dense, homogeneous deployment, a frequently used approximation is to take the hop distance between nodes to be proportional to the Euclidean distance between them. In this work, we analyze this approximation through two complementary analyses. We assume that the mesh topology is a random geometric graph on the nodes; and that some nodes are designated as anchors with known locations. First, we obtain high probability bounds on the Euclidean distances of all nodes that are h hops away from a fixed anchor node. In the second analysis, we provide a heuristic argument that leads to a direct approximation for the density function of the Euclidean distance between two nodes that are separated by a hop distance h. This approximation is shown, through simulation, to very closely match the true density function. Localization algorithms that draw upon the preceding analyses are then proposed and shown to perform better than some of the well-known algorithms present in the literature. Belief-propagation-based message-passing is then used to further enhance the performance of the proposed localization algorithms. To our knowledge, this is the first usage of message-passing for hop-count-based self-localization.
We study the power-aware buffering problem in battery-powered sensor networks, focusing on the fixed-size and fixed-interval buffering schemes. The main motivation is to address the yet poorly understood size variation-induced effect on power-aware buffering schemes. Our theoretical analysis elucidates the fundamental differences between the fixed-size and fixed-interval buffering schemes in the presence of data-size variation. It shows that data-size variation has detrimental effects on the power expenditure of the fixed-size buffering in general, and reveals that the size variation induced effects can be either mitigated by a positive skewness or promoted by a negative skewness in size distribution. By contrast, the fixed-interval buffering scheme has an obvious advantage of being eminently immune to the data-size variation. Hence the fixed-interval buffering scheme is a risk-averse strategy for its robustness in a variety of operational environments. In addition, based on the fixed-interval buffering scheme, we establish the power consumption relationship between child nodes and parent node in a static data-collection tree, and give an in-depth analysis of the impact of child bandwidth distribution on the parent's power consumption. This study is of practical significance: it sheds new light on the relationship among power consumption of buffering schemes, power parameters of radio module and memory bank, data arrival rate, and data-size variation, thereby providing well-informed guidance in determining an optimal buffer size (interval) to maximize the operational lifespan of sensor networks.
We study the problem of optimal sensor placement in the context of soil moisture sensing. We show that the soil moisture data possesses some unique features that can be used together with the commonly used Gaussian assumption to construct more scalable, robust, and better performing placement algorithms. Specifically, there exists a coarse-grained monotonic ordering of locations in their soil moisture level over time, both in terms of its first and second moments, a feature much more stable than the soil moisture process itself at these locations. This motivates a clustered sensor placement scheme, where locations are classified into clusters based on the ordering of the mean, with the number of sensors placed in each cluster determined by the ordering of the variances. We show that under idealized conditions the greedy mutual information maximization algorithm applied globally is equivalent to that applied cluster by cluster, but the latter has the advantage of being more scalable. Extensive numerical experiments are performed on a set of three-dimensional soil moisture data generated by a state-of-the-art soil moisture simulator. Our results show that our clustering approach outperforms applying the same algorithms globally, and is very robust to lack of training and errors in training data.
Let consider a set of anonymous moving objects to be tracked in a binary sensor network. This article studies the problem of associating deterministically a track revealed by the sensor network with the trajectory of an unique anonymous object, namely the multiple object tracking and identification (MOTI) problem. In our model, the network is represented by a sparse connected graph where each vertex represents a binary sensor and there is an edge between two sensors if an object can pass from one sensed region to another one without activating any other sensor. The difficulty of MOTI lies in the fact that the trajectories of two or more objects can be so close that the corresponding tracks on the sensor network can no longer be distinguished (track merging), thus confusing the deterministic association between an object trajectory and a track. The article presents several results. We first show that MOTI cannot be solved on a general graph of ideal binary sensors even by an omniscient external observer if all the objects can freely move on the graph. Then we describe restrictions that can be imposed a priori either on the graph, on the object movements, or on both, to make the MOTI problem always solvable. In the absence of an omniscient observer, we show how our results can lead to the definition of distributed algorithms that are able to detect when the system is in a state where MOTI becomes unsolvable.
Should there be a standard in the way of algorithms languages to improve productivity and remove ambiguity?
The XRDS blog highlights a range of topics from security and privacy to neuroscience. Selected blog posts, edited for print, will be featured in every issue. Please visit xrds.acm.org/blog to read each post in its entirety.
Quantum computing is not merely a recipe for new computing devices, but a new way of looking at the world.
We already know algorithms can make our lives and our work more efficient, but how can we go beyond that to create trustworthy, fair, and enjoyable workplaces in which workers can find meaning and continuously learn?
Programs written in multiple, partially disjoint versions of a high-level language may have to migrate for program compilation in the environment in which the versions are distributed. The decisions made during program compilation concerning which node is to be selected from the possible ones supporting those disjoint features constitute a process migration path. The shortest paths which minimize the time spent in process migration can be determined using a dynamic programming approach. The approach is versatile in that it determines some of the shortest paths in a branch-and-bound way and determines all of the shortest paths in a non-brute force way by pruning those branches at the turning points that version expansion is not necessary.
We present a method to simplify the implementation of a kernel supporting a high level concurrent language on a bare multiprocessor system.User and system level languages (with as much common syntax as possible) are defined. The system language consists of those constructs which do not require concurrency support, plus other facilities which make it well suited to write the kernel supporting the user language. Such constructs must provide the equivalent of a supervisor call interface within a high level language.All machine dependencies for both the system and user languages have been encapsulated in the code generation phase of the compilation process. Our method reduces the retargeting problem of a kernel to the orientability problem of the code generator.Finally our experiences in using such methodology are discussed.
The performance of any computing system is seriously affected by the performance of the resident operating system. Distributed computing systems are no different. There have been many successful attempts at harnessing the full potential offered by distributed computing's inherent flexibility. However by far the majority of these implementations have been made on homogeneous configurations; the complexity implicit in connecting a number of heterogeneous sub-systems into a single, unified distributed computing system, has precluded their realisation. The system designer has to compare the many characteristics of a design before deciding upon the final solution. In doing so the designer needs a framework, or model, upon which he can assess alternative strategies. Most of the models to date have been based upon the layered approach which simplfies the software strucutre but it cannot reflect the control infrastructure which is necessary to effect such facilities as reconfigurability, resilience. etc.
This article presents TiledVS, a fast external algorithm and implementation for computing viewsheds. TiledVS is intended for terrains that are too large for internal memory, even more than 100,000�100,000 points. It subdivides the terrain into tiles that are stored compressed on disk and then paged into memory with a custom cache data structure and least recently used algorithm. If there is sufficient available memory to store a whole row of tiles, which is easy, then this specialized data management is faster than relying on the operating system�s virtual memory management. Applications of viewshed computation include siting radio transmitters, surveillance, and visual environmental impact measurement. TiledVS runs a rotating line of sight from the observer to points on the region boundary. For each boundary point, it computes the visibility of all terrain points close to the line of sight. The running time is linear in the number of points. No terrain tile is read more than twice. TiledVS is very fast, for instance, processing a 104,000�104,000 terrain on a modest computer with only 512MB of RAM took only 1� hours. On large datasets, TiledVS was several times faster than competing algorithms, such as the ones included in GRASS. The source code of TiledVS is freely available for nonprofit researchers to study, use, and extend. A preliminary version of this algorithm appeared in a four-page ACM SIGSPATIAL GIS 2012 conference paper, �More Efficient Terrain Viewshed Computation on Massive Datasets Using External Memory.� This more detailed version adds the fast lossless compression stage that reduces the time by 30% to 40%, and many more experiments and comparisons.
In this article, we present an algorithmic system for determining the proper correspondence between place markers and their labels in historical maps. We assume that the locations of place markers (usually pictographs) and labels (pieces of text) have already been determined -- either algorithmically or by hand -- and we want to match the labels to the markers. This time-consuming step in the digitization process of historical maps is nontrivial even for humans but provides valuable metadata (e.g., when subsequently georeferencing the map). To speed up this process, we model the problem in terms of combinatorial optimization, solve that problem efficiently, and show how user interaction can be used to improve the quality of the results. We also consider a version of the model where we are given label fragments and additionally have to decide which fragments go together. We show that this problem is NP-hard. However, we give a polynomial-time algorithm for a restricted version of this fragment assignment problem. We have implemented the algorithm for the main problem and tested it on a manually extracted ground truth for eight historical maps with a combined total of more than 12,800 markers and labels. On average, the algorithm correctly matches 96% of the labels and is robust against noisy input. It furthermore performs a sensitivity analysis and in this way computes a measure of confidence for each of the matches. We use this as the basis for an interactive system where the user�s effort is directed to checking those parts of the map where the algorithm is unsure; any corrections the user makes are propagated by the algorithm. We discuss a prototype of this system and statistically confirm that it successfully locates those areas on the map where the algorithm needs help.
Comparing two geometric graphs embedded in space is important in the field of transportation network analysis. Given street maps of the same city collected from different sources, researchers often need to know how and where they differ. However, the majority of current graph comparison algorithms are based on structural properties of graphs, such as their degree distribution or their local connectivity properties, and do not consider their spatial embedding. This ignores a key property of road networks since the similarity of travel over two road networks is intimately tied to the specific spatial embedding. Likewise, many current algorithms specific to street map comparison either do not provide quality guarantees or focus on spatial embeddings only. Motivated by road network comparison, we propose a new path-based distance measure between two planar geometric graphs that is based on comparing sets of travel paths generated over the graphs. Surprisingly, we are able to show that using paths of bounded link-length, we can capture global structural and spatial differences between the graphs. We show how to utilize our distance measure as a local signature in order to identify and visualize portions of high similarity in the maps. Finally, we present an experimental evaluation of our distance measure and its local signature on street map data from Berlin, Germany and Athens, Greece.
Deficiencies in computer-related education for end users have contributed to the lack of successful integration of computer-based information systems (CBIS) into organizations. Data gathered from 20 companies whose CBIS exert a high strategic impact on existing operating systems were studied to determine how these companies accomplished "education integration." About 80% of the companies reported training budgets between zero and two percent of the CBIS budget, with a majority of the participants reported training staffs consisting of two or less full time trainers. Of the seven educational techniques used by the companies, the Resident Expert technique rated superior in terms of quantity and quality. A description of the activities in four companies with unique approaches to the education of their user community provides additional insights into the role of organizations in promoting the integration and use of CBIS among end users.
In this article, the expectation maximization (EM) algorithm is applied for modeling the throughput of emergency departments via available time-series data. The dynamics of emergency department throughput is developed and evaluated, for the first time, as a stochastic dynamic model that consists of the noisy measurement and first-order autoregressive (AR) stochastic dynamic process. By using the EM algorithm, the model parameters, the actual throughput, as well as the noise intensity, can be identified simultaneously. Four real-world time series collected from an emergency department in West London are employed to demonstrate the effectiveness of the introduced algorithm. Several quantitative indices are proposed to evaluate the inferred models. The simulation shows that the identified model fits the data very well.
In a landmark paper, Papadimitriou and Roughgarden described a polynomial-time algorithm ("Ellipsoid Against Hope") for computing sample correlated equilibria of concisely-represented games. Recently, Stein, Parrilo and Ozdaglar showed that this algorithm can fail to find an exact correlated equilibrium, but can be easily modified to efficiently compute approximate correlated equilibria. It remained an open problem to determine whether the algorithm can be modified to compute an exact correlated equilibrium. In a new paper, we showed that it can, presenting a variant of the Ellipsoid Against Hope algorithm that guarantees the polynomial-time identification of exact correlated equilibrium. Our new algorithm differs from the original primarily in its use of a separation oracle that produces cuts corresponding to pure-strategy profiles. As a result, we no longer face the numerical precision issues encountered by the original approach, and both the resulting algorithm and its analysis are considerably simplified. Our new separation oracle can be understood as a derandomization of Papadimitriou and Roughgarden's original separation oracle via the method of conditional expectations.
We consider auction settings where the seller is constrained in the amount and nature of information he may reveal about the good being sold. This is encountered, for example, in online advertising auctions, where communicating precise details of every viewer to interested advertisers is impractical, costly, and possibly socially undesirable. We initiate the study of constrained signaling in such settings, where a seller must choose which information to reveal subject to exogenous constraints on the signaling policy. We consider a seller employing the second-price auction, and present algorithms and hardness results for approximating the welfare and revenue maximizing signaling policies under a variety of constraints.
Among other solution concepts, the notion of the pure Nash equilibrium plays a central role in Game Theory. Pure Nash equilibria in a game characterize situations with non-cooperative deterministic players in which no player has any incentive to unilaterally deviate from the current situation in order to achieve a higher payoff. Unfortunately, it is well known that there are games that do not have pure Nash equilibria. Furhermore, even in games where the existence of equilibria is guaranteed, their computation can be a computationally hard task. Such negative results significantly question the importance of pure Nash equilibria as solution concepts that characterize the behavior of rational players. Approximate pure Nash equilibria, which characterize situations where no player can significantly improve her payoff by unilaterally deviating from her current strategy, could serve as alternative solution concepts provided that they exist and can be computed efficiently. In this letter, we discuss recent positive algorithmic results for approximate pure Nash equilibria in congestion games.
In our recent paper [Rubinstein 2016] we rule out a PTAS for the 2-Player Nash Equilibrium Problem. More precisely, we prove that there exists a constant ? > 0 such that, assuming the Exponential Time Hypothesis for PPAD, computing an ?-approximate Nash equilibrium in a two-player n � n game requires time nlog1?o(1) n. This matches (up to the o (1) term) the algorithm of Lipton, Markakis, and Mehta [Lipton et al. 2003].
One of the most appealing aspects of correlated equilibria and coarse correlated equilibria is that natural dynamics quickly arrive at approximations of such equilibria, even in games with many players. In addition, there exist polynomial-time algorithms that compute exact correlated and coarse correlated equilibria. However, in general these dynamics and algorithms do not provide a guarantee on the quality (say, in terms of social welfare) of the resulting equilibrium. In light of these results, a natural question is how good are the correlated and coarse correlated equilibria---in terms natural objectives such as social welfare or Pareto optimality---that can arise from any efficient algorithm or dynamics. We address this question, and establish strong negative results. In particular, we show that in multiplayer games that have a succinct representation, it is NP-hard to compute any coarse correlated equilibrium (or approximate coarse correlated equilibrium) with welfare strictly better than the worst possible. The focus on succinct games ensures that the underlying complexity question is interesting; many multiplayer games of interest are in fact succinct. We show that analogous hardness results hold for correlated equilibria, and persist under the egalitarian objective or Pareto optimality. To complement the hardness results, we develop an algorithmic framework that identifies settings in which we can efficiently compute an approximate correlated equilibrium with near-optimal welfare. We use this framework to develop an efficient algorithm for computing an approximate correlated equilibrium with near-optimal welfare in aggregative games.
Some recent advances in the identification of tractable classes of combinatorial auctions are discussed. In particular, the work of [Gottlob and Greco 2007] is illustrated, where a research question raised in [Conitzer et al. 2004] is solved by showing that the class of structured item graphs is not efficiently recognizable (i.e., deciding the membership of instances is NP-hard), and where this difficulty is overcome trough a different approach based on the notion of hypertree decomposition.
We present two new critical domains where security games are applied to generate randomized patrol schedules. For each setting, we present the current research that we have produced. We then propose two new challenges to build accurate schedules that can be deployed effectively in the real world. The first is a planning challenge. Current schedules cannot handle interruptions. Thus, more expressive models, that allow for reasoning over stochastic actions, are needed. The second is a learning challenge. In several security domains, data can be used to extract information about both the environment and the attacker. This information can then be used to improve the defender's strategies.
A Stackelberg game is played between a leader and a follower. The leader first chooses an action, and then the follower plays his best response, and the goal of the leader is to pick the action that will maximize his payoff given the follower's best response. Stackelberg games capture, for example, the following interaction between a retailer and a buyer. The retailer chooses the prices of the goods he produces, and then the buyer chooses to buy a utility-maximizing bundle of goods. The goal of the retailer here is to set prices to maximize his profit---his revenue minus the production cost of the purchased bundle. It is quite natural that the retailer in this example would not know the buyer's utility function. However, he does have access to revealed preference feedback---he can set prices, and then observe the purchased bundle and his own profit. We give algorithms for efficiently solving, in terms of both computational and query complexity, a broad class of Stackelberg games in which the follower's utility function is unknown, using only "revealed preference" access to it. This class includes the profit maximization problem, as well as the optimal tolling problem in nonatomic congestion games, when the latency functions are unknown. Surprisingly, we are able to solve these problems even though the corresponding maximization problems are not concave in the leader's actions.
Fair allocation of indivisible objects under ordinal preferences is an important problem. Unfortunately, a fairness notion like envy- freeness is both incompatible with Pareto optimality and is also NP-complete to achieve. To tackle this predicament, we consider a different notion of fairness, namely proportionality. We frame allocation of indivisible objects as randomized assignment but with integrality requirements. We then use the stochastic dominance relation to define two natural notions of proportionality. Since an assignment may not exist even for the weaker notion of proportionality, we propose relaxations of the concepts --- optimal weak proportionality and optimal proportionality. For both concepts, we propose algorithms to compute fair assignments under ordinal preferences. Both new fairness concepts appear to be desirable in view of the following: they are compatible with Pareto optimality, admit efficient algorithms to compute them, are based on proportionality, and are guaranteed to exist.
We solve the optimal multi-dimensional mechanism design problem when either the number of bidders is a constant or the number of items is a constant. In the first setting, we need that the values of each bidder for the items are i.i.d., but allow different distributions for each bidder. In the second setting, we allow the values of each bidder for the items to be arbitrarily correlated, but assume that the bidders are i.i.d. For all ? > 0, we obtain an efficient additive ?-approximation, when the value distributions are bounded, or a multiplicative (1--?)-approximation when the value distributions are unbounded, but satisfy the Monotone Hazard Rate condition. When there is a single bidder, we generalize these results to independent but not necessarily identically distributed value distributions, and to independent regular distributions.
Two fundamental problems in economics are voting and assignment. In both settings, random serial dictatorship is a well-established mechanism that satisfies anonymity, ex post efficiency, and strategyproofness. We present an overview of recent results on the computational complexity of problems related to random serial dictatorship.
The design and analysis of online algorithms, where the input to the algorithm is revealed over time and the algorithm has to make decisions immediately without knowing the future input, has received a revived interest in the last few years primarily due to their application to online advertising. The canonical problem is the Adwords problem, which is motivated by the problem of optimally allocating ad slots on search queries to budget constrained advertisers. It involves simplifications that ignore certain aspects of the actual way this allocation is done. For instance, it assumes a "first-price" pay-per-impression scheme, ignoring the game theoretic aspects, and considers only one slot per query. To be precise, the Adwords problem is as follows.
Routing games are frequently used to model the behavior of traffic in large networks, such as road networks. In transportation research, the problem of adding capacity to a road network in a cost-effective manner to minimize the total delay at equilibrium is known as the Network Design Problem, and has received considerable attention. However, prior to our work, little was known about guarantees for polynomial-time algorithms for this problem. We obtain tight approximation guarantees for general and series-parallel networks, and present a number of open questions for future work.
In k-Facility Location games, n strategic agents report their locations on the real line and a mechanism maps them to k facilities. Each agent seeks to minimize her connection cost to the nearest facility and the mechanism should be strategyproof and approximately efficient. Facility Location games have received considerable attention in the framework of approximate mechanism design without money. In this letter, we discuss some recent positive results on the approximability of k-Facility Location by randomized strategyproof mechanisms. Interestingly, these results hold even if the agents' connection cost is a concave cost function of the distance.
Understanding when equilibria are guaranteed to exist is a central theme in economic theory, seemingly unrelated to computation. In this note we survey our main result from [Roughgarden and Talgam-Cohen 2015], which shows that the existence of pricing equilibria is inextricably connected to the computational complexity of related optimization problems: demand oracles, revenue-maximization and welfare-maximization. We demonstrate how this relationship implies, under suitable complexity assumptions, a host of impossibility results. We also suggest a complexity-theoretic explanation for the lack of useful extensions of the Walrasian equilibrium concept: such extensions seem to require the invention of novel polynomial-time algorithms for welfare-maximization.
In this note we report recent results on the problem of leading natural dynamics to good behavior in games that have both high-quality and low-quality equilibria. We show how a central agency can use a public-service advertising campaign to help "nudge" players' behavior towards a high-quality equilibrium, even if only a fraction of players pay attention. We also discuss results analyzing how well-motivated learning rules, when given additional global information about a game, can be used to effectively reach high-quality equilibria.
Although a finite envy-free cake-cutting protocol has been known for more than twenty years, it had been open whether a protocol exists in which the number of steps taken by the protocol is bounded by a function of the number of agents. In this letter, we report on our recent results on discrete, bounded, and envy-free cake-cutting protocols.
Game-theoretic solution concepts, such as Nash equilibrium, are playing an ever increasing role in the study of systems of autonomous agents. A common criticism of Nash equilibrium is that its existence relies on the possibility of randomizing over actions, which in many cases is deemed unsuitable, impractical, or even infeasible.
We show that algorithms that follow the relax-and-round paradigm translate approximation guarantees into Price of Anarchy guarantees, provided that the rounding is oblivious and the relaxation is smooth. We use this meta result to obtain simple, near-optimal mechanisms for a broad range of optimization problems such as combinatorial auctions, the maximum traveling salesman problem, and packing integer programs. In each case the resulting mechanism matches or beats the performance guarantees of known mechanisms.
We identify and address algorithmic and game-theoretic issues arising from welfare maximization in the well-studied two-stage stochastic optimization framework. In contrast, prior work in algorithmic mechanism design has focused almost exclusively on optimization problems without uncertainty. We show both positive results, by demonstrating a mechanism that implements the social welfare maximizer in sequential ex post equilibrium, and also negative results, by showing the impossibility of dominant-strategy implementation. In this letter, we describe the relationship between mechanism design and stochastic optimization, and highlight our key technical results. An extended abstract will appear in WINE 2007, and a journal version is under preparation.
Nash equilibria of two-player games are much easier to compute in practice than those of n-player games, even though the two problems have the same asymptotic complexity. We used a recent constructive reduction to solve general games using a two-player algorithm. However, the reduction increases the game size too much to be practically usable. An open problem is to find a more compact constructive reduction, which might make this approach feasible.
Recent results in complexity theory suggest that various economic theories require agents to solve intractable problems. However, such results assume the agents are optimizing explicit utility functions, whereas the economic theories merely assume the agents' behavior is rationalizable by the optimization of some utility function. For a major economic theory, the theory of the consumer, we show that behaving in a rationalizable way is easier than the corresponding optimization problem. Specifically, if an agent's behavior is at all rationalizable, then it is rationalizable using a utility function that is easy to maximize in every budget set.
Sorting is a well-known problem frequently used in many aspects of the world of computational applications. Sorting means arranging a set of records (or a list of keys) in some (increasing or decreasing) order. In this paper, we propose a graph based comparison sorting algorithm, designated as RKPianGraphSort, that takes time ?(n2) in the worst-case, where n is the number of records in the given list to be sorted.
Ant Colony Optimization and Swarm Optimization are the classical areas of researches in the field of Computer Science. Computer Scientists are trying to map the Biological and Natural Solution with the Artificial one, since last 2 decades. Finally, Genetic Algorithm (GA), Genetic Programming (GP) and Artificial Neural Network (ANN), ACO[13], etc were derived from the Biological, Genetical and Natural characteristics of the living organisms.We have developed a population based stochastic optimization technique inspired by social behavior of Mosquito (Female). This research will open a new side of 'Particle Swarm Optimization', in future.
During the past 35 years the evolutionary computation research community has been studying properties of evolutionary algorithms. Many claims have been made---these varied from a promise of developing an automatic programming methodology to solving virtually any optimization problem (as some evolutionary algorithms are problem independent). However, the most important claim was related to applicability of evolutionary algorithms to solving very complex business problems, i.e. problems, where other techniques failed. So it might be worthwhile to revisit this claim and to search for evolutionary algorithm-based software applications, which were accepted by businesses and industries. In this article Zbigniew Michalewicz attempts to identify reasons for the mismatch between the efforts of hundreds of researchers who make substantial contribution to the field of evolutionary computation and the number of real-world applications, which are based on concepts of evolutionary algorithms.
This paper examines how a new software-implemented data error recovery scheme can be so effective in comparison to conventional Error Correction Codes (ECC) during the execution time of an application. The proposed algorithm is three times faster than the conventional software-implemented ECC and application program designers can easily implement the proposed scheme because of its simplicity while designing their fault tolerant applications at no extra hardware cost. The proposed software-implemented scheme for execution-time data-error detection and correction relies on threefold replication of application data set as a basis for fault tolerance.
Finding a feasible point that satisfies a set of constraints is a common task in scientific computing; examples are the linear feasibility problem and the convex feasibility problem. Finitely convergent sequential algorithms can be used for solving such problems; an example of such an algorithm is ART3, which is defined in such a way that its control is cyclic in the sense that during its execution it repeatedly cycles through the given constraints. Previously we found a variant of ART3 whose control is no longer cyclic, but which is still finitely convergent and in practice usually converges faster than ART3. In this article we propose a general methodology for automatic transformation of finitely convergent sequential algorithms in such a way that (1) finite convergence is retained, and (2) the speed of convergence is improved. The first of these properties is proven by mathematical theorems, the second is illustrated by applying the algorithms to a practical problem.
Finite element methods approximate solutions of partial differential equations by restricting the problem to a finite dimensional function space. In hp adaptive finite element methods, one defines these discrete spaces by choosing different polynomial degrees for the shape functions defined on a locally refined mesh. Although this basic idea is quite simple, its implementation in algorithms and data structures is challenging. It has apparently not been documented in the literature in its most general form. Rather, most existing implementations appear to be for special combinations of finite elements, or for discontinuous Galerkin methods. In this article, we discuss generic data structures and algorithms used in the implementation of hp methods for arbitrary elements, and the complications and pitfalls one encounters. As a consequence, we list the information a description of a finite element has to provide to the generic algorithms for it to be used in an hp context. We support our claim that our reference implementation is efficient using numerical examples in two dimensions and three dimensions, and demonstrate that the hp-specific parts of the program do not dominate the total computing time. This reference implementation is also made available as part of the Open Source deal.II finite element library.
To overcome difficulties in polynomial evaluation caused by overflow or unnecessary underflow, we introduce a simple scaling procedure into Horner's method.
Algorithm 644 computes all major Bessel functions of a complex argument and of nonnegative order. Single-precision routine CBESY and double-precision routine ZBESY are modified to reduce the computation time for the Y Bessel function by approximately 25% over a wide range of arguments and orders. Quick check (driver) programs that exercise the package are also modified to make tests more meaningful over larger regions of the complex plane.
ADMIT-1 enables the computation of sparse Jacobian and Hessian matrices, using automatic differentiation technology, from a MATLAB environment. Given a function to be differentiated, ADMIT-1 will exploit sparsity if present to yield sparse derivative matrices (in sparse MATLAB form). A generic automatic differentiation tool, subject to some functionality requirements, can be plugged into ADMIT-1; examples include ADOL-C (C/C++ target functions)and ADMAT (MATLAB target funcitons). ADMIT-1 also allows for the calculation of gradients and has several other related functions. This article provides an introduction to the design and usage of ADMIT-1.
This article develops an affine-scaling method for linear programming in standard primal form. Its descent search directions are formulated in terms of the null-space of the linear programming matrix, which, in turn, is defined by a suitable basis matrix. We describe some basic properties of the method and an experimental implementation that employs a periodic basis change strategy in conjunction with inexact computation of the search direction by an iterative method, specifically, the conjugate-gradient method with diagonal preconditioning. The result of a numerical study on a number of nontrivial problems representative of problems that arise in practice are reported and discussed.A key advantage of the primal null-space affine-scaling method is its compatibility with the primal simplex method. This is considered in the concluding section, along with implications for the development of a more practical implementation.
Global polynomial approximation methods applied to piecewise continuous functions exhibit the well-known Gibbs phenomenon. We summarize known methods to remove the Gibbs oscillations and present a collection of Matlab programs that implement the methods. The software features a Graphical User Interface that allows easy access to the postprocessing algorithms for benchmarking and educational purposes.
This article presents BiqCrunch, an exact solver for binary quadratic optimization problems. BiqCrunch is a branch-and-bound method that uses an original, efficient semidefinite-optimization-based bounding procedure. It has been successfully tested on a variety of well-known combinatorial optimization problems, such as Max-Cut, Max-k-Cluster, and Max-Independent-Set. The code is publicly available online; a web interface and many conversion tools are also provided.
The flexible statistical models incorporating the log-F distribution are little used because of numeric difficulties. We describe a method for calculating the log-likelihood and two derivatives with respect to the data argument. Fortran subroutines incorporating these calculations are provided.
Wavelets and their associated transforms are highly efficient when approximating and analyzing one-dimensional signals. However, multivariate signals such as images or videos typically exhibit curvilinear singularities, which wavelets are provably deficient in sparsely approximating and also in analyzing in the sense of, for instance, detecting their direction. Shearlets are a directional representation system extending the wavelet framework, which overcomes those deficiencies. Similar to wavelets, shearlets allow a faithful implementation and fast associated transforms. In this article, we will introduce a comprehensive carefully documented software package coined ShearLab 3D (www.ShearLab.org) and discuss its algorithmic details. This package provides MATLAB code for a novel faithful algorithmic realization of the 2D and 3D shearlet transform (and their inverses) associated with compactly supported universal shearlet systems incorporating the option of using CUDA. We will present extensive numerical experiments in 2D and 3D concerning denoising, inpainting, and feature extraction, comparing the performance of ShearLab 3D with similar transform-based algorithms such as curvelets, contourlets, or surfacelets. In the spirit of reproducible research, all scripts are accessible on www.ShearLab.org.
Understanding the behavior of Runge-Kutta codes when stability considerations restrict the stepsize provides useful information for stiffness detection and other implementation details. Analysis of equilibrium states on test problems is presented which provides predictions and insights into this behavior. The implications for global error are also discussed.
The most widely used ordering scheme to reduce fills and operations in sparse matrix computation is the minimum-degree algorithm. The notion of multiple elimination is introduced here as a modification to the conventional scheme. The motivation is discussed using the k-by-k grid model problem. Experimental results indicate that the modified version retains the fill-reducing property of (and is often better than) the original ordering algorithm and yet requires less computer time. The reduction in ordering time is problem dependent, and for some problems the modified algorithm can run a few times faster than existing implementations of the minimum-degree algorithm. The use of external degree in the algorithm is also introduced.
We present four basic Fortran subroutines for nondifferentiable optimization with simple bounds and general linear constraints. Subroutine PMIN, intended for minimax optimization, is based on a sequential quadratic programming variable metric algorithm. Subroutines PBUN and PNEW, intended for general nonsmooth problems, are based on bundle-type methods. Subroutine PVAR is based on special nonsmooth variable metric methods. Besides the description of methods and codes, we propose computational experiments which demonstrate the efficiency of this approach.
The computation of Fermi-Dirac integrals *** is discussed for the values *** = -1, 1/2, 3/2, 5/2. We derive Chebyshev polynomial expansions which allow the computation of these functions to double precision IEEE accuracy.
DSDP implements the dual-scaling algorithm for semidefinite programming. The source code for this interior-point algorithm, written entirely in ANSI C, is freely available under an open source license. The solver can be used as a subroutine library, as a function within the Matlab environment, or as an executable that reads and writes to data files. Initiated in 1997, DSDP has developed into an efficient and robust general-purpose solver for semidefinite programming. Its features include a convergence proof with polynomially bounded worst-case complexity, primal and dual feasible solutions when they exist, certificates of infeasibility when solutions do not exist, initial points that can be feasible or infeasible, relatively low memory requirements for an interior-point method, sparse and low-rank data structures, extensibility that allows applications to customize the solver and improve its performance, a subroutine library that enables it to be linked to larger applications, scalable performance for large problems on parallel architectures, and a well-documented interface and examples of its use. The package has been used in many applications and tested for efficiency, robustness, and ease of use.
BACOL is a new, high quality, robust software package in Fortran 77 for solving one-dimensional parabolic PDEs, which has been shown to be significantly more efficient than any other widely available software package of the same class (to our knowledge), especially for problems with solutions exhibiting rapid spatial variation. A novel feature of this package is that it employs high order, adaptive methods in both time and space, controlling and balancing both spatial and temporal error estimates. The software implements a spline collocation method at Gaussian points, with a B-spline basis, for the spatial discretization. The time integration is performed using a modification of the popular DAE solver, DASSL. Based on the computation of a second, higher order, global solution, a high quality a posteriori spatial error estimate is obtained after each successful time step. The spatial error is controlled by a sophisticated new mesh selection algorithm based on an equidistribution principle. In this article we describe the overall structure of the BACOL package, and in particular the modifications to the DASSL package that improve its performance within BACOL. An example is provided in the online Appendix to illustrate the use of the package.
We present a Pascal implementation of the one-pass algorithm for constructing dynamic Huffman codes that is described and analyzed in a companion paper. The program runs in real time; that is, the processing time for each letter of the message is proportional to the length of its codeword. The number of bits used to encode a message of t letters is less than t bits more than that used by the well-known two-pass algorithm. This is best possible for any one-pass Huffman scheme. In practice, it uses fewer bits than all other Huffman schemes. The algorithm has applications in file compression and network transmission.
In this article we discuss a new software package, BACOLR, for the numerical solution of a general class of time-dependent 1-D PDEs. This package employs high-order adaptive methods in time and space within a method-of-lines approach and provides tolerance control of the spatial and temporal errors. The DAEs resulting from the spatial discretization (based on B-spline collocation) are handled by a substantially modified version of the Runge-Kutta solver, RADAU5. For each time step, the RADAU5 code computes an estimate of the temporal error and requires it to satisfy the user tolerance. After each time step BACOLR then computes a high-order estimate of the spatial error and requires this error estimate to satisfy the user tolerance. BACOLR was developed through a substantial modification of the adaptive method-of-lines package, BACOL. In this article we introduce the BACOLR package and present numerical results to show that the performance of BACOLR is comparable to and in some cases significantly superior to that of BACOL, which was shown in previous work to be more efficient, reliable and robust than other existing codes, especially for problems with solutions exhibiting narrow spikes or boundary layers.
We propose FORTRAN subroutines for approximately solving the feedback vertex and arc set problems on directed graphs using a Greedy Randomized Adaptive Search Procedure (GRASP). Implementation and usage of the package is outlined and computational experiments are reported illustrating solution quality as a function of running time.
Algorithm 708 (BRATIO) was run on 2730 test cases. Comparison of these results with the results from an algorithm using a continued fraction of Tretter and Walster were performed using a high-precision version of the latter algorithm implemented in Maple. Accuracy of BRATIO ranged from 9.64 significant digits to a full machine double-precision, 15.65 significant digits, with the lower value occurring when a was nearly equal to b, and a was large.