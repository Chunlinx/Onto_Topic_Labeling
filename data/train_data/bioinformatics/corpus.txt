The ability to properly compare RNA secondary conformations is of prominent importance in many RNA structural investigations, and particularly in the study of non-coding RNAs (ncRNAs). Several different metrics have been proposed to catch topological dissimilarities between RNA secondary structures, but so far specific metric features have not been assessed yet against extensive datasets. The choice of proper metrics is also crucial for the characterization of structural ensembles, so to be able to proficiently analyze them. The current availability of large ncRNA databases has made it possible to perform an extensive comparison of different metrics. Correlation analysis has uncovered the relative descriptive power of such metrics, providing indications on their possible practical use in different contexts.
Understanding the relationship between neuronal activation patterns of specific brain areas resulting from sensorial experiences is a challenging problem. In this context, we analyzed the levels of similarity between neuronal activation patterns using a semi-supervised method and data acquired from microelectrode arrays implanted on specific brain areas of rats, during an experiment of tactile exploration of four classes of physical objects in the dark. Eight factors were considered (animal, brain region, pair of objects, clustering algorithm, clustering evaluation metric, bin size, window size and contact interval), resulting in 294.912 similarity measurements. Hypotheses regarding the relationship of each of the factors were statistically tested. Not all degrees of similarity between the patterns extracted from pairs of different exploration intervals, for two different objects, were equivalent to a given treatment. This indicated that the similarity between the patterns is sensitive to all the factors analyzed and provides evidence about the complexity of neuronal coding in the brain.
We consider the problem of mining gene expression data in order to single out interesting features characterizing healthy/unhealthy samples of an input dataset. We present an approach based on a network model of the input gene expression data, where there is a labelled graph for each sample. To the best of our knowledge, this is the first attempt to build a different graph for each sample and, then, to have a database of graphs for representing a sample set. Our main goal is that of singling out interesting differences between healthy and unhealthy samples, through the extraction of "discriminative patterns" among graphs belonging to the two different sample sets. Differently from the other approaches presented in the literature, our techniques is able to take into account important local similarities, and also collaborative effects involving interactions between multiple genes. In particular, we use edge-labelled graphs and we measure the discriminative power of a pattern based on such edge weights, which are representative of how much relevant is the co-expression between two genes.
As text mining advances rapidly in the biomedical field, the importance of text data is increasing. Most text data is obtained through a Medical Subjects Headings (MeSH) term search; in this process, a large amount of valuable data is missed because the data is not indexed yet with MeSH terms. In this paper, we propose a method for obtaining additional text data in addition to that obtained using a conventional MeSH term search. In order to obtain additional data, we used the Support Vector Machine (SVM) as the data mining method for classifying documents to related or unrelated. We evaluated the results using a frequency-based text mining approach measuring the quality of data in study of lung cancer. This was confirmed that the data extracted using our method provided as much valuable information as searching using MeSH terms. Further, we found that the amount of information found was increased by 40% using additional extracted data.
The purpose of research on biomedical literature-based discovery is to bring out new knowledge from the existing biomedical information. Beginning with Dr. Swanson's ABC model, many studies extended or applied the ABC model to find new associations between biomedical entities. While the methods applied to data have advanced, in most cases biomedical literature has been used for the text data. Assuming that web crawl data is helpful in studying literature-based discovery as well as biomedical literature which is the existing but rather limited data source, we discovered new disease-drug associations using web crawl data in addition to biomedical literature. We also analyzed how helpful the additional use of web crawl data is for biomedical literature mining. Literature-based discovery using web crawl data has its significance as a pioneering work utilizing new data.
Text mining is widely used to infer relationships between biological entities. Most text-mining algorithms utilize a co-occurrence-based approach. The term co-occurrence denotes a relationship between two interesting entities if they appear in the same sentence. Using these approaches current studies have extracted relationships between biological entities such as disease-gene relationships. However, these approaches cannot provide specific information for inferred relationships such as the role of the gene in the disease. To overcome this limitation, we propose a novel approach for inferring disease-gene relationship that provides specific knowledge of the inferred relationships. To implement this method, we first built terms based on text analysis to extract opinion sentences that include disease-gene relationships. We then extracted these opinion sentences and inferred disease-gene relationships by using disease-related and gene-related terms in the opinion sentences. Using these extracted relationships and terms, we inferred disease-related genes and constructed a disease-specific gene network. To validate our approach, we investigated the top k (k = 20) inferred genes for prostate cancer and analyzed the constructed gene network using three network analysis measures. Our approach found more disease-gene relationships than comparable method, and inferred describable disease-gene relationships.
Recent advances in Bioinformatics and in Computer simulation and modelling have positively impacted the drug discovery process by turning viable the rational drug design (RDD). One of the major challenges in RDD is the understanding about protein-ligand interaction simulated at the atomic level by molecular docking algorithms. Virtual screening (VS) is defined as a computational approach applied to the analyses of large libraries of chemical structures in order to identify possible drug candidates to a target. The major challenge of VS based on molecular docking is the time required to run each experiment and the countless parameters and characteristics that should be defined by the researcher such as: the target(s) receptor, one or a set of ligands, the receptor binding site and so on. In order to perform more realistic docking simulations it is also necessary to account for the receptor and ligand flexibility. Therefore, this paper presents a framework for VS, where the user configure an experiment in a Web based platform informing the path of input and output files as well as the size, center and variation of the binding site(s). Then, the proposed framework generates a Python script that performs the VS experiment on the users personal computer. We expect that researchers from diverse backgrounds as Biology, Physics, Pharmacy, etc. can easily prepare VS experiments without the necessity of learning how to write scripts. To validate our proposed framework we performed five different case studies considering the AcrB protein as target receptor. All the case studies were easily realized using the proposed framework. The results show that the framework is effective to configure the VS experiments with different characteristics. Besides, the experiments can help on the search for new drug candidates for this important target.
This paper presents the application of shape distributions to diagnose lung nodules in computerized tomography images. Our study uses the nodule surface and also interior surfaces generated by their voxel's joint attributes and distribution. The surfaces are captured using 3D Alpha Shapes algorithm. All surfaces are characterized using shape distributions D1, D2, D3, D4 and A3 in order to represent shape behavior based on statistical distribution of their points in a 3D contour. We evaluate experiments with these features using Support Vector Machines and the combination of all descriptors proving above of 90% of accuracy. This indicates a promising feature to distinguish malignant from benign nodules for lung cancer diagnosis.
Life science data integration and interoperability is one of the most challenging problems facing bioinformatics today. In the current age of the life sciences, investigators have to interpret many types of information from a variety of sources: lab instruments, public databases, gene expression profiles, raw sequence traces, single nucleotide polymorphisms, chemical screening data, proteomic data, putative metabolic pathway models, and many others. Unfortunately, scientists are not currently able to easily identify and access this information because of the variety of semantics, interfaces, and data formats used by the underlying data sources.
Bioinformatics: Managing Scientific Data tackles this challenge head-on by discussing the current approaches and variety of systems available to help bioinformaticians with this increasingly complex issue. The heart of the book lies in the collaboration efforts of eight distinct bioinformatics teams that describe their own unique approaches to data integration and interoperability. Each system receives its own chapter where the lead contributors provide precious insight into the specific problems being addressed by the system, why the particular architecture was chosen, and details on the system's strengths and weaknesses. In closing, the editors provide important criteria for evaluating these systems that bioinformatics professionals will find valuable.
The development and application of systems strategies to biology and disease are transforming medical research and clinical practice in an unprecedented rate. In the foreseeable future, clinicians, medical researchers, and ultimately the consumers and patients will be increasingly equipped with a deluge of personal health information, e.g., whole genome sequences, molecular profiling of diseased tissues, and periodic multi-analyte blood testing of biomarker panels for disease and wellness. The convergence of these practices will enable accurate prediction of disease susceptibility and early diagnosis for actionable preventive schema and personalized treatment regimes tailored to each individual. It will also entail proactive participation from all major stakeholders in the health care system. We are at the dawn of predictive, preventive, personalized, and participatory (P4) medicine, the fully implementation of which requires marrying basic and clinical researches through advanced systems thinking and the employment of high-throughput technologies in genomics, proteomics, nanofluidics, single-cell analysis, and computation strategies in a highly-orchestrated discipline we termed translational systems medicine
MicroRNAs (miRNAs) are a class of short non-coding RNA molecules that have attracted tremendous attention from the biological and biomedical research communities over the past decade. With over 1900 miRNAs discovered in humans to date, many of them have already been implicated in common human disorders. Facilitated by high-throughput genomics and bioinformatics in conjunction with traditional molecul ar biology techniques and animal models, miRNA research is now positioned to make the transition from laboratories to clinics to deliver profound benefits to public health. Herein, we overview the progress of miRNA research related to human diseases, as well as the potential for miRNA to becoming the next generation of diagnostics and therapeutics. 
MicroRNAs (miRNAs) are approximately 22-nucleotide-long non-coding RNAs that are important regulators of gene expression in eukaryotes. miRNAs are first transcribed as long primary transcripts, which then undergo a series of processing steps to produce the single-stranded mature miRNAs. This article reviews our current knowledge of the mechanism and regulation of mammalian miRNA expression and points out areas of research that may enhance our understanding of how the specificity and efficiency of miRNA production is controlled in vivo.
The ENCyclopedia Of DNA Elements (ENCODE) project is an international research consortium that aims to identify all functional elements in the human genome sequence. The second phase of the project comprised 1640 datasets from 147 different cell types, yielding a set of 30 publications across several journals. These data revealed that 80.4% of the human genome displays some functionality in at least one cell type. Many of these regulatory elements are physically associated with one another and further form a network or three-dimensional conformation to affect gene expression. These elements are also related to sequence variants associated with diseases or traits. All these findings provide us new insights into the organization and regulation of genes and genome, and serve as an expansive resource for understanding human health and disease.
Muscle wasting is common in mammals during extended periods of immobility. However, many small hibernating mammals manage to avoid muscle atrophy despite remaining stationary for long periods during hibernation. Recent research has highlighted roles for short non-coding microRNAs (miRNAs) in the regulation of stress tolerance. We proposed that they could also play an important role in muscle maintenance during hibernation. To explore this possibility, a group of 10 miRNAs known to be normally expressed in skeletal muscle of non-hibernating mammals were analyzed by RT-PCR in hibernating little brown bats, Myotis lucifugus. We then compared the expression of these miRNAs in euthermic control bats and bats in torpor. Our results showed that compared to euthermic controls, significant, albeit modest (1.2-1.6 fold), increases in transcript expression were observed for eight mature miRNAs, including miR-1a-1, miR-29b, miR-181b, miR-15a, miR-20a, miR-206 and miR-128-1, in the pectoral muscle of torpid bats. Conversely, expression of miR-21 decreased by 80% during torpor, while expression of miR-107 remained unaffected. Interestingly, these miRNAs have been either validated or predicted to affect multiple muscle-specific factors, including myostatin, FoxO3a, HDAC4 and SMAD7, and are likely involved in the preservation of pectoral muscle mass and functionality during bat hibernation. 
There is an unmet need for new cardiovascular biomarkers. Despite this only few biomarkers for the diagnosis or screening of cardiovascular diseases have been implemented in the clinic. Thousands of proteins can be analysed in plasma by mass spectrometry-based proteomics technologies. Therefore, this technology may therefore identify new biomarkers that previously have not been associated with cardiovascular diseases. In this review, we summarize the key challenges and considerations, including strategies, recent discoveries and clinical applications in cardiovascular proteomics that may lead to the discovery of novel cardiovascular biomarkers.
Chitin is one of the most abundant biopolymers widely distributed in the marine and terrestrial environments. Chitinase enzyme has received increased attention due to its wide range of biotechnological applications, especially in agriculture for biocontrol of phytopathogenic fungi and harmful insects. In the present study, 58 bacterial isolates were screened for chitinolytic activity and on the basis of chitin hydrolysis zone 6 isolates were selected for chitinase production in broth media. Based on enzyme production, two most potent isolates identified as Aeromonas hydrophila HS4 and Aeromonas punctata HS6 were selected for further study. The effects of media composition and various fermentation conditions for optimization of chitinase production were studied. The maximum chitinase production was obtained at 37 °C and pH 8.0 after 24-48 h of incubation by HS4; and at 37 °C and pH 7 after 48 h incubation by HS6. Among the substrates colloidal chitin was the best for both the strains. Regarding carbon sources, starch (1%) was the best for both strains; while malt and yeast extract (1%) was found as the best nitrogen source for HS4 and HS6, respectively. Out of metal ions Mn 2+ and Cu 2+ enhanced enzyme production in the case of HS6. However, Co 2+ was the most appropriate for HS4.
Antioxidant and antimicrobial activities of nutmeg (Myristica fragrans Houtt) seed extracts were evaluated. Seeds were extracted with acetone, ethanol, methanol, butanol and water. All the extracts have shown significant antioxidant and antimicrobial activities against the tested microorganisms. Among all extracts, acetone extract has shown the highest antioxidant activity. The acetone extract showed 93.12 ± 1.48 mg gallic acid equivalents (GAE)/100 g dry weight total phenolic content, DPPH scavenging activity of 63.04 ± 1.56%, chelating activity of 64.11 ± 2.21% and 74.36 ± 1.94% inhibition of β-carotene bleaching, at 1 mg/mL extract concentration. Out of all extracts, acetone extract was able to exert antimicrobial activity against all tested bacteria and fungi. Acetone extract has shown the strongest antibacterial and antifungal activity with Staphylococcus aureus (13.8 ± 0.42 mm) and Aspergillus niger (14.4 ± 0.37 mm), respectively. GC-MS analysis of acetone extract has revealed the presence of 32 compounds of extract representing 99.49%. Sabinene (28.61%) has shown the highest occurrence in the extract. β-Pinene (10.26), α-pinene (9.72), myristicin (4.30%), isoeugenol (2.72%), p-cymene (1.81%), carvacrol (1.54%), eugenol (0.89%) and β-caryophellene (0.82%) were reported as possible contributor for antioxidant and antimicrobial activity of nutmeg.
Chromium, specifically hexavalent chromium is one of the most toxic pollutants that are released into soils by various anthropogenic activities. It has numerous adverse effects not only on plant system but also on beneficial soil microorganisms which are the indicators of soil fertility and health. Recent emergence of phytoremediation as an environmental friendly and economical approach to decontaminate the chromium stressed soils has received wider attention. But major drawback of this process is that it takes long time. Application of multifunctional plant-growth-promoting bacteria (PGPB) exhibiting chromium resistance and reducing traits when used as bioinoculants with phytoremediating plants, has resulted in a better plant growth and chromium remediating efficiency in a short time span. PGPB improve chromium uptake by modifying root architecture, secreting metal sequestering molecules in rhizosphere and alleviating chromium induced phytotoxicity. The purpose of this review is to highlight the plant-beneficial traits of PGPB to accelerate plant-growth and concurrently ameliorate phytoremediation of chromium contaminated soils.
Academy of Scientific Research and Technology. α-Amylase was extracted and purified from soybean seeds to apparent homogeneity by affinity precipitation. The homogeneous enzyme preparation was immobilized on gelatin matrix using glutaraldehyde as an organic hardener. Response surface methodology (RSM) and 3-level-3-factor Box-Behnken design was employed to evaluate the effects of immobilization parameters, such as gelatin concentration, glutaraldehyde concentration and hardening time on the activity of immobilized α-amylase. The results showed that 20% gelatin (w/v), 10% glutaraldehyde (v/v) and 1 h hardening time yielded an optimum immobilization of 82.5%.
Protease producing halotolerant bacterium was isolated from saltern pond sediment (Tuticorin) and identified as Bacillus licheniformis (TD4) by 16S rRNA gene sequencing. Protease production was enhanced by optimizing the culture conditions. The nutritional factors such as carbon and nitrogen sources, NaCl and also physical parameters like incubation time, pH, agitation, inoculum size were optimized for the maximum yield of protease. Studies on the effect of different carbon and nitrogen sources revealed that xylose and urea enhances the enzyme production. Thus, with selected C-N sources along with 1 M NaCl the maximum protease production (141.46 U/mg) was obtained in the period of 24 h incubation at pH 8 under 250 rpm compared to the initial enzyme production (89.87 U/mg).
Protease enzyme from Bacillus megaterium was successively purified by ammonium sulfate precipitation, ion exchange chromatography on DEAE-cellulose and gel filtration chromatography on Sephadex G-200. The purification steps of protease resulted in the production of two protease fractions namely protease P1 and P2 with specific activities of 561.27 and 317.23 U mg -1 of protein, respectively. The molecular weights of B. megaterium P1 and P2 were 28 and 25 KDa, respectively. The purified fractions P1 and P2 were rich in aspartic acid and serine. Relatively higher amounts of alanine, leucine, glycine, valine, thereonine valine and glutamic acid were also present. The maximum protease activities for both enzyme fractions were attained at 50 °C, pH 7.5, 1% of gelatine concentration and 0.5 enzyme concentrations. P1 and P2 fractions were more stable over pH 7.0-8.5 and able to prolong their thermal stability up to 80 °C. The effect of different inhibitors on the protease activity of both enzyme fractions was also studied. The enzyme was found to be serine active as it had been affected by lower concentrations of phenylmethylsulfonyl fluoride (PMSF). Complete dehairing of the enzyme-treated skin was achieved in 12 h, at room temperature.
Academy of Scientific Research and Technology. A psychrophilic bacterium producing cold-active lipase upon growth at low temperature was isolated from the soil samples of Gangotri glacier and identified as Microbacterium luteolum. The bacterial strain produced maximum lipase at 15 °C, at a pH of 8.0. Beef extract served as the best organic nitrogen source and ammonium nitrate as inorganic for maximum lipase production. Castor oil served as an inducer and glucose served as an additional carbon source for production of cold-active lipase. Ferric chloride as additional mineral salt in the medium, highly influenced the lipase production with an activity of 8.01 U ml -1 . The cold-active lipase was purified to 35.64-fold by DEAE-cellulose column chromatography. It showed maximum activity at 5 °C and thermostability up to 35 °C. The purified lipase was stable between pH 5 and 9 and the optimal pH for enzymatic hydrolysis was 8.0. Lipase activity was stimulated in presence of all the solvents (5%) tested except with acetonitrile. Lipase activity was inhibited in presence of Mn 2+ , Cu 2+ , and Hg 2+ ; whereas Fe + , Na + did not have any inhibitory effect on the enzyme activity. The purified lipase was stable in the presence of SDS; however, EDTA and dithiothreitol inhibited enzyme activity. Presence of Ca 2+ along with inhibitors stabilized lipase activity. The cold active lipase thus exhibiting activity and stability at a low temperature and alkaline pH appears to be practically useful in industrial applications especially in detergent formulations.
Academy of Scientific Research and Technology. A promising protocol for achievement the accumulation rate of inulin compound in a suspension culture of Jerusalem artichoke (Helianthus tuberosus) was established. The effect of incorporated of cell cultures in combining with two type of biotic elicitors Aspergillus niger extract and Methyl-Jasmonate incorporation feeding medium on leaf cell growth patterns and production of inulin was investigated. The maximum value of cell growth parameters and highest content of inulinase activity (0.395 u/ml) were resulted from elicitation of augmented MS-medium with A. niger extract at the level of 0.2% in combination with Methyl-Jasmonate (150 μM) as compared with other concentrations after 2 weeks of cultivation. The chemical analyses of the different cell lines were spectro-photometerically performed. This study clearly indicates that combining of A. niger and Methyl-Jasmonate elicitors plays a critical role on inulin process and its accumulation in Jerusalem artichoke cell cultures.
Implantation of biomaterials poses a huge risk of local inflammation therefore affecting the implant function leading to mortality in a significant number of cases. Thus, alternatively, naturally derived drugs if developed to treat implant induced inflammation, would therefore sharply decrease the largest risk of implant rejection. This study was aimed to investigate the anti inflammatory effect of Withania somnifera on stainless steel implant induced inflammation in adult zebrafish model. Fish were divided into four experimental groups of 6 fish each. Group 1 served as the control; Group 2 fish were stainless steel implant (SSI) inserted fish without treatment; Group 3 fish were SSI inserted + Thin layer chromatography (TLC) separated portion of supernatant of W. somnifera and Group 4 fish were SSI inserted + Ibuprofen treated. Fish were assessed for reduced inflammation by histopathology, local apoptosis using fluorescent quantification, reverse transcriptase polymerase chain reaction (RT-PCR) of inflammatory genes. The characterization of the TLC separated portion of the supernatant of W. somnifera was also performed. The histopathology result of Group 2 showed crypt architectural distortion in the muscle which was not found in the control fish, whereas simultaneously TLC separated portion of the supernatant of W. somnifera showed reduced fatty changes and fibrosis of the submucosa, muscular hyperplasia. RT-PCR result revealed that the TLC separated portion of supernatant of W. somnifera has a significant inhibition of TNFα in the adult zebrafish. In conclusion the observed anti-inflammatory activity of TLC separated portion of the supernatant of W. somnifera might be due to rich phenolic acids and flavonoids.
Silver nanoparticle (AgNP) synthesis and characterization is an area of vast interest due to their broader application in the fields of science and technology and medicine. Plants are an attractive source for AgNP synthesis because of its ability to produce a wide range of secondary metabolites with strong reducing potentials. Thus, the present study describes the synthesis of AgNPs using aqueous rhizome extract of Acorus calamus (sweet flag). The AgNP formation was evaluated at different temperatures, incubation time and concentrations of AgNO 3 using Response surface methodology based Box-Behnken design (BBD). The synthesized AgNPs were characterized by UV-Visible spectroscopy, Fourier transform infra-red spectroscopy (FTIR), X-ray diffraction (XRD), and Scanning electron microscopy-energy-dispersive spectroscopy (SEM-EDS). The surface plasmon resonance found at 420 nm confirmed the formation of AgNPs. SEM images reveal that the particles are spherical in nature. The EDS analysis of the AgNPs, using an energy range of 2-4 keV, confirmed the presence of elemental silver without any contamination. The antibacterial activity of synthesized AgNPs was evaluated against the clinical isolates Staphylococcus aureus and Escherichia coli and it was found that bacterial growth was significantly inhibited in a dose dependent manner. The results suggest that the AgNPs from rhizome extract could be used as a potential antibacterial agent for commercial application.
Academy of Scientific Research and Technology. Staphylococcus aureus and Escherichia coli were enumerated and isolated from ready-to-eat vegetables salad and meat luncheon on their selective media (Baird-parker and Macconkey agar, respectively). Twenty suspicious colonies of each (10 from each product) were randomly chosen and identified using conventional based on morphological and physiological characteristics. S. aureus and E. coli isolates which gave the highest pathogenicity were chosen for identification and confirmation with molecular method based on 16S rRNA gene. The PCR amplification method of 16S rRNA gave the same identification results as conventional method, but it was sensitive and fast. This molecular method takes about 48 h in comparison with 6 days for conventional method. The 16S rRNA of S. aureus and E. coli were deposited in the Genebank database under accessions (AB599719.1 and AB599716.1, respectively).
Academy of Scientific Research and Technology. Knowledge of the origin and domestication history of crop plants is important for studies aiming at avoiding the erosion of genetic resources due to the loss of ecotypes and landraces and habitats and increased urbanization. Such knowledge also strengthens the capacity of modern farming system to develop and scale-up the domestication of high value potential crops that can be achieved by improving the knowledge that help to identify and select high value plant species within their locality, identify and apply the most appropriate propagation techniques for improving crops and integrate improved crop species into the farming systems. The study of domestication history and ancestry provide means for germplasm preservation through establishment of gene banks, largely as seed collections, and preservation of natural habitats. Information about crop evolution and specifically on patterns of genetic change generated by evolution prior, during, and after domestication, is important to develop sound genetic conservation programs of genetic resources of crop plants and also increases the efficiency of breeding programs. In recent years, molecular approaches have contributed to our understanding of the aspects of plant evolution and crops domestication. In this article, aspects of crops domestication are outlined and the role of molecular data in elucidating the ancestry and domestication of crop plants are outlined. Particular emphasis is given to the contribution of molecular approaches to the origin and domestication history of barley and the origin and ancestry of the Egyptian clover.
Statistical optimization is an effective technique for the investigation of complex processes with minimal number of experimental runs. In this study, statistical approach was used to study the optimization of media components for lipase production from Yarrowia lipolytica MTCC 35. Mahua cake, glucose, MnCl 2 and KH 2 PO 4 were screened to be the most significant variables among the nine medium variables that were tested to determine influence on lipase production by Plackett-Burman design. Central Composite Design was used for further optimization of these screened variables for enhanced lipase production. The determination coefficient (R 2 ) value of 0.922 showed that the regression models adequately explain the data variation and represent the actual relationships between the variables and response. The optimum values of investigated variables for the maximum lipase production were 6.0% Mahua cake, 2.0% glucose, 0.2% MnCl 2 and 0.2% KH 2 PO 4 . The maximum lipase production (9.40 U mL -1 ) was obtained under optimal condition.
The main aim of the work was the fabrication of novel silver releasing nanocomposite scaffolds, for bone treatment, by the gas foaming/particulate-leaching technique. Silver doped bioactive glass nanoparticles were used as a filler, to provide the scaffolds with bioactivity, as well as anti-bacterial properties. Nanocomposite scaffolds containing 0, 20 and 40 wt% glass contents were prepared and coded as PAg0, PAg20 and PAg40, respectively. The scaffolds were characterized by SEM/EDXA, FTIR and TGA. Examination of SEM microphotographs showed that, the produced scaffolds had well interconnected structures. For PAg0, PAg20 and PAg40, the maximum pore sizes were about 250, 150 and 100 μm, respectively, while their porosities % were 92%, 89% and 83%, respectively. Degradation studies were carried out, by incubating the scaffolds in simulated body fluid, for a month. Results revealed the possibility to modulate and improve the degradation of the scaffolds by increasing their glass contents. The final weight losses measured for PAg0, PAg20 and PAg40 were 12.76%, 14.61% and 17.42%, respectively. On the other hand, the highest water absorption values recorded for those scaffolds were 61.89%, 240.36% and 270.87%, respectively, indicating that, the addition of glass nanoparticles to the scaffolds improved their water absorption abilities. Both PAg20 and PAg40 induced an apatite layer on their surfaces, had anti-bacterial effect in agar plates, and their silver releasing profiles followed a diffusion-controlled mechanism. Therefore they could be used for bone treatment.
The objective of the present study was to isolate halotolerant bacteria from the sediment sample collected from Marakanam Solar Salterns, Tamil Nadu, India using NaCl supplemented media and screened for amylase production. Among the 22 isolates recovered, two strains that had immense potential were selected for amylase production and designated as P1 and P2. The phylogenetic analysis revealed that P1 and P2 have highest homology with Pontibacillus chungwhensis (99%) and Bacillus barbaricus (100%). Their amylase activity was optimized to obtain high yield under various temperature, pH and NaCl concentration. P1 and P2 strain showed respective, amylase activity maximum at 35 °C and 40 °C; pH 7.0 and 8.0; 1.5 M and 1.0 M NaCl concentration. Further under optimized conditions, the amylase activity of P1 strain (49.6 U mL -1 ) was higher than P2 strain. Therefore, the amylase enzyme isolated from P. chungwhensis P1 was immobilized in sodium alginate beads. Compared to the free enzyme form (49.6 U mL -1 ), the immobilized enzyme showed higher amylase activity as 90.3 U mL -1 . The enzyme was further purified partially and the molecular mass was determined as 40 kDa by SDS-PAGE. Thus, high activity of amylase even under increased NaCl concentration would render immense benefits in food processing industries.
The enormous demand for new rootstock genotypes in Prunus spp. makes us to use micropropagation as an unavoidable propagation method. Therefore, the study on micropropagation of a new semi-dwarf vegetative rootstock namely Tetra (Prunus empyrean 3) was carried out to develop an optimized protocol. Culture establishment using nodal segments was enhanced using WPM (woody plant medium) medium lacking growth regulators. From various shoot multiplication treatments, the highest number of shoots per explant (30.4) was found on ME (Media created specifically) medium supplemented with 0.8 mg l -1 BAP and 0.05 mg l -1 IBA. 100% in vitro rooting was achieved on 1/2 strength MS medium with 0.5 mg l -1 IBA, 1.6 mg l -1 thiamine and 150 mg l -1 iron sequestrene.
The objective of the presented work is to demonstrate the metabolism of 1,2 propandiol by Lactobacillus reuteri and to elucidate the metabolites produced during the process. This Metabolic pathway is crucial for biotechnological applications using L. reuteri in bioconversion of glycerol to industrially important plate-form chemicals. L. reuteri grown on minimal media containing 1,2 propanediol was able to utilize the compound as a sole carbon and energy source. The growth of the bacteria was linear with time; however the specific growth rate was significantly low compared to bacteria grown on the same media in the presence of glucose.The fermentation of 1,2 propanediol by L. reuteri in presence and absence of glucose was followed for 72 h and the metabolites produced during the process were detected using HPLC. 1,2 Propanediol was completely converted to propionaldhyde in a time dependent fashion, this process had a higher rate in presence of glucose. Consequently the produced propionaldhyde was converted to propionic acid and propanol in a skewed equimolar manner. In presence of glucose: acetic acid, lactic acid, succinic acid and ethanol were detected while in absence of glucose only minute amounts of acetic acid and lactic acid were detected which indicates presence of different metabolic pathways for glucose and 1,2 propanediol metabolism. Resting cells of L. reuteri induced in presence of 1,2 propanediol have shown significant capabilities to convert aqueous glycerol to 1,3 propanediol, 3-hydroxypropionaldhyde and a compound proposed to be 3-hydroxypropionic acid as detected by gas chromatographic technique.
In the present investigation were evaluated the antifungal and antibacterial activities of Nano-silver (NS). Two separate experiments were done to evaluate the potential of silver nanoparticles in controlling the contamination of G × N15 micro-propagation. In the first experiment, a factorial experiment based on a completely randomized design with 15 treatments including five different NS concentrations (0, 50, 100, 150 and 200 ppm) and three soaking time of explants (3, 5 and 7 min) with five replications was conducted. In the other experiment, NS was added to Murashige and Skoog (MS) medium with concentrations of 0, 50, 100, 150 and 200 ppm in a completely randomized design. Results showed that the application of 100 and 150 ppm NS both as an immersion and as added directly to the culture medium significantly reduces internal and external contaminations compared with the control group. Using NS in culture medium was more efficient to reduce fungal and bacterial contamination than immersion. High concentrations of NS had an adverse effect on the viability of G × N15 single nodes and this effect was more serious in immersed explants in NS than directly added NS ones regarding the viability of buds and plantlet regeneration. In conclusion, due to high contamination of woody plants especially fruit trees and also adverse environmental effects of mercury chloride, the NS solution can be used as a low risk bactericide in micro-propagation of G × N15 and can be an appropriate alternative to mercury chloride in the future.
Academy of Scientific Research and Technology. Lepidium sativum L. is a fast growing edible herb which belongs to family Brassicaceae. The seeds of L. sativum are aperient, diuretic, tonic, demulcent, carminative, galatogogue and emmenagogue. They have been used in the treatment of bacterial and fungal infections, as an aperient and also possess antibacterial and antifungal properties. The seeds of this plant possess rapid bone fracture healing ability. Despite of its diverse medicinal properties no molecular data for diversity analysis is available till date. During this study random amplified polymorphic DNA (RAPD) markers were used to detect genetic variations of L. sativum. Initially 50 decamer primers were screened, out of which only 32 primers showed reproducible fragments with easily recordable bands. A total of 414 reproducible and clear bands were distinguished across the selected primers and statistical analysis showed 361 polymorphic bands and 53 monomorphic bands. Cluster analysis of the genotypes based on UPGMA divided the 18 genotypes into two main clusters, with first cluster having only HCS-20 genotype of L. sativum and other having rest of all 17 genotypes. The dendrogram based on similarity matrix revealed 23-66% genetic relatedness among 18 genotypes. The results of the present study can be used for molecular breeding and improvement of L. sativum for various desired traits through hybridization in future.
Hyaluronidase " venom spreading factor" is a common component of snake venoms and indirectly potentiates venom toxicity. It may cause permanent local tissue destruction at the bite site/systemic collapse of the envenomated victim. The present study was performed to assess the benefits of inhibiting the hyaluronidase activity of Egyptian horned viper, Cerastes cerastes (Cc). The aqueous extracts of some medicinal plants were screened for their inhibitory effect on hyaluronidase activity of Cc venom. The results revealed that the Rosmarinus officinalis (Ro) extract is the most potent hyaluronidase inhibitor among the tested extracts. The Ro extract is more potent inhibitory effect on the hyaluronidase activity than the prepared rabbit monoclonal antiserum of previously purified hyaluronidase enzyme from Cc venom (anti-CcHaseII). In addition, the Ro extract is efficiently inhibited the activity of hemorrhagic toxin previously purified from Cc venom, and it also neutralized the edema inducing activity of the Cc venom in vivo. Furthermore, the Ro extract markedly increased the survival time of experimental mice injected with lethal dose of Cc venom up to 7 h in compared to mice injected with venom alone or with venom/anti-CcHaseII (15 ± 5, 75 ± 4 min), respectively. Our findings imply the significance of plant-derived hyaluronidase inhibitor in the neutralization of local effects of Cc venom and retardation of death time. Therefore, it may use as a therapeutic value in complementary snakebite therapy.
The challenging task of bringing high efficiency transformed plants attracts lot of attention in recent times. In search for this, there have been many attempts made using, different techniques like tissue culture and plant breeding methods. Here we report a suitable alternative facile route, where cyanobacterial extracellular products are utilized as growth regulators and its performance validated on Gossypium hirsutum L. MS medium is tested with cyanobacterial extracellular products of Nostoc ellipsosporum, Dolichospermum flos-aquae and Oscillatoria acuminata.Our best results show that the addition of O. acuminata extracellular product with plant growth hormones gives the excellent induction and elongation in cotton. In addition to this, the multiple shoot was obtained on MS medium fortified with 1.0 mg L -1 BA with 8% O. acuminata and 1.5 mg L -1 TDZ with 12% O. acuminata. High frequency of shoot elongation supplemented with MS medium, iP 2.5 mg L -1 and 16% O. acuminata and root production MS medium fortified with 12% O. acuminata best responsible for regeneration in cotton plants. The rooted plants were hardened and transferred to soil with 90% survival rate.
Methods were developed in the present investigation for cloning and large scale plant production of Passiflora foetida L. germplasm selected from the East-Coast region of South India. Nodal shoot segments were used as explants. The explants were dressed and surface sterilized with 0.1% (w/v) HgCl 2 . Multiple shoots were induced (6.13 ± 0.22 shoots per explant) by proliferation of nodal shoot meristems on Murashige and Skoog (MS) semi-solid medium + 2.0 mg l -1 6-benzylaminopurine (BAP). The shoots of P. foetida were further multiplied (16.45 ± 0.44 shoots per explant) on MS medium + 0.5 mg l -1 each of BAP and Kinetin (Kin). The in vitro generated shoots were rooted on half-strength MS medium containing 2.5 mg l -1 indole-3 butyric acid (IBA). By this method 67% shoots were rooted. About 97% shoots were rooted ex vitro (8.33 ± 0.29 roots per shoot) when the cut ends of the shoots were treated with 300 mg l -1 IBA for 5 min. The in vitro rooted plants were hardened and acclimatized in the greenhouse and successfully (100%) transplanted to the field.
The objective of this research was to assess the effects of different media i.e. Murashige and Skoog (MS) and Quoirin and Lepoivre (QL), cytokinin type i.e. 6-Benzyladenin (BA) and 6-Benzylaminopurine (BAP) and cytokinin concentration on in vitro proliferation of the G × N15 rootstock. To evaluate the effects of different media and cytokinin type, two separate experiments were conducted as factorial based on completely randomized design, and single nodes were used as explants. The results showed that MS nutrient medium was found to be superior to QL nutrient medium. Regarding the interaction between media and growth regulators, the best interaction was found in MS medium supplemented with 1 mg l -1 BAP resulting in 8.5 new micro shoots/explant while 7.75 shoots were observed in MS medium containing 1.25 mg l -1 BA. The longest length of new micro-shoots (2.10 cm) was obtained in hormone-free MS medium. Findings of this study showed that there is a significant correlation between the hormone level and plantlet height and formed callus weight so that an increase in BAP and BA levels in both of MS and QL media resulted significantly in height decrease and callus weight increase. The results also suggest that the best and the worst plantlets in terms of quality were observed in hormone-free QL medium and MS medium supplemented with 1.25 mg l -1 , respectively. These results reflect the fact that the presence of high amounts of NH 4 NO 3 and cytokinin especially BAP in culture medium triggered inhibitory effect on shoot growth.
Thirty two morphologically different bacterial were isolated from different soil samples and screened for their ability to produce lipolytic enzymes. Among all isolates, the isolate coded AZ1 was selected due to its high potency to produce lipase at elevated temperature up to 65 °C. Phylogenetic analysis based on 16SrDNA sequence revealed its close relationship to Geobacillus thermodenitrificans. The effect of ten culture variable on lipase production was evaluated by implementing Plackett-Burman statistical design. d-sucrose, peptone and soy bean flour were the most significant variables affecting lipase production. A pre-optimized medium based on this experiment yielded an enzyme activity of 260 U min -1 ml -1 . For further optimization, a fourteen trials' multi-factorial Box-Behnken experimental design was applied to find out the optimum level of each of the significant variables. The tested variables, namely: d-sucrose (X 1 ); peptone (X 2 ) and soy bean flour (X 3 ) were examined, each at three different levels coded -1, 0, +1. The optimal levels of the three components were founded to be (g/L): d-sucrose, 6.56; peptone, 6.35; and soy bean flour, 6.92, with a predicted activity of approximately 610 U min -1 ml -1 . According to the results of the Plackett-Burman and Box-Behnken designs the following medium composition is expected to be optimum (g/L): d-sucrose 6.56, peptone 6.35, soy bean flour 6.92, CaCl 2 0.02, Y.E. 2.5, K 2 HPO 4 1.0, MgSO 4 .7H 2 O 0.2 and Fe 2 (SO 4 ) 3 0.02; pH, 8; cultivation temperature 55 °C and incubation time 24 h, the enzyme activity measured in the medium was approximately 593 U min -1 ml -1 .
Amylase production by Bacillus cereus IND4 was investigated by solid state fermentation (SSF) using cow dung substrate. The SSF conditions were optimized by using one-variable-at-a-time approach and two level full factorial design. Two level full factorial design demonstrated that moisture, pH, fructose, yeast extract and ammonium sulphate have significantly influenced enzyme production (p < 0.05). A central composite design was employed to investigate the optimum concentration of these variables affecting amylase production. Maximal amylase production of 464 units/ml of enzyme was observed in the presence of 100% moisture, 0.1% fructose and 0.01% ammonium sulphate. The enzyme production increased three fold compared to the original medium. The optimum pH and temperature for the activity of amylase were found to be 8.0 and 50 °C, respectively. This enzyme was highly stable at wide pH range (7.0-9.0) and showed 32% enzyme activity after initial denaturation at 50 °C for 1 h. This is the first detailed report on the production of amylase by microorganisms using cow dung as the low cost medium.
Support vector machine (SVM) is an extensively used machine learning method with many biomedical signal classification applications. In this study, a novel PSO-SVM model has been proposed that hybridized the particle swarm optimization (PSO) and SVM to improve the EMG signal classification accuracy. This optimization mechanism involves kernel parameter setting in the SVM training procedure, which significantly influences the classification accuracy. The experiments were conducted on the basis of EMG signal to classify into normal, neurogenic or myopathic. In the proposed method the EMG signals were decomposed into the frequency sub-bands using discrete wavelet transform (DWT) and a set of statistical features were extracted from these sub-bands to represent the distribution of wavelet coefficients. The obtained results obviously validate the superiority of the SVM method compared to conventional machine learning methods, and suggest that further significant enhancements in terms of classification accuracy can be achieved by the proposed PSO-SVM classification system. The PSO-SVM yielded an overall accuracy of 97.41% on 1200 EMG signals selected from 27 subject records against 96.75%, 95.17% and 94.08% for the SVM, the k-NN and the RBF classifiers, respectively. PSO-SVM is developed as an efficient tool so that various SVMs can be used conveniently as the core of PSO-SVM for diagnosis of neuromuscular disorders. © 2013 Elsevier Ltd.
In this paper an Empirical Mode Decomposition (EMD) based ECG signal enhancement and QRS detection algorithm is proposed. Being a non-invasive measurement, ECG is prone to various high and low frequency noises causing baseline wander and power line interference, which act as a source of error in QRS and other feature extraction. EMD is a fully adaptive signal decomposition technique that generates Intrinsic Mode Functions (IMF) as decomposition output. Here, first baseline wander is corrected by selective reconstruction based slope minimization technique from IMFs and then high frequency noise is removed by eliminating a noisy set of lower order IMFs with a statistical peak correction as high frequency noise elimination is accompanied by peak deformation of sharp characteristic waves. Then a set of IMFs are selected that represents QRS region and a nonlinear transformation is done for QRS enhancement. This improves detection accuracy, which is represented in the result section. Thus in this method a single fold processing of each signal is required unlike other conventional techniques. © 2011 Elsevier Ltd.
Diabetes mellitus may cause alterations in the retinal microvasculature leading to diabetic retinopathy. Unchecked, advanced diabetic retinopathy may lead to blindness. It can be tedious and time consuming to decipher subtle morphological changes in optic disk, microaneurysms, hemorrhage, blood vessels, macula, and exudates through manual inspection of fundus images. A computer aided diagnosis system can significantly reduce the burden on the ophthalmologists and may alleviate the inter and intra observer variability. This review discusses the available methods of various retinal feature extractions and automated analysis. © 2013 Elsevier Ltd.
Automatic seizure detection is significant for long-term monitoring of epilepsy, as well as for diagnostics and rehabilitation, and can decrease the duration of work required when inspecting the EEG signals. In this study we propose a novel method for feature extraction and pattern recognition of ictal EEG, based upon empirical mode decomposition (EMD) and support vector machine (SVM). First the EEG signal is decomposed into Intrinsic Mode Functions (IMFs) using EMD, and then the coefficient of variation and fluctuation index of IMFs are extracted as features. SVM is then used as the classifier for recognition of ictal EEG. The experimental results show that this algorithm can achieve the sensitivity of 97.00% and specificity of 96.25% for interictal and ictal EEGs, and the sensitivity of 98.00% and specificity of 99.40% for normal and ictal EEGs on Bonn data sets. Besides, the experiment with interictal and ictal EEGs from Qilu Hospital dataset also yields a satisfactory sensitivity of 98.05% and specificity of 100%. © 2013 Elsevier Ltd.
Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results. © 2014.
The performance of (bio-)signal classification strongly depends on the choice of suitable features (also called parameters or biomarkers). In this article we evaluate the discriminative power of ordinal pattern statistics and symbolic dynamics in comparison with established heart rate variability parameters applied to beat-to-beat intervals. As an illustrative example we distinguish patients suffering from congestive heart failure from a (healthy) control group using beat-to-beat time series. We assess the discriminative power of individual features as well as pairs of features. These comparisons show that ordinal patterns sampled with an additional time lag are promising features for efficient classification. © 2011 Elsevier Ltd.
This paper presents a method for breast cancer diagnosis in digital mammogram images. Multiresolution representations, wavelet or curvelet, are used to transform the mammogram images into a long vector of coefficients. A matrix is constructed by putting wavelet or curvelet coefficients of each image in row vector, where the number of rows is the number of images, and the number of columns is the number of coefficients. A feature extraction method is developed based on the statistical t-test method. The method is ranking the features (columns) according to its capability to differentiate the classes. Then, a dynamic threshold is applied to optimize the number of features, which can achieve the maximum classification accuracy rate. The method depends on extracting the features that can maximize the ability to discriminate between different classes. Thus, the dimensionality of data features is reduced and the classification accuracy rate is improved. Support vector machine (SVM) is used to classify between the normal and abnormal tissues and to distinguish between benign and malignant tumors. The proposed method is validated using 5-fold cross validation. The obtained classification accuracy rates demonstrate that the proposed method could contribute to the successful detection of breast cancer. © 2011 Elsevier Ltd.
The present work aims at automatic identification of various sleep stages like, sleep stages 1, 2, slow wave sleep (sleep stages 3 and 4), REM sleep and wakefulness from single channel EEG signal. Automatic scoring of sleep stages was performed with the help of pattern recognition technique which involves feature extraction, selection and finally classification. Total 39 numbers of features from time domain, frequency domain and from non-linear analysis were extracted. After extraction of features, SVM based recursive feature elimination (RFE) technique was used to find the optimum number of feature subset which can provide significant classification performance with reduced number of features for the five different sleep stages. Finally for classification, binary SVMs were combined with one-against-all (OAA) strategy. Careful extraction and selection of optimum feature subset helped to reduce the classification error to 8.9 for training dataset, validated by k-fold cross-validation (CV) technique and 10.61 in the case of independent testing dataset. Agreement of the estimated sleep stages with those obtained by expert scoring for all sleep stages of training dataset was 0.877 and for independent testing dataset it was 0.8572. The proposed ensemble SVM-based method could be used as an efficient and cost-effective method for sleep staging with the advantage of reducing stress and burden imposed on subjects. © 2012 Elsevier Ltd.
Recently, the increasing demand for telemedicine services has raised interest in the use of medical image protection technology. Conventional block ciphers are poorly suited to image protection due to the size of image data and increasing demand for real-time teleradiology and other online telehealth applications. To meet this challenge, this paper presents a novel chaos-based medical image encryption scheme. To address the efficiency problem encountered by many existing permutation-substitution type image ciphers, the proposed scheme introduces a substitution mechanism in the permutation process through a bit-level shuffling algorithm. As the pixel value mixing effect is contributed by both the improved permutation process and the original substitution process, the same level of security can be achieved in a fewer number of overall rounds. The results indicate that the proposed approach provides an efficient method for real-time secure medical image transmission over public networks. © 2013 Elsevier Ltd.
In this paper, a novel method for lung nodule detection, segmentation and recognition using computed tomography (CT) images is presented. Our contribution consists of several steps. First, the lung area is segmented by active contour modeling followed by some masking techniques to transfer non-isolated nodules into isolated ones. Then, nodules are detected by the support vector machine (SVM) classifier using efficient 2D stochastic and 3D anatomical features. Contours of detected nodules are then extracted by active contour modeling. In this step all solid and cavitary nodules are accurately segmented. Finally, lung tissues are classified into four classes: namely lung wall, parenchyma, bronchioles and nodules. This classification helps us to distinguish a nodule connected to the lung wall and/or bronchioles (attached nodule) from the one covered by parenchyma (solitary nodule). At the end, performance of our proposed method is examined and compared with other efficient methods through experiments using clinical CT images and two groups of public datasets from Lung Image Database Consortium (LIDC) and ANODE09. Solid, non-solid and cavitary nodules are detected with an overall detection rate of 89%; the number of false positive is 7.3/scan and the location of all detected nodules are recognized correctly. © 2012 Elsevier Ltd.
We propose a computer-aided detection (CAD) system which can detect small-sized (from 3. mm) pulmonary nodules in spiral CT scans. A pulmonary nodule is a small lesion in the lungs, round-shaped (parenchymal nodule) or worm-shaped (juxtapleural nodule). Both kinds of lesions have a radio-density greater than lung parenchyma, thus appearing white on the images. Lung nodules might indicate a lung cancer and their early stage detection arguably improves the patient survival rate. CT is considered to be the most accurate imaging modality for nodule detection. However, the large amount of data per examination makes the full analysis difficult, leading to omission of nodules by the radiologist. We developed an advanced computerized method for the automatic detection of internal and juxtapleural nodules on low-dose and thin-slice lung CT scan. This method consists of an initial selection of nodule candidates list, the segmentation of each candidate nodule and the classification of the features computed for each segmented nodule candidate.The presented CAD system is aimed to reduce the number of omissions and to decrease the radiologist scan examination time. Our system locates with the same scheme both internal and juxtapleural nodules. For a correct volume segmentation of the lung parenchyma, the system uses a Region Growing (RG) algorithm and an opening process for including the juxtapleural nodules. The segmentation and the extraction of the suspected nodular lesions from CT images by a lung CAD system constitutes a hard task. In order to solve this key problem, we use a new Stable 3D Mass-Spring Model (MSM) combined with a spline curves reconstruction process. Our model represents concurrently the characteristic gray value range, the directed contour information as well as shape knowledge, which leads to a much more robust and efficient segmentation process. For distinguishing the real nodules among nodule candidates, an additional classification step is applied; furthermore, a neural network is applied to reduce the false positives (FPs) after a double-threshold cut. The system performance was tested on a set of 84 scans made available by the Lung Image Database Consortium (LIDC) annotated by four expert radiologists. The detection rate of the system is 97% with 6.1. FPs/CT. A reduction to 2.5. FPs/CT is achieved at 88% sensitivity. We presented a new 3D segmentation technique for lung nodules in CT datasets, using deformable MSMs. The result is a efficient segmentation process able to converge, identifying the shape of the generic ROI, after a few iterations. Our suitable results show that the use of the 3D AC model and the feature analysis based FPs reduction process constitutes an accurate approach to the segmentation and the classification of lung nodules. © 2012 Elsevier Ltd.
The Electrocardiogram (ECG) is the P-QRS-T wave depicting the cardiac activity of the heart. The subtle changes in the electric potential patterns of repolarization and depolarization are indicative of the disease afflicting the patient. These clinical time domain features of the ECG waveform can be used in cardiac health diagnosis. Due to the presence of noise and minute morphological parameter values, it is very difficult to identify the ECG classes accurately by the naked eye. Various computer aided cardiac diagnosis (CACD) systems, analysis methods, challenges addressed and the future of cardiovascular disease screening are reviewed in this paper. Methods developed for time domain, frequency transform domain, and time-frequency domain analysis, such as the wavelet transform, cannot by themselves represent the inherent distinguishing features accurately. Hence, nonlinear methods which can capture the small variations in the ECG signal and provide improved accuracy in the presence of noise are discussed in greater detail in this review. A CACD system exploiting these nonlinear features can help clinicians to diagnose cardiovascular disease more accurately. © 2014 Elsevier Ltd.
MicroRNA (miRNA) family is a group of miRNAs that derive from the common ancestor. Normally, members from the same miRNA family have similar physiological functions; however, they are not always conserved in primary sequence or secondary structure. Proper family prediction from primary sequence will be helpful for accurate identification and further functional annotation of novel miRNA. Therefore, we introduced a novel machine learning-based web server, the miRClassify, which can rapidly identify miRNA from the primary sequence and classify it into a miRNA family regardless of similarity in sequence and structure. Additionally, the medical implication of the miRNA family is also provided when it is available in PubMed. The web server is accessible at the link http://datamining.xmu.edu.cn/software/MIR/home.html. © 2013.
In this paper, the semi-numerical techniques known as the optimal homotopy analysis method (HAM) and Differential Transform Method (DTM) are applied to study the magneto-hemodynamic laminar viscous flow of a conducting physiological fluid in a semi-porous channel under a transverse magnetic field. The two-dimensional momentum conservation partial differential equations are reduced to ordinary form incorporating Lorentizian magnetohydrodynamic body force terms. These ordinary differential equations are solved by the homotopy analysis method, the differential transform method and also a numerical method (fourth-order Runge-Kutta quadrature with a shooting method), under physically realistic boundary conditions. The homotopy analysis method contains the auxiliary parameter ℏ, which provides us with a simple way to adjust and control the convergence region of solution series. The differential transform method (DTM) does not require an auxiliary parameter and is employed to compute an approximation to the solution of the system of nonlinear differential equations governing the problem. The influence of Hartmann number (Ha) and transpiration Reynolds number (mass transfer parameter, Re) on the velocity profiles in the channel are studied in detail. Interesting fluid dynamic characteristics are revealed and addressed. The HAM and DTM solutions are shown to both correlate well with numerical quadrature solutions, testifying to the accuracy of both HAM and DTM in nonlinear magneto-hemodynamics problems. Both these semi-numerical techniques hold excellent potential in modeling nonlinear viscous flows in biological systems. © 2013 Elsevier Ltd.
We propose a multivariate dynamical adjustment (MDA) modeling approach to assess the strength of baroreflex and cardiopulmonary couplings from spontaneous cardiovascular variabilities. Open loop MDA (OLMDA) and closed loop MDA (CLMDA) models were compared. The coupling strength was assessed during progressive sympathetic activation induced by graded head-up tilt. Both OLMDA and CLMDA models suggested that baroreflex coupling progressively increased with tilt table inclination. Only CLMDA model indicated that cardiopulmonary coupling due to the direct link from respiration to heart period gradually decreased with tilt table angles, while that due to the indirect link mediated by systolic arterial pressure progressively increased. © 2011 Elsevier Ltd.
Diabetic Retinopathy (DR) is an eye abnormality in which the human retina is affected due to an increasing amount of insulin in blood. The early detection and diagnosis of DR is vital to save the vision of diabetes patients. The early signs of DR which appear on the surface of the retina are microaneurysms, haemorrhages, and exudates. In this paper, we propose a system consisting of a novel hybrid classifier for the detection of retinal lesions. The proposed system consists of preprocessing, extraction of candidate lesions, feature set formulation, and classification. In preprocessing, the system eliminates background pixels and extracts the blood vessels and optic disc from the digital retinal image. The candidate lesion detection phase extracts, using filter banks, all regions which may possibly have any type of lesion. A feature set based on different descriptors, such as shape, intensity, and statistics, is formulated for each possible candidate region: this further helps in classifying that region. This paper presents an extension of the m-Mediods based modeling approach, and combines it with a Gaussian Mixture Model in an ensemble to form a hybrid classifier to improve the accuracy of the classification. The proposed system is assessed using standard fundus image databases with the help of performance parameters, such as, sensitivity, specificity, accuracy, and the Receiver Operating Characteristics curves for statistical analysis. © 2013 Elsevier Ltd.
Signal distortion of photoplethysmographs (PPGs) due to motion artifacts has been a limitation for developing real-time, wearable health monitoring devices. The artifacts in PPG signals are analyzed by comparing the frequency of the PPG with a reference pulse and daily life motions, including typing, writing, tapping, gesturing, walking, and running. Periodical motions in the range of pulse frequency, such as walking and running, cause motion artifacts. To reduce these artifacts in real-time devices, a least mean square based active noise cancellation method is applied to the accelerometer data. Experiments show that the proposed method recovers pulse from PPGs efficiently. © 2011 Elsevier Ltd.
This paper proposed a new entropy measure, Fuzzy Measure Entropy (FuzzyMEn), for the analysis of heart rate variability (HRV) signals. FuzzyMEn was calculated based on the fuzzy set theory and improved the poor statistical stability in the approximate entropy (ApEn) and sample entropy (SampEn). The simulation results also demonstrated that the FuzzyMEn had better algorithm discrimination ability when compared with the recently published fuzzy entropy (FuzzyEn), The validity of FuzzyMEn was tested for clinical HRV analysis on 120 subjects (60 heart failure and 60 healthy control subjects). It is concluded that FuzzyMEn could be considered as a valid and reliable method for a clinical HRV application. © 2012 Elsevier Ltd.
The motor unit action potentials (MUAPs) in an electromyographic (EMG) signal provide a significant source of information for the assessment of neuromuscular disorders. In this work, different types of machine learning methods were used to classify EMG signals and compared in relation to their accuracy in classification of EMG signals. The models automatically classify the EMG signals into normal, neurogenic or myopathic. The best averaged performance over 10 runs of randomized cross-validation is also obtained by different classification models. Some conclusions concerning the impacts of features on the EMG signal classification were obtained through analysis of the classification techniques. The comparative analysis suggests that the fuzzy support vector machines (FSVM) modelling is superior to the other machine learning methods in at least three points: slightly higher recognition rate; insensitivity to overtraining; and consistent outputs demonstrating higher reliability. The combined model with discrete wavelet transform (DWT) and FSVM achieves the better performance for internal cross validation (External cross validation) with the area under the reciever operating characteristic (ROC) curve (AUC) and accuracy equal to 0.996 (0.970) and 97.67% (93.5%), respectively. These results show that the proposed model have the potential to obtain a reliable classification of EMG signals, and to assist the clinicians for making a correct diagnosis of neuromuscular disorders. © 2012 Elsevier Ltd.
A computational model was developed for studying the flow field and particle deposition in a human upper airway system, including: nasal cavity, nasopharynx, oropharynx, larynx and trachea. A series of coronal CT scan images of a 24 year old woman was used to construct the 3D model. The Lagrangian and Eulerian approaches were used, respectively, to find the trajectories of micro-particles and concentration of nano-particles. The total and regional deposition fractions of micro/nanoparticles were evaluated and the major hot spots for the deposition of inhaled particles were found. © 2011 Elsevier Ltd.
© 2015 Elsevier Ltd. Prostate cancer is the second most diagnosed cancer of men all over the world. In the last few decades, new imaging techniques based on Magnetic Resonance Imaging (MRI) have been developed to improve diagnosis. In practise, diagnosis can be affected by multiple factors such as observer variability and visibility and complexity of the lesions. In this regard, computer-aided detection and computer-aided diagnosis systems have been designed to help radiologists in their clinical practice. Research on computer-aided systems specifically focused for prostate cancer is a young technology and has been part of a dynamic field of research for the last 10. years. This survey aims to provide a comprehensive review of the state-of-the-art in this lapse of time, focusing on the different stages composing the work-flow of a computer-aided system. We also provide a comparison between studies and a discussion about the potential avenues for future research. In addition, this paper presents a new public online dataset which is made available to the research community with the aim of providing a common evaluation framework to overcome some of the current limitations identified in this survey.
Diabetes mellitus (DM) affects considerable number of people in the world and the number of cases is increasing every year. Due to a strong link to the genetic basis of the disease, it is extremely difficult to cure. However, it can be controlled to prevent severe consequences, such as organ damage. Therefore, diabetes diagnosis and monitoring of its treatment is very important. In this paper, we have proposed a non-invasive diagnosis support system for DM. The system determines whether or not diabetes is present by determining the cardiac health of a patient using heart rate variability (HRV) analysis. This analysis was based on nine nonlinear features namely: Approximate Entropy (ApEn), largest Lyapunov exponet (LLE), detrended fluctuation analysis (DFA) and recurrence quantification analysis (RQA). Clinically significant measures were used as input to classification algorithms, namely AdaBoost, decision tree (DT), fuzzy Sugeno classifier (FSC), k-nearest neighbor algorithm ( k-NN), probabilistic neural network (PNN) and support vector machine (SVM). Ten-fold stratified cross-validation was used to select the best classifier. AdaBoost, with least squares (LS) as weak learner, performed better than the other classifiers, yielding an average accuracy of 90%, sensitivity of 92.5% and specificity of 88.7%. © 2013 Elsevier Ltd.
Many biological research areas such as drug design require gene regulatory networks to provide clear insight and understanding of the cellular process in living cells. This is because interactions among the genes and their products play an important role in many molecular processes. A gene regulatory network can act as a blueprint for the researchers to observe the relationships among genes. Due to its importance, several computational approaches have been proposed to infer gene regulatory networks from gene expression data. In this review, six inference approaches are discussed: Boolean network, probabilistic Boolean network, ordinary differential equation, neural network, Bayesian network, a nd dynamic Bayesian network. These approaches are discussed in terms of introduction, methodology and recent applications of these approaches in gene regulatory network construction. These approaches are also compared in the discussion section. Furthermore, the strengths and weaknesses of these computational approaches are described. © 2014 Elsevier Ltd.
Knowing the type of an uncharacterized membrane protein often provides a useful clue in both basic research and drug discovery. With the explosion of protein sequences generated in the post genomic era, determination of membrane protein types by experimental methods is expensive and time consuming. It therefore becomes important to develop an automated method to find the possible types of membrane proteins. In view of this, various computational membrane protein prediction methods have been proposed. They extract protein feature vectors, such as PseAAC (pseudo amino acid composition) and PsePSSM (pseudo position-specific scoring matrix) for representation of protein sequence, and then learn a distance metric for the KNN (K nearest neighbor) or NN (nearest neighbor) classifier to predicate the final type. Most of the metrics are learned using linear dimensionality reduction algorithms like Principle Components Analysis (PCA) and Linear Discriminant Analysis (LDA). Such metrics are common to all the proteins in the dataset. In fact, they assume that the proteins lie on a uniform distribution, which can be captured by the linear dimensionality reduction algorithm. We doubt this assumption, and learn local metrics which are optimized for local subset of the whole proteins. The learning procedure is iterated with the protein clustering. Then a novel ensemble distance metric is given by combining the local metrics through Tikhonov regularization. The experimental results on a benchmark dataset demonstrate the feasibility and effectiveness of the proposed algorithm named ProClusEnsem. © 2012 Elsevier Ltd.
Pregnancy leads to physiological changes in various parameters of the cardiovascular system. The aim of this study was to investigate longitudinal changes in the structure and complexity of heart rate variability (HRV) and QT interval variability during the second half of normal gestation. We analysed 30-min high-resolution ECGs recorded monthly in 32 pregnant women, starting from the 20th week of gestation. Heart rate and QT variability were quantified using multiscale entropy (MSE) and detrended fluctuation analyses (DFA). DFA of HRV showed significantly higher scaling exponents towards the end of gestation (p < 0.0001). MSE analysis showed a significant decrease in sample entropy of HRV with progressing gestation on scales 1-4 (p < 0.05). MSE analysis and DFA of QT interval time series revealed structures significantly different from those of HRV with no significant alteration during the second half of gestation.In conclusion, pregnancy is associated with increases in long-term correlations and regularity of HRV, but it does not affect QT variability. The structure of QT time series is significantly different from that of RR time series, despite its close physiological dependence. © 2011 Elsevier Ltd.
Extensive efforts have been made in both academia and industry in the research and development of smart wearable systems (SWS) for health monitoring (HM). Primarily influenced by skyrocketing healthcare costs and supported by recent technological advances in micro- and nanotechnologies, miniaturisation of sensors, and smart fabrics, the continuous advances in SWS will progressively change the landscape of healthcare by allowing individual management and continuous monitoring of a patient's health status. Consisting of various components and devices, ranging from sensors and actuators to multimedia devices, these systems support complex healthcare applications and enable low-cost wearable, non-invasive alternatives for continuous 24-h monitoring of health, activity, mobility, and mental status, both indoors and outdoors. Our objective has been to examine the current research in wearable to serve as references for researchers and provide perspectives for future research. Methods: Herein, we review the current research and development of and the challenges facing SWS for HM, focusing on multi-parameter physiological sensor systems and activity and mobility measurement system designs that reliably measure mobility or vital signs and integrate real-time decision support processing for disease prevention, symptom detection, and diagnosis. For this literature review, we have chosen specific selection criteria to include papers in which wearable systems or devices are covered. Results: We describe the state of the art in SWS and provide a survey of recent implementations of wearable health-care systems. We describe current issues, challenges, and prospects of SWS. Conclusion: We conclude by identifying the future challenges facing SWS for HM. Elsevier B.V.
Computerized analysis of pigmented skin lesions (PSLs) is an active area of research that dates back over 25. years. One of its main goals is to develop reliable automatic instruments for recognizing skin cancer from images acquired in vivo. This paper presents a review of this research applied to microscopic (dermoscopic) and macroscopic (clinical) images of PSLs. The review aims to: (1) provide an extensive introduction to and clarify ambiguities in the terminology used in the literature and (2) categorize and group together relevant references so as to simplify literature searches on a specific sub-topic. Methods and material: The existing literature was classified according to the nature of publication (clinical or computer vision articles) and differentiating between individual and multiple PSL image analysis. We also emphasize the importance of the difference in content between dermoscopic and clinical images. Results: Various approaches for implementing PSL computer-aided diagnosis systems and their standard workflow components are reviewed and summary tables provided. An extended categorization of PSL feature descriptors is also proposed, associating them with the specific methods for diagnosing melanoma, separating images of the two modalities and discriminating references according to our classification of the literature. Conclusions: There is a large discrepancy in the number of articles published on individual and multiple PSL image analysis and a scarcity of reported material on the automation of lesion change detection. At present, computer-aided diagnosis systems based on individual PSL image analysis cannot yet be used to provide the best diagnostic results. Furthermore, the absence of benchmark datasets for standardized algorithm evaluation is a barrier to a more dynamic development of this research area. Elsevier B.V.
Background: The bilateral loss of the grasp function associated with a lesion of the cervical spinal cord severely limits the affected individuals' ability to live independently and return to gainful employment after sustaining a spinal cord injury (SCI). Any improvement in lost or limited grasp function is highly desirable. With current neuroprostheses, relevant improvements can be achieved in end users with preserved shoulder and elbow, but missing hand function.  The aim of this single case study is to show that (1) with the support of hybrid neuroprostheses combining functional electrical stimulation (FES) with orthoses, restoration of hand, finger and elbow function is possible in users with high-level SCI and (2) shared control principles can be effectively used to allow for a brain-computer interface (BCI) control, even if only moderate BCI performance is achieved after extensive training. Patient and methods: The individual in this study is a right-handed 41-year-old man who sustained a traumatic SCI in 2009 and has a complete motor and sensory lesion at the level of C4. He is unable to generate functionally relevant movements of the elbow, hand and fingers on either side. He underwent extensive FES training (30-45. min, 2-3 times per week for 6 months) and motor imagery (MI) BCI training (415 runs in 43 sessions over 12 months). To meet individual needs, the system was designed in a modular fashion including an intelligent control approach encompassing two input modalities, namely an MI-BCI and shoulder movements. Results: After one year of training, the end user's MI-BCI performance ranged from 50% to 93% (average: 70.5%). The performance of the hybrid system was evaluated with different functional assessments. The user was able to transfer objects of the grasp-and-release-test and he succeeded in eating a pretzel stick, signing a document and eating an ice cream cone, which he was unable to do without the system. Conclusion: This proof-of-concept study has demonstrated that with the support of hybrid FES systems consisting of FES and a semiactive orthosis, restoring hand, finger and elbow function is possible in a tetraplegic end-user. Remarkably, even after one year of training and 415 MI-BCI runs, the end user's average BCI performance remained at about 70%. This supports the view that in high-level tetraplegic subjects, an initially moderate BCI performance cannot be improved by extensive training. However, this aspect has to be validated in future studies with a larger population. Elsevier B.V.
Objectives: Brain-computer interfaces (BCIs) are no longer only used by healthy participants under controlled conditions in laboratory environments, but also by patients and end-users, controlling applications in their homes or clinics, without the BCI experts around. But are the technology and the field mature enough for this? Especially the successful operation of applications - like text entry systems or assistive mobility devices such as tele-presence robots - requires a good level of BCI control. How much training is needed to achieve such a level? Is it possible to train naïve end-users in 10 days to succes sfully control such applications? Materials and methods: In this work, we report our experiences of training 24 motor-disabled participants at rehabilitation clinics or at the end-users' homes, without BCI experts present. We also share the lessons that we have learned through transferring BCI technologies from the lab to the user's home or clinics. Results: The most important outcome is that 50% of the participants achieved good BCI performance and could successfully control the applications (tele-presence robot and text-entry system). In the case of the tele-presence robot the participants achieved an average performance ratio of 0.87 (max. 0.97) and for the text entry application a mean of 0.93 (max. 1.0). The lessons learned and the gathered user feedback range from pure BCI problems (technical and handling), to common communication issues among the different people involved, and issues encountered while controlling the applications. Conclusion: The points raised in this paper are very widely applicable and we anticipate that they might be faced similarly by other groups, if they move on to bringing the BCI technology to the end-user, to home environments and towards application prototype control. Elsevier B.V.
Clinical pathway analysis, as a pivotal issue in ensuring specialized, standardized, normalized and sophisticated therapy procedures, is receiving increasing attention in the field of medical informatics. Clinical pathway pattern mining is one of the most important components of clinical pathway analysis and aims to discover which medical behaviors are essential/critical for clinical pathways, and also where temporal orders of these medical behaviors are quantified with numerical bounds. Even though existing clinical pathway pattern mining techniques can tell us which medical behaviors are frequently performed and in which order, they seldom precisely provide quantified temporal order information of critical medical behaviors in clinical pathways. Methods: This study adopts process mining to analyze clinical pathways. The key contribution of the paper is to develop a new process mining approach to find a set of clinical pathway patterns given a specific clinical workflow log and minimum support threshold. The proposed approach not only discovers which critical medical behaviors are performed and in which order, but also provides comprehensive knowledge about quantified temporal orders of medical behaviors in clinical pathways. Results: The proposed approach is evaluated via real-world data-sets, which are extracted from Zhejiang Huzhou Central hospital of China with regard to six specific diseases, i.e., bronchial lung cancer, gastric cancer, cerebral hemorrhage, breast cancer, infarction, and colon cancer, in two years (2007.08-2009.09). As compared to the general sequence pattern mining algorithm, the proposed approach consumes less processing time, generates quite a smaller number of clinical pathway patterns, and has a linear scalability in terms of execution time against the increasing size of data sets. Conclusion: The experimental results indicate the applicability of the proposed approach, based on which it is possible to discover clinical pathway patterns that can cover most frequent medical behaviors that are most regularly encountered in clinical practice. Therefore, it holds significant promise in research efforts related to the analysis of clinical pathways. Elsevier B.V.
Background: Bipolar disorders are characterized by a series of both depressive and manic or hypomanic episodes. Although common and expensive to treat, the clinical assessment of bipolar disorder is still ill-defined.  In the current literature several correlations between mood disorders and dysfunctions involving the autonomic nervous system (ANS) can be found. The objective of this work is to develop a novel mood recognition system based on a pervasive, wearable and personalized monitoring system using ANS-related biosignals. Materials and methods: The monitoring platform used in this study is the core sensing system of the personalized monitoring systems for care in mental health (PSYCHE) European project. It is comprised of a comfortable sensorized t-shirt that can acquire the inter-beat interval time series, the heart rate, and the respiratory dynamics for long-term monitoring during the day and overnight. In this study, three bipolar patients were followed for a period of 90 days during which up to six monitoring sessions and psychophysical evaluations were performed for each patient. Specific signal processing techniques and artificial intelligence algorithms were applied to analyze more than 120. h of data. Results: Experimental results are expressed in terms of confusion matrices and an exhaustive descriptive statistics of the most relevant features is reported as well. A classification accuracy of about 97% is achieved for the intra-subject analysis. Such an accuracy was found in distinguishing relatively good affective balance state (euthymia) from severe clinical states (severe depression and mixed state) and is lower in distinguishing euthymia from the milder states (accuracy up to 88%). Conclusions: The PSYCHE platform could provide a viable decision support system in order to improve mood assessment in patient care. Evidences about the correlation between mood disorders and ANS dysfunction s were found and the obtained results are promising for an effective biosignal-based mood recognition. Elsevier B.V.
An electroencephalogram-based (EEG-based) brain-computer-interface (BCI) provides a new communication channel between the human brain and a computer. Amongst the various available techniques, artificial neural networks (ANNs) are well established in BCI research and have numerous successful applications. However, one of the drawbacks of conventional ANNs is the lack of an explicit input optimization mechanism. In addition, results of ANN learning are usually not easily interpretable. In this paper, we have applied an ANN-based method, the genetic neural mathematic method (GNMM), to two EEG channel selection and classification problems, aiming to address the issues above. Methods and materials: Pre-processing steps include: least-square (LS) approximation to determine the overall signal increase/decrease rate; locally weighted polynomial regression (Loess) and fast Fourier transform (FFT) to smooth the signals to determine the signal strength and variations. The GNMM method consists of three successive steps: (1) a genetic algorithm-based (GA-based) input selection process; (2) multi-layer perceptron-based (MLP-based) modelling; and (3) rule extraction based upon successful training. The fitness function used in the GA is the training error when an MLP is trained for a limited number of epochs. By averaging the appearance of a particular channel in the winning chromosome over several runs, we were able to minimize the error due to randomness and to obtain an energy distribution around the scalp. In the second step, a threshold was used to select a subset of channels to be fed into an MLP, which performed modelling with a large number of iterations, thus fine-tuning the input/output relationship. Upon successful training, neurons in the input layer are divided into four sub-spaces to produce if-then rules (step 3).Two datasets were used as case studies to perform three classifications. The first data were electrocorticography (ECoG) recordings that have been used in the BCI competition III. The data belonged to two categories, imagined movements of either a finger or the tongue. The data were recorded using an 8. ×. 8 ECoG platinum electrode grid at a sampling rate of 1000. Hz for a total of 378 trials. The second dataset consisted of a 32-channel, 256. Hz EEG recording of 960 trials where participants had to execute a left- or right-hand button-press in response to left- or right-pointing arrow stimuli. The data were used to classify correct/incorrect responses and left/right hand movements. Results: For the first dataset, 100 samples were reserved for testing, and those remaining were for training and validation with a ratio of 90%:10% using . K-fold cross-validation. Using the top 10 channels selected by GNMM, we achieved a classification accuracy of 0.80. ±. 0.04 for the testing dataset, which compares favourably with results reported in the literature. For the second case, we performed multi-time-windows pre-processing over a single trial. By selecting 6 channels out of 32, we were able to achieve a classification accuracy of about 0.86 for the response correctness classification and 0.82 for the actual responding hand classification, respectively. Furthermore, 139 regression rules were identified after training was completed. Conclusions: We demonstrate that GNMM is able to perform effective channel selections/reductions, which not only reduces the difficulty of data collection, but also greatly improves the generalization of the classifier. An important step that affects the effectiveness of GNMM is the pre-processing method. In this paper, we also highlight the importance of choosing an appropriate time window position. Elsevier B.V..
Within this work an auditory P300 brain-computer interface based on tone stream segregation, which allows for binary decisions, was developed and evaluated. Methods and materials: Two tone streams consisting of short beep tones with infrequently appearing deviant tones at random positions were used as stimuli. This paradigm was evaluated in 10 healthy subjects and applied to 12 patients in a minimally conscious state (MCS) at clinics in Graz, Würzburg, Rome, and Liège. A stepwise linear discriminant analysis classifier with 10. ×. 10 cross-validation was used to detect the presence of any P300 and to investigate attentional modulation of the P300 amplitude. Results: The results for healthy subjects were promising and most classification results were better than random. In 8 of the 10 subjects, focused attention on at least one of the tone streams could be detected on a single-trial basis. By averaging 10 data segments, classification accuracies up to 90.6. % could be reached. However, for MCS patients only a small number of classification results were above chance level and none of the results were sufficient for communication purposes. Nevertheless, signs of consciousness were detected in 9 of the 12 patients, not on a single-trial basis, but after averaging of all corresponding data segments and computing significant differences. These significant results, however, strongly varied across sessions and conditions. Conclusion: This work shows the transition of a paradigm from healthy subjects to MCS patients. Promising results with healthy subjects are, however, no guarantee of good results with patients. Therefore, more investigations are required before any definite conclusions about the usability of this paradigm for MCS patients can be drawn. Nevertheless, this paradigm might offer an opportunity to support bedside clinical assessment of MCS patients and eventually, to provide them with a means of communication. Elsevier B.V.
Background: For many years the reestablishment of communication for people with severe motor paralysis has been in the focus of brain-computer interface (BCI) research. Recently applications for entertainment have also been developed. Brain Painting allows the user creative expression through painting pictures.  The second, revised prototype of the BCI Brain Painting application was evaluated in its target function - free painting - and compared to the P300 spelling application by four end users with severe disabilities. Methods: According to the International Organization for Standardization (ISO), usability was evaluated in terms of effectiveness (accuracy), efficiency (information transfer rate (ITR)), utility metric, subjective workload (National Aeronautics and Space Administration Task Load Index (NASA TLX)) and user satisfaction (Quebec User Evaluation of Satisfaction with assistive Technology (QUEST) 2.0 and Assistive Technology Device Predisposition Assessment (ATD PA), Device Form). Results: The results revealed high performance levels (M≥. 80% accuracy) in the free painting and the copy painting conditions, ITRs (4.47-6.65. bits/min) comparable to other P300 applications and only low to moderate workload levels (5-49 of 100), thereby proving that the complex task of free painting did neither impair performance nor impose insurmountable workload. Users were satisfied with the BCI Brain Painting application. Main obstacles for use in daily life were the system operability and the EEG cap, particularly the need of extensive support for adjustment. Conclusion: The P300 Brain Painting application can be operated with high effectiveness and efficiency. End users with severe motor paralysis would like to use the application in daily life. User-friendliness, specifically ease of use, is a mandatory necessity when bringing BCI to end users. Early and active involvement of users and iterative user-centered evaluation enable developers to work toward this goal. Elsevier B.V.
In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time. The goal in this paper is to develop a general purpose (non-disease-specific) computational/artificial intelligence (AI) framework to address these challenges. This framework serves two potential functions: (1) a simulation environment for exploring various healthcare policies, payment methodologies, etc., and (2) the basis for clinical artificial intelligence - an AI that can " think like a doctor" Methods: This approach combines Markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting, sometimes synergistic interactions of various components in the healthcare system. It can operate in partially observable environments (in the case of missing observations or data) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans as actions are performed and new observations are obtained. This framework was evaluated using real patient data from an electronic health record. Results: The results demonstrate the feasibility of this approach; such an AI framework easily outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service models of healthcare. The cost per unit of outcome change (CPUC) was $189 vs $497 for AI vs. TAU (where lower is considered optimal) - while at the same time the AI approach could obtain a 30-35% increase in patient outcomes. Tweaking certain AI model parameters could further enhance this advantage, obtaining approximately 50% more improvement (outcome change) for roughly half the costs. Conclusion: Given careful design and problem formulation, an AI simulation framework can approximate optimal decisions even in complex and uncertain environments. Future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine. Elsevier B.V.
Connect-Four, a new sensorimotor rhythm (SMR) based brain-computer interface (BCI) gaming application, was evaluated by four severely motor restricted end-users; two were in the locked-in state and had unreliable eye-movement. Methods: Following the user-centred approach, usability of the BCI prototype was evaluated in terms of effectiveness (accuracy), efficiency (information transfer rate (ITR) and subjective workload) and users' satisfaction. Results: Online performance varied strongly across users and sessions (median accuracy (%) of end-users: A. = .65; B. = .60; C. = .47; D. = .77). Our results thus yielded low to medium effectiveness in three end-users and high effectiveness in one end-user. Consequently, ITR was low (0.05-1.44. bits/min). Only two end-users were able to play the game in free-mode. Total workload was moderate but varied strongly across sessions. Main sources of workload were mental and temporal demand. Furthermore, frustration contributed to the subjective workload of two end-users. Nevertheless, most end-users accepted the BCI application well and rated satisfaction medium to high. Sources for dissatisfaction were (1) electrode gel and cap, (2) low effectiveness, (3) time-consuming adjustment and (4) not easy-to-use BCI equipment. All four end-users indicated ease of use as being one of the most important aspect of BCI. Conclusion: Effectiveness and efficiency are lower as compared to applications using the event-related potential as input channel. Nevertheless, the SMR-BCI application was satisfactorily accepted by the end-users and two of four could imagine using the BCI application in their daily life. Thus, despite moderate effectiveness and efficiency BCIs might be an option when controlling an application for entertainment. Elsevier B.V.
Predicting or prioritizing the human genes that cause disease, or " disease genes" , is one of the emerging tasks in biomedicine informatics. Research on network-based approach to this problem is carried out upon the key assumption of " the network-neighbour of a disease gene is likely to cause the same or a similar disease" , and mostly employs data regarding well-known disease genes, using supervised learning methods. This work aims to find an effective method to exploit the disease gene neighbourhood and the integration of several useful omics data sources, which potentially enhance disease gene predictions. Methods: We have presented a novel method to effectively predict disease genes by exploiting, in the semi-supervised learning (SSL) scheme, data regarding both disease genes and disease gene neighbours via protein-protein interaction network. Multiple proteomic and genomic data were integrated from six biological databases, including Universal Protein Resource, Interologous Interaction Database, Reactome, Gene Ontology, Pfam, and InterDom, and a gene expression dataset. Results: By employing a 10 times stratified 10-fold cross validation, the SSL method performs better than the k-nearest neighbour method and the support vector machines method in terms of sensitivity of 85%, specificity of 79%, precision of 81%, accuracy of 82%, and a balanced F-function of 83%. The other comparative experimental evaluations demonstrate advantages of the proposed method given a small amount of labeled data with accuracy of 78%. We have applied the proposed method to detect 572 putative disease genes, which are biologically validated by some indirect ways. Conclusion: Semi-supervised learning improved ability to study disease genes, especially a specific disease when the known disease genes (as labeled data) are very often limited. In addition to the computational improvement, the analysis of predicted disease proteins indicates that the findings are beneficial in deciphering the pathogenic mechanisms. © 2011 Elsevier B.V.
Elsevier B.V.. Introduction: The counting and classification of blood cells allow for the evaluation and diagnosis of a vast number of diseases. The analysis of white blood cells (WBCs) allows for the detection of acute lymphoblastic leukaemia (ALL), a blood cancer that can be fatal if left untreated. Currently, the morphological analysis of blood cells is performed manually by skilled operators. However, this method has numerous drawbacks, such as slow analysis, non-standard accuracy, and dependences on the operator's skill. Few examples of automated systems that can analyse and classify blood cells have been reported in the literature, and most of these systems are only partially developed. This paper presents a complete and fully automated method for WBC identification and classification using microscopic images. Methods: In contrast to other approaches that identify the nuclei first, which are more prominent than other components, the proposed approach isolates the whole leucocyte and then separates the nucleus and cytoplasm. This approach is necessary to analyse each cell component in detail. From each cell component, different features, such as shape, colour and texture, are extracted using a new approach for background pixel removal. This feature set was used to train different classification models in order to determine which one is most suitable for the detection of leukaemia. Results: Using our method, 245 of 267 total leucocytes were properly identified (92% accuracy) from 33 images taken with the same camera and under the same lighting conditions. Performing this evaluation using different classification models allowed us to establish that the support vector machine with a Gaussian radial basis kernel is the most suitable model for the identification of ALL, with an accuracy of 93% and a sensitivity of 98%. Furthermore, we evaluated the goodness of our new feature set, which displayed better performance with each evaluated classification model. Conclusions: The proposed method permits the analysis of blood cells automatically via image processing techniques, and it represents a medical tool to avoid the numerous drawbacks associated with manual observation. This process could also be used for counting, as it provides excellent performance and allows for early diagnostic suspicion, which can then be confirmed by a haematologist through specialised techniques.
Background: The multiplicity of information sources for data acquisition in modern intensive care units (ICUs) makes the resulting databases particularly susceptible to missing data. Missing data can significantly affect the performance of predictive risk modeling, an important technique for developing medical guidelines. The two most commonly used strategies for managing missing data are to impute or delete values, and the former can cause bias, while the later can cause both bias and loss of statistical power. Objectives: In this paper we present a new approach for managing missing data in ICU databases in order to improve overall modeling performance. Methods: We use a statistical classifier followed by fuzzy modeling to more accurately determine which missing data should be imputed and which should not. We firstly develop a simulation test bed to evaluate performance, and then translate that knowledge using exactly the same database as previously published work by [13]. Results: In this work, test beds resulted in datasets with mi ssing data ranging 10-50%. Using this new approach to missing data we are able to significantly improve modeling performance parameters such as accuracy of classifications by an 11%, sensitivity by 13%, and specificity by 10%, including also area under the receiver-operator curve (AUC) improvement of up to 13%. Conclusions: In this work, we improve modeling performance in a simulated test bed, and then confirm improved performance replicating previously published work by using the proposed approach for missing data classification. We offer this new method to other researchers who wish to improve predictive risk modeling performance in the ICU through advanced missing data management. Elsevier B.V.
Elsevier B.V.  In pattern recognition and medical diagnosis, similarity measure is an important mathematical tool. To overcome some disadvantages of existing cosine similarity measures of simplified neutrosophic sets (SNSs) in vector space, this paper proposed improved cosine similarity measures of SNSs based on cosine function, including single valued neutrosophic cosine similarity measures and interval neutrosophic cosine similarity measures. Then, weighted cosine similarity measures of SNSs were introduced by taking into account the importance of each element. Further, a medical diagnosis method using the improved cosine similarity measures was proposed to solve medical diagnosis problems with simplified neutrosophic information. Materials and methods: The improved cosine similarity measures between SNSs were introduced based on cosine function. Then, we compared the improved cosine similarity measures of SNSs with existing cosine similarity measures of SNSs by numerical examples to demonstrate their effectiveness and rationality for overcoming some shortcomings of existing cosine similarity measures of SNSs in some cases. In the medical diagnosis method, we can find a proper diagnosis by the cosine similarity measures between the symptoms and considered diseases which are represented by SNSs. Then, the medical diagnosis method based on the improved cosine similarity measures was applied to two medical diagnosis problems to show the applications and effectiveness of the proposed method. Results: Two numerical examples all demonstrated that the improved cosine similarity measures of SNSs based on the cosine function can overcome the shortcomings of the existing cosine similarity measures between two vectors in some cases. By two medical diagnoses problems, the medical diagnoses using various similarity measures of SNSs indicated the identical diagnosis results and demonstrated the effectiveness and rationality of the diagnosis method proposed in this paper. Conclusions: The improved cosine measures of SNSs based on cosine function can overcome some drawbacks of existing cosine similarity measures of SNSs in vector space, and then their diagnosis method is very suitable for handling the medical diagnosis problems with simplified neutrosophic information and demonstrates the effectiveness and rationality of medical diagnoses.
Implement and perform pilot testing of web-based clinical decision support services using a novel framework for creating and managing clinical knowledge in a distributed fashion using the cloud. The pilot sought to (1) develop and test connectivity to an external clinical decision support (CDS) service, (2) assess the exchange of data to and knowledge from the external CDS service, and (3) capture lessons to guide expansion to more practice sites and users. Materials and methods: The Clinical Decision Support Consortium created a repository of shared CDS knowledge for managing hypertension, diabetes, and coronary artery disease in a community cloud hosted by Partners HealthCare. A limited data set for primary care patients at a separate health system was securely transmitted to a CDS rules engine hosted in the cloud. Preventive care reminders triggered by the limited data set were returned for display to clinician end users for review and display. During a pilot study, we (1) monitored connectivity and system performance, (2) studied the exchange of data and decision support reminders between the two health systems, and (3) captured lessons. Results: During the six month pilot study, there were 1339 patient encounters in which information was successfully exchanged. Preventive care reminders were displayed during 57% of patient visits, most often reminding physicians to monitor blood pressure for hypertensive patients (29%) and order eye exams for patients with diabetes (28%). Lessons learned were grouped into five themes: performance, governance, semantic interoperability, ongoing adjustments, and usability. Discussion: Remote, asynchronous cloud-based decision support performed reasonably well, although issues concerning governance, semantic interoperability, and usability remain key challenges for successful adoption and use of cloud-based CDS that will require collaboration between biomedical informatics and computer science disciplines. Conclusion: Decision support in the cloud is feasible and may be a reasonable path toward achieving better support of clinical decision-making across the widest range of health care providers.
One of the hardest technical tasks in employing Bayesian network models in practice is obtaining their numerical parameters. In the light of this difficulty, a pressing question, one that has immediate implications on the knowledge engineering effort, is whether precision of these parameters is important. In this paper, we address experimentally the question whether medical diagnostic systems based on Bayesian networks are sensitive to precision of their parameters. Methods and materials: The test networks include Hepar II, a sizeable Bayesian network model for diagnosis of liver disorders and six other medical diagnostic networks constructed from medical data sets available through the Irvine Machine Learning Repository. Assuming that the original model parameters are perfectly accurate, we lower systematically their precision by rounding them to progressively courser scales and check the impact of this rounding on the models' accuracy. Results: Our main result, consistent across all tested networks, is that imprecision in numerical parameters has minimal impact on the diagnostic accuracy of models, as long as we avoid zeroes among parameters. Conclusion: The experiments' results provide evidence that as long as we avoid zeroes among model parameters, diagnostic accuracy of Bayesian network models does not suffer from decreased precision of their parameters. Elsevier B.V.
The array of available brain-computer interface (BCI) paradigms has continued to grow, and so has the corresponding set of machine lear ning methods which are at the core of BCI systems. The latter have evolved to provide more robust data analysis solutions, and as a consequence the proportion of healthy BCI users who can use a BCI successfully is growing. With this development the chances have increased that the needs and abilities of specific patients, the end-users, can be covered by an existing BCI approach. However, most end-users who have experienced the use of a BCI system at all have encountered a single paradigm only. This paradigm is typically the one that is being tested in the study that the end-user happens to be enrolled in, along with other end-users. Though this corresponds to the preferred study arrangement for basic research, it does not ensure that the end-user experiences a working BCI. In this study, a different approach was taken; that of a user-centered design. It is the prevailing process in traditional assistive technology. Given an individual user with a particular clinical profile, several available BCI approaches are tested and - if necessary - adapted to him/her until a suitable BCI system is found. Methods: Described is the case of a 48-year-old woman who suffered from an ischemic brain stem stroke, leading to a severe motor- and communication deficit. She was enrolled in studies with two different BCI systems before a suitable system was found. The first was an auditory event-related potential (ERP) paradigm and the second a visual ERP paradigm, both of which are established in literature. Results: The auditory paradigm did not work successfully, despite favorable preconditions. The visual paradigm worked flawlessly, as found over several sessions. This discrepancy in performance can possibly be explained by the user's clinical deficit in several key neuropsychological indicators, such as attention and working memory. While the auditory paradigm relies on both categories, the visual paradigm could be used with lower cognitive workload. Besides attention and working memory, several other neurophysiological and -psychological indicators - and the role they play in the BCIs at hand - are discussed. Conclusion: The user's performance on the first BCI paradigm would typically have excluded her from further ERP-based BCI studies. However, this study clearly shows that, with the numerous paradigms now at our disposal, the pursuit for a functioning BCI system should not be stopped after an initial failed attempt. The Authors.
The present work has the objective of developing an automatic methodology for the detection of lung nodules. Methodology: The proposed methodology is based on image processing and pattern recognition techniques and can be summarized in three stages. In the first stage, the extraction and reconstruction of the pulmonary parenchyma is carried out and then enhanced to highlight its structures. In the second stage, nodule candidates are segmented. Finally, in the third stage, shape and texture features are extracted, selected and then classified using a support vector machine. Results: In the testing stage, with 140 new exams from the Lung Image Database Consortium image collection, 80% of which are for training and 20% are for testing, good results were achieved, as indicated by a sensitivity of 85.91%, a specificity of 97.70% and an accuracy of 97.55%, with a false positive rate of 1.82 per exam and 0.008 per slice and an area under the free response operating characteristic of 0.8062. Conclusion: Lung cancer presents the highest mortality rate in addition to one of the smallest survival rates after diagnosis. An early diagnosis considerably increases the survival chance of patients. The methodology proposed herein contributes to this diagnosis by being a useful tool for specialists who are attempting to detect nodules. Elsevier B.V.
In this study, a methodology is presented for an automated levodopa-induced dyskinesia (LID) assessment in patients suffering from Parkinson's disease (PD) under real-life conditions. Methods and Material: The methodology is based on the analysis of signals recorded from several accelerometers and gyroscopes, which are placed on the subjects' body while they were performing a series of standardised motor tasks as well as voluntary movements. Sixteen subjects were enrolled in the study. The recordings were analysed in order to extract several features and, based on these features, a classification technique was used for LID assessment, i.e. detection of LID symptoms and classification of their severity. Results: The results were compared with the clinical annotation of the signals, provided by two expert neurologists. The analysis was performed related to the number and topology of sensors used; several different experimental settings were evaluated while a 10-fold stratified cross validation technique was employed in all cases. Moreover, several different classification techniques were examined. The ability of the methodology to be generalised was also evaluated using leave-one-patient-out cross validation. The sensitivity and positive predictive values (average for all LID severities) were 80.35% and 76.84%, respectively. Conclusions: The proposed methodology can be applied in real-life conditions since it can perform LID assessment in recordings which include various PD symptoms (such as tremor, dyskinesia and freezing of gait) of several motor tasks and random voluntary movements. Elsevier B.V..
This paper describes a methodology which enables computer-aided support for the planning, visualization and execution of personalized patient treatments in a specific healthcare process, taking into account complex temporal constraints and the allocation of institutional resources. To this end, a translation from a time-annotated computer-interpretable guideline (CIG) model of a clinical protocol into a temporal hierarchical task network (HTN) planning domain is presented. Materials and methods: The proposed method uses a knowledge-driven reasoning process to translate knowledge previously described in a CIG into a corresponding HTN Planning and Scheduling domain, taking advantage of HTNs known ability to (i) dynamically cope with temporal and resource constraints, and (ii) automatically generate customized plans. The proposed method, focusing on the representation of temporal knowledge and based on the identification of workflow and temporal patterns in a CIG, makes it possible to automatically generate time-annotated and resource-based care pathways tailored to the needs of any possible patient profile. Results: The proposed translation is illustrated through a case study based on a 70 pages long clinical protocol to manage Hodgkin's disease, developed by the Spanish Society of Pediatric Oncology. We show that an HTN planning domain can be generated from the corresponding specification of the protocol in the Asbru language, providing a running example of this translation. Furthermore, the correctness of the translation is checked and also the management of ten different types of temporal patterns represented in the protocol. By interpreting the automatically generated domain with a state-of-art HTN planner, a time-annotated care pathway is automatically obtained, customized for the patient's and institutional needs. The generated care pathway can then be used by clinicians to plan and manage the patients long-term care. Conclusion: The described methodology makes it possible to automatically generate patient-tailored care pathways, leveraging an incremental knowledge-driven engineering process that starts from the expert knowledge of medical professionals. The presented approach makes the most of the strengths inherent in both CIG languages and HTN planning and scheduling techniques: for the former, knowledge acquisition and representation of the original clinical protocol, and for the latter, knowledge reasoning capabilities and an ability to deal with complex temporal and r esource constraints. Moreover, the proposed approach provides immediate access to technologies such as business process management (BPM) tools, which are increasingly being used to support healthcare processes. Elsevier B.V.
Information in critical care environments is distributed across multiple sources, such as paper charts, electronic records, and support personnel. For decision-making tasks, physicians have to seek, gather, filter and organize information from various sources in a timely manner. The objective of this research is to characterize the nature of physicians' information seeking process, and the content and structure of clinical information retrieved during this process. Method: Eight medical intensive care unit physicians provided a verbal think-aloud as they performed a clinical diagnosis task. Verbal descriptions of physicians' activities, sources of information they used, time spent on each information source, and interactions with other clinicians were captured for analysis. The data were analyzed using qualitative and quantitative approaches. Results: We found that the information seeking process was exploratory and iterative and driven by the contextual organization of information. While there was no significant differences between the overall time spent paper or electronic records, there was marginally greater relative information gain (i.e., more unique information retrieved per unit time) from electronic records (t(6)=1.89, p=0.1). Additionally, information retrieved from electronic records was at a higher level (i.e., observations and findings) in the knowledge structure than paper records, reflecting differences in the nature of knowledge utilization across resources. Conclusion: A process of local optimization drove the information seeking process: physicians utilized information that maximized their information gain even though it required significantly more cognitive effort. Implications for the design of health information technology solutions that seamlessly integrate information seeking activities within the workflow, such as enriching the clinical information space and supporting efficient clinical reasoning and decision-making, are discussed. Elsevier B.V.
Background: Digital traces left on the Internet by web users, if properly aggregated and analyzed, can represent a huge information dataset able to inform syndromic surveillance systems in real time with data collected directly from individuals. Since people use everyday language rather than medical jargon (e.g. runny nose vs. respiratory distress), knowledge of patients' terminology is essential for the mining of health related conversations on social networks. Objectives: In this paper we present a methodology for early detection and analysis of epidemics based on mining Twitter messages. In order to reliably trace messages of patients that actually complain of a disease, first, we learn a model of naïve medical language, second, we adopt a symptom-driven, rather than disease-driven, keyword analysis. This approach represents a major innovation compared to previous published work in the field. Method: We first developed an algorithm to automatically learn a variety of expressions that people use to describe their health conditions, thus improving our ability to detect health-related "concepts" expressed in non-medical terms and, in the end, producing a larger body of evidence. We then implemented a Twitter monitoring instrument to finely analyze the presence and combinations of symptoms in tweets. Results: We first evaluate the algorithm's performance on an available dataset of diverse medical condition synonyms, then, we assess its utility in a case study of five common syndromes for surveillance purposes. We show that, by exploiting physicians' knowledge on symptoms positively or negatively related to a given disease, as well as the correspondence between patients' "naïve" terminology and medical jargon, not only can we analyze large volumes of Twitter messages related to that disease, but we can also mine micro-blogs with complex queries, performing fine-grained tweets classification (e.g. those reporting influenza-like illness (ILI) symptoms vs. common cold or allergy). Conclusions: Our approach yields a very high level of correlation with flu trends derived from traditional surveillance systems. Compared with Google Flu, another popular tool based on query search volumes, our method is more flexible and less sensitive to changes in web search behaviors. Elsevier B.V.
Elsevier B.V. Objective This paper presents benchmarking results of human epithelial type 2 (HEp-2) interphase cell image classification methods on a very large dataset. The indirect immunofluorescence method applied on HEp-2 cells has been the gold standard to identify connective tissue diseases such as systemic lupus erythematosus and Sjögren's syndrome. However, the method suffers from numerous issues such as being subjective, time consuming and labor intensive. This has been the main motivation for the development of various computer-aided diagnosis systems whose main task is to automatically classify a given cell image into one of the predefined classes. Methods and material The benchmarking was performed in the form of an international competition held in conjunction with the International Conference of Image Processing in 2013: fourteen teams, composed of practitioners and researchers in this area, took part in the initiative. The system developed by each team was trained and tested on a very large HEp-2 cell dataset comprising over 68,000 images of HEp-2 cell. The dataset contains cells with six different staining patterns and two levels of fluorescence intensity. For each method we provide a brief description highlighting the design choices and an in-depth analysis on the benchmarking results. Results The staining pattern recognition accuracy attained by the methods varies between 47.91% and slightly above 83.65%. However, the difference between the top performing method and the seventh ranked method is only 5%. In the paper, we also study the performance achieved by fusing the best methods, finding that a recog nition rate of 85.60% is reached when the top seven methods are employed. Conclusions We found that highest performance is obtained when using a strong classifier (typically a kernelised support vector machine) in conjunction with features extracted from local statistics. Furthermore, the misclassification profiles of the different methods highlight that some staining patterns are intrinsically more difficult to recognize. We also noted that performance is strongly affected by the fluorescence intensity level. Thus, low accuracy is to be expected when analyzing low contrasted images.
Objectives: To investigate whether (1) machine learning classifiers can help identify nonrandomized studies eligible for full-text screening by systematic reviewers; (2) classifier performance varies with optimization; and (3) the number of citations to screen can be reduced. Methods: We used an open-source, data-mining suite to process and classify biomedical citations that point to mostly nonrandomized studies from 2 systematic reviews. We built training and test sets for citation portions and compared classifier performance by considering the value of indexing, various feature sets, and optimization. We conducted our experiments in 2 phases. The design of phase I with no optimization was: 4 classifiers × 3 feature sets × 3 citation portions. Classifiers included k-nearest neighbor, naïve Bayes, complement naïve Bayes, and evolutionary support vector machine. Feature sets included bag of words, and 2- and 3-term n-grams. Citation portions included titles, titles and abstracts, and full citations with metadata. Phase II with optimization involved a subset of the classifiers, as well as features extracted from full citations, and full citations with overweighted titles. We optimized features and classifier parameters by manually setting information gain thresholds outside of a process for iterative grid optimization with 10-fold cross-validations. We independently tested models on data reserved for that purpose and statistically compared classifier performance on 2 types of feature sets. We estimated the number of citations needed to screen by reviewers during a second pass through a reduced set of citations. Results: In phase I, the evolutionary support vector machine returned the best recall for bag of words extracted from full citations; the best classifier with respect to overall performance was k-nearest neighbor. No classifier attained good enough recall for this task without optimization. In phase II, we boosted performance with optimization for evolutionary support vector machine and complement naïve Bayes classifiers. Generalization performance was better for the latter in the independent tests. For evolutionary support vector machine and complement naïve Bayes classifiers, the initial retrieval set was reduced by 46% and 35%, respectively. Conclusions: Machine learning classifiers can help identify nonrandomized studies eligible for full-text screening by systematic reviewers. Optimization can markedly improve performance of classifiers. However, generalizability varies with the classifier. The number of citations to screen during a second independent pass through the citations can be substantially reduced. Elsevier B.V.
Registration of pre- and intra-interventional data is one of the key technologies for image-guided radiation therapy, radiosurgery, minimally invasive surgery, endoscopy, and interventional radiology. In this paper, we survey those 3D/2D data registration methods that utilize 3D computer tomography or magnetic resonance images as the pre-interventional data and 2D X-ray projection images as the intra-interventional data. The 3D/2D registration methods are reviewed with respect to image modality, image dimensionality, registration basis, geometric transformation, user interaction, optimization procedure, subject, and object of registration. © 2010 Elsevier B.V.
Organ shape plays an important role in various clinical practices, e.g., diagnosis, surgical planning and treatment evaluation. It is usually derived from low level appearance cues in medical images. However, due to diseases and imaging artifacts, low level appearance cues might be weak or misleading. In this situation, shape priors become critical to infer and refine the shape derived by image appearances. Effective modeling of shape priors is challenging because: (1) shape variation is complex and cannot always be modeled by a parametric probability distribution; (2) a shape instance derived from image appearance cues (input shape) may have gross errors; and (3) local details of the input shape are difficult to preserve if they are not statistically significant in the training data. In this paper we propose a novel Sparse Shape Composition model (SSC) to deal with these three challenges in a unified framework. In our method, a sparse set of shapes in the shape repository is selected and composed together to infer/refine an input shape. The a priori information is thus implicitly incorporated on-the-fly. Our model leverages two sparsity observations of the input shape instance: (1) the input shape can be approximately represented by a sparse linear combination of shapes in the shape repository; (2) parts of the input shape may contain gross errors but such errors are sparse. Our model is formulated as a sparse learning problem. Using L1 norm relaxation, it can be solved by an efficient expectation-maximization (EM) type of framework. Our method is extensively validated on two medical applications, 2D lung localization in X-ray images and 3D liver segmentation in low-dose CT scans. Compared to state-of-the-art methods, our model exhibits better performance in both studies. © 2011 Elsevier B.V.
Deformable registration of images obtained from different modalities remains a challenging task in medical image analysis. This paper addresses this important problem and proposes a modality independent neighbourhood descriptor (MIND) for both linear and deformable multi-modal registration. Based on the similarity of small image patches within one image, it aims to extract the distinctive structure in a local neighbourhood, which is preserved across modalities. The descriptor is based on the concept of image self-similarity, which has been introduced for non-local means filtering for image denoising. It is able to distinguish between different types of features such as corners, edges and homogeneously textured regions. MIND is robust to the most considerable differences between modalities: non-functional intensity relations, image noise and non-uniform bias fields. The multi-dimensional descriptor can be efficiently computed in a dense fashion across the whole image and provides point-wise local similarity across modalities based on the absolute or squared difference between descriptors, making it applicable for a wide range of transformation models and optimisation algorithms. We use the sum of squared differences of the MIND representations of the images as a similarity metric within a symmetric non-parametric Gauss-Newton registration framework. In principle, MIND would be applicable to the registration of arbitrary modalities. In this work, we apply and validate it for the registration of clinical 3D thoracic CT scans between inhale and exhale as well as the alignment of 3D CT and MRI scans. Experimental results show the advantages of MIND over state-of-the-art techniques such as conditional mutual information and entropy images, with respect to clinically annotated landmark locations. Elsevier B.V.
The problem of respiratory motion has proved a serious obstacle in developing techniques to acquire images or guide interventions in abdominal and thoracic organs. Motion models offer a possible solution to these problems, and as a result the field of respiratory motion modelling has become an active one over the past 15. years. A motion model can be defined as a process that takes some surrogate data as input and produces a motion estimate as output. Many techniques have been proposed in the literature, differing in the data used to form the models, the type of model employed, how this model is computed, the type of surrogate data used as input to the model in order to make motion estimates and what form this output should take. In addition, a wide range of different application areas have been proposed. In this paper we summarise the state of the art in this important field and in the process highlight the key papers that have driven its advance. The intention is that this will serve as a timely review and comparison of the different techniques proposed to date and as a basis to inform future research in this area.
Graphics processing units (GPUs) are used today in a wide range of applications, mainly because they can dramatically accelerate parallel computing, are affordable and energy efficient. In the field of medical imaging, GPUs are in some cases crucial for enabling practical use of computationally demanding algorithms. This review presents the past and present work on GPU accelerated medical image processing, and is meant to serve as an overview and introduction to existing GPU implementations. The review covers GPU acceleration of basic image processing operations (filtering, interpolation, histogram estimation and distance transforms), the most commonly used algorithms in medical imaging (image registration, image segmentation and image denoising) and algorithms that are specific to individual modalities (CT, PET, SPECT, MRI, fMRI, DTI, ultrasound, optical imaging and microscopy). The review ends by highlighting some future possibilities and challenges. Elsevier B.V.
This paper proposes a new algorithm for the efficient, automatic detection and localization of multiple anatomical structures within three-dimensional computed tomography (CT) scans. Applications include selective retrieval of patients images from PACS systems, semantic visual navigation and tracking radiation dose over time.The main contribution of this work is a new, continuous parametrization of the anatomy localization problem, which allows it to be addressed effectively by multi-class random regression forests. Regression forests are similar to the more popular classification forests, but trained to predict continuous, multi-variate outputs, where the training focuses on maximizing the confidence of output predictions. A single pass of our probabilistic algorithm enables the direct mapping from voxels to organ location and size.Quantitative validation is performed on a database of 400 highly variable CT scans. We show that the proposed method is more accurate and robust than techniques based on efficient multi-atlas registration and template-based nearest-neighbor detection. Due to the simplicity of the regressor's context-rich visual features and the algorithm's parallelism, these results are achieved in typical run-times of only ~4. s on a conventional single-core machine. Elsevier B.V.
Magnetic resonance (MR) imaging is often used to characterize and quantify multiple sclerosis (MS) lesions in the brain and spinal cord. The number and volume of lesions have been used to evaluate MS disease burden, to track the progression of the disease and to evaluate the effect of new pharmaceuticals in clinical trials. Accurate identification of MS lesions in MR images is extremely difficult due to variability in lesion location, size and shape in addition to anatomical variability between subjects. Since manual segmentation requires expert knowledge, is time consuming and is subject to intra- and inter-expert variability, many methods have been proposed to automatically segment lesions.The objective of this study was to carry out a systematic review of the literature to evaluate the state of the art in automated multiple sclerosis lesion segmentation. From 1240. hits found initially with PubMed and Google scholar, our selection criteria identified 80 papers that described an automatic lesion segmentation procedure applied to MS. Only 47 of these included quantitative validation with at least one realistic image. In this paper, we describe the complexity of lesion segmentation, classify the automatic MS lesion segmentation methods found, and review the validation methods applied in each of the papers reviewed. Although many segmentation solutions have been proposed, including some with promising results using MRI data obtained on small groups of patients, no single method is widely employed due to performance issues related to the high variability of MS lesion appearance and differences in image acquisition. The challenge remains to provide segmentation techniques that work in all cases regardless of the type of MS, duration of the disease, or MRI protocol, and this within a comprehensive, standardized validation framework. MS lesion segmentation remains an open problem. Elsevier B.V.
This paper proposes two new methods for the three-dimensional denoising of magnetic resonance images that exploit the sparseness and self-similarity properties of the images. The proposed methods are based on a three-dimensional moving-window discrete cosine transform hard thresholding and a three-dimensional rotationally invariant version of the well-known nonlocal means filter. The proposed approaches were compared with related state-of-the-art methods and produced very competitive results. Both methods run in less than a minute, making them usable in most clinical and research settings. © 2011 Elsevier B.V..
Diabetic macular edema (DME) is a common vision threatening complication of diabetic retinopathy. In a large scale screening environment DME can be assessed by detecting exudates (a type of bright lesions) in fundus images. In this work, we introduce a new methodology for diagnosis of DME using a novel set of features based on colour, wavelet decomposition and automatic lesion segmentation. These features are employed to train a classifier able to automatically diagnose DME through the presence of exudation. We present a new publicly available dataset with ground-truth data containing 169 patients from various ethnic groups and levels of DME. This and other two publicly available datasets are employed to evaluate our algorithm. We are able to achieve diagnosis performance comparable to retina experts on the MESSIDOR (an independently labelled dataset with 1200 images) with cross-dataset testing (e.g., the classifier was trained on an independent dataset and tested on MESSIDOR). Our algorithm obtained an AUC between 0.88 and 0.94 depending on the dataset/features used. Additionally, it does not need ground truth at lesion level to reject false positives and is computationally efficient, as it generates a diagnosis on an average of 4.4. s (9.3. s, considering the optic nerve localisation) per image on an 2.6. GHz platform with an unoptimised Matlab implementation. © 2011 Elsevier B.V.
Anatomical segmentation of structures of interest is critical to quantitative analysis in medical imaging. Several automated multi-atlas based segmentation propagation methods that utilise manual delineations from multiple templates appear promising. However, high levels of accuracy and reliability are needed for use in diagnosis or in clinical trials. We propose a new local ranking strategy for template selection based on the locally normalised cross correlation (LNCC) and an extension to the classical STAPLE algorithm by Warfield et al. (2004), which we refer to as STEPS for Similarity and Truth Estimation for Propagated Segmentations. It addresses the well-known problems of local vs. global image matching and the bias introduced in the performance estimation due to structure size. We assessed the method on hippocampal segmentation using a leave-one-out cross validation with optimised model parameters; STEPS achieved a mean Dice score of 0.925 when compared with manual segmentation. This was significantly better in terms of segmentation accuracy when compared to other state-of-the-art fusion techniques. Furthermore, due to the finer anatomical scale, STEPS also obtains more accurate segmentations even when using only a third of the templates, reducing the dependence on large template databases. Using a subset of Alzheimer's Disease Neuroimaging Initiative (ADNI) scans from different MRI imaging systems and protocols, STEPS yielded similarly accurate segmentations (Dice=0.903). A cross-sectional and longitudinal hippocampal volumetric study was performed on the ADNI database. Mean±SD hippocampal volume ( mm3 ) was 5195±656 for controls; 4786±781 for MCI; and 4427±903 for Alzheimer's disease patients and hippocampal atrophy rates (%/year) of 1.09±3.0, 2.74±3.5 and 4.04±3.6 respectively. Statistically significant (p < 10 - 3 ) differences were found between disease groups for both hippocampal volume and volume change rates. Finally, STEPS was also applied in a multi-label segmentation propagation scenario using a leave-one-out cross validation, in order to parcellate 83 separate structures of the brain. Comparisons of STEPS with state-of-the-art multi-label fusion algorithms showed statistically significant segmentation accuracy improvements (p < 10 - 4 ) in several key structures. Elsevier B.V.
In this paper, we give a short introduction to machine learning and survey its applications in radiology. We focused on six categories of applications in radiology: medical image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis and neurological disease diagnosis from fMR images, content-based image retrieval systems for CT or MRI images, and text analysis of radiology reports using natural language processing (NLP) and natural language understanding (NLU). This survey shows that machine learning plays a key role in many radiology applications. Machine learning identifies complex patterns automatically and helps radiologists make intelligent decisions on radiology data such as conventional radiographs, CT, MRI, and PET images and radiology reports. In many applications, the performance of machine learning-based automatic detection and diagnosis systems has shown to be comparable to that of a well-trained and experienced radiologist. Technology development in machine learning and radiology will benefit from each other in the long run. Key contributions and common characteristics of machine learning techniques in radiology are discussed. We also discuss the problem of translating machine learning applications to the radiology clinical setting, including advantages and potential barriers. .
"Shape" and "appearance", the two pillars of a deformable model, complement each other in object segmentation. In many medical imaging applications, while the low-level appearance information is weak or mis-leading, shape priors play a more important role to guide a correct segmentation, thanks to the strong shape characteristics of biological structures. Recently a novel shape prior modeling method has been proposed based on sparse learning theory. Instead of learning a generative shape model, shape priors are incorporated on-the-fly through the sparse shape composition (SSC). SSC is robust to non-Gaussian errors and still preserves individual shape characteristics even when such characteristics is not statistically significant. Although it seems straightforward to incorporate SSC into a deformable segmentation framework as shape priors, the large-scale sparse optimization of SSC has low runtime efficiency, which cannot satisfy clinical requirements. In this paper, we design two strategies to decrease the computational complexity of SSC, making a robust, accurate and efficient deformable segmentation system. (1) When the shape repository contains a large number of instances, which is often the case in 2D problems, K-SVD is used to learn a more compact but still informative shape dictionary. (2) If the derived shape instance has a large number of vertices, which often appears in 3D problems, an affinity propagation method is used to partition the surface into small sub-regions, on which the sparse shape composition is performed locally. Both strategies dramatically decrease the scale of the sparse optimization problem and hence speed up the algorithm. Our method is applied on a diverse set of biomedical image analysis problems. Compared to the original SSC, these two newly-proposed modules not only significant reduce the computational complexity, but also improve the overall accuracy. Elsevier B.V.
Cardiac resynchronisation therapy (CRT) is an effective treatment for patients with congestive heart failure and a wide QRS complex. How ever, up to 30% of patients are non-responders to therapy in terms of exercise capacity or left ventricular reverse remodelling. A number of controversies still remain surrounding patient selection, targeted lead implantation and optimisation of this important treatment. The development of biophysical models to predict the response to CRT represents a potential strategy to address these issues. In this article, we present how the personalisation of an electromechanical model of the myocardium can predict the acute haemodynamic changes associated with CRT. In order to introduce such an approach as a clinical application, we needed to design models that can be individualised from images and electrophysiological mapping of the left ventricle. In this paper the personalisation of the anatomy, the electrophysiology, the kinematics and the mechanics are described. The acute effects of pacing on pressure development were predicted with the in silico model for several pacing conditions on two patients, achieving good agreement with invasive haemodynamic measurements: the mean error on dP/dt max is 47.5±35mmHgs -1 , less than 5% error. These promising results demonstrate the potential of physiological models personalised from images and electrophysiology signals to improve patient selection and plan CRT. © 2011 Elsevier B.V..
A deformable registration method is described that enables automatic alignment of magnetic resonance (MR) and 3D transrectal ultrasound (TRUS) images of the prostate gland. The method employs a novel " model-to-image" registration approach in which a deformable model of the gland surface, derived from an MR image, is registered automatically to a TRUS volume by maximising the likelihood of a particular model shape given a voxel-intensity-based feature that represents an estimate of surface normal vectors at the boundary of the gland. The deformation of the surface model is constrained by a patient-specific statistical model of gland deformation, which is trained using data provided by biomechanical simulations. Each simulation predicts the motion of a volumetric finite element mesh due to the random placement of a TRUS probe in the rectum. The use of biomechanical modelling in this way also allows a dense displacement field to be calculated within the prostate, which is then used to non-rigidly warp the MR image to match the TRUS image. Using data acquired from eight patients, and anatomical landmarks to quantify the registration accuracy, the median final RMS target registration error after performing 100 MR-TRUS registrations for each patient was 2.40. mm. © 2010 Elsevier B.V.
Elsevier B.V. Retinal imaging provides a non-invasive opportunity for the diagnosis of several medical pathologies. The automatic segmentation of the vessel tree is an important pre-processing step which facilitates subsequent automatic processes that contribute to such diagnosis.We introduce a novel method for the automatic segmentation of vessel trees in retinal fundus images. We propose a filter that selectively responds to vessels and that we call B-COSFIRE with B standing for bar which is an abstraction for a vessel. It is based on the existing COSFIRE (Combination Of Shifted Filter Responses) approach. A B-COSFIRE filter achieves orientation selectivity by computing the weighted geometric mean of the output of a pool of Difference-of-Gaussians filters, whose supports are aligned in a collinear manner. It achieves rotation invariance efficiently by simple shifting operations. The proposed filter is versatile as its selectivity is determined from any given vessel-like prototype pattern in an automatic configuration process. We configure two B-COSFIRE filters, namely symmetric and asymmetric, that are selective for bars and bar-endings, respectively. We achieve vessel segmentation by summing up the responses of the two rotation-invariant B-COSFIRE filters followed by thresholding.The results that we achieve on three publicly available data sets (DRIVE: Se. =. 0.7655, Sp. =. 0.9704; STARE: Se. =. 0.7716, Sp. =. 0.9701; CHASE_DB1: Se. =. 0.7585, Sp. =. 0.9587) are higher than many of the state-of-the-art methods. The proposed segmentation approach is also very efficient with a time complexity that is significantly lower than existing methods.
Multi-atlas segmentation provides a general purpose, fully-automated approach for transferring spatial information from an existing dataset (" atlases") to a previously unseen context (" target") through image registration. The method to resolve voxelwise label conflicts between the registered atlases (" label fusion") has a substantial impact on segmentation quality. Ideally, statistical fusion algorithms (e.g., STAPLE) would result in accurate segmentations as they provide a framework to elegantly integrate models of rater performance. The accuracy of statistical fusion hinges upon accurately modeling the underlying process of how raters err. Despite success on human raters, current approaches inaccurately model multi-atlas behavior as they fail to seamlessly incorporate exogenous intensity information into the estimation process. As a result, locally weighted voting algorithms represent the de facto standard fusion approach in clinical applications. Moreover, regardless of the approach, fusion algorithms are generally dependent upon large atlas sets and highly accurate registration as they implicitly assume that the registered atlases form a collectively unbiased representation of the target. Herein, we propose a novel statistical fusion algorithm, Non-Local STAPLE (NLS). NLS reformulates the STAPLE framework from a non-local means perspective in order to learn what label an atlas would have observed, given perfect correspondence. Through this reformulation, NLS (1) seamlessly integrates intensity into the estimation process, (2) provides a theoretically consistent model of multi-atlas observation error, and (3) largely diminishes the need for large atlas sets and very high-quality registrations. We assess the sensitivity and optimality of the approach and demonstrate significant improvement in two empirical multi-atlas experiments. Elsevier B.V.
One of the main challenges for computer-assisted surgery (CAS) is to determine the intra-operative morphology and motion of soft-tissues. This information is prerequisite to the registration of multi-modal patient-specific data for enhancing the surgeon's navigation capabilities by observing beyond exposed tissue surfaces and for providing intelligent control of robotic-assisted instruments. In minimally invasive surgery (MIS), optical techniques are an increasingly attractive approach for in vivo 3D reconstruction of the soft-tissue surface geometry. This paper reviews the state-of-the-art methods for optical intra-operative 3D reconstruction in laparoscopic surgery and discusses the technical challenges and future perspectives towards clinical translation. With the recent paradigm shift of surgical practice towards MIS and new developments in 3D optical imaging, this is a timely discussion about technologies that could facilitate complex CAS procedures in dynamic and deformable anatomical regions. Elsevier B.V.
Elsevier B.V.. Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, et al. (2004), Klein, et al. (2005), and Heckemann, etal. (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of "atlases" (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003-2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.
Respiratory motion models have potential application for estimating and correcting the effects of motion in a wide range of applications, for example in PET-MR imaging. Given that motion cycles caused by breathing are only approximately repeatable, an important quality of such models is their ability to capture and estimate the intra- and inter-cycle variability of the motion. In this paper we propose and describe a technique for free-form nonrigid respiratory motion correction in the thorax. Our model is based on a principal component analysis of the motion states encountered during different breathing patterns, and is formed from motion estimates made from dynamic 3-D MRI data. We apply our model using a data-driven technique based on a 2-D MRI image navigator. Unlike most previously reported work in the literature, our approach is able to capture both intra- and inter-cycle motion variability. In addition, the 2-D image navigator can be used to estimate how applicable the current motion model is, and hence report when more imaging data is required to update the model. We also use the motion model to decide on the best positioning for the image navigator. We validate our approach using MRI data acquired from 10 volunteers and demonstrate improvements of up to 40.5% over other reported motion modelling approaches, which corresponds to 61% of the overall respiratory motion present. Finally we demonstrate one potential application of our technique: MRI-based motion correction of real-time PET data for simultaneous PET-MRI acquisition. © 2011 Elsevier B.V.
In this paper, we contribute to the development of context-aware operating rooms by introducing a novel approach to modeling and monitoring the workflow of surgical interventions. We first propose a new representation of interventions in terms of multidimensional time-series formed by synchronized signals acquired over time. We then introduce methods based on Dynamic Time Warping and Hidden Markov Models to analyze and process this data. This results in workflow models combining low-level signals with high-level information such as predefined phases, which can be used to detect actions and trigger an event. Two methods are presented to train these models, using either fully or partially labeled training surgeries. Results are given based on tool usage recordings from sixteen laparoscopic cholecystectomies performed by several surgeons. © 2010 Elsevier B.V.
This paper presents a new registration algorithm, called Temporal Diffeomorphic Free Form Deformation (TDFFD), and its application to motion and strain quantification from a sequence of 3D ultrasound (US) images. The originality of our approach resides in enforcing time consistency by representing the 4D velocity field as the sum of continuous spatiotemporal B-Spline kernels. The spatiotemporal displacement field is then recovered through forward Eulerian integration of the non-stationary velocity field. The strain tensor is computed locally using the spatial derivatives of the reconstructed displacement field. The energy functional considered in this paper weighs two terms: the image similarity and a regularization term. The image similarity metric is the sum of squared differences between the intensities of each frame and a reference one. Any frame in the sequence can be chosen as reference. The regularization term is based on the incompressibility of myocardial tissue. TDFFD was compared to pairwise 3D FFD and 3D+t FFD, both on displacement and velocity fields, on a set of synthetic 3D US images with different noise levels. TDFFD showed increased robustness to noise compared to these two state-of-the-art algorithms. TDFFD also proved to be more resistant to a reduced temporal resolution when decimating this synthetic sequence. Finally, this synthetic dataset was used to determine optimal settings of the TDFFD algorithm. Subsequently, TDFFD was applied to a database of cardiac 3D US images of the left ventricle acquired from 9 healthy volunteers and 13 patients treated by Cardiac Resynchronization Therapy (CRT). On healthy cases, uniform strain patterns were observed over all myocardial segments, as physiologically expected. On all CRT patients, the improvement in synchrony of regional longitudinal strain correlated with CRT clinical outcome as quantified by the reduction of end-systolic left ventricular volume at follow-up (6 and 12. months), showing the potential of the proposed algorithm for the assessment of CRT. © 2011 Elsevier B.V.
Compressed sensing MRI (CS-MRI) has shown great potential in reducing data acquisition time in MRI. Sparsity or compressibility plays an important role to reduce the image reconstruction error. Conventional CS-MRI typically uses a pre-defined sparsifying transform such as wavelet or finite difference, which sometimes does not lead to a sufficient sparse representation for the image to be reconstructed. In this paper, we design a patch-based nonlocal operator (PANO) to sparsify magnetic resonance images by making use of the similarity of image patches. The definition of PANO results in sparse representation for similar patches and allows us to establish a general formulation to trade the sparsity of these patches with the data consistency. It also provides feasibility to inc orporate prior information learnt from undersampled data or another contrast image, which leads to optimized sparse representation of images to be reconstructed. Simulation results on in vivo data demonstrate that the proposed method achieves lower reconstruction error and higher visual quality than conventional CS-MRI methods. Elsevier B.V.
The standard approach to multi-modal registration is to apply sophisticated similarity metrics such as mutual information. The disadvantage of these metrics, in comparison to measuring the intensity difference with, e.g. L1 or L2 distance, is the increase in computational complexity and consequently the increase in runtime of the registration. An alternative approach, which has not yet gained much attention in the literature, is to find image representations, so called structural representations, that allow for the application of the L1 and L2 distance for multi-modal images. This has not only the advantage of a faster similarity calculation but enables also the application of more sophisticated optimization strategies. In this article, we theoretically analyze the requirements for structural representations. Further, we introduce two approaches to create such representations, which are based on the calculation of patch entropy and manifold learning, respectively. While the application of entropy has practical advantages in terms of computational complexity, the usage of manifold learning has theoretical advantages, by presenting an optimal approximation to one of the theoretical requirements. We perform experiments on multiple datasets for rigid, deformable, and groupwise registration with good results with respect to both, runtime and quality of alignment. © 2011 Elsevier B.V..
We present a novel method for the joint segmentation of anatomical and functional images. Our proposed methodology unifies the domains of anatomical and functional images, represents them in a product lattice, and performs simultaneous delineation of regions based on random walk image segmentation. Furthermore, we also propose a simple yet effective object/background seed localization method to make the proposed segmentation process fully automatic. Our study uses PET, PET-CT, MRI-PET, and fused MRI-PET-CT scans (77 studies in all) from 56 patients who had various lesions in different body regions. We validated the effectiveness of the proposed method on different PET phantoms as well as on clinical images with respect to the ground truth segmentation provided by clinicians. Experimental results indicate that the presented method is superior to threshold and Bayesian methods commonly used in PET image segmentation, is more accurate and robust compared to the other PET-CT segmentation methods recently published in the literature, and also it is general in the sense of simultaneously segmenting multiple scans in real-time with high accuracy needed in routine clinical use.
Elsevier B.V. In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data.We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.
Recent advances in genome typing and sequencing technologies have enabled quick generation of a vast amount of molecular data at very low cost. The mining and computational analysis of this type of data can help shape new diagnostic and therapeutic strategies in biomedicine.
Russ Altman discusses how computational biology is rapidly transforming clinical practice, particularly in his own field of pharmacogenomics.
Technology can address biological threats like viral epidemics and bioterrorism that could put humankind on the brink of biological disaster.
Web and semantic technologies will form the foundation for ecosystems of machines that interact with each other and with people as never before.
Computational methods can be used to find associations between our genome and our traits, and new optimizations to these computations promise to do it much faster.
In an interdisciplinary effort to model protein dependency networks, biologists measure signals from certain proteins within cells over a given interval of time. Using this time series data, the goal is to deduce protein dependency relationships. The mathematical challenges is to statistically measure correlations between given proteins over time in order to conjecture probable relationships. Biologists can then consider these relationships with more scrutiny, in order to confirm their conjectures. One algorithm for finding such relationships makes use of interpolation of the data to produce next-state functions for each protein and the Deegan-Packel Index of Power voting method to measure the strength of correlations between pairs of proteins. The algorithm was previously implemented, but limitations associated with the original language required the algorithm to be re-implemented in a more computationally efficient language. Because of the algebraic focus of the Computational Commutative Algebra language, or CoCoA, the algorithm was re-implemented in this language, and results have been produced much more efficiently. In this paper I discuss the algorithm, the CoCoA language, the implementation of the algorithm in CoCoA, and the quality of the results.
Undergraduate students find that a genetically engineered machine can solve Hamiltonian Path Problems.
Suchi Saria of Johns Hopkins University shares how big data and machine learning can help improve the practice of healthcare, and how computing students can contribute.
An introduction to Markov models, their significance, and an explanation of how a hidden Markov model can be used to model the ultrasonic calls made by mice.
Somewhere between the studies of information technology and organic chemistry, researchers are trying to make tiny robots out of DNA molecules.
The intersection of biology and computer science is pushing computation beyond its traditional limits---forget algorithms think evolution.
How data collection and reporting standards have shaped what we know and do not know about water contamination in Hoosick Falls, NY.
The director of Stanford University's Pande Lab discusses how his work with large-scale, distributed simulation is being used to study protein folding and its connection to disease.
Pressured by escalating costs, continual demand for high quality, and the speed of technological advances, the need for change and improvisation has become a critical priority for the healthcare industry. Now society demands that healthcare providers offer better patient care through the careful use of information technologies. For that, practitioners are urged to expand the boundaries of innovative IS design strategies. Unfortunately, research on healthcare information systems (HIS) improvisation remains relatively underdeveloped. Thus, this study uses the organizational improvisation and bricolage theoretical lenses, from the perspective of a case study, to examine how strategic improvisation might give rise to fruitful HIS novel design performances. Theoretically, we provide an inductively derived strategic conceptual model of improvisation that couples with network, structure, and institutional bricolage to execute a 'resource-time-effort' model. This enables us to improvise a superior HIS that offers quality patient-centric healthcare delivery and a valuable improvisation model. Professionally, this study contributes three key insights for IS improvisation in the healthcare industry.
In this paper we demonstrate the relevance of abstraction, reuse, objects, classes, component and inheritance hierarchies, multiplicity, visual modeling, and other current software development best practices. We show how it is possible to start with a direct diagrammatic representation of a biological structure such as a cell, using terminology familiar to biologists, and by following a process of gradually adding more and more detail, arrive at a system with structure and behavior of arbitrary complexity that can run and be observed on a computer. We will be discussing various tools of UML used to describe biological modeling.
Auditory prostheses (AP) using Cochlear Implant System a.k.a bionic ears are widely used electronic devices that electrically stimulate the auditory nerve using an electrode array, surgically placed in the inner ear for patients suffering from severe to profound senosorineural deafness. The AP mainly contains an external Body Worn Speech Processor (BWSP) and an internal Implantable Receiver Stimulator (IRS). The BWSP receives an external sound or speech and generates encoded speech data bits for transmission to the IRS via a Radio Frequency transcutaneous link to excite the electrode array. After surgical placement of the electrode array in the inner ear, the BWSP should be fine-tuned to achieve the 80 to 100% speech reception abilities of the patient by an audiologist using Clinical Programming Software (CPS). The tuning process involves several tasks such as identifying the active electrode contacts, determining the detection and pain threshold of each active electrode, and loading these values into BWSP by reprogramming the BWSP. The main objective of this paper is to describe a simple personal-computer based, user-friendly CPS, which fine tunes the BWSP to achieve the best possible speech reception abilities of each patient and to perform post-operative fitting procedures by an audiologist. The CPS was developed to perform the post-operative fine tuning procedures such as (i) measurement of electrode tissue impedance, (ii) fitting to determine the hearing threshold and comfort levels for each active electrode, and (iii) reprogramming the speech processor using the identified threshold and comfort values. Finally, experimental results are presented.
Sepsis and septic shock are common and potentially fatal conditions that often occur in intensive care unit (ICU) patients. Early prediction of patients at risk for septic shock is therefore crucial to minimizing the effects of these complications. Potential indications for septic shock risk span a wide range of measurements, including physiological data gathered at different temporal resolutions and gene expression levels, leading to a nontrivial prediction problem. Previous works on septic shock prediction have used small, carefully curated datasets or clinical measurements that may not be available for many ICU patients. The recent availability of a large, rich ICU dataset called MIMIC-II has provided the opportunity for more extensive modeling of this problem. However, such a large clinical dataset inevitably contains a substantial amount of missing data. We investigate how different imputation selection criteria and methods can overcome the missing data problem. Our results show that imputation methods in conjunction with predictive modeling can lead to accurate septic shock prediction, even if the features are restricted primarily to noninvasive measurements. Our models provide a generalized approach for predicting septic shock in any ICU patient.
The need to improve population monitoring and enhance surveillance of infectious diseases has never been more pressing. Factors such as air travel act as a catalyst in the spread of new and existing viruses. The unprecedented user-generated activity on social networks over the last few years has created real-time streams of personal data that provide an invaluable tool for monitoring and sampling large populations. Epidemic intelligence relies on constant monitoring of online media sources for early warning, detection, and rapid response; however, the real-time information available in social networks provides a new paradigm for the early warning function. The communication of risk in any public health emergency is a complex task for governments and healthcare agencies. This task is made more challenging in the current situation when the public has access to a wide range of online resources, ranging from traditional news channels to information posted on blogs and social networks. Twitter�s strength is its two-way communication nature --- both as an information source but also as a central hub for publishing, disseminating and discovering online media. This study addresses these two challenges by investigating the role of Twitter during the 2009 swine flu pandemic by analysing data collected from the SN, and by Twitter using the opposite way for dissemination information through the network. First, we demonstrate the role of the social network for early warning by detecting an upcoming spike in an epidemic before the official surveillance systems by up to two weeks in the U.K. and up to two to three weeks in the U.S. Second, we illustrate how online resources are propagated through Twitter at the time of the WHO�s declaration of the swine flu �pandemic�. Our findings indicate that Twitter does favour reputable t bogus information can still leak into the network.
Automated monitoring algorithms operating on live video streamed from a home can effectively aid in several assistive monitoring goals, such as detecting falls or estimating daily energy expenditure. Use of video raises obvious privacy concerns. Several privacy enhancements have been proposed such as modifying a person in video by introducing blur, silhouette, or bounding-box. Person extraction is fundamental in video-based assistive monitoring and degraded in the presence of privacy enhancements; however, privacy enhancements have characteristics that can opportunistically be adapted to. We propose two adaptive algorithms for improving assistive monitoring goal performance with privacy-enhanced video: specific-color hunter and edge-void filler. A nonadaptive algorithm, foregrounding, is used as the default algorithm for the adaptive algorithms. We compare nonadaptive and adaptive algorithms with 5 common privacy enhancements on the effectiveness of 8 automated monitoring goals. The nonadaptive algorithm performance on privacy-enhanced video is degraded from raw video. However, adaptive algorithms can compensate for the degradation. Energy estimation accuracy in our tests degraded from 90.9% to 83.9%, but the adaptive algorithms significantly compensated by bringing the accuracy up to 87.1%. Similarly, fall detection accuracy degraded from 1.0 sensitivity to 0.86 and from 1.0 specificity to 0.79, but the adaptive algorithms compensated accuracy back to 0.92 sensitivity and 0.90 specificity. Additionally, the adaptive algorithms were computationally more efficient than the nonadaptive algorithm, averaging 1.7% more frames processed per second.
Business process analysis and process mining, particularly within the health care domain, remain under-utilized. Applied research that employs such techniques to routinely collected health care data enables stakeholders to empirically investigate care as it is delivered by different health providers. However, cross-organizational mining and the comparative analysis of processes present a set of unique challenges in terms of ensuring population and activity comparability, visualizing the mined models, and interpreting the results. Without addressing these issues, health providers will find it difficult to use process mining insights, and the potential benefits of evidence-based process improvement within health will remain unrealized. In this article, we present a brief introduction on the nature of health care processes, a review of process mining in health literature, and a case study conducted to explore and learn how health care data and cross-organizational comparisons with process-mining techniques may be approached. The case study applies process-mining techniques to administrative and clinical data for patients who present with chest pain symptoms at one of four public hospitals in South Australia. We demonstrate an approach that provides detailed insights into clinical (quality of patient health) and fiscal (hospital budget) pressures in the delivery of health care. We conclude by discussing the key lessons learned from our experience in conducting business process analysis and process mining based on the data from four different hospitals.
In electronic medical record (EMR) systems, administrators often provide EMR users with broad access privileges, which may leave the system vulnerable to misuse and abuse. Given that patient care is based on a coordinated workflow, we hypothesize that care pathways can be represented as the progression of a patient through a system and introduce a strategy to model the patient�s flow as a sequence of accesses defined over a graph. Elements in the sequence correspond to features associated with the access transaction (e.g., reason for access). Based on this motivation, we model patterns of patient record usage, which may indicate deviations from care workflows. We evaluate our approach using several months of data from a large academic medical center. Empirical results show that this framework finds a small portion of accesses constitute outliers from such flows. We also observe that the violation patterns deviate for different types of medical services. Analysis of our results suggests greater deviation from normal access patterns by nonclinical users. We simulate anomalies in the context of real accesses to illustrate the efficiency of the proposed method for different medical services. As an illustration of the capabilities of our method, it was observed that the area under the receiver operating characteristic (ROC) curve for the Pediatrics service was found to be 0.9166. The results suggest that our approach is competitive with, and often better than, the existing state-of-the-art in its outlier detection performance. At the same time, our method is more efficient, by orders of magnitude, than previous approaches, allowing for detection of thousands of accesses in seconds.
When a medical practitioner encounters a patient with rare symptoms that translates to rare occurrences in the local database, it is quite valuable to draw conclusions collectively from such occurrences in other hospitals. However, for such rare conditions, there will be a huge imbalance in classes among the relevant base population. Due to regulations and privacy concerns, collecting data from other hospitals will be problematic. Consequently, distributed decision support systems that can use just the statistics of data from multiple hospitals are valuable. We present a system that can collectively build a distributed classification model dynamically without the need of patient data from each site in the case of imbalanced data. The system uses a voting ensemble of experts for the decision model. The imbalance condition and number of experts can be determined by the system. Since only statistics of the data and no raw data are required by the system, patient privacy issues are addressed. We demonstrate the outlined principles using the Nationwide Inpatient Sample (NIS) database. Results of experiments conducted on 7,810,762 patients from 1050 hospitals show improvement of 13.68% to 24.46% in balanced prediction accuracy using our model over the baseline model, illustrating the effectiveness of the proposed methodology.
Computing and information technology professionals have exhibited high standards of engagement with ethical issues relating to privacy, information security and abuse of the technical capabilities they have been responsible for developing. But one can argue that computing capability is implicated in ethical controversies that receive relatively little discussion within the IT community. Stem cell research, nanotechnologies and other controversial areas of science would be impossible without the computational capacity of information processing. In many instances, the downstream applications of computer technology are deeply involved in the issues surrounding contested technologies.
In this thirteenth piece to the Ubiquity symposium discussing What is computation? Melanie Mitchell discusses the idea that biological computation is a process that occurs in nature, not merely in computer simulations of nature. --Editor
The work of a multidisciplinary genomic research lab in Berkeley may yield big changes in drug therapy and medicine. Roger Brent is President and Research Director of the Molecular Sciences Institute, an independent nonprofit research laboratory in Berkeley, CA, that combines genomic experimentation with computer modeling. The mission of the MSI is to predict the behavior of cells and organisms in response to defined genetic and environmental changes.
Pervasive care and chronic disease management to reduce institutionalization is a priority for most western countries. The realization of next generation ubiquitous and pervasive healthcare systems will be a challenging task, as these systems are likely to involve a complex structure. Such systems will consist of various devices, ranging from resource-constrained sensors and actuators to complex multimedia devices, supporting time critical applications. This is further compounded by cultural and socio-economical factors that must be addressed for next generation healthcare systems to be widely diffused and used. These factors have a direct impact on the system and security models and will require further understanding to encourage users to embrace and adopt the new technology. These models must capture not only the perceived value of the new technology and its ease of use, but most importantly the perceived risk of using this technology. This paper outlines the design space of pervasive health monitoring with body sensor networks and derives the requirements for connected pervasive medical care systems. Commercial and academic mobile medical sensor systems have been mapped to the requirements derived and a comparative analysis of state of the technology is given.
In this work, we investigate the feasibility and effectiveness of using human contact traces collected from mobile phones to derive social community information to reduce the disease propagation rate in the healthcare domain. Our trace-driven simulation results showed that our scheme is highly effective, and thus opens more opportunities for utilizing social relationships information to support healthcare related applications.
DNP (dynamic networks of processes) is a variant of the language introduced by Kahn and MacQueen [11, 12]. In the language it is possible to create new processes dynamically. We present a complete, formal denotational semantics for the language, along the lines sketched by Kahn and MacQueen. An informal explanation of the formal semantics is also given.
Practicing and playing a sport causes athletes' bodies to adapt to the movements they regularly perform. Unfortunately, this can cause muscle imbalances, which might impair performance or worse, cause an injury. It is always best to find the root cause of a muscle imbalance, and to make a precise effort to fix it. Muscle imbalance shouldn't be taken lightly-it could create bigger problems, from posture to spinal positioning, which can ultimately lead to issues in walking, sitting and even lying down, as time progresses. However, muscle imbalances can't be easily evaluated using X-rays, CT scans, or other high-tech devices. But it's possible to address the problem in other ways. In general, the "strong" muscle is measured against the "weaker" muscle. Using the infrared (IR) camera, Kinect can recognize users and track their skeletons in the field of view of the sensor. Kinect sensor can locate the joints of the tracked users in space and track their movements over time. This allows Kinect sensor to recognize people (postures) and follow their actions (movements). Hence, the primary aim of this research is to investigate patterns of muscle imbalance among athletes and evaluate those patterns based on the posture, balance, gait and movement variations using Kinect sensor. Ideally the expected outcome of this research would be a physically meaningful & robust method to identify the muscle imbalance of an athlete.
With the increase of data collected and computation power available, modern recommender systems are ever facing new challenges. While complex models are developed in academia, industry practice seems to focus on relatively simple techniques that can deal with the magnitude of data and the need to distribute the computation. The workshop on large-scale recommender systems (LSRS) is a meeting place for industry and academia to discuss the current and future challenges of applied large-scale recommender systems.
Despite their high priority, healthy nutrition, physical activity and other preventive health factors are rarely adopted over a long term. Traditional nutrition support systems lack of practical everyday knowledge, social support and motivation as well the consideration of the personal context. We address these impediments with a holistic decision support system that empowers people to change their lifestyle successfully. We aim at analyzing previous approaches from various disciplines and integrating them into one concept for nutrition support giving personalized context aware recommendations to each user while presenting and teaching practical information about healthy nutrition. Additionally, we consider the ease of usage of such an application by automating necessary burdens and motivating participants with social incentives. The decision support system is tested in a 6-month intervention study using regression analysis on usage patterns and matrix factorial designs for interacting features. The social and interactive components are observed in a 1-year field study, utilizing a realistic environment.
The first Workshop on Engendering Health with Recommender Systems was organized in conjunction with ACM RecSys 2016. The focus of the workshop was on bringing together researchers and practitioners from diverse areas of health, well-being, decision support, and behavioral change. Health-related issues in recommender systems have been a growing research topic in the recent years and this was a initial attempt at bringing together academics and practitioners to share their experiences on working on related issues.
We present MARS, a muscle activity recognition system that uses inertial sensors to capture the vibrations of active muscle. Specifically, we demonstrate how accelerometer data capturing these vibrations in the quadriceps, hamstrings and calf muscles of the human leg, can be leveraged to create muscle vibration signatures. We finally show that these vibration signatures can be used to distinguish these muscles from each other with greater than 85% precision and recall.
Poor posture and incorrect muscle usage are a leading cause of many injuries in sports and fitness. For this reason, non- invasive, fine-grained sensing and monitoring of human motion and muscles is important for mitigating injury and improving fitness efficacy. Current sensing systems either de- pend on invasive techniques or unscalable approaches whose accuracy is highly dependent on body sensor placement. As a result these systems are not suitable for use in active sports or fitness training where sensing needs to be scalable, accurate and un-inhibitive to the activity being performed. We present MARS, a system that detects both body motion and individual muscle group activity during physical human activity by only using unobtrusive, non-invasive in- ertial sensors. MARS not only accurately senses and recreates human motion down to the muscles, but also allows for fast personalized system setup by determining the individual identities of the instrumented muscles, obtained with minimal system training. In a real world human study con- ducted to evaluate MARS, the system achieves greater than 95% accuracy in identifying muscle groups.
Image transmission in biomedical wireless sensor networks often requires low complexity, low power consumption with soft real-time constraint. However, the full separation of source coding and channel coding usually comes with large delay and/or complexity. To address these requirements, we propose a new direct mapping of source sample to channel sample that utilizes large bandwidth of ultra wideband (UWB). Our system combines a 5/3 integer lifting scheme wavelet transform followed by a pulse position modulation (PPM). The system's major advantages are low complexity, low power consumption, low processing delay and robustness to error at sensor node. Evaluation on an image transmission system over additive white Gaussian noise (AWGN) channel shows that our proposed system can achieve lossless or near lossless image with significant energy saving and very low delay.
A wrist watch based system, which can measure electrocardiogram (ECG) and photoplethysmogram (PPG), is presented in this work. By using both ECG and PPG we measure pulse transit time (PTT), which is known to correlate well with the blood pressure (BP) [1]. This system, called BioWatch, can potentially facilitate continuous and ubiquitous monitoring of ECG, PPG and BP.
A variety of health and behavioral states can potentially be inferred from physiological measurements that can now be collected in the natural free-living environment. The major challenge, however, is to develop computational models for automated detection of health events that can work reliably in the natural field environment. In this paper, we develop a physiologically-informed model to automatically detect drug (cocaine) use events in the free-living environment of participants from their electrocardiogram (ECG) measurements. The key to reliably detecting drug use events in the field is to incorporate the knowledge of autonomic nervous system (ANS) behavior in the model development so as to decompose the activation effect of cocaine from the natural recovery behavior of the parasympathetic nervous system (after an episode of physical activity). We collect 89 days of data from 9 active drug users in two residential lab environments and 922 days of data from 42 active drug users in the field environment, for a total of 11,283 hours. We develop a model that tracks the natural recovery by the parasympathetic nervous system and then estimates the dampening caused to the recovery by the activation of the sympathetic nervous system due to cocaine. We develop efficient methods to screen and clean the ECG time series data and extract candidate windows to assess for potential drug use. We then apply our model on the recovery segments from these windows. Our model achieves 100% true positive rate while keeping the false positive rate to 0.87/day over (9+ hours/day of) lab data and to 1.13/day over (11+ hours/day of) field data.
The respiration rate of a person provides critical information about their well-being. Conventionally, contact sensing is used for breathing monitoring; however, it is expensive, uncomfortable, and immobile. In-home non-contact breathing monitoring is now possible via Doppler radar and motion capture video sensors, yet these technologies are limited in mobility, among other limitations. When monitoring a patient who is free to move around his or her home, it is dificult to scale current non-contact sensors to cover the large area. Our RUBreathing sensor system uses RF received signal strength (RSS) in a network to estimate breathing rate in real-time with high accuracy over a wide area. In this demonstration, we show the sensor continuously estimating a patient's respiration rate from non-contact RSS measurements between wireless devices.
Providing healthcare in remote locations can prove to be costly. Using a static videoconference system in the patient's home has its limitations. A remotely operated mobile robot platform could provide a better interaction with the patient located at home. This paper presents Telerobot, a teleoperated mobile robotic platform equipped with videoconferencing capabilities. Developed by a team of roboticists and clinical experts, the system is designed specifically for the provision of in-home telerehabilitation services. A usability study was done in order to qualify the robot user control scheme and the clinician-patient interaction.
By initiating physical contact with people, robots can be more useful. For example, a robotic caregiver might make contact to provide physical assistance or facilitate communication. So as to better understand how people respond to robot-initiated touch, we conducted a 2x2 between-subjects experiment with 56 people in which a robotic nurse autonomously touched and wiped the subject's forearm. Our independent variables were whether or not the robot verbally warned the person before contact, and whether the robot verbally indicated that the touch was intended to clean the person's skin (instrumental touch) or to provide comfort (affective touch). On average, regardless of the treatment, participants had a generally positive subjective response. However, with instrumental touch people responded significantly more favorably. Since the physical behavior of the robot was the same for all trials, our results demonstrate that the perceived intent of the robot can significantly influence a person's subjective response to robot-initiated touch. Our results suggest that roboticists should consider this factor in addition to the mechanics of physical interaction. Unexpectedly, we found that participants tended to respond more favorably without a verbal warning. Although inconclusive, our results suggest that verbal warnings prior to contact should be carefully designed, if used at all.
In this case study, we examine the functional utility of an embodied agent as an interactive medium in stroke rehab. A set of physical rehab exercises is conducted through the direct engagement of an embodied agent, the uBot-5. Based on the preliminary data, we argue that a general-purpose embodied agent has a potential to functionally complement human therapists in providing rehab to stroke patients.
This work is targeted towards the development of a recommender system that fosters the well-being of elderly people in their domestic environment. Since seniors can have a rather high barrier of using newer technologies, the interaction device utilized in such a system has to be selected carefully. This paper compares an anthropomorphic presentation of recommendations by a social robot with a graphical presentation on a tablet PC in terms of the perceived usability, the users' experience and the system's persuasion.
For many users of myoelectric prostheses there is a set of functionality which remains out of reach with current technology. In this work, we provide a first assessment of an extension to classical myoelectric prostheses control approaches that introduces simple automation that is shapable, using EMG signals. The idea is not to replace classical techniques, but to introduce automation for tasks, like those which require the coordination of multiple degrees of freedom, for which automation is well-suited. A prototype system is developed in simulation and an exploratory user study is performed to provide a first assessment, that evaluates our proposed approach and provides guidance for future development. A comparison is made between different formulations for the shaping controls, as well as to a classical control paradigm. Results from the user study are promising: showing significant performance improvements when using the automated controllers, and also unanimous preference for the use of automated controllers on this task. Additionally, some questions about the optimal user interaction with the system are revealed. All of these results support the case for continued development of the proposed approach, including more extensive user studies.
This work presents the preliminary results of an eight- week study of the seal-like robot PARO being used in a sensory therapy activity in a local nursing home. Participants were older adults with different levels of cognitive impairment,. We analyzed participant behaviors in video recorded during the weekly interactions between older adults, a therapist, and PARO. We found that PARO's continued use led to a steady increase in physical interaction between older adults and the robot and an increasing willingness among participants to interact with it.
We studied the multimodal nonverbal and verbal relationship between autistic children and a mobile toy robot during free spontaneous game play. A range of cognitive nonverbal criteria including eye contact, touch, manipulation, and posture were analyzed; the frequency of the words and verbs was calculated. Embedded multimodal interactions of autistic children and a mobile toy robot suggest that this robot could be used as a neural orthesis in order to improve children's brain activity and incite child to express language.
ALIZ-E is a Europe-wide project focusing on long-term child-robot interaction, specifically as a means of educating diabetic children on their condition. This video showcases a recent field study at "SugarKidsClub", a camp devoted to helping 7-12 year-olds handle type-1 diabetes. A wide range of CRI activities developed by ALIZ-E were employed, including a large "SandTray" touch table running a tile-sorting game and a "Handshake" touch-inducing activity designed to strengthen the child-robot bond. Apart from helping kids with their unfortunate affliction, the day at "SugarKidsClub" provided us a chance to use new technologies developed for the aforementioned activities as well as furthering our relationship with our primary stakeholders. This playful video highlights some of the footage taken that day within an entertaining story centered on Charlie, one of the NAO robots used for our field study with a pension for battery theft.
We report the results of standardized tests on a single subject with a stroke at 4, 20 and 28 weeks after completion of the study. These results follow from previous work[1]. The subject demonstrated sustained improvement in motor function 28 weeks after completing the study. In addition to quantitative results, the questionnaire results by the subject and the spouse testify that the subjective user experience was also positive. This further advocates the use of general purpose robots to complement human therapists.
This paper highlights initial observations from a user study performed in an assisted living facility in Spain. We introduced the NAO robot to assist in geriatric physiotherapy rehabilitation. The NAO is introduced in order to take over one of the usual roles of the physiotherapist: modeling movements for the inpatients. We also introduced a virtual version of the NAO in order to see whether this role of modeling is equally effective in a screen-based modality. Preliminary results show the inpatients adjust their movements to the NAO, although they react differently to the virtual and the physical robot.
In Japan, which has already become a super aging society, it is important that elderly people maintain their healthy condition both physically and mentally. We have developed a portable robot, "TechTech," which encourages them to take a walk. We infer that the human-robot interaction is crucially important for increasing their motivation to exercise by walking. TechTech talks to elderly people based on information obtained from various sensors: TechTech can judge weather and indoor-outdoor environments. It can also detect sounds and to count steps. It can vary the words and the intervals in conversations. Results obtained from experiments demonstrate that the impressions of dialogues with the robot differ according to a person's age. Elderly people prefer long intervals in conversations.
Fast Health Interoperability Services (FHIR) is the most recent in the line of standards for healthcare resources. FHIR represents different types of medical artifacts as resources and also provides recommendations for their authorized disclosure using web-based protocols including O-Auth and OpenId Connect and also defines security labels. In most cases, Role Based Access Control (RBAC) is used to secure access to FHIR resources. We provide an alternative approach based on Attribute Based Access Control (ABAC) that allows attributes of subjects and objects to take part in authorization decision. Our system allows various stakeholders to define policies governing the release of healthcare data. It also authenticates the end user requesting access. Our system acts as a middle-layer between the end-user and the FHIR server. Our system provides efficient release of individual and batch resources both during normal operations and also during emergencies. We also provide an implementation that demonstrates the feasibility of our approach.
The stethoscope has been used for many years, and has been very effective to diagnose certain cardiologic and pulmonologic sounds. For many years healthcare professionals would listen quietly to patients internal organs so they could diagnose from specific sounds of such internal organs. The objective is to develop a Peripheral Interface Controller based digital Stethoscope to capture the heart sound. The proposed designed device consists of following hardware stages: Front-end pickup circuitry, PIC18F2550 microcontroller, (128x64) graphic LCD and a Serial EEPROM. The captured data can be sent to PC for software analysis using LabVEIW. A digital stethoscope would help healthcare professional record their findings on a Serial EEPROM. Once the data is stored, the healthcare professionals can hear and plot the graph. This would be quick and more effective since there is a visual and audio representation to diagnose such cardiologic sounds.
This paper reports about design and development of a novel mobile electronic nose assembly, which is an inspection cum controlling system. It is used to detect and control the fungi growth. The Model is tested in North-Eastern (Shillong) Region of India. The diseases like Aspergillosis, mucor amphibiorum, penicillium marneffei and hypersensitivity pneumonic are caused due to long term exposure of Aspergillus Sp., Mucor Sp. and Penicillum Sp. fungi which causes severe breathing problem, bleeding lungs, cancer and even death. The developed system can be used in the room/laboratory/library or any closed environment in which the fungal growth control is needed to prevent the spread of fatal diseases.
This paper presents the analysis of affective speech of human for fatigue detection. Experimental results revealed that, there is a dependence of fatigue on human voice [3]. If a person is carrying fatigue or feeling depressed, then it is clearly reflected from his speech. Current research work on fatigue detection is based on normal human speech. This paper analyses the advantages of affective speech for precise determination of the state of fatigue. The analysis is carried out on the basis of various features such as fundamental frequency, formant frequencies, cepstrum, short-time energy etc.
The technology is changing at a faster rate. Information technology and communication technology have witnessed rapid technological advancements. These advancements will get appreciable respect if these are transformed into some beneficiary use to the citizens and reach to the common man. Inadequate healthcare facilities, shortage of healthcare staff and experts post major threats to meet the healthcare requirements of the population of underdeveloped countries or developing countries. The only way to cope up with these technologies is though the use of these advancements in the healthcare. This paper discusses about healthcare scenario in developing countries and developed countries, role of information technologies/communication technologies in healthcare.
Detection and classification of Atrial complexes from the ECG is of considerable importance in critical patient care monitoring of dangerous heart conditions. Accurate detection of Paroxysmal Atrial Fibrillation (PAF) using Atrial Premature Complexes(APC) is particularily important in relation to life threatening arrhythmias. PAF is a type of progressive cardiac arrhythmia that poses severe health risks, sometimes leading to ventricular arrhythmia. The electrocardiogram (ECG) data from the PhysioNet Online Database is used to develop a technique to screen, detect, and predict the onset of PAF. By considering a set of feature derived from RR intervals and P wave morphology it is possible to discriminate between PAF patients and healthy individuals. Result demonstrated that feature based on RR intervals is most successful. The RR based algorithm could be incorporated into medical devices with the potential of contributing to new healthcare technology.
There is a great need to constantly monitor the vital physiological parameters like ECG, heart rate, etc. for the elderly and chronic patients to take care of their health effectively and to provide treatment during emergency. The proposed designed system detects abnormal conditions in a patient and informs to the concerned medical personnel, without the intervention of even the patient himself. It processes the signals to detect any anomalies. If detected, it sends the data collected of the signal to the mobile phone of the concerned doctor in the form of messages by the use of GSM technology. At the doctor's end, the message received can be reconstructed back to the ECG waveform with the help of Java to Mobile Edition (J2ME) software. The proposed design is simple and compact and provides data with real time analysis to improvise patient care.
<u>Trypanosoma brucei</u> is a rational target for anti-trypanosomatid drug design because glycolysis provides virtually all the energy for the bloodstream form of this parasite. Its GAPDH enzyme and of human has established profound differences between them around the Adenosyl pocket of GAPDH. A cleft is present near the ribosyl ring of NAD+ in case of T.brucei, but it is filled by the ILE in human GAPDH. During the project, the NCI-DATABASE was screened in search for a ligand, which has greater affinity towards GAPDH enzyme of T.brucei than NAD+ by AutoDock (version 4). Finally, two ligands were found, which have substantially very high affinity towards the T.brucei GAPDH and comparatively low affinity towards human GAPDH. Therefore, these ligands can be proposed to act as the tight binding competitive inhibitor of GAPDH enzyme of T.brucei.
As times proceed people are depending more and more on computer technology. More enhancements are made on existing technologies to satisfy the improving needs of user. DNA computing is similar to parallel computing. It is a form of computing which uses DNA and molecular biology, instead of traditional silicon based computer technologies. A single gram of DNA can hold as much information as a trillion compact disk. It is possible to take different molecules of DNA to try different possibilities at once. This paper deals with evolution and need of DNA computer, example of DNA computing, its applications, advantages and disadvantages.
A variety of eHealth apps exist today ranging from ovulation calculators, such as Glow, to more sophisticated systems for determining the right therapist via semantic analysis, such as Talkspace, or ZocDoc. Despite their promising functionality, existing systems offer limited capabilities in terms of search filters, reviews, and doctor recommendations. In this paper, we propose FindMyDoc, a novel peer-to-peer healthcare platform that goes beyond existing traditional healthcare models. It provides doctor recommendations by allowing proper filtering based on treatment procedures, quality of treatment, and reviews of healthcare providers. In addition, the search results are refined using a recommendation engine that employs user-based collaborative filtering and exploits a set of predefined review options provided by the patients in order to match them with doctors.
Research indicates that various disorders in aging ranging from mild cognitive impairment to dementia and Alzheimer's could be diagnosed early based on the study of locomotion. Sensor platforms integrated into clothing provide the possibility of reliable locomotion monitoring. In this work, we describe a system using inexpensive, off-the-shelf inertial sensors to analyze locomotion. We present our measurements for coordination and consistency and discuss the preliminary results of our efforts to establish a baseline for these measurements for normal adult locomotion.
Cognitive functions, motoric expression, and changes in physiology are often studied separately, with little attention to the relationships, or correlations, among these entities. In this study, we implement an integrated approach by combining motion capture (action) and EMG (physiological) parameters as synchronized data streams resulting from the action and associated physiological data. Our experiments were designed to measure the preparatory movement capabilities of the upper extremities. In particular, measurement of changes in preparatory activity during the aging process are of interest to us, as the attempt is to develop means to compensate for loss of adaptive capabilities that aging entails. To achieve this goal, it is necessary to quantify preparation phases (timing and intensity). We measured motion capture and EMG parameters when subjects raised their arms without constraint (condition one) and raised their arms while holding a ball (second condition). Furthermore, on comparing aging and young participants, we confirmed that with aging the temporal relationships between actual movement and the preceding EMG signal change.
Breathing is one of the vital signs that are considered important from medical point of view. The quality of breathing is evaluated by considering several factors. In this paper we present the results of experiments that use a smart phone to evaluate some of the factors to determine the quality of breathing. The accelerometer in the smart phone is used to measure the breathing. We measure subjects with normal breathing, slow breathing, fast breathing and irregular breathing. Our results show that we can evaluate the rate of breathing using a smart phone with an accuracy ranging from 95% to 100%. We can also evaluate the regularity and the effort of breathing.
Among different types of human movement, falls are the most important since they related with high social and economic costs. Falls can cause various unintentional injuries such as fractures or in the worst-case scenario even lead to death, elderly citizen. Wearable devices present a growing interest in health care applications since they can detect signals of human activity and continuously monitoring critical parameters, offering a reliable and inexpensive solution. In this paper, an attitude and heading reference system - inertial measurement unit (IMU) is used in order to detect human movement and especially different type of falls.
Remote health monitoring (RHM) can help save the cost burden of unhealthy lifestyles. Of increased popularity is the use of smartphones to collect data, measure physical activity, and provide coaching and feedback to users. One challenge with this method is to improve adherence to prescribed medical regimens. In this paper we present a new battery optimization method that increases the battery lifetime of smartphones which monitor physical activity. We designed a system, WANDA-CVD, to test our battery optimization method. The focus of this report describes our in-lab pilot study and a study aimed at reducing cardiovascular disease (CVD) in young women, the Women's Heart Health study. Conclusively, our battery optimization technique improved battery lifetime by 300%. This method also increased participant adherence to the remote health monitoring system in the Women's Heart Health study by 53%.
Computer vision systems offer a new promising solution which can help older people stay at home by providing a secure environment and improve their quality of life. One application area of video surveillance is to analyse human behaviour and detect unusual behaviour. Falls are one of the greatest risks for the elderly living at home. This paper presents a novel approach for detecting falls, based on a combination of motion information and human shape variation. The motion information of a segmented silhouette, when extracted can provide a useful cue for classifying different behaviours. Also, the variation in human shape can used to establish the pose and hence fall events. The approach presented here extracts motion information, use variation in shape and in addition use best-fit approximated ellipse around the human body to further improved the accuracy of falls detection. Result of our approach demonstrates a 20% improvement over motion information only implementations.
Sleep constitutes a big portion of our lives and is a major part of health and well-being. The vital repair and regeneration tasks carried out during sleep are essential for our physical, mental and emotional health. Obstructive sleep apnea (OSA) is a sleep disorder that is characterized by repeated pauses in breathing during sleep. These pauses, or apneas, deplete the brain and the rest of the body of oxygen and disrupt the normal sleep cycle. OSA is associated with a number of adverse safety and health consequences, including excessive daytime sleepiness and fatigue, which increase the risk for motor vehicle and work-related accidents. OSA also results in an increased risk for hypertension, cardiovascular disease, the development of diabetes and even premature death. The gold standard method for diagnosing OSA patients is polysomnography (PSG). PSG is an overnight sleep test that monitors a participant's biophysical changes (EEG, ECG, etc.) that occur during sleep. Despite its wide use and multi-parametric nature, there are multiple complications associated with that test that make it ineffective as an early-stage diagnosis tool. In this paper, we propose a daytime OSA screening tool that addresses the shortcomings of PSG. The framework consists of a data collection component that acquires information about the subject being tested, and a prediction component that analyzes the collected data and makes a diagnosis. We identify patients' key physiological, psychological and contextual features and apply advanced machine learning algorithms to build effective prediction models that help identify OSA patients in the comfort of their own home. The system was deployed in a pilot sleep apnea study of 16 patients. Results demonstrate the proposed system's great potential in helping sleep specialists in the initial assessment of patients with suspected OSA.
In this paper, we concentrate on refining the results of segmenting human presence from indoors videos acquired by a fisheye camera, using a 3D mathematical model of the camera. The model has been calibrated according to the specific indoor environment that is being monitored. Human segmentation is implemented using a standard established technique. The fisheye camera used for video acquisition is modeled using a spherical element, while the parameters of the camera model are determined only once, using the correspondence of a number of user-defined landmarks, both in real world coordinates and on the acquired video frame. Subsequently, each pixel of the video frame is inversely mapped to the direction of view in the real world and the relevant data are stored in look-up tables for very fast utilization in real-time video processing. The proposed fisheye camera model enables the inference of possible real world positions of a segmented cluster of pixels in the video frame. In this work, we utilize the constructed camera model to achieve a simple geometric reasoning that corrects gaps and mistakes of the human figure segmentation. Initial results are also presented for a small number of video sequences, which prove the efficiency of the proposed method.
Parkinson's Disease (PD) is a disabling neurodegenerative disease that affects approximately 1 million people in the United States. PD symptoms such as tremor, rigidity, bradykinesia and gait disturbances can be alleviated with drug treatments or deep brain stimulation; however, these treatments need to be appropriately adjusted over time. Monitoring the disease severity during intermittent physician visits for this purpose is notoriously imprecise. Remote and continuous monitoring of the severity of parkinsonism in these patients could therefore significantly improve the patient's health and quality of life. In recent work, we showed it is possible to discriminate between varying levels of parkinsonism and normal brain activity, based on motor cortex electroencephalograms (EEGs). We believe it may be possible to detect similar patterns in ambulatory human EEGs collected periodically during home health care visits. In this study, we investigate how these EEG readings can be used together with continuous smartphone gyroscope and accelerometer movement measurements to allow improved management of Parkinson's Disease treatments.
COPD (Chronic Obstructive Pulmonary Disease) is a serious lung disease that causes difficulty in breathing. COPD patients require lung function examinations and perform breathing exercises on a regular basis in order to manage and be more aware of their health status. In this paper, we designed and developed a mobile-phone based system for lung function diagnosis, called mCOPD. Besides enabling accurate COPD examinations at home, the mCOPD system also offers a video-game based guidance system for breathing exercises. We evaluated mCOPD in controlled and uncontrolled environments with 40 subjects. The experimental results show that our system is a promising tool for remote medical treatment of COPD.
The estimation of human attention as input modality has been suggested as a method for an advanced human-computer interaction. With an increasing interest and development of augmented reality tools, the advent of Microsoft HoloLens glasses and increasingly affordable wearable eye-tracking devices, monitoring the human attention will soon become ubiquitous. Also visual heat-maps have become very popular and simpler to create in the 2D space over the last few years. They are very compelling and can be effective in summarizing and communicating data. The innovation in our work is the implementation of visual 3D heat-maps of the real world combined with advanced Computer Vision libraries. Finally, we have incorporated the visual 3D heat-maps for rehabilitation purposes that deal with the loss of concentration in children with learning disabilities, or disabled patients to select items of interest for them across a room.
We present the application of a recently proposed probabilistic logical formalism, on the task of sensor data fusion in the USEFIL project. USEFIL seeks to extract valuable knowledge concerning the well-being of elderly people by combining information coming from low-cost, unobtrusive monitoring devices. The approach we adopt to device its data fusion component is based on the Event Calculus and the stochastic logic programming language ProbLog and aims towards constructing a semantic representation of the received data, usable by a Decision Support System that will assist elderly people in their every day activities and will provide to doctors, relatives and carers insights on the user's behaviour and health.
Chronic mental illnesses pose a great burden on the lives of citizens worldwide. In modern health-care, decentralization and enabling the self management of patients at home are crucial factors in improving the every-day lives of patients and the people close to them. People in general tend to dislike obtrusive monitoring on their daily activities, so how can we implement a platform that can provide clinicians with adequate and concise information on their patients health status and at the same time be unobtrusive and easy to use. Moreover, how can we make such an unobtrusive system capable of providing the doctor with high-impact warnings on the patient's health status only when it is needed, thus relieving him of unnecessary workload? In this paper, the authors present a reconfigurable Event Detection mechanism used in the ALADDIN platform for Risk Assessment and Analysis.
Clinical practice guidelines (CPGs) are systematically developed documents that assist healthcare practitioners in making decisions about appropriate care for specific diseases based on evidence. In this context, acute gastroenteritis (AGE) in pediatrics is a suitable model for guidelines implementation and CPGs are available and very straightforward. Unfortunately, CPGs for AGE are poorly applied and most children receive not needed interventions. Mobile learning is a collaborative form of learning that can be used as a strategy of implementation for CPGs, for its portability and spreading capacity at a reasonable cost. In this paper we describe the development of a smartphone/tablet application for physicians providing guided decision making and carrying guidelines content about pediatric AGE in order to implement the pivotal recommendations from the CPGs and to foster their diffusion.
In this paper, we present a low-cost solution for real-time tracking of a human user's head position with respect to a video display source for eye gaze estimation in an assistive setting. The solution utilizes a wearable headset equipped with sensors found in commercially available off-the-shelf video gaming devices in order to minimize hardware complexity and expense. A pair of Nintendo Wiimote imaging sensors are used to create a stereo camera for 6DOF position tracking of the headset, while a modified Playstation Eye monocular camera is used to track the pupil position. The resulting tracking hardware is able to measure the 3D position of four infrared LEDs mounted at known locations on the video display using triangulation of the stereo camera data. Integration of the head tracking estimate with a computer vision based pupil tracking solution in order to compute the user's point of gaze is also described.
Human behaviour can be difficult to interpret even with the sophistication of modern smart homes, yet an understanding of the way people conduct their activities of daily living is essential for any attempts to detect problems. We discuss the key indicators for various activities that can be relatively robustly measured, and how these indicators can lead to a holistic measure for the activity. Combining indicators to give a metric for an activity evolution such as sleep can assist in extracting trends which may indicate some change in well-being.
Ebola crisis has taken its human and economic toll. It is an illustrative example of how inadequate healthcare infrastructure at the community level can have global implications. We are developing a low-cost continuous temperature acquisition system with embedded functionality for monitoring at the local, national and global level. Our system aims to not only track the health of patients in a range of resource constrained environments (e.g. small villages), empowering communities to contain potential outbreaks before they become unmanageable, but also to monitor healthcare workers during the course of their contact with patients and upon return to their local communities minimizing the probability of further disease transmission. We are confident that our system can be deployed in a number of use scenarios for Ebola tracking and management to improve the efficiency of healthcare resource allocation, control the spread of disease, and ultimately save lives.
In this paper, we investigate the potentials of an architecture for enabling the context-aware content adaptation and specialized delivery of information, based on a Future Internet architecture; the Publish/Subscribe Internetworking (PSI or ? for short) architecture. ? is an information oriented architecture built for the Future Internet using the so-called publish/subscribe paradigm. We advocate that by bringing information in the center of our solution as well as by applying ?'s particular security features we can achieve seamless information morphing and effective access control policies
Falls are a major cause of fatal injury for the elderly population. To improve the quality of living for seniors, a wide range of monitoring systems with fall detection functionality have been proposed over recent years. This article is a survey of systems and algorithms which aim at automatically detecting cases where a human falls and may have been injured. Existing fall detection methods can be categorized as using sensors, or being exclusively vision-based. This literature review focuses on vision-based methods.
The monitoring of human respiratory rate is essential in many clinical applications including the detection and monitoring of sleep disorders, the monitoring of newborns for Sudden Infant Death Syndrome (SIDS), and identifying patients at high risk up to 24 hours before an adverse event like stroke and cardiac arrest [1]. Traditional noninvasive respiratory rate measurements in a hospital setting rely on clinical staff to visually track a patient's chest movement for a period of time to derive the respiratory rate from the number of movements observed. Failure to perform continuous and quantified measurements of respiratory rate could result in an inability to rescue a patient exhibiting respiratory distress. Severe after effects hinder recovery and result in loss of time, cost, or even life. This paper proposes an e-textile pressure sensitive bed sheet to non-invasively and accurately measure respiratory rate by analyzing time-stamped pressure distribution sequences. The bed sheet provides a 24/7 quantified on-bed respiratory rate monitoring service. It is made of e-textile and is similar to a regular bed sheet in comfort. As a result, it can seamlessly fit in common clinical or home environments, reducing the possible interference with a patient's regular sleeping habits and resulting in a type of inconspicuous monitoring.
This paper presents a quantitative assessment solution for an upper extremities rehabilitative gaming application [1]. This assessment solution consists of a set of stand-alone hardware, including SmartGlove and Kinect, a depth capturing sensor made by Microsoft. SmartGlove is a specially designed motion and finger angle extraction device which is packaged in an easy-to-wear and adjustable manner for a patient with an upper extremity impairment. Sensor data extraction, alignment, and visualization algorithms were designed for integrating hand-mounted sensors data streams into skeleton coordinates captured by the Kinect. This enhanced skeleton information can be summarized and replayed as upper extremity joint coordinate animations which can be used for physical therapists to quantify rehabilitation progress. In addition, to serve as an assessment tool, enhanced skeleton information can be used to extend the capability of the Kinect vision system, such as providing motion capture of the upper extremities, even when the testing subject is out of camera scope or one's upper extremities are occluded by the body.
Consequences of advancing in years include declining health and mobility, leading to increased risk of injury due to accidents, especially at home. People currently use very basic passive or reactive methods concerning accidents such as panic or watch-strap buttons. More intelligent and proactive safety products leveraging the recent advances in pervasive computing, services and communication are not yet part of the home. The objective of this research is to introduce active safety products on the example of vacuum cleaner that are functionally enhanced by networked services towards personal safety and comfort.
The monitoring of sleeping behavioral patterns is of major importance for various reasons such as the detection and treatment of sleep disorders, the assessment of the effect of different medical conditions or medications on the sleep quality, and the assessment of mortality risks associated with sleeping patterns in adults and children. Sleep monitoring by itself is a difficult problem due to both privacy and technical considerations. The proposed system uses a combination of non-invasive sensors to assess and report sleep patterns and breathing activity: a contact-based pressure mattress and a non-contact 2D image acquisition device. To evaluate our system, we used real data collected in Heracleia Lab's assistive living apartment. Our system uses Machine Learning and Computer Vision techniques to automatically analyze the collected data, recognize sleep patterns and track the breathing behavior. It is non-invasive, as it does not disrupt the user's usual sleeping behavior and it can be used both at the clinic and at home with minimal cost. Going one step beyond, we developed a mobile application for visualizing the analyzed data and monitor the patient's sleep status remotely.
The automated human behavior modeling is highly desired in the context of an assistive environment. In this paper, we describe a software video processing and analysis system to assist the near real time detection of human activity. The video data are acquired indoors from fixed cameras in the living environment. The proposed system uses image-processing techniques to segment the human figure from the background, suppress the appearance of its shadow and detect the path and velocity of its motion. Detail performance measurements of the proposed algorithm are given in terms of execution time. Initial results are presented for a small number of video sequences.
Due to demographic changes in developed industrial countries and a better medical care system, the number of elderly people who still live in their home environment is rapidly growing because there they feel more comfortable and independent as in a clinical environment or in a residential care home. The elderly often live alone and receive only irregular visits. Due to impaired physical skills the probability of falls significantly increases. The detection of falls is a crucial aspect in the care of elderly. Falls are often detected very late with severe consequential damages. There are existing approaches for automatic fall detection. They usually deploy special external devices. Elderly people often do not accept these devices because they expose their frailty. In this paper, we present a location-independent fall detection method implemented as a smartphone application for an inconspicuous use in nearly every situation of the daily life. The difficulty of our approach is in the low resolution range of integrated acceleration sensors and the limited energy supply of the smartphone. As solution, we apply a modular threshold-based algorithm which uses the acceleration sensor with moderate energy consumption. Its fall detection rate is in the average of current relevant research.
Given the rise of long term conditions, and focus on living independently in the community, we launched a free crowd sourcing community app called 'ifOnly' to encourage people with disabilities to share the problems they encounter in everyday life. The app allows people to record, upload and share videos and audios that demonstrate everyday problems they face at home. These were then shared on the 'ifOnly' website - www.ifonlyitworked.com. Designers, recruited nationally, were asked to come up with innovative design solutions via a competition hosted by the University of Bath. The designs were evaluated for novelty and commercial potential by a panel of stakeholders.