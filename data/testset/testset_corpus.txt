artificial_intelligence	Nature is very simple and efficient in everything she makes, and is extremely obvious. We humans like to simulate in an extremely complicated manner what exists quite simply in nature, and what we succeed in simulating falls in the category of artificial intelligence. Artificial intelligence has limits of scope, but they fade away when compared with the performances of natural intelligence. In this study, we undertake to outline some limits of artificial intelligence compared to natural intelligence and some clear-cut differences that exist between the two.
artificial_intelligence	Experiment replication and reproduction are key requirements for empirical research methodology, and an important open issue in the field of Recommender Systems. When an experiment is repeated by a different researcher and exactly the same result is obtained, we can say the experiment has been replicated. When the results are not exactly the same but the conclusions are compatible with the prior ones, we have a reproduction of the experiment. Reproducibility and replication involve recommendation algorithm implementations, experimental protocols, and evaluation metrics. While the problem of reproducibility and replication has been recognized in the Recommender Systems community, the need for a clear solution remains largely unmet, which motivates the present workshop.
artificial_intelligence	In this paper, we summarize RecTour 2016 -- a workshop on recommenders in tourism co-located with RecSys 2016. There was a great variety of submissions, i.e., research papers, demo papers and position papers, addressing fundamental challenges of recommender systems in the tourism domain. The main topics included group recommendations, context-aware recommenders, choice-based recommenders and event recommendations.
artificial_intelligence	While content-based recommendation has been applied successfully in many different domains, it has not seen the same level of attention as collaborative filtering techniques have. However, there are many recommendation domains and applications where content and metadata play a key role, either in addition to or instead of ratings and implicit usage data. For some domains, such as movies, the relationship between content and usage data has seen thorough investigation already, but for many other domains, such as books, news, scientific articles, and Web pages we still do not know if and how these data sources should be combined to provided the best recommendation performance. The CBRecSys 2014 workshop aims to address this by providing a dedicated venue for papers dedicated to all aspects of content-based recommendation.
artificial_intelligence	We present a robust and efficient method for extracting textual information in the metro and train stations. The textual information in the train stations is destined to guide passengers about the directions, name of the station, etc. Our proposed method for text detection is based on the adaptive boosting method (AdaBoost algorithm) to construct a precise classifier by selecting and combining several features and weak classifiers of different families. We have studied, in detail, the behavior of the particular boosting algorithm when several features and weak classifiers are employed in the feature selection process. The evaluation is done on a challenging metro station image database which contains text of various sizes and fonts. The experiments show that the text detector is robust at detecting text in complex backgrounds.
artificial_intelligence	A common way of performing Feature Selection in Text Categorization consists in keeping the features with highest score according to certain measures, like linear ones which have been successfully proposed in [1]. Its disadvantage is that they need to previously determine the parameter which defines them. Until now, this drawback has been overcome by taking manually a set of values for such parameter. This paper proposes a method for automatically determining optimal values of the parameter by means of solving a univariate maximization problem.
artificial_intelligence	Music has changed and evolved through time and time tells that it has always been for the better. In our study, we have implemented Genetic Algorithms for the generation of music by music mixing to provide willing music lovers, the first step to personalize music to suit their liking. A music loop library, containing a list of all tentative loops which are to be used in the remix production alongwith the parameters which define them, is used for this purpose. A loop is an endless band of tape or music allowing continuous repetition. These loops cover all the major music instrument families with particular emphasis on Indian musical instruments as they have been extracted from Indian songs after careful observation using a sound editor. The combinations of different loops for the analysis is run through genetic algorithms. Some important parameters of a loop from the point of view of music mixing are: Depth, BPM (beats per minute), length of the loop and type of the instrument. A fitness function is calculated for the genetic algorithm to rank various combinations of loops for the next generation. The best loop combination can be selected out of the final set of population.
artificial_intelligence	Methods are known for the exact computation of the solution of integer systems of linear equations AX = B with a nonsingular coefficient matrix A by congruence techniques. These methods are now generalized for systems with an arbitrary integer coefficient matrix A. To make congruence techniques applicable, a common denominator of all elements of the solution X = A+B must be computed. This is achieved by defining the natural denominator CODE of A+ and describing it by some formulas. Methods for the exact computation of additional results (consistency, null space, solution of at most R nonzero elements), a recursive test to save computing time, and a comparison with some results from the literature are presented.
artificial_intelligence	The ability to learn from interaction with the exterior world as well as variability are two main features of living organisms. The aim of this study is to present and discuss the property of a stochastic reinforcement learning based model of upper limb posture generation that exhibits both properties. The capacity of the model to discover suitable postures satisfying task and obstacle avoidance constraints is demonstrated by simulation. Also, task equivalent configurations that can be linked to recent findings in the motor control literature are generated by the proposed formalism due to its stochastic nature.
artificial_intelligence	Topographic and overcomplete representations of natural images/videos are important problems in computational neuroscience. We propose a new method using both topographic and overcomplete representations of natural images, showing emergence of properties similar to those of complex cells in primary visual cortex (V1). This method can be considered as an extension of model in Hyvarinen et al. [Topographic independent component analysis, Neural Comput. 13 (7) (2001) 1527-1558], which uses complete topographic representation. We utilize a sparse and approximately uncorrelated decompositions and define a topographic structure on coefficients (the dot products between basis vectors and whitened observed data vectors). The overcomplete topographic basis vectors can be learned via estimation of independent component analysis (ICA) model based on the prior assumption upon basis vectors. Computer simulations are provided to show the relationship between our model and the basic properties of complex cells in V1 cortex. The learned bases are shown to have better coding efficiency than ordinary topographic ICA (TICA) bases.
artificial_intelligence	Biological cortical neurons form functional networks through a complex set of developmental steps. A key process in early development is the transition of the spontaneous network dynamics from slow synchronous activity to a mature firing profile with complex high-order patterns of spikes and bursts. In the present modeling study we investigate the required properties of the network to initialize this transition by the shift of the chloride reversal potential, which switches the effect of the GABA synapses from depolarizing to hyperpolarizing. The simulated networks are generated by a statistical description of parameters for the neuron model and the network architecture.
artificial_intelligence	An important problem in artificial intelligence (AI) is to find calculation procedures to save the semantic gap between the analytic formulations of the neuronal models and the concepts of the natural language used to describe the cognitive processes. In this work we explore a way of saving this gap for the case of the attentional processes, consisting in (1) proposing in first place a conceptual model of the attention double bottom-up/top-down organization, (2) proposing afterwards a neurophysiological model of the cortical and sub-cortical involved structures, (3) establishing the correspondences between the entities of (1) and (2), (4) operationalizing the model by using biologically inspired calculation mechanisms (algorithmic lateral inhibition and accumulative computation) formulated at symbolic level, and, (5) assessing the validity of the proposal by accommodating the works of the research team on diverse aspects of attention associated to visual surveillance tasks. The results obtained support in a reasonable way the validity of the proposal and enable its application in surveillance tasks different from the ones considered in this work. In particular, this is the case when linking the geometric descriptions of a scene with the corresponding activity level.
artificial_intelligence	We propose a new biologically motivated dynamic bottom-up selective attention model, which can generate a saliency map (SM) by considering dynamics of continuous input scenes as well as saliency of the primitive features of a static input scene. The maximum entropy algorithm is used to develop the dynamic selective attention model, whereby the input consists of the static bottom-up SMs for the successive static scenes. The experimental results show that the proposed model can generate more plausible scan paths for a dynamic scene compared with those obtained by the static bottom-up attention model.
artificial_intelligence	Constructing genetic regulatory networks is one of the most important issues in system biology research. Yet, building regulatory models manually is a tedious task, especially when the number of genes involved increases with the complexity of regulation. To automate the procedure of network construction, in this work we establish a clustering-based approach to infer recurrent neural networks as regulatory systems. Our approach also deals with the scalability problem by developing a clustering method with several data analysis techniques. To verify the presented approach, experiments have been conducted and the results show that it can be used to infer gene regulatory networks successfully.
artificial_intelligence	The local edge detector cells play a fundamental role in the visual system. They may be involved in the control of eye movement and visual attention. However, few studies have been devoted to modeling their behaviors. In this study, a feedforward multi-subunit spatiotemporal model is proposed for local edge detector cells in the cat retina. The model is able to describe their responses to drifting sinusoidal gratings, alternating sinusoidal gratings, flashing spots and annuli, and these model responses are qualitatively consistent with the physiological observations. The organization of the model maps the anatomical structure of the cat retina well, and the model may be useful in explaining ON-OFF retinal cells in other vertebrates.
artificial_intelligence	This paper concerns the blind separation of P complex convolutive mixtures of N statistically independent complex sources, with underdetermined or noisy mixtures i.e. P
artificial_intelligence	The neuronal regulator of the lower urinary tract is a very complex nervous system that consists of a heterogeneous group of neuronal centres. We have developed a new system from a model based in a multi-agent system in which each neuronal centre corresponds with an agent. This system incorporates a heuristic in order to make it more robust in the presence of possible inconsistencies. The heuristic used is based on a neural network (orthogonal associative memory). Knowledge through training has been added to the system, using correct patterns of behaviour of the urinary tract and behaviour patterns resulting from dysfunctions in two neuronal centres as a minimum. The experiments prove that the model is robust and its functioning coincides with the behaviour of the biological system. This work fulfils the expectations of providing a model of the regulator system that allows breaking the problem into simple modules each with its own entity.
artificial_intelligence	We describe a framework for the development of a gaming environment for the assessment of children with a spectrum of disabilities (GE-CDA tool). A typical case example is that of Cerebral Palsy (CP). We follow an evidence-based approach to identify the required domains which need to be addressed in the GE-CDA tool. Three different axes are considered, namely the domain of disability (cognitive, physical, psychological and behavioral), the function to be assessed for those domains (i.e. hand-eye coordination, working memory capacity) and the assessment method for each one of the functions. We plan to use Minecraft to develop the assessment tool, since it supports activity based mini-games (fishing, archery, maze, puzzles, etc.) and also allows for modifications, serving as an ideal platform for our GE-CDA tool.
artificial_intelligence	Roundoff errors cannot be avoided when implementing numerical programs with finite precision. The ability to reason about rounding is especially important if one wants to explore a range of potential representations, for instance, for FPGAs or custom hardware implementations. This problem becomes challenging when the program does not employ solely linear operations as non-linearities are inherent to many interesting computational problems in real-world applications. Existing solutions to reasoning possibly lead to either inaccurate bounds or high analysis time in the presence of nonlinear correlations between variables. Furthermore, while it is easy to implement a straightforward method such as interval arithmetic, sophisticated techniques are less straightforward to implement in a formal setting. Thus there is a need for methods that output certificates that can be formally validated inside a proof assistant. We present a framework to provide upper bounds on absolute roundoff errors of floating-point nonlinear programs. This framework is based on optimization techniques employing semidefinite programming and sums of squares certificates, which can be checked inside the Coq theorem prover to provide formal roundoff error bounds for polynomial programs. Our tool covers a wide range of nonlinear programs, including polynomials and transcendental operations as well as conditional statements. We illustrate the efficiency and precision of this tool on non-trivial programs coming from biology, optimization, and space control. Our tool produces more accurate error bounds for 23% of all programs and yields better performance in 66% of all programs.
artificial_intelligence	Sponsored search auctions are the main source of revenue for search engines. In such an auction, a set of utility maximizing advertisers competes for a set of ad slots. The assignment of advertisers to slots depends on the bids they submit; these bids may be different than the true valuations of the advertisers for the slots. Variants of the celebrated VCG auction mechanism guarantee that advertisers act truthfully and, under some assumptions, lead to revenue or social welfare maximization. Still, the sponsored search industry mostly uses generalized second price (GSP) auctions; these auctions are known to be nontruthful and suboptimal in terms of social welfare and revenue. In an attempt to explain this tradition, we study a Bayesian setting wherein the valuations of advertisers are drawn independently from a common regular probability distribution. In this setting, it is well known from the work of Myerson [1981] that the optimal revenue is obtained by the VCG mechanism with a particular reserve price that depends on the probability distribution. We show that, by appropriately setting the reserve price, the revenue over any Bayes-Nash equilibrium of the game induced by the GSP auction is at most a small constant factor away from the optimal revenue, improving previous results of Lucier et al. [2012]. Our analysis is based on the Bayes-Nash equilibrium conditions and the improved results are obtained by bounding the utility of each player at equilibrium using infinitely many deviating bids and also by developing novel prophet-like inequalities.
artificial_intelligence	Efficient data collection and event detection in the deep sea, as special applications of delay tolerant networks, pose some unique challenges due to the need for timeliness of data and event reporting of coverage areas and the delay of acoustic transmission in the ocean. Usually, autonomous underwater vehicles (AUVs) deployed in searching need to surface frequently to transmit collected data and events, as communications in the air can be done more quickly than communications under the water. However, extra delay is introduced at each resurfacing as AUVs are usually operated in the deep sea. In this paper, we attempt to optimize the frequency of surfacing with the objective of minimizing the average data and event reporting delay. We also study trajectory planning using an extended Euler circuit, where the search space is a set of connected line segments in the deep sea.
artificial_intelligence	In the text summarization field, based on how to create the summary, there are two main theoretic approaches (E. Lloret [22], K. Jezek and J. Steinberger [13]): the trend based on "extraction" in which the most important sentences in the source text will be chosen; the trend based on "abstraction" in which a new reduced grammatical and meaningful text will be created based on understanding the original text. In this paper, we focus on "abstraction" approach to summarize Vietnamese texts having only two sentences. We also restrict this research to consider verbs representing "actions" and "states", which appear in both two sentences of source text. Our method is based on modeling and processing Consequent-Relations between the verbs in the first sentence and the second sentence. The aim of this research is to generate new Vietnamese meaning-summarizing sentences which satisfy two requirements: (i) summarize the true meaning of the source pair of sentences; (ii) have natural expression for summarization. We also experiment and assess our approach by proposing a new quantitative evaluation method that suitable for the research approach.
artificial_intelligence	The application of Linked Open Data (LOD) principles to legal information (URI naming of resources, assertions about named relationships between resources or between resources and data values, and the possibility to easily extend, update and modify these relationships and resources) could offer better access and understanding of regulatory information to individual citizens, businesses and government agencies and administrations, and allow its sharing and reuse across applications, organizations and jurisdictions.
artificial_intelligence	The uptake of mobile phones has been especially remarkable in the developing world. For the first time in history people at the bottom of the income pyramid can also take part in the telecommunication society. Mobile phones can play a unique role in reaching those who are outside the scope of formal or institutionalized schooling and open doors to out-of-school learning practices. Based on ethnographic fieldwork in Wesbank, an impoverished community in Cape Town, we look at the (informal) learning practices "illiterate" and "low literate" women engage in, in an attempt to gain voice in new communicative realities by learning how to be(come) cell phone literate and at the different levels of competence this informal learning generates. The mobile phone has become a learning tool, nourishing learning practices in emerging "communities of practice" in which learning is a social activity in which anyone with any knowledge on mobile phones and mobile phone literacies becomes a potential tutor.
artificial_intelligence	We developed an emotional model, which could help supporting robots to accommodate humans during a working task inside an industrial setting. The robot would recognize when a human is experiencing increased stress and decides whether it should assist the human or should do other tasks. We propose the model as a framework which was developed as part of "The Smart Virtual Worker"-project within the context of human-robot interactions. The emotional model is able to estimate a worker's emotional valence throughout a typical work task by applying a hierarchical reinforcement learning algorithm. Since emotions are generated by the human brain based on an individual's interpretation of a stimulus, we linked the genesis of emotions to empirical findings of the sports sciences in order to infer an emotional reaction. Furthermore, the model reproduces sympathetic reactions of the human body and is capable of remembering past actions in order to include possible future time constraints as an initiator for emotional responses in the upcoming iterations. This capability is crucial for accommodating long-term experiences since the emotional reaction is not only based on the present situation, but on the whole experimental setting.
artificial_intelligence	Despite the great progress in robotic navigation in the past decades, navigating a human environment remains a hard task for a robot, due to the lack of formal rules guiding traffic, the lack of explicit communication among agents and the unpredictability of human behavior. Inspired by the efficiency of human navigation, we employ the insights of sociology studies on pedestrian behavior and psychology studies on action interpretation to design an online planning framework that leverages the power of implicit communication to generate legible robot behaviors in pedestrian environments. The foundation of our approach is a novel topological representation, based on braid groups. Preliminary results demonstrate the efficiency of our approach in simulation, whereas planned experiments with human subjects are expected to enable us to extract realistic predictive models and get user feedback. Finally, we plan on evaluating our approach by running our algorithms on our social robot in crowded environments.
artificial_intelligence	The study of the First Passage Time (FPT) problem (also known as first passage problem, FPP) started more than a century ago, but its diverse applications in science and engineering mostly emerged in the last two to three decades. Assuming that X(t) is a one-dimensional stochastic process, the First Passage Time is defined as the time (T) when X(t) first crosses a threshold. Engineering reliability is obviously a suitable application domain, and indeed applications such as optimal dam design in hydrology and analysis of structural failure in civil and mechanical engineering are typical examples. Although we envision that the FPT problem has great potential in network and software reliability, it should be more useful for network security and survivability because the approaches developed for the FPT problem are mostly analytical. The assumption for this inference is that in reliability analysis, experimental or historical data are often more readily available, which makes statistical approaches such as survival analysis more convenient and likely more realistic. In contrast, data is generally more difficult to obtain in security and survivability analyses, and analytical approaches can be leveraged to play more important roles. Furthermore, security and survivability often have to deal with malicious actions that may be driven by sophisticated cognition and behavioral processes, which are highly variable over time and very difficult to detect with short term data. If the behavior of an intruder can be characterized with some stochastic process such as Brownian motion, then the FPT approach may be applied to find the closed-form solution of the probability density function (PDF) of the first passage time, which can be the time when the system breaks down or when the hacker is successful in compromising a network. In addition, the solutions to FPT depend on boundary and initial conditions of the corresponding partial differential equations, and they also describe the evolution of PDF over time. This may suggest that it is possible to model the behavior changes of an intruder over time and circumstances. Another advantage of FPT analysis is that it may help solve some non-Markov stochastic process problems in reliability analysis and survival analysis. In this article, we first briefly introduce the FPT problem with Brownian motion as an example, and then suggest its potential applications in software reliability and network security.
artificial_intelligence	Affective characteristics are crucial factors that influence human behavior, and often, the prevalence of either emotions or reason varies on each individual. We aim to facilitate the development of agents� reasoning considering their affective characteristics. We first identify core processes in an affective BDI agent, and we integrate them into an affective agent architecture (GenIA3). These tasks include the extension of the BDI agent reasoning cycle to be compliant with the architecture, the extension of the agent language (Jason) to support affect-based reasoning, and the adjustment of the equilibrium between the agent�s affective and rational sides.
artificial_intelligence	This paper introduces AuraConf, the first programming language with a unified means to specify access-control and confidentially policies. In concert with a proof-carrying access control mechanism, to known-techniques for describing access-control, AuraConf allows confidentially policies to be specified declaratively using types and enforced via cryptography. Programs written in AuraConf enjoy a formal security guarantee via noninterference. Additionally, the language definition introduces a novel type system where the typechecker may use resources (i.e., private keys) and knowledge of an object's provenance (i.e., how a ciphertext was computed) to guide analysis.
artificial_intelligence	In this paper, an observer based cyber-attack detection and estimation methodology for cyber physical systems is presented. The cyber-attack is considered to influence the physical part of the cyber physical system that compromises human safety. The cyber-attacks are considered to affect the sensors and the actuators in the sub-systems as well as the software programs of the control systems in the cyber physical system. The whole system is modeled as a hybrid system to incorporate the discrete and continuous part of the cyber physical system and a sliding mode based observer is designed for the detection of these cyber-attacks. For simulation purposes, this paper considers different cyber-attacks on the battery sub-system of modern automobiles and the simulation results of attack detection are presented in the paper.
artificial_intelligence	Software agents are increasingly being used to represent humans in on-line auctions. Such agents have the advantages of being able to systematically monitor a wide variety of auctions and then make rapid decisions about what bids to place in what auctions. They can do this continuously and repetitively without losing concentration. Moreover, in complex multiple auction settings, agents may need to modify their behavior in one auction depending on what is happening in another. To provide a means of evaluating and comparing (benchmarking) research methods in this area, the Trading Agent Competition (TAC) was established. This competition involves a number of agents bidding against one another in a number of related auctions (operating different protocols) to purchase travel packages for customers. Against this background, this artcle describes the design, implementation and evaluation of our adaptive autonomous trading agent, SouthamptonTAC, one of the most successful participants in TAC 2002.
artificial_intelligence	Early design decisions such as architectural class and instruction set selection largely determine the performance and energy consumption of application specific processors (ASIPs). However, making decisions that effectively reflect in high performance require that a careful analysis of the target application is done by an experienced designer. Such process is extremely time consuming, and a confirmation that the processor meets the application requirements can only be extracted after costly architectural implementation, synthesis and simulation. To shorten design times, this work couples High-Level Synthesis (HLS) with pre-architectural performance estimation. We do so with the aim of providing designers with an initial architectural seed together with quantitative feedback about its performance. This enables to perform a light-weight refinement process based on the obtained feedback, such that time-consuming microarchitectural implementation is done only once at the end of the refinement steps. We employed our flow to generate four potential ASIPs for a 1024-point FFT. Estimates validation and gain evaluation is performed on actual ASIP implementations, which achieve performance gains of up to 8.42x and energy gains up to 1.32x over an existing VLIW processor.
artificial_intelligence	Indoor air pollution can provoke temporary uneasiness, headhache, sometimes sore throats or burning eyes and noses, but in the long term it can cause serious health problems both for young people, for the elderly and for those with existing diseases. It's easy to understand that knowing the quality of the air we are breathing is valuable. But most of all, having an easy readable object that shows you if you are in an unhealthy situation can simplify the task. A textile flower that opens and closes if the air quality is good or not could be a pleasurable object to wear and a indicator of the air quality.
artificial_intelligence	Low-cost mini-drones with advanced sensing and maneuverability enable a new class of intelligent visual sensing systems. This potential motivated several research efforts to employ drones as standalone surveillance systems or to assist legacy deployments. However, several fundamental challenges remain unsolved including: 1) Adequate coverage of sizable targets; 2) Target orientation that render coverage effective only from certain directions; 3) Occlusion by elements in the environment, including other targets. In this paper, we present Argus, a system that provides visual coverage of wide and oriented targets, using camera-mounted drones, taking into account the challenges stated above. Argus relies on a geometric model that captures both target shapes and coverage constraints. With drones being the scarcest resource in Argus, we study the problem of minimizing the number of drones required to cover a set of such targets and derive a best-possible approximation algorithm. Building upon that, we present a sampling heuristic that performs favorably, while running up to 100x faster compared to the approximation algorithm. We implement a complete prototype of Argus to demonstrate and evaluate the proposed coverage algorithms within a fully autonomous surveillance system. Finally, we evaluate the proposed algorithms via simulations to compare their performance at scale under various conditions.
artificial_intelligence	The on-going work at German Aerospace Center (DLR) and European Space Agency (ESA) on the Meteron Supvis-Justin space telerobotic experiment utilizing supervised autonomy is presented. The Supvis-Justin experiment will employ a tablet UI for an astronaut on the International Space Station (ISS) to communicate task level commands to a service robot. The goal is to explore the viability of supervised autonomy for space telerobotics. For its validation, survey, navigation, inspection, and maintenance tasks will be commanded to DLR's service robot, Rollin' Justin, to be performed in a simulated extraterrestrial environment constructed at DLR. The experiment is currently slated for late 2015-2016.
artificial_intelligence	This paper presents a novel reflection on interaction devices between human and robots. Robots are most currently seen as tools, extensions of the human body designed and meant to serve its needs. The development of artificial intelligence forces us to reconsider that paradigm, and to ask ourselves the question of who, during the human-machine dialogue, is really in control. In the overwhelming majority of situations where robots and users are expected to collaborate or interrelate, users are required to fully trust the machine and its reactions. The authors propose a methodology for the design of interfaces that question the very core of this trust issue, through a reversed interface allowing the machine to physically control humans seen as mere peripherals. After laying down the design principles of this approach, the authors describe an art performance during which the consequences of this reversal are explored to their very limits. The radical approach of this research is made possible through the unconventional study context provided by the artistic nature of the attempt. The experimental feedback of the artist is discussed. It is followed by a survey of the audience's reactions that allows to withdraw significant conclusions from the work.
artificial_intelligence	This paper describes the kernel and user interface designs of a Network Operating System for a Local Heterogeneous Area Network. The main goal of the project is to create a kernel to manage the distribution of a restricted set of functions or services between the network nodes, allowing portability of these functions within machine and Operating Systems heteregeneities upon which the kernel is built.Facing the problem throughout the Object and Client-Server Paradigms we attempt to provide the user with a dynamic availability of, till now isolated, resources by means of a user-friendly interface.
artificial_intelligence	In this special section of OSR, we present a selection of research papers that relate to Akamai's platform and technology.
artificial_intelligence	Address Resolution Protocol (ARP) is used to map the network address (IP address) to a physical address (MAC address). Being a stateless protocol and lacking proper authentication mechanism in the ARP messages, ARP is vulnerable for cache poisoning attack. Attacker can perform Man-In-The-Middle (MITM) attack or Denial of Service (DoS) attack and can access sensitive information, modify the contents, or deny the host from getting services. Different techniques for the detection and prevention of ARP cache poisoning attack have been proposed. Detection techniques (such as ARPWatch and Intrusion Detection techniques) generate false positives. Some prevention technique makes change in the switch itself and some uses cryptographic techniques. Secure-ARP and Ticket based ARP (TARP) are cryptographic techniques but suffer from single point failure and ticket flooding attacks respectively. ARP is a stateless protocol and ARP messages lacks the address authentication mechanism. As an ARP reply is unicast, all host systems in the LAN are not aware of the attacker present in the LAN. In this paper, we have proposed a protocol known as "Genuine Address Resolution Protocol (GARP)". Two novel concepts, viz., broadcastbased reply, and the Certifier for proof of IP address ownership have been proposed in GARP. As a reply is broadcast, the host, whose IP the attacker is using for attack, is aware of the attacker and subsequently makes other hosts in the LAN also aware of the attacker. Thus, the protocol prevents possible attack from the same attacker in the future. Statefulness is achieved by two tables, viz., the pending table and the blacklist table. The pending table holds the reply till its genuineness is proved and the blacklist table holds the MAC of attacker. Furthermore, the Certificate Authority is responsible for monitoring the ARP activities, which intervenes with appropriate messages at appropriate instances. The Dynamic Host Configuration Protocol (DHCP) server could be loaded with the additional service of monitoring ARP activities. The protocol has been implemented on Linux operating system. GARP was tested for various possible cases of ARP cache poisoning attack. From the results, it could be inferred that the GARP provides security against ARP cache poisoning attacks.
artificial_intelligence	The global economy and society increasingly depends on computer networks linked together by the Internet. The importance of computer networks reaches far beyond the telecommunications sector since they have become a critical factor for many other crucial infrastructures and markets. With threats mounting and security incidents becoming more frequent, concerns about network security grow. It is an acknowledged fact that some of the most fundamental network protocols that make the Internet work are exposed to serious threats. One of them is the Border Gateway Protocol (BGP) which determines how Internet traffic is routed through the topology of administratively independent networks that the Internet is comprised of. Despite the existence of a steadily growing number of BGP security proposals, to date none of them has been adopted. Using a precise definition of BGP robustness we experimentally show that the degree of robustness is distributed unequally across the administrative domains of the Internet, the so-called Autonomous Systems (ASes). The experiments confirm the intuition that the contribution ASes are able to make towards securing the correct working of the inter-domain routing infrastructure by deploying countermeasures against routing attacks differ depending on their position in the AS topology. We also show that the degree of this asymmetry can be controlled by the choice of the security strategy. We compare the strengths and weaknesses of two fundamentally different approaches in increasing BGP's robustness which we termed ingress and egress detection of false route advertisements and indicate their implications. Our quantitative results have important implications for Internet security policy, in particular with respect to the crucial question where to start the deployment of which type of security scheme in order to maximize the Internet's robustness to routing attacks.
artificial_intelligence	This paper presents an application-specific intrusion detection framework in order to address the problem of detecting intrusions in individual applications when their traffic exhibits anomalies. The system is based on the assumption that authorized traffic analyzers have access to a trustworthy binding between network traffic and the source application responsible for it. Given traffic flows generated by individual genuine application, we exploit the GMM-UBM (Gaussian Mixture Model-Universal Background Model) method to build models for genuine applications, and thereby form our detection system. The system was evaluated on a public dataset collected from a real network. Favorable results indicate the success of the framework.
artificial_intelligence	In traditional logistic regression model, every value of feature has the same weight. In this paper, we propose a new weighting method for logistic regression, which assigns a different weight to each feature value. A gradient approach is used to calculate the optimal weights of feature values.
artificial_intelligence	Nowadays, huge amount of documents are increasingly transferred to the remote servers due to the appealing features of cloud computing. On the other hand, privacy and security of the sensitive information in untrusted cloud environment is a big concern. To alleviate such concerns, encryption of sensitive data before its transfer to the cloud has become an important risk mitigation option. Encrypted storage provides protection at the expense of a significant increase in the data management complexity. For effective management, it is critical to provide efficient selective document retrieval capability on the encrypted collection. In fact, considerable amount of searchable symmetric encryption schemes have been designed in the literature to achieve this task. However, with the emergence of big data everywhere, available approaches are insufficient to address some crucial real-world problems such as scalability. In this study, we focus on practical aspects of a secure keyword search mechanism over encrypted data. First, we propose a provably secure distributed index along with a parallelizable retrieval technique that can easily scale to big data. Second, we integrate authorization into the search scheme to limit the information leakage in multi-user setting where users are allowed to access only particular documents. Third, we offer efficient updates on the distributed secure index. In addition, we conduct extensive empirical analysis on a real dataset to illustrate the efficiency of the proposed practical techniques.
artificial_intelligence	In this paper, we present SobTrA, a Software-based Trust Anchor for ARM Cortex-A processors to protect systems against software-based attacks. SobTrA enables the implementation of a software-based secure boot controlled by a third party independent from the manufacturer. Compared to hardware-based trust anchors, our concept provides some other advantages like being updateable and also usable on legacy hardware. The presented software-based trust anchor involves a trusted third party device, the verifier, locally connected to the untrusted device, e.g., via the microSD card slot of a smartphone. The verifier is verifying the integrity of the untrusted device by making sure that a piece of code is executed untampered on it using a timing-based approach. This code can then act as an anchor for a chain of trust similar to a hardware-based secure boot. Tests on our prototype showed that tampered and untampered execution of SobTrA can be clearly and reliably distinguished.
artificial_intelligence	Versatile algorithms for document image content extraction (DICE) were investigated in [1, 2, 3, 4]. That is, to extract the image layers that contain the contents of interests, such as handwriting, machine-print text, photographs and blank, etc. The DICE classifier based on tight ground truth data can delimit the regions of interests approximately. In this paper, taking the result of DICE classifier as the input, we extended the work by trying to completely separate the pixels of characters from the background and the other contents using image post-processing techniques and pattern recognition methods. First of all, we applied the color space analysis on the detected text regions. Then we segmented the image into regions (connected components) that contain pixels of similar colors and content labels, and generated patches containing multiple connected components that are within a selected distance to their neighbors. Finally we classified the generated patches using the structure features and DICE labels. The preliminary experiment results of the proposed model are promising.
artificial_intelligence	Online content exhibits rich temporal dynamics, and diverse realtime user generated content further intensifies this process. However, temporal patterns by which online content grows and fades over time, and by which different pieces of content compete for attention remain largely unexplored. We study temporal patterns associated with online content and how the content's popularity grows and fades over time. The attention that content receives on the Web varies depending on many factors and occurs on very different time scales and at different resolutions. In order to uncover the temporal dynamics of online content we formulate a time series clustering problem using a similarity metric that is invariant to scaling and shifting. We develop the K-Spectral Centroid (K-SC) clustering algorithm that effectively finds cluster centroids with our similarity measure. By applying an adaptive wavelet-based incremental approach to clustering, we scale K-SC to large data sets. We demonstrate our approach on two massive datasets: a set of 580 million Tweets, and a set of 170 million blog posts and news media articles. We find that K-SC outperforms the K-means clustering algorithm in finding distinct shapes of time series. Our analysis shows that there are six main temporal shapes of attention of online content. We also present a simple model that reliably predicts the shape of attention by using information about only a small number of participants. Our analyses offer insight into common temporal patterns of the content on theWeb and broaden the understanding of the dynamics of human attention.
artificial_intelligence	Friendships are dynamic. Previous studies have converged to suggest that social interactions, in both online and offline social networks, are diagnostic reflections of friendship relations (also called social ties). However, most existing approaches consider a social tie as either a binary relation, or a fixed value (named tie strength). In this paper, we investigate the dynamics of dyadic friend relationships through online social interactions, in terms of a variety of aspects, such as reciprocity, temporality, and contextuality. In turn, we propose a model to predict repliers and retweeters given a particular tweet posted at a certain time in a microblog-based social network. More specifically, we have devised a learning-to-rank approach to train a ranker that considers elaborate user-level and tweet-level features (like sentiment, self-disclosure, and responsiveness) to address these dynamics. In the prediction phase, a tweet posted by a user is deemed a query and the predicted repliers/retweeters are retrieved using the learned ranker. We have collected a large dataset containing 73.3 million dyadic relationships with their interactions (replies and retweets). Extensive experimental results based on this dataset show that by incorporating the dynamics of friendship relations, our approach significantly outperforms state-of-the-art models in terms of multiple evaluation metrics, such as MAP, NDCG and Topmost Accuracy. In particular, the advantage of our model is even more promising in predicting the exact sequence of repliers/retweeters considering their orders. Furthermore, the proposed approach provides emerging implications for many high-value applications in online social networks.
artificial_intelligence	Data mining and machine learning approaches can be incorporated into business intelligence (BI) systems to help users for decision support in many real-life applications. Here, in this paper, we propose a machine learning approach for BI applications. Specifically, we apply structural support vector machines (SSVMs) to perform classification on complex inputs such as the nodes of a graph structure. We connect collaborating companies in the information technology sector in a graph structure and use an SSVM to predict positive or negative movement in their stock prices. The complexity of the SSVM cutting plane optimization problem is determined by the complexity of the separation oracle. It is shown that (i) the separation oracle performs a task equivalent to maximum a posteriori (MAP) inference and (ii) a minimum graph cutting algorithm can solve this problem in the stock price case in polynomial time. Experimental results show the practicability of our proposed machine learning approach in predicting stock prices.
artificial_intelligence	Frequent set mining searches for sets of items that are frequently co-occurring together. Existing algorithms mainly find all the frequent sets from precise data. However, there are real-life situations in which users are interested in only some tiny portions of the entire collection of frequent sets and/or the data to be mined are uncertain. Recently, a tree-based system was proposed to mine uncertain data for frequent sets that satisfy user-specified succinct constraints. However, non-succinct constraints exist. In this paper, we extend such a system to mine uncertain data for frequent sets that satisfy succinct as well as non-succinct constraints by effectively exploiting properties of these constraints.
artificial_intelligence	Frequent pattern mining searches data for sets of items that are frequently co-occurring together. Most of algorithms find all the frequent patterns. However, there are many real-life situations in which users is interested in only some small portions of the entire collection of frequent patterns. To mine patterns that satisfy the user aggregate constraints in the form of agg(X.attr)?const, properties of constraints are exploited. When agg is sum, the mining can be complicated. Existing mining systems or algorithms usually make assumptions about the value or range of X.attr and/or const. In this paper, we propose a frequent pattern mining system that avoids making these assumptions and that effectively handles the sum constraints as well as other aggregate constraints.
artificial_intelligence	Research into (semi-)supervised clustering has been increasing. Supervised clustering aims to group similar data that are partially guided by the user's supervision. In this supervised clustering, there are many choices for formalization. For example, as a type of supervision, one can adopt labels of data points, must/cannot links, and so on. Given a real clustering task, such as grouping documents or image segmentation, users must confront the question "How should we mathematically formalize our task?" To help answer this question, we propose the classification of real clusterings into absolute and relative clusterings, which are defined based on the relationship between the resultant partition and the data set to be clustered. This categorization can be exploited to choose a type of task formalization.
artificial_intelligence	To cluster a data stream is a more challenging task than its regular batch version, having stricter performance constraints. In this paper an approach to this problem is presented, based on WiSARD, a memory-based artificial neural network (ANN) model. This model functioning was reviewed and improved, in order to adapt it to this task. The experimental results obtained support the use of this system for the analysis of data streams in an informative way.
artificial_intelligence	We propose a relatively simple yet powerful model for choosing relevant and non-redundant pieces of information. The model addresses data mining or information retrieval settings where relevance is measured with respect to a set of key or query objects, either specified by the user or obtained by a data mining step. The problem addressed is not only to identify other relevant objects, but also ensure that they are not related to possible negative query objects, and that they are not redundant with respect to each other. The model proposed here only assumes a similarity or distance function for the objects. It has simple parameterization to allow for different behaviors with respect to query objects. We analyze the model and give two efficient, approximate methods. We illustrate and evaluate the proposed model on different applications: linguistics and social networks. The results indicate that the model and methods are useful in finding a relevant and non-redundant set of results. While this area has been a popular topic of research, our contribution is to provide a simple, generic model that covers several related approaches while providing a systematic model for taking account of positive and negative query objects as well as non-redundancy of the output.
artificial_intelligence	The history and the future of Artificial Intelligence could be summarized into three distinctive phases: embryonic, embedded and embodied. We briefly describe early efforts in AI aiming to mimic intelligent behavior, evolving later into a set of the useful, embedded and practical technologies. We project the possible future of embodied intelligent systems, able to model and understand the environment and learn from interactions, while learning and evolving in constantly changing circumstances. We conclude with the (heretical) thought that in the future, AI should re-emerge as research in complex systems. One particular embodiment of a complex system is the Intelligent Enterprise.
artificial_intelligence	Unmanned aerial vehicle (UAV) swarms provide situation awareness in tasks such as emergency response, search and rescue, etc. However, most of these scenarios take place in GPS-denied environments, where accurately localizing each UAV is challenging. Heterogeneous UAV swarms, in which only a subset of the drones carry cameras, face the additional challenge of identifying each individual UAV to avoid sending position updates to the wrong drone, thus crashing. This work presents an identification mechanism based on the correlation between motion observed from external camera, and acceleration measured on each UAV's accelerometer.
artificial_intelligence	In this video, "Nao Haka", four robots and a haka leader perform a traditional Maori Haka. The Haka leader, who performs the main actions is supported by Aldebaran Nao Robots, which are controlled by an external performer, using a Microsoft Kinect as the input device. This device allows for full-body user tracking. This Video was made as a supportive gesture towards the All Blacks Rugby World Cup Campaign 2011.
artificial_intelligence	The purpose of this study is to verify the research model that coolness and perceived usefulness are effective predictor variables, attitude is mediating variable, and purchase intention is criterion variable. In total, 41 respondents with no prior exposure robot JIBO completed the on-line survey after watching the scenario movie of explaining its usage. Coolness and perceived usefulness is significant predictor for attitude, and attitude has positive impact on purchase intention. Based on the results, a "cool" product is more likely to arouse potential consumer's purchase intention directly and indirectly in the market. Theoretical and practical implications were discussed in detail.
artificial_intelligence	Visual Focus of Attention is an important mechanism to support successful interactions. In order to communicate effectively and intentionally (issuing cues when a person is paying attention, for example), a robot must have an understanding of this Visual Focus of Attention behavior in the Human-Robot Interaction space. A real-world interaction study was conducted with 24 unsolicited participants to explore attention behavior towards robots in this space. The results suggest there is no generalizable attention pattern between people, and thus that online, in situ Visual Focus of Attention estimation would be advantageous to Human-Robot Interaction.
artificial_intelligence	Autonomous robots acting as companions or assistants in real social environments should be able to sustain and operate over an extended period of time. Generally, autonomous mobile robots draw power from batteries to operate various sensors, actuators and perform tasks. Batteries have a limited power life and take a long time to recharge via a power source, which may impede human-robot interaction and task performance. Thus, it is important for social robots to manage their energy, this paper discusses an approach to manage power resources on mobile robot with regard to social aspects for creating life-like autonomous social robots.
artificial_intelligence	Various robot platforms have been designed and developed to perform given tasks in a hazardous environment for surveillance, reconnaissance, search and rescue, etc. We considered a terrain-adaptive and transformable hybrid robot platform that is equipped for rapid navigation on flat floors and good performance in overcoming stairs or obstacles. The mode transition is determined and implemented by adaptive driving mode control of the mobile robot. The terrain-adaptive and user-friendly remote control was verified through its navigation performance experiments in real and test-bed environments.
artificial_intelligence	Pointing gestures are a common and intuitive way to draw somebody's attention to a certain object. While humans can easily interpret robot gestures, the perception of human behavior using robot sensors is more difficult. In this work, we propose a method for perceiving pointing gestures using a Time-of-Flight (ToF) camera. To determine the intended pointing target, frequently the line between a person's eyes and hand is assumed to be the pointing direction. However, since people tend to keep the line-of-sight free while they are pointing, this simple approximation is inadequate. Moreover, depending on the distance and angle to the pointing target, the line between shoulder and hand or elbow and hand may yield better interpretations of the pointing direction. In order to achieve a better estimate, we extract a set of body features from depth and amplitude images of a ToF camera and train a model of pointing directions using Gaussian Process Regression. We evaluate the accuracy of the estimated pointing direction in a quantitative study. The results show that our learned model achieves far better accuracy than simple criteria like head-hand, shoulder-hand, or elbow-hand line.
artificial_intelligence	Advances in voice recognition have made possible applications in robotics controlled by voice only. However, user input through gestures and robot output gestures both create a more vivid interaction experience. In this article, we present an aloud reading application offering all these interaction methods for the HRI-research robot Maggie. It gives us a testbed for user studies investigating the effect of these additional interaction methods.
artificial_intelligence	This work introduces a universal Quantum-Dot Cellular Automata logic gate (UQCALG) for synthesizing symmetric functions with the target to reduce wire crossings in a design as well as the number of operating clock cycles. It is realized with the coupled majority-minority gate (CMVMIN) structure. The proposed UQCALG structure not only improves performance of a QCA design in terms of wire crossings and clocking, but also the simultaneous access to its four outputs ensures the cost-effective implementation of functions that may not be possible with that of conventional universal logic gates.
artificial_intelligence	Breast cancer affects many women, and early detection aids in fast and effective treatment. Mammography, which is currently the most popular method of breast screening, has some limitations, and microwave imaging offers an attractive alternative. The objective of this paper is to design an simple and cost effective wide slot aperture coupled patch antenna for Breast Cancer Detection. The radiation characteristics and the performance of the proposed design are compared with stacked patch method. The results of this study show that the wide-slot antenna has excellent performance across the required frequency range. This new wide slot stacked patch antenna, offers better radiation coverage to the breast.
artificial_intelligence	This paper calls out several research challenges in the art of recommendation technology as applied in Web media sites. One particular characteristic of such recommendation settings is the relative low cost of falsely recommending an irrelevant item, which means that recommendation schemes can be less conservative and more exploratory. This also creates opportunities for better item cold-start handling. Other technical difficulties include analyzing offline data that is heavily biased by the site's appearance, and in a related vein -- once a recommendation module's appearance has been designed -- defining the correct metrics by which to measure it. Also called out are tradeoffs between personalization and contextualization, as are novel schemes that aim at recommending sets and sequences of items.
artificial_intelligence	Classical recommender systems provide users with ranked lists of recommendations, where each one consists of a single item. However, these ranked lists are not suitable for applications such as trip planning, which deal with heterogeneous items. In this paper, we focus on the problem of recommending a set of packages to the user, where each package is constituted with a set of different Points of Interest that may constitute a tour. Given a collection of POIs, our goal is to recommend the most interesting packages for the user, where each package satisfies the budget constraints. We formally define the problem and we present a novel composite recommendation system, inspired from composite retrieval. Experimental evaluation of our proposed system, using a real-world dataset demonstrates its quality and its ability to improve both diversity and relevance of recommendations.
artificial_intelligence	Context-Aware Recommender Systems locally adapt to a specific contextual situation the rating prediction computed by a traditional context-free recommender. In this paper we present a novel semantic pre-filtering approach that can be tuned to the optimal level of contextualization by aggregating contextual situations that are similar to the target one. The similarities of contextual situations are derived from the available contextually tagged users' ratings according to how similarly the contextual conditions influence the user's rating behavior. We present an extensive evaluation of the performance of our pre-filtering approach on several data sets, showing that it outperforms state-of-the-art context-aware Matrix Factorization approaches.
artificial_intelligence	A methodology for the optimal design of projection patterns for stereometric structured light systems is presented. The similarity as well as the difference between the design of projection patterns and the design of optimal signals for digital communication are discussed. The design of K projection patterns for a structured light system with L distinct planes of light is shown to be equivalent to the placement of L points in a K dimensional space subject to certain constraints. optimal design in the MSE sense is defined, but shown to lead to an intractable multi-parameter global optimization problem. Intuitively appealing suboptimal solutions derived from the family of K dimensional space-filling Hilbert curves are obtained. Preliminary experimental results are presented.
artificial_intelligence	We present a new method for the global registration of several overlapping 3D surfaces sampled on an object. The method is based on the ICP (iterative closest point) algorithm and on a segmentation of the sampled points in an optimized set of z-buffers. This multi-z-buffer technique provides a 3D space partitioning which greatly accelerates the search of the nearest neighbours in the establishment of the point-to-point correspondence between overlapping surfaces. Then a randomized iterative registration is processed on the surface set. We have tested an implementation of this technique on real sampled surfaces. It appears to be rapid accurate and robust, especially in the case of highly curved objects.
artificial_intelligence	Collaborative filtering algorithms attempt to predict a user's interests based on his past feedback. In real world applications, a user's feedback is often continuously collected over a long period of time. It is very common for a user's interests or an item's popularity to change over a long period of time. Therefore, the underlying recommendation algorithm should be able to adapt to such changes accordingly. However, most existing algorithms do not distinguish current and historical data when predicting the users' current interests. In this paper, we consider a new problem - online evolutionary collaborative filtering, which tracks user interests over time in order to make timely recommendations. We extended the widely used neighborhood based algorithms by incorporating temporal information and developed an incremental algorithm for updating neighborhood similarities with new data. Experiments on two real world datasets demonstrated both improved effectiveness and efficiency of the proposed approach.
artificial_intelligence	In field environments it is often not possible to provide robot teams with detailed a priori environment and task models. In such unstructured environments, robots will need to create a dimensionally accurate three-dimensional geometric model of its surroundings by performing appropriate sensor actions. However, uncertainties in robot locations and sensing limitations/occlusions make this difficult. A new algorithm, based on iterative sensor planning and sensor redundancy, is proposed to build a geometrically consistent dimensional map of the environment for mobile robots that have articulated sensors. The aim is to acquire new information that leads to more detailed and complete knowledge of the environment. The robot(s) is controlled to maximize geometric knowledge gained of its environment using an evaluation function based on Shannon's information theory. Using the measured and Markovian predictions of the unknown environment, an information theory based metric is maximized to determine a robotic agent's next best view (NBV) of the environment. Data collected at this NBV pose are fused using a Kalman filter statistical uncertainty model to the measured environment map. The process continues until the environment mapping process is complete. The work is unique in the application of information theory to enhance the performance of environment sensing robot agents. It may be used by multiple distributed and decentralized sensing agents for efficient and accurate cooperative environment modeling. The algorithm makes no assumptions of the environment structure. Hence, it is robust to robot failure since the environment model being built is not dependent on any single agent frame, but is set in an absolute reference frame. It accounts for sensing uncertainty, robot motion uncertainty, environment model uncertainty and other critical parameters. It allows for regions of higher interest receiving greater attention by the agents. This algorithm is particularly well suited to unstructured environments, where sensor uncertainty and occlusions are significant. Simulations and experiments show the effectiveness of this algorithm.
artificial_intelligence	One of the important aspects in the development of high-throughput platforms based on a fleet of scientific instruments in the form of miniature wireless robots designed for fast operations at the nanometer-scale, is the conception of an embedded locomotion system capable of fast displacements between two successive locations while being accurate enough to position the robot within the range of the embedded instrument, typically within a few tenths of nanometers. This paper describes not only the fundamental principles of the locomotion method and mechanisms but the main constraints, challenges, and environmental conditions that must be taken into account in the implementation of such a system. Preliminary experimental results show the validity of this approach.
artificial_intelligence	Haptic feedback is one of the missing links in robotized minimally invasive telesurgery. The teleoperation controllers are optimized so as to offer the surgeon a reliable perception of the stiffness of soft tissue, rather than following the traditional approach where tracking and force reflection fidelity are considered. The experimental results show that optimization allows for better focus on the quality of the haptic information in the performance-stability trade-off. A force sensor to measure the interaction forces with the environment is found to be indispensable for high-quality touch feedback. Next to optimization for realistic feedback of the environment stiffness, in this paper we describe how to extend human perception beyond differential thresholds and thus enhance sensitivity. Experiments on a one-dimensional system demonstrate that the operator is able to discriminate tinier differences using a telemanipulation system with enhanced sensitivity than through direct manipulation.
artificial_intelligence	We present a new sampling-based algorithm for complete motion planning. Our algorithm relies on computing a star-shaped roadmap of the free space. We partition the free space into star-shaped regions such that a single point, called the guard, can see every point in the star-shaped region. The resulting set of guards capture the intra-region connectivity--the connectivity between points belonging to the same star-shaped region. We capture the inter-region connectivity by computing connectors that link guards of adjacent regions. The guards and connectors are combined to obtain the star-shaped roadmap. We present an efficient algorithm to compute the roadmap in a deterministic manner without explicit computation of the free space. We show that the star-shaped roadmap captures the connectivity of the free space, thereby enabling us to perform complete motion planning. Our approach is relatively simple to implement. We apply our approach to perform motion planning of robots with translational and rotational degrees of freedom (dof). We highlight its performance in challenging scenarios with narrow passages or when there is no collision-free path for robots with low degrees of freedom.
artificial_intelligence	The spring-loaded inverted pendulum (SLIP) is a simple, passively-elastic two-degree-of-freedom model for legged locomotion that describes the center-of-mass dynamics of many animal species and some legged robots. Conventionally, SLIP models employ a single support leg during stance and, while they can exhibit stable steady gaits when motions are confined to the sagittal plane, three-dimensional gaits are unstable to lateral toppling. In this paper it is shown that multiple stance legs can confer stability. Three SLIP-inspired models are studied: a passive bipedal kangaroo-hopper, an actuated insect model, and passive and actuated versions of a hexapedal robot model. The latter models both employ tripod stance phases. The sources of lateral stability are identified and, for the passive systems, analytical estimates of critical parameters are provided. Throughout, rotations are ignored and only center-of-mass translational dynamics are considered.
artificial_intelligence	In [Don4], we described a manipulation task for cooperating mobile robots that can push large, heavy objects. There, we asked whether explicit local and global communication between the agents can be removed from a family of pushing protocols. In this paper, we answer in the affirmative. We do so by using the general methods of [Don4] analyzing information invariants. We discuss several measures for the information complexity of the task: (a) How much internal state should the robot retain? (b) How many cooperating agents are required, and how much communication between them is necessary? (c) How can the robot change (side-effect) the environment in order to record state or sensory information to perform a task? (d) How much information is provided by sensors? and (e) How much computation is required by the robot? To answer these questions, we develop a notion of information invariants. We develop a technique whereby one sensor can be constructed from others by adding, deleting, and rellocating (a) (e) among collaborating autonomous agents. We add a resource to (a) (e) and ask: (f) How much information is provided by the task mechnics? By answering this question, we hope to develop information invariants that explicitly trade-off resource (f) with resources (a) (e). The protocols we describe here have been implemented in several different forms, and report on experiments to measure and analyze information invariants using a pair of cooperating mobile robots for manipulation experiments in our laboratory.
artificial_intelligence	Unlike conventional haptic devices, an encountered-type device is not held by a user all the time. Instead, the device remains at the location of a virtual object and waits for the user to encounter it. In this paper, we extend this concept to fingertip contacts and design an encountered-type haptic display for multiple fingertip contacts to simulate tasks of grasping an object with any shape and size. Before designing the device, we intensively observed human grasping behaviors. This observation was very helpful to determine the mechanism of the device. An encountered-type device for three-fingered grasping was actually prototyped based on our design.
artificial_intelligence	A real-time joint trajectory generator for planar walking bipeds is proposed. In the near future this trajectory planner will be implemented on the robot "Lucy", which is actuated by pleated pneumatic artificial muscles. The trajectory planner generates dynamically stable motion patterns by using a set of objective locomotion parameters as its input, and by tuning and exploiting the natural upper body dynamics. The latter can be determined and manipulated by using the angular momentum equation. Basically, trajectories for hip and swing foot motion are generated, which guarantee that the objective locomotion parameters attain certain prescribed values. Additionally, the hip trajectories are slightly modified such that the upper body motion is steered naturally, meaning that it requires practically no actuation. This has the advantage that the upper body actuation hardly influences the position of the Zero Moment Point. The effectiveness of the strategy developed is demonstrated by simulation results. A first simulation is performed under the assumption of perfect tracking by the controllers of the different actuators. This allows one to verify the effectiveness of the trajectory planner and to evaluate the postural stability. A second simulation is performed while taking the control architecture of the real robot into account. In order to have a more realistic simulation the proposed control architecture is evaluated with a full hybrid dynamic simulation model of the biped "Lucy". This simulator combines the dynamical behaviour of the robot with the thermodynamical effects that take place in the muscle-valves actuation system. The observed hardware limitations of the real robot and expected model errors are taken into account in order to give a realistic qualitative evaluation of the control performance and to test the robustness.
artificial_intelligence	When multiple robots cooperatively explore an environment, maps from individual robots must be merged to produce a single globally consistent map. This is a challenging problem when the robots do not have a common reference frame or global positioning. In this paper, we describe an algorithm for merging embedded topological maps. Topological maps provide a concise description of the navigability of an environment, and, with measurements easily collected during exploration, the vertices of the map can be embedded in a metric space. Our algorithm uses both the structure and the geometry of topological maps to determine the best correspondence between maps with single or multiple overlapping regions. Experiments with simulated and real-world data demonstrate the efficacy of our algorithm.
artificial_intelligence	We present a new roadmap for a rod-shaped robot operating in a three-dimensional workspace, whose configuration space is diffeomorphic to R3 X S2. This roadmap is called the rod hierarchical generalized Voronoi graph (rod-HGVG) and can be used to find a path between any two points in an unknown configuration space using only the sensor data. More importantly, the rod-HGVG serves as a basis for an algorithm to explore an unknown configuration space without explicitly constructing it. Once the rod-HGVG is constructed, the planner can use it to plan a path between any two configurations. One of the challenges in defining the roadmap revolves around a homotopy theory result, which asserts that there cannot be a one-dimensional deformation retract of a non-contractible space with dimension greater than two. Instead, we define an exact cellular decomposition on the free configuration space and a deformation retract in each cell (each cell is contractible). Next, we "connect" the deformation retracts of each of the cells using a roadmap of the workspace. We call this roadmap a piecewise retract because it comprises many deformation retracts. Exploiting the fact that the rod-HGVG is defined in terms of workspace distance measurements, we prescribe an incremental procedure to construct the rod-HGVG that uses the distance information that can be obtained from conventional range sensors.
artificial_intelligence	In this paper we compare models and experiments involving Scout II, an untethered four-legged running robot with only one actuator per compliant leg. Scout II achieves dynamically stable running of up to 1.3 m s-1 on flat ground via a bounding gait. Energetics analysis reveals a highly efficient system with a specific resistance of only 1.4. The running controller requires no task-level or body-state feedback, and relies on the passive dynamics of the mechanical system. These results contribute to the increasing evidence that apparently complex dynamically dexterous tasks may be controlled via simple control laws. We discuss general modeling issues for dynamically stable legged robots. Two simulation models are compared with experimental data to test the validity of common simplifying assumptions. The need for including motor saturation and non-rigid torque transmission characteristics in simulation models is demonstrated. Similar issues are likely to be important in other dynamically stable legged robots as well. An extensive suite of experimental results documents the robot's performance and the validity of the proposed models.
artificial_intelligence	Modern ubiquitous services demand the fusion of data from multiple modalities, but so far few approaches have achieved to present such solutions. This challenge becomes even more demanding when the AAL environment comes to serve the requirements of elderly people who need continuous monitoring and care. In light of this, this paper presents a multi -- modal decision making system that consists of mixed knowledge and non-knowledge based subsystems that deliver the appropriate intelligence among three modalities (i.e. ambient, health, fall detection). The main effort has been given on the health status assessment module, where a SVM classifier has been trained using the Physionet's MIT-BIH Arrhythmia database in order to detect abnormal heart beats based on time-domain and statistical features. An initial study on the classification scheme showed satisfactory results for the purposes of a system that is responsible of early screening and dangerous event detection.
artificial_intelligence	We employ recent work on computational noise to obtain near-optimal difference estimates of the derivative of a noisy function. Our analysis relies on a stochastic model of the noise without assuming a specific form of distribution. We use this model to derive theoretical bounds for the errors in the difference estimates and obtain an easily computable difference parameter that is provably near-optimal. Numerical results closely resemble the theory and show that we obtain accurate derivative estimates even when the noisy function is deterministic.
artificial_intelligence	A new training algorithm for hierarchical hybrid fuzzy-neural networks (HHFNN) based on Gaussian membership function is proposed in this paper. This new algorithm adjusts the widths of Gaussian membership functions of the IF parts of fuzzy rules in the lower fuzzy sub-systems, and updates the weights and bias terms of the upper neural network by gradient-descent method. Two advantages of the proposed algorithm are shown in this paper: firstly, its parameters are usually fewer, compared with the existing training algorithm for HHFNN and standard BP algorithm; secondly, it outperforms the latter two algorithms in accuracy according to simulation results.
artificial_intelligence	The demand for automatically annotating and retrieving medical images is growing faster than ever. In this paper, we present a novel medical image retrieval method for a special medical image retrieval problem where the images in the retrieval database can be annotated into one of the pre-defined labels. Even more, a user may query the database with an image that is close to but not exactly what he/she expects. The retrieval consists of the deducible retrieval and the traditional retrieval. The deducible retrieval is a special semantic retrieval and is to retrieve the label that a user expects while the traditional retrieval is to retrieve the images in the database which belong to this label and are most similar to the query image in appearance. The deducible retrieval is achieved using SEMI-supervised Semantic Error-Correcting output Codes (SEMI-SECC). The active learning method is also exploited to further reduce the number of the required ground truthed training images. Relevance feedbacks (RFs) are used in both retrieval steps: in the deducible retrieval, RF acts as a short-term memory feedback and helps identify the label that a user expects; in the traditional retrieval, RF acts as a long-term memory feedback and helps ground truth the unlabelled training images in the database. The experimental results on IMAGECLEF 2005 [] annotation data set clearly show the strength and the promise of the presented methods.
artificial_intelligence	This paper studies the delay-interval-dependent stability of the equilibrium point of a general class of recurrent neural networks with time-varying delays that may exclude zero. By constructing the appropriate Lyapunov-Krasovskii functional, two sufficient conditions ensuring the global asymptotic stability of the equilibrium point of such networks with interval-time-varying delays are established. The present results, together with two numerical examples, show that the equilibrium points of the considered networks may be globally asymptotically stable in some delay interval(s) even though the equilibrium points of the corresponding delay-free recurrent neural networks are not globally asymptotically stable.
artificial_intelligence	Self-organizing maps capable of processing graph structured information are a relatively new concept. This paper describes a novel concept on the processing of graph structured information using the self-organizing map framework which allows the processing of much more general types of graphs, e.g. cyclic graphs, directed graphs. Previous approaches to this problem were limited to the processing of bounded graphs, their computational complexity can grow rapidly with the level of connectivity of the graphs concerned, and are restricted to the processing of positional graphs. The novel concept proposed in this paper, namely, by using the clusters formed in the state space of the self-organizing map to represent the ''strengths'' of the activation of the neighboring vertices, rather than as in previous approaches, using the state space of the surrounding vertices to represent such ''strengths'' of activations. Such an approach resulted in reduced computational demand, and in allowing the processing of non-positional graphs.
artificial_intelligence	In this article, an adaptive neural network algorithm is developed for control issue of rigid-link electrically-driven (RLED) robot systems. First, an virtual control algorithm is designed to deal with the mechanical dynamics. Next, an actual neural network controller is used to handle the uncertainty in the mechanical and electrical dynamics. The stability is guaranteed by using a rigid stability proof. Finally, a simulation is given to show the effectiveness of the proposed algorithm.
artificial_intelligence	Based on Lyapunov-Krasovskii functional or Lyapunov-Razumikhin functional method and invariant set principle, we presented a new method to estimate the domain of attraction for general recurrent neural networks with time-varying delays. Convex optimization method is proposed to enlarge and estimate the domain of attraction. Local exponential stability conditions are derived, which can be expressed as linear matrix inequalities (LMIs) in terms of all the varying parameters and hence can be easily checked in both analysis and design.
artificial_intelligence	In this paper, a novel scheme is proposed which monitors and discriminates the different operating conditions (normal, magnetizing inrush current, over excitation of the core, internal faults and CT saturation due to external faults) of power transformers. The Particle Swarm Optimization (PSO) technique is used to train the multi-layered feed forward neural networks to discriminate the different operating conditions. The proposed scheme has been realized through two different Artificial Neural Network (ANN) architectures. One is used as an internal fault detector and the other one detects and discriminates the other operating conditions like normal, inrush, over excitation, and CT saturation due to external faults. These two ANNs were trained using Back Propagation Neural Network Algorithm (BPN) and PSO technique and the simulated results are compared. Simulations are performed for the practical power transformer ratings obtained from Tamilnadu Electricity Board (TNEB), India. Comparing the simulated results of the above two cases, training the neural network by PSO technique gives more accurate (in terms of sum square error) and also faster (in terms of number of iterations and simulation time) results than BPN. The PSO trained ANN-based differential protection scheme provides faster, accurate, more secured and dependable results for power transformers.
artificial_intelligence	We propose a novel method for the autonomous determination of endmembers that employs recent results from the theory of lattice based auto-associative memories. In contrast to several other existing methods, the endmembers determined by the proposed method are physically linked to the data set spectra. Numerical examples are provided to illustrate lattice theoretical concepts and a hyperspectral image subcube, from the Cuprite site in Nevada, is used to find all endmember candidates in a single pass.
artificial_intelligence	This paper applies the minimum gradient method (MGM) to denoise signals in engineering problems. The MGM is a novel technique based on the complexity control, which defines the learning as a bi-objective problem in such a way to find the best trade-off between the empirical risk and the machine complexity. A neural network trained with this method can be used to pre-process data aiming at increasing the signal-to-noise ratio (SNR). After training, the neural network behaves as an adaptive filter which minimizes the cross-validation error. By applying the general singular value decomposition (GSVD), we show the relation between the proposed approach and the Wiener filter. Some results are presented, including a toy example and two complex engineering problems, which prove the effectiveness of the proposed approach.
artificial_intelligence	Facial sketch synthesis (FSS) is crucial in sketch-based face recognition. This paper proposes an automatic FSS algorithm with local strategy based on embedded hidden Markov model (E-HMM) and selective ensemble (SE). By using E-HMM to model the nonlinear relationship between a photo-sketch patch pair, a series of pseudo-sketch patches, generated based on several learned models for a given photo patch, are integrated with SE strategy to synthesize a finer face pseudo-sketch patch. Finally, the intact pseudo-sketch can be generated by combining all synthesized patches. Experimental results illustrate that the proposed FSS algorithm works well.
artificial_intelligence	Neural networks are widely used in many applications including astronomical physics, image processing, recognition, robotics and automated target tracking, etc. Their ability to approximate arbitrary functions is the main reason for this popularity. The main result of this paper is a constructive proof of a formula for the upper bound of the approximation error by feedforward neural networks with one hidden layer of sigmoidal units and a linear output. The result can also be used to estimate complexity of the maximum error network. An example to demonstrate the theoretical result is given.
artificial_intelligence	Blind source extraction (BSE) is a special class of blind source separation (BSS) method. Due to its low computation load and fast processing speed, BSE has become one of the promising methods in signal processing and analysis. This paper addresses BSE problem when a desired source signal has temporal structures. Based on the generalized autocorrelations of the desired signal and the non-Gaussianity of its innovations, we develop an objective function. Maximizing this objective function, we present a BSE algorithm and further give its stability analysis in this paper. Simulations on image data and electrocardiogram (ECG) data indicate its better performance and the better property of tolerance to the estimation error of the time delay.
artificial_intelligence	The paper presents the study concerning the application of a single-class support vector machine (SVM) and directed transfer function method for the localization of the region of the brain containing the epileptic focus on the basis of EEG registration. The results of the performed numerical experiments for the localization of the seizure focus in the brain will be demonstrated on the examples of EEG for few patients.
artificial_intelligence	At large scales the brain is a complex network of anatomical structures, or neural populations, with the small-world characteristics of dense local clustering between neighboring populations and a short path length between any two populations. There have been extensive studies of the structural and dynamical advantages of small-world networks over the last decade, providing a number of possible reasons for the evolution of this small-world architecture. However, there has been little work on the stability of small-world brain networks. A network model, which is a variant of the Watts-Strogatz model, is used to generate random-rewire networks (RRNs). Depending on their parameters these networks can be regular, small-world, or random. Stability of small-world networks is investigated with a physiologically based model of brain electrical activity and compared with the results for regular, random, and experimentally determined cortical networks. The stability of RRNs is independent of network size; and small-world brain networks are less likely to be stable than random networks with the same number of populations and average number of connections. For this network model our results suggest that if stability is the dominant constraint on network structure then brain networks are more likely to have random rather than small-world connectivity. Thus a specific type of network architecture may be required for brain networks to be small-world and maintain marginal stability.
artificial_intelligence	In this paper, global exponential stability in Lagrange sense is further studied for various continuous-time delayed recurrent neural network with two different types of activation functions. Based on the parameters of the systems, detailed estimation of global exponential attractive sets and positive invariant sets are presented without any hypothesis on the existence. It is also verified that outside the global exponential attracting set; i.e., within the global attraction domain, there is no equilibrium state, periodic state, almost periodic state, and chaos attractor of the neural network. These theoretical analysis narrows the search field of optimization computation, associative memories, chaos control and synchronization and provide convenience for applications.
artificial_intelligence	The minimum average correlation energy (MACE) filter is a well known correlation filter for object recognition. Recently, a nonlinear extension to the MACE filter using the correntropy function in feature space has been introduced. Correntropy is a positive definite function that generalizes the concept of correlation by utilizing higher order moment information of signal structure. Since the MACE is a spatial matched filter for an image class, the correntropy MACE (CMACE) can potentially improve its performance. Both the MACE and CMACE are basically memory-based algorithms and due to the high dimensionality of the image data, the computational cost of the CMACE filter is one of the critical issues in practical applications. We propose to use a dimensionality reduction method based on random projections (RP), which has emerged as a powerful method for dimensionality reduction in machine learning. We apply the CMACE filter with random projection (CMACE-RP) to face recognition and show that it indeed outperforms the traditional linear MACE in both generalization and rejection abilities with small degradation in performance, but great savings in storage and computational complexity.
bioinformatics	Finding effective solutions for cold-starting Context-Aware Recommender Systems (CARSs) is important because usually low quality recommendations are produced for users, items or contextual situations that are new to the system. In this paper, we tackle this problem with a switching hybrid solution that exploits a custom selection of two CARS algorithms, each one suited for a particular cold-start situation, and switches between these algorithms depending on the detected recommendation situation (new user, new item or new context). We evaluate the proposed algorithms in an off-line experiment by using various contextually-tagged rating datasets. We illustrate some significant performance differences between the considered algorithms and show that they can be effectively combined into the proposed switching hybrid to cope with different types of cold-start problems.
bioinformatics	The new user experience is one of the important problems in recommender systems. Past work on recommending for new users has focused on the process of gathering information from the user. Our work focuses on how different algorithms behave for new users. We describe a methodology that we use to compare representatives of three common families of algorithms along eleven different metrics. We find that for the first few ratings a baseline algorithm performs better than three common collaborative filtering algorithms. Once we have a few ratings, we find that Funk's SVD algorithm has the best overall performance. We also find that ItemItem, a very commonly deployed algorithm, performs very poorly for new users. Our results can inform the design of interfaces and algorithms for new users.
bioinformatics	Wireless sensor networks present a growing interest in health care applications since they can replace wired devices for detecting signals of physiological origin and continuously monitoring health parameters, offering a reliable and inexpensive solution. In this paper a low cost open architecture wearable sensor network for health care applications is presented. Through this study, an experimental wireless sensor network (WSN) architecture has been built from scratch in order to investigate and present the development procedure and the corresponding capabilities and limitations of such a system. Moreover, technological aspects regarding implementation are also presented.
bioinformatics	In this paper we tackle the problem of recommendation in the scenarios with binary relevance data, when only a few (k) items are recommended to individual users. Past work on Collaborative Filtering (CF) has either not addressed the ranking problem for binary relevance datasets, or not specifically focused on improving top-k recommendations. To solve the problem we propose a new CF approach, Collaborative Less-is-More Filtering (CLiMF). In CLiMF the model parameters are learned by directly maximizing the Mean Reciprocal Rank (MRR), which is a well-known information retrieval metric for measuring the performance of top-k recommendations. We achieve linear computational complexity by introducing a lower bound of the smoothed reciprocal rank metric. Experiments on two social network datasets demonstrate the effectiveness and the scalability of CLiMF, and show that CLiMF significantly outperforms a naive baseline and two state-of-the-art CF methods.
bioinformatics	Measuring the error in rating prediction has been by far the dominant evaluation methodology in the Recommender Systems literature. Yet there seems to be a general consensus that this criterion alone is far from being enough to assess the practical effectiveness of a recommender system. Information Retrieval metrics have started to be used to evaluate item selection and ranking rather than rating prediction, but considerable divergence remains in the adoption of such metrics by different authors. On the other hand, recommendation utility includes other key dimensions and concerns beyond accuracy, such as novelty and diversity, user engagement, and business performance. While the need for further extension, formalization, clarification and standardization of evaluation methodologies is recognized in the community, this need is still unmet for a large extent. The RUE 2012 workshop sought to identify and better understand the current gaps in recommender system evaluation methodologies, help lay directions for progress in addressing them, and contribute to the consolidation and convergence of experimental methods and practice.
bioinformatics	Food recommenders have been touted as a useful tool to help people achieve a healthy diet. Here we incorporate nutrition into the recommender problem by examining the feasibility of algorithmically creating daily meal plans for a sample of user profiles (n=100), combined with a diverse set of food preference data (n=64) collected in a natural setting. Our analyses demonstrate it is possible to recommend plans for a large percentage of users which meet the guidelines set out by international health agencies
bioinformatics	Classical recommender systems provide users with a list of recommendations where each recommendation consists of a single item, e.g., a book or DVD. However, several applications can benefit from a system capable of recommending packages of items, in the form of sets. Sample applications include travel planning with a limited budget (price or time) and twitter users wanting to select worthwhile tweeters to follow given that they can deal with only a bounded number of tweets. In these contexts, there is a need for a system that can recommend top-k packages for the user to choose from. Motivated by these applications, we consider composite recommendations, where each recommendation comprises a set of items. Each item has both a value (rating) and a cost associated with it, and the user specifies a maximum total cost (budget) for any recommended set of items. Our composite recommender system has access to one or more component recommender system, focusing on different domains, as well as to information sources which can provide the cost associated with each item. Because the problem of generating the top recommendation (package) is NP-complete, we devise several approximation algorithms for generating top-k packages as recommendations. We analyze their efficiency as well as approximation quality. Finally, using two real and two synthetic data sets, we subject our algorithms to thorough experimentation and empirical analysis. Our findings attest to the efficiency and quality of our approximation algorithms for top-k packages compared to exact algorithms.
bioinformatics	The advantage of Factorization Machines over other factorization models is their ability to easily integrate and efficiently exploit auxiliary information to improve Collaborative Filtering. Until now, this auxiliary information has been drawn from external knowledge sources beyond the user-item matrix. In this paper, we demonstrate that Factorization Machines can exploit additional representations of information inherent in the user-item matrix to improve recommendation performance. We refer to our approach as 'Free Lunch' enhancement since it leverages clusters that are based on information that is present in the user-item matrix, but not otherwise directly exploited during matrix factorization. Borrowing clustering concepts from codebook sharing, our approach can also make use of 'Free Lunch' information inherent in a user-item matrix from a auxiliary domain that is different from the target domain of the recommender. Our approach improves performance both in the joint case, in which the auxiliary and target domains share users, and in the disjoint case, in which they do not. Although 'Free Lunch' enhancement does not apply equally well to any given domain or domain combination, our overall conclusion is that Factorization Machines present an opportunity to exploit information that is ubiquitously present, but commonly under-appreciated by Collaborative Filtering algorithms.
bioinformatics	Group recommender systems (RS) are used to support groups in making common decisions when considering a set of alternatives. Current approaches generate group recommendations based on the users' individual preferences models. We believe that members of a group can reach an agreement more effectively by exchanging proposals suggested by a conventional RS. We propose to use a critiquing RS that has been shown to be effective in single-user recommendation. In the group recommendation context, critiquing allows each user to get new recommendations similar to the proposals made by the other group members and to communicate the rationale behind their own counterproposals. We describe a mobile application implementing the proposed approach and its evaluation in a live user experiment.
bioinformatics	Personalized location recommendation is a special topic of recommendation. It is related to human mobile behavior in the real world regarding various contexts including spatial, temporal, social, and content. The development of this topic is subject to the availability of human mobile data. The recent rapid growth of location-based social networks has alleviated such limitation, which promotes the development of various location recommendation techniques. This tutorial offers an overview, in a data mining perspective, of personalized location recommendation on location-based social networks. It introduces basic concepts, summarizes unique LBSN characteristics and research opportunities, elaborates associated challenges, reviews state-of-the-art algorithms with illustrative examples and real-world LBSN datasets, and discusses effective evaluation methods.
bioinformatics	Pairwise algorithms are popular for learning recommender systems from implicit feedback. For each user, or more generally context, they try to discriminate between a small set of selected items and the large set of remaining (irrelevant) items. Learning is typically based on stochastic gradient descent (SGD) with uniformly drawn pairs. In this work, we show that convergence of such SGD learning algorithms slows down considerably if the item popularity has a tailed distribution. We propose a non-uniform item sampler to overcome this problem. The proposed sampler is context-dependent and oversamples informative pairs to speed up convergence. An efficient implementation with constant amortized runtime costs is developed. Furthermore, it is shown how the proposed learning algorithm can be applied to a large class of recommender models. The properties of the new learning algorithm are studied empirically on two real-world recommender system problems. The experiments indicate that the proposed adaptive sampler improves the state-of-the art learning algorithm largely in convergence without negative effects on prediction quality or iteration runtime.
bioinformatics	While various technical approaches for constructing Ambient Assisted Living (AAL) applications/services have been proposed, it has become apparent that interoperability of AAL systems is the key challenge that has to be tackled for exploiting AAL technologies in their full potential. Equally important, there is a lack of tools that can support AAL system/application developers to implement systemic and affordable solutions. The objective of this work is to illustrate the design and development of a basic AAL application devoted to physical activity monitoring by exploiting the universAAL open platform and tools. Our main goal is to illustrate the procedure that developers have to follow for such a development as well as the benefits offered from the adoption of universAAL in terms of extensibility and interoperability.
bioinformatics	Predicting the arrival and residence time of individuals at their relevant places enables a plethora of novel applications. In this work we first analyze the theoretical predictability of arrival and residence times and then evaluate the performance of eight different residence time predictors. We show that these predictors tend to underestimate the time a user will spend at her relevant places.
bioinformatics	In many experimental domains, especially e-Science, workflow management systems are gaining increasing attention to design and execute in-silico experiments involving data analysis tools. As a by-product, a repository of workflows is generated, that formally describes experimental protocols and the way different tools are combined inside experiments. In this paper we describe the use of the SUBDUE graph clustering algorithm to discover sub-workflows from a repository. Since sub-workflows represent significant usage patterns of tools, the discovered knowledge can be exploited by scientists to learn by-example about design practices, or to retrieve and reuse workflows. Such a knowledge, ultimately, leverages the potential of scientific workflow repositories to become a knowledge-asset. A set of experiments is conducted on the my Experiment repository to assess the effectiveness of the approach.
bioinformatics	To cluster a data stream is a more challenging task than its regular batch version, having stricter performance constraints. In this paper an approach to this problem is presented, based on WiSARD, a memory-based artificial neural network (ANN) model. This model functioning was reviewed and improved, in order to adapt it to this task. The experimental results obtained support the use of this system for the analysis of data streams in an informative way.
bioinformatics	In this work we propose a novel algorithm for human silhouette segmentation, which combines characteristics from a number of well established and state of the art algorithms, such as the Gaussian mixture models, the Self Organizing Maps and the Illumination Sensitive method. The proposed algorithm is evaluated against user-defined ground truth segmentation for two different types of indoor video sequences, one of which was obtained by a hemispheric camera. The behavior of the algorithm with respect to its controlling parameters is investigated and its computational burden is studied.
bioinformatics	Inter-Component Communication (ICC) enables useful interactions between mobile apps. However, misuse of ICC exposes users to serious threats such as intent hijacking/spoofing and app collusions, allowing malicious apps to access privileged user data via another app. Unfortunately, existing ICC analyses are largely incompetent in both accuracy and scale. This poster points out the need and technical challenges of prioritized analysis of inter-app ICC risks. In this poster, we propose MR-Droid, a MapReduce-based computing framework for accurate and scalable inter-app ICC analysis in Android. MR-Droid extracts data-flow features between multiple communicating apps and the target apps to build a large-scale ICC graph. Our approach is to leverage the ICC graph to provide contexts for inter-app communications to produce precise alerts and prioritize risk assessments. This process requires large app-pair data, which is enabled by our MapReduce-based program analysis. Our initial extensive experiments on 11,996 apps from 24 app categories (13 million pairs) demonstrate the scalability of our approach.
bioinformatics	This article presents the appointment terminal, a new component of the KopAL orientation system for elderly and dementia patients. The appointment terminal is an intuitive to use screen that enables elderly to overview and arrange their appointments. A brief overview of the current appointment architecture of KopAL is given. The focused user group has special characteristics, like low technological competence and physical handicaps that results into special demands on the terminal design. As a result the terminal is based on touch-screens as presentation and interaction device, which are extended by RFID capabilities for authentication purposes. The evaluation of the terminal with heterogeneous users resulted in several ideas for future tasks.
bioinformatics	Sentiment analysis is the process of identifying the polarity of sentiments held in opinions found in pieces of text and classifying them as positive, negative or neutral. In this paper, we propose the implementation of a sentiment analysis tool that is conducted over text found in Arabic new media including web forums, comments on newspaper articles and other websites with evaluative content. The expected input of the tool, which is informal Colloquial Arabic, is characterized to be of highly non-structured nature and subject to trends used to express sentiments. Our solution is a novel technique that merges the area of human computation with the task of natural language processing.
bioinformatics	Over the recent years, there has been a growing interest in developing new research evaluation methods that could go beyond the traditional citation-based metrics. This interest is motivated on one side by the wider availability or even emergence of new information evidencing research performance, such as article downloads, views and Twitter mentions, and on the other side by the continued frustrations and problems surrounding the application of purely citation-based metrics to evaluate research performance in practice. Semantometrics are a new class of research evaluation metrics which build on the premise that full-text is needed to assess the value of a publication. This paper reports on the analysis carried out with the aim to investigate the properties of the semantometric contribution measure [Knoth, 2014], which uses semantic similarity of publications to estimate research contribution, and provides a comparative study of the contribution measure with traditional bibliometric measures based on citation counting.
bioinformatics	Advances in both technology and publishing practices continue to increase the quantity of scientific literature that is available electronically. In this paper, we introduce the Information Synthesis process, a new approach that enables scientists to visualize, explore, and resolve contradictory findings that are inevitable when multiple empirical studies explore the same natural phenomena. Central to the Information Synthesis approach is a cyber-infrastructure that provides a scientist with both primary and secondary information from an article and structured information resources. To demonstrate this approach, we have developed the Multi-User, Information Extraction for Information Synthesis (METIS) System. METIS is an interactive system that automates critical tasks within the Information Synthesis process. We provide two case-studies that demonstrate the utility of the Information Synthesis approach.
bioinformatics	Surveys show that around 70% of US Internet users consult the Internet when they require medical information. People seek this information using both traditional search engines and via social media. The information created using the search process offers an unprecedented opportunity for applications to monitor and improve the quality of life of people with a variety of medical conditions. In recent years, research in this area has addressed public-health questions such as the effect of media on development of anorexia, developed tools for measuring influenza rates and assessing drug safety, and examined the effects of health information on individual wellbeing. This tutorial will show how Internet data can facilitate medical research, providing an overview of the state-of-the-art in this area. During the tutorial we will discuss the information which can be gleaned from a variety of Internet data sources, including social media, search engines, and specialized medical websites. We will provide an overview of analysis methods used in recent literature, and show how results can be evaluated using publicly-available health information and online experimentation. Finally, we will discuss ethical and privacy issues and possible technological solutions. This tutorial is intended for researchers of user generated content who are interested in applying their knowledge to improve health and medicine.
bioinformatics	The Web is populated with many Web sites containing unstructured textual information. These Web sites are a source of knowledge for various interests. As semantic annotations are only rarely used on Web sites, an automated harvesting of the knowledge without additional effort is not possible. Thus, elaborated approaches for information extraction are required. In our work we face the challenge of identifying business address data on Web sites since we see the need for this data in various applications. In order to accomplish our aim, we have developed a hybrid approach combining patterns and gazetteers obtained from freely available knowledge resources such as OpenStreetMap. Experimental evaluation on a corpus of heterogeneous Web sites shows a high recall and precision. The approach can be adapted for identification of addresses considering the different formats in various countries.
bioinformatics	In this paper, we outline a specific application domain for serious games, i.e. the social intervention for the promotion of safe and healthy behavior in the nightlife. The potential of serious games in this domain are synthesized and a set of design guidelines, derived from interviewing stakeholders and operators, is provided.
bioinformatics	It is often desirable to identify the concepts that are present in a corpus. A popular way to deal with this objective is to discover clusters of words or topics, for which many algorithms exist in the literature. Yet most of these methods lack the interpretability that would enable interaction with a user not familiar with their inner workings. The paper proposes a graph-based topic extraction algorithm, which can also be viewed as a soft-clustering of words present in a given corpus. Each topic, in the form of a set of words, represents an underlying concept in the corpus. The method allows easy interpretation of the clustering process, and hence enables the scope of user involvement at various steps. For a quantitative evaluation of the topics extracted, we use them as features to get a compact representation of documents for classification tasks. We compare the classification accuracy achieved by a reduced feature set obtained with our method versus other topic extraction techniques, namely Latent Dirichlet Allocation and Non-negative Matrix Factorization. While the results from all the three algorithms are comparable, the speed and easy interpretability of our algorithm makes it more appropriate to be used interactively by lay users.
bioinformatics	In this paper, we describe Berkeley Prosopography Services (BPS), a new set of tools for prosopography - the identification of individuals and study of their interactions - in support of humanities research. The BPS tools include 1) functionality to import TEI documents and convert to our data model, 2) a disambiguation engine to associate names to persons based upon configurable heuristic rules, 3) an assertion model that supports flexible researcher curation and tracks provenance, 4) social network analysis and 5) graph visualization tools to analyze and understand social relations, and 6) a workspace model supporting exploratory research and collaboration. We contrast the BPS model that uses configurable heuristic rules to other approaches for automated text analysis, and explain how our model facilitates interpretation by humanist researchers. We describe the significance of our curation model that improves upon traditional curation and annotation as a fact-based model by adding a more flexible model in which researchers assert conclusions or possibilities, allowing them to override automated inference, to explore ideas in what-if scenarios, and to formally publish and subscribe-to asserted annotations among colleagues, and/or with students. We detail the architecture and our implementation of the tools as a set of reusable web services and web application UI. We present an initial evaluation of researchers' experience using the tools to study corpora of cuneiform tablets, and describe plans to expand the application of the tools to a broader range of corpora.
bioinformatics	Linked data technologies make it possible to publish and link structured data on the Web. Although RDF is not about text, many RDF data providers publish their data in their own language. Cross-lingual interlinking aims at discovering links between identical resources across knowledge bases in different languages. In this paper, we present a method for interlinking RDF resources described in English and Chinese using the BabelNet multilingual lexicon. Resources are represented as vectors of identifiers and then similarity between these resources is computed. The method achieves an F-measure of 88%. The results are also compared to a translation-based method.
bioinformatics	Design of the Watch Dog health care application is presented in this paper. Solution is based on the client running on the smartphone and the server side running in the cloud. Sensors embedded in the smartphone are used for the measurement of the different information about the monitored person such as position, temperature, breath frequency etc. Basic algorithms evaluating current person's health status run on the smartphone. Measured data are sent to the second part of the application running in the cloud for deeper analysis. Suitability of the cloud solution for this application is discussed in this paper.
bioinformatics	This paper compares three techniques for Arabic text classification; these techniques are Support Vector Machine (SVM) with Sequential Minimal Optimization (SMO), Na�ve Bayesian (NB), and J48. The main objective of this paper is to measure the accuracy for each classifier and to determine which classifier is more accurate for Arabic text classification based on stop words elimination. The accuracy for classifier is measured by Percentage split method (holdout), and K-fold cross validation methods,. The results show that the SMO classifier achieves the highest accuracy and the lowest error rate, and shows that the time needed to build the SMO model is the smallest time.
bioinformatics	In this paper we describe an algorithm for heart sound classification (classes Normal, Murmur and Extrasystole) based on the discretization of sound signals using the SAX (Symbolic Aggregate Approximation) representation. The general strategy is to automatically discover relevant top frequent motifs and relate them with the occurrence of systolic (S1) and diastolic (S2) sounds in the audio signals. The algorithm was tuned using motifs generated from a collection of audio signals obtained from a clinical trial in a hospital. Validation was performed on a separate set of unlabeled audio signals. Results indicate ability to improve the precision of the classification of the classes Normal and Murmur.
bioinformatics	Emerging non-volatile memory (NVRAM) technologies, like phase change memory, envision persistent memory architectures. In case of power failure, operations on persistent memory should be in transactional semantics by adopting techniques such as WAL. To ensure consistency and atomicity, persist barriers are widely adopted, to prevent persistent memory controller from scheduling writes and exploiting bank-level parallelism of NVRAM devices. Besides, unified retention time for persistent writes, i.e., log and data writes, further reduces the performance of persistent memory system, while retention time for log writes does not need to be so long due to periodic truncation. In this paper, we study how NVRAM write latency affects the system throughput and propose DP2, which consists of two main techniques: differential persistency and dual persistency. Differential persistency distinguishes log writes from data writes, and enhances the NVRAM memory controller to schedule log writes across persist barriers to fully utilize bank level parallelism. Dual persistency relaxes the retention time of log writes to reduce write latency and the iterations per write, which in turn enhances lifetime of NVRAM devices. Evaluation results show that our proposed techniques improve system throughput up by 43% on average and extend lifetime up by 47%, with 104-s retention time for log writes.
bioinformatics	Reducing infant mortality is one of the primary Millennium Development Goals 2015. A lot of effort has been made to reduce infant mortality but it remains high in most of the developing countries and the underdeveloped world. Perinatal Mortality is a cause of great emotional pain and social unrest. The main cause of pregnancy failure in the developed world is obesity but in the under-developed world the main cause remains malnutrition. However, their are a mix of factors that affect pregnancy failure in the developing countries. Pakistan has a very high infant mortality rate which stands at 78 deaths per 1000 births. The reasons for this are many including lack of proper healthcare. This is because of a severe shortage of healthcare professionals and specialists in Pakistan. The gap in healthcare may be overcome by leveraging IT to provide automated healthcare. In this paper, we show how machine learning may be used to predict perinatal failure. We examine the relationship between pre-pregnancy weight, weight gain during pregnancy and the body mass index (BMI) to investigate how they relate to foetal failure. We employ the K Nearest Neighbor (K-NN) technique to automatically differentiate between successful and failed pregnancies. Our method is able to predict the the outcome of a pregnancy with about 95% accuracy.
bioinformatics	Wheat, corn, and rice provide 60 percent of the world's food intake every day, and just 15 plant species make up 90 percent of the world's food intake. As such there is tremendous agricultural and scientific interest to sequence and study plant genomes, especially to develop a reference sequence to direct plant breeding or to identify functional elements. DNA sequencing technologies can now generate sequence data for large genomes at low cost, however, it remains a substantial computational challenge to assemble the short sequencing reads into their complete genome sequences. Even one of the simpler ancestral species of wheat, Aegilops tauschii, has a genome size of 4.36 gigabasepairs (Gbp), nearly fifty percent larger than the human genome. Assembling a genome this size requires computational resources, especially RAM to store the large assembly graph, out of reach for most institutions. In this paper, we describe a collaborative effort between Cold Spring Harbor Laboratory and the Pittsburgh Supercomputing Center to assemble large, complex cereal genomes starting with Ae. tauschii, using the XSEDE shared memory supercomputer Blacklight. We expect these experiences using Blacklight to provide a case study and computational protocol for other genomics communities to leverage this or similar resources for assembly of other significant genomes of interest.
bioinformatics	Two monitors were implemented to collect information about the behavior of the online system developed and run at Stanford. The response of this online system was slow and main memory was a critical resource. The goal was to extract desired information by a method that requires only a negligible amount of monitored system resources. Results presented in this paper indicate that this effort has been successful. A software monitor that requires less than 700 bytes of main memory collects statistics about utilization of special online system resources and about the scheduler mechanism, detects system deadlocks, and measures online executive overhead. This software monitor helped to discover various facts about the system behavior; however, to understand the reasons behind certain situations, it was necessary to learn more about properties of individual terminal tasks. Since a software monitor would cause an intolerable system degradation and hardware monitoring is not directly applicable for such measurements, a hardware/software monitor interface was implemented which enables recording of software events by a hardware monitor. The monitoring artifact is thus kept close to zero. This technique has been applied to measure time a task spends in various states and it has many other uses.
bioinformatics	RNA aptamers are small oligonucleotide molecules whose composition and resulting folded structure enable them to bind with high affinity and high selectivity to target ligands and therefore hold great promise as potential therapeutic drugs. Functional aptamers are selected from a large, randomized initial library in a process known as SELEX (systematic evolution of ligands by exponential enrichment). This is an iterative process involving numerous rounds of binding, elution, and amplification against a specific target substrate. During each iteration -- or round of selection -- we enrich for the species with the highest binding affinity to the target. After multiple rounds, we ideally have an enriched aptamer library suitable for subsequent investigation. Modern techniques employ massively parallel sequencing, enabling the generation of large libraries (~106 sequences) in a matter of hours for each round of selection. As RNA is single-stranded, covariance models (CMs) are ideal for representing motifs in their secondary structures, allowing us to discover patterns within functional aptamer populations following each round. CMs have been implemented in Infernal, a program that infers RNA alignments based on RNA sequence and structure. Calibrating a single CM in Infernal can take several hours and is a significant performance bottleneck for our work. However, as each CM calculation is itself independently determined and requires defined processing and memory resources, their computation in parallel offers a potential solution to this problem. In this paper, we describe using the Open Science Grid (OSG) to facilitate the identification of aptamer motifs by running CM calibrations and refinements in parallel across up to ten OSG clients. We use the Simple API for Grid Applications (SAGA) to interface with OSG and manage job submissions and file transfers. When run in parallel, our results show a significant speed up, constrained by typical latencies and QoS associated with nominal OSG usage. Our work demonstrates the ability of SAGA and the OSG to assist in parallelizing solutions to complex sequencing-based biomedical challenges.
bioinformatics	The development of genomic resources of non-model organisms is now becoming commonplace as the cost of sequencing continues to decrease. The Genome Informatics Facility in collaboration with the Southwest Fisheries Science Center (SWFSC), NOAA is creating these resources for sustainable aquaculture in Seriola lalandi. Gene prediction and annotation are common steps in the pipeline to generate genomic resources, which are computationally intense and time consuming. In our steps to create genomic resources for Seriola lalandi, we found BLAST to be one of our most rate limiting steps. Therefore, we took advantage of our XSEDE Extended Collaborative Support Services (ECSS) to reduce the amount of time required to process our transcriptome data by 300 percent. In this paper, we describe an optimized method for the BLAST tool on the Stampede cluster, which works with any existing datasets or database, without any modification. At modest core counts, our results are similar to the MPI-enabled BLAST algorithm (mpiBLAST), but also allow the much needed and improved flexibility of output formats that the latest versions of BLAST provide. Reducing this time-consuming bottleneck in BLAST will be broadly applicable to the annotation of large sequencing datasets for any organism.
bioinformatics	DNA Subway bundles research-grade bioinformatics tools, high-performance computing, and databases into easy-to-use workflows. Students have been "riding" different lines since 2010, to predict and annotate genes in up to 150kb of raw DNA sequence (Red Line), identify homologs in sequenced genomes (Yellow Line), identify species using DNA barcodes and construct phylogenetic trees (Blue Line), and examine RNA sequence (RNA-Seq) datasets for transcript abundance and differential expression (Green Line). With support for plant and animal genomes, DNA Subway engages students in their own learning, bringing to life key concepts in molecular biology, genetics, and evolution. Integrated DNA barcoding and RNA extraction wet-lab experiments support a variety of inquiry-based projects using student-generated data. Products of student research can be exported, published, and used in follow-up experiments. To date, DNA Subway has over 8,000 registered users who have produced 51,000 projects. Based on the popular Tuxedo Protocol, the Green Line was introduced in January 2014 as an easy-to-use workflow to analyze RNA-Seq datasets. The workflow uses iPlant's APIs (http://agaveapi.co/) to access high-performance compute resources of NSF's Extreme Scientific and Engineering Discovery Environment (XSEDE), providing the first easy "on ramp" to biological supercomputing.
bioinformatics	This work explores the relationship between a robot's embodiment and people's ability to mimic its behavior. It presents a study in which participants were asked to mimic a 3D mixed-embodied robotic head and a 2D version of the same character. Quantitative and qualitative analysis were performed from questionnaires. Quantitative results show no significant influence of the character's embodiment on the self-assessed ability to mimic it, while qualitative ones indicate a preference for mimicking the robotic head.
bioinformatics	Social robots are being used to create better educational scenarios, boosting children's motivation and engagement. The focus of the research is to explore new ways to support children in acquisition of their handwriting skills with the help of a social robot. With this perspective, three studies are discussed to investigate aspects related to the learning modes of child-robot interaction, children's impression of a social robot and classification of children's common handwriting difficulties.
bioinformatics	A demand response method is expected to reduce carbon dioxide emissions and make stable power supply. A demand control depends on electric power suppliers' intention in many cases and therefore users are forced to be inconvenient in their way of life. We now need to develop an innovative method which encourages users to participate in a demand response and electric power saving action without their patience. Usually, the electric power saving behaviors are motivated by pecuniary incentives or environmental concern such as carbon dioxide emissions reduction and power failure probability. On such background, this paper evaluates the effectiveness of the attempt to encourage people's engagement in electric power saving action with the method of presenting information such as numeral, visualized, and anthropomorphized information regarding pecuniary incentives and environmental concern. The result shows that the method of presenting anthropomorphized information strongly induced users' electric power saving behaviors than other methods.
bioinformatics	Reinforcement learning is one of research fields in artificial intelligence. The learning method usually assumes a discrete state in computer simulations. However, we must treat a continuous value in a realistic situation. In this paper, we investigate various techniques of the tile cording scheme which is a representative technique to handle continuous states. We check the performance of single tiling, multiple tiling, time-shift method and the proposed method in the issue of space search.
bioinformatics	The purpose of our study is to investigate human teaching behavior and robot learning behavior when a human teaches a robot. Agents for learning support need to build a pedagogical relationship, in which a teacher agent and a student agent change their behaviors as they recognize the other's characteristic behaviors. In order to investigate how a robot that behaves as a student should respond to humans' teaching behaviors in a pedagogical relationship between human and robot, we conducted a case study using a game played on a tablet with a robot. In the case study, we analyzed how humans changed their teaching behaviors when the humanoid robot failed to understand what they taught. From the results of this case study, we observed that some subjects carefully taught the robot in each trial in order to allow the robot to understand the subjects. Moreover, we also observed that subjects' teaching behavior changed when the subject received feedback from the robot about the teaching.
bioinformatics	In order to make people truly benefit from data sharing, we need technical solutions to assuring the trustworthiness of data received from parties one may not have encountered in the past. Assured data provenance is an important means for this purpose because it (i) allows data providers to get credited for their contribution or sharing of data, (ii) is able to hold the data providers accountable for the data they contributed, and (iii) enables the data providers to supply high-quality data in a self-healing fashion. While the above (i) and (ii) have been investigated to some extent, the above (iii) is a new perspective that, to our knowledge, has not been investigated in the literature. In this paper, we introduce a novel cryptographic technique that can simultaneously offer these properties. Our technique is called editable signatures, which allow a user, Bob, to edit (e.g., replace, modify, and insert) some portions of the message that is contributed and signed by Alice such that the resulting edited message is jointly signed by Alice and Bob in some fashion. While it is easy to see that the above (i) and (ii) are achieved, the above (iii) is also achieved because Bob may have a better knowledge of the situation that allows him to provide more accurate/trustworthy information than Alice, who may intentionally or unintentionally enter inaccurate or even misleading data into an information network. This is useful because Alice's inaccurate or even misleading information will never be released into an information network if it can be ``cleaned" or "healed" by Bob. Specifically, we propose two novel cryptographic constructions that can be used to realize the above functions in some practical settings.
bioinformatics	With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, Cloud computing is defined and the architecture for creating Clouds with market-oriented resource allocation are provided by leveraging technologies such as Virtual Machines (VMs). Also insights on market-based resource management strategies that encompass both customer-driven service management and computational have been provided. Cloud computing customers generally do not own the physical infrastructure, instead avoiding capital expenditure by renting usage from a third-party provider. They consume resources as a service and pay only for resources that they use. Many cloud-computing offerings employ the utility computing model, which is analogous to how traditional utility services (such as electricity) are consumed, while others bill on a subscription basis. Sharing "perishable and intangible" computing power among multiple tenants can improve utilization rates, as servers are not unnecessarily left idle (which can reduce costs significantly while increasing the speed of application development). A side effect of this approach is that overall computer usage rises dramatically, as customers do not have to engineer for peak load limits. Additionally, "increased high-speed bandwidth" makes it possible to receive the same response times from centralized infrastructure at other sites. The risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, the early thoughts are being revealed on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Furthermore, the difference between High Performance Computing (HPC) workload and Internet-based services workload are also highlighted.
bioinformatics	The rapid technological advances have made the use of domestic assistive environments vital in the daily life of elderly and disabled people. A major challenge in an assistive environment is the effective coordination and scheduling of the interdependent tasks required for the analysis and processing of the collected sensory data, in order to timely response to a life-threatening emergency event. These complex jobs are usually scheduled on distributed heterogeneous resources that may be part of a computational grid or cloud. We investigate by simulation the impact of the processor and network heterogeneity on the timeliness of such critical jobs.
bioinformatics	Developers usually use in-memory data caching system like Hazelcast with the application server to offload the backend database for scaling Internetware. Unfortunately, such caches do not integrate well with the database or the application. Developers need to take a large effort to rewrite the existing data access logic and manually manage the caching data. In this paper, we present EasyCache, a novel data caching approach, which provides transparent cache pre-loading, accessing and consistency maintenance to relieve developers of the burden of cache using and management. First, EasyCache translates each row of data in the existing database table into application cached object to pre-load cache data. Second, EasyCache allows applications to access the data cache using SQL statements and translates them into key/value based cache operations. Finally, EasyCache provides asynchronous/synchronous strategies to persist the cache data changes into the backend database. We design and implement EasyCache as a JDBC driver with Hazelcast as the caching layer. To evaluate our prototype, a detailed set of experiments were performed using the TPC-W benchmark. In the experiments, the only programming effort with EasyCache is point the application to the EasyCache JDBC driver. In contrast, when using Hazelcast as a traditional application-level caching system, we need to modify the TPC-W code over 2000 lines for 15 man days. Our experiments also show that, compared to a system with no cache and with query result cache, using EasyCache leads to up to 692� and 77� performance improvement respectively.
bioinformatics	The shift to more parallel and distributed computer architectures changed how data is managed consequently giving birth to a new generation of software products, namely NoSQL. These products offer a scalable and reliable solution for "Big Data", but none of them solves the problem of analyzing and visualizing multidimensional data. There are solutions for scaling analytic workloads, for creating distributed databases and for indexing multidimensional data, but there is no single solution that points to all three goals together. We propose the D8-tree, a De-normalized Octa-tree index that supports all three goals. It works with both analytical and data-thinning queries on multidimensional data ensuring, at the same time, low latency and a linear scalability. We have implemented a D8-tree prototype, and we compared it with PostGIS on a set of queries required by an in-house HPC application. We found the D8-tree outperforming PostGIS in all tested queries, with a performance gain up to 47 times.
bioinformatics	Autism spectrum disorder (ASD) are characterized by difficulties in sensory integration and a body image which differs from the normal, healthy one. In order to enable healthy people to experience and thus develop a deeper understanding of the different body image that people with developmental disorders possess the author is pursuing research into the simulation of sensory experiences common to ASD. This study focuses on a particular sensory characteristic of ASD where difficulty is experienced in locating the source of sound in an environment and the development of a device for simulating the sense of hearing experienced in such a disorder. Workshops for children were carried out using the developed device, with interviews indicating that the majority of participants experienced feelings of ambiguity in relation to their own senses. Such a feeling in one's sensory boundaries is a phenomenon which is common in ASD research. It was concluded that the device developed in this study allowed people to vicariously experience the senses of people with ASD.
bioinformatics	In the past decade, the rates of overweight and obesity have been increasing dramatically worldwide, which lead to serious health risks, including heart diseases, diabetes, and various cancers. Clearly, to improve the health conditions, physical activities and diet are the most important factors for people to control the weight and thus achieve healthy lifestyle. Motivated by the amazingly growth of smartphone ownership and the sensing technologies, in this paper, we propose a mobile-sensing based health recognition and recommendation framework, namely, H-Rec2. The main idea is to use smartphone to unobtrusively record and analyze the user's physical activity and health status, and at the same time obtain the personalized health food recommendations from the remote server. To demonstrate the idea, we implemented a prototype system and conduct systematic experiments to evaluate performance. The evaluation results confirm the proposed approaches with regard to the effectiveness and usability.
bioinformatics	GPUs (Graphics Processing Unit) and other accelerators are nowadays commonly found in desktop machines, mobile devices and even data centres. While these highly parallel processors offer high raw performance, they also dramatically increase program complexity, requiring extra effort from programmers. This results in difficult-to-maintain and non-portable code due to the low-level nature of the languages used to program these devices. This paper presents a high-level parallel programming approach for the popular Java programming language. Our goal is to revitalise the old Java slogan -- Write once, run anywhere --- in the context of modern heterogeneous systems. To enable the use of parallel accelerators from Java we introduce a new API for heterogeneous programming based on array and functional programming. Applications written with our API can then be transparently accelerated on a device such as a GPU using our runtime OpenCL code generator. In order to ensure the highest level of performance, we present data management optimizations. Usually, data has to be translated (marshalled) between the Java representation and the representation accelerators use. This paper shows how marshal affects runtime and present a novel technique in Java to avoid this cost by implementing our own customised array data structure. Our design hides low level data management from the user making our approach applicable even for inexperienced Java programmers. We evaluated our technique using a set of applications from different domains, including mathematical finance and machine learning. We achieve speedups of up to 500? over sequential and multi-threaded Java code when using an external GPU.
bioinformatics	The Dynamic Time Warping (DTW) algorithm is a commonly used algorithm in matching time sequence data in many applications that require some kind of similarity measure. Though effective, DTW is computationally intensive, and therefore is not suitable for real-time situations. In the past 30 years, there has been some research work on implementing DTW in hardware as a stand-alone processing unit, or as a co-processor within a larger system. This work gives a brief survey on previous work done in DTW hardware design and implementation. For many modern-day Web and intelligent applications, one must consider the real time and hardware footprint aspects of the system. A DTW single-element processing unit is proposed in order to investigate the suitability of using it as a building block for more complex architecture for embedded applications. This simple unit is designed and simulated as a Field Programmable Gate Array (FPGA) implementation using Xilinx tools. The performance results of both area and speed show great potential and ascertain DTW hardware is a worthwhile endeavor to pursue further in a systematic fashion.
bioinformatics	The ability to detect mental states, whether relaxation or stressed, would be useful in categorizing places according to their impact on our brains and many other domains. Newly available, affordable and dry-electrode devices make electroencephalography headsets (EEG) feasible to use outside the lab, for example in open spaces and shopping malls. The purpose of this pervasive experimental manipulation is to analyze brain signals in order to label outdoor places according to how users perceive them with a focus on ---relaxing and ---stressful mental states. That is, when the user is experiencing tranquil brain waves or not when visiting a particular place. This paper demonstrates the potential of exploiting the temporal structure of EEG signals in making sense of outdoor places. The EEG signals induced by the place stimuli are analyzed and exploited to distinguish what we refer to as a place signature.
bioinformatics	According to the reported numbers of fatal crashes, road tunnels on highways pose a severe safety risk. Based on our own experience and observations, it is supposed that one substantial reason is an increased level of anger or stress, observable through elevated levels of cognitive demand. To confirm these assumptions, we conducted an on-road experiment aimed at revealing how the human mind is affected (in terms of stress levels) when traveling through tunnels compared to driving on the open road. Test subjects were continuously monitored with an ECG device and had to steer the vehicle on a 100km long highway segment with 24 tunnel passages. Qualitative and quantitative evaluation reveals that individuals in fact experience an elevated stress level, while feeling uncomfortable with the situation. A combination of these two circumstances suggests that, in critical situations, the body might react with tonical immobility as part of its acute stress response, having devastating consequences on the road. Future work is therefore strongly encouraged to identify major stressors, so to alleviate increased stress and, thus, to increase traffic safety.
bioinformatics	Formulation of perceptual objective metrics for haptic interaction evaluation is still a challenge. These metrics are created including the human perception and the distortion intensity of the haptic signals, however, they disregarded peculiarities of tasks to be performed during human-computer interaction. Thus, we proposed a metric specialized in healthcare training for haptic interaction evaluation, that considers the importance degree of the different steps of a task and distortion intensity. The novel perceptual objective metric was validated with experts from dentistry area. The task selected was needle insertion, a common task in healthcare training systems. Our metric was compared to a state of the art metric using position haptic attribute and presented more coherent results with users perception in some cases.
bioinformatics	This project addresses how to link scattered health-related data from different Web communities, and provide integrated knowledge of health information. Specifically, we integrate data from social media-based patient communities, curated sites with expert content, and the research community. Our approach is based on medical concept extraction using the Unified Medical Language System (UMLS), Resource Description Framework (RDF) semantic modeling to represent diverse social health and medical experiences, and summarization of integrated health data. A prototype implementation annotates medical terms occurring in blogs with summarized health experience data, medical expert data and medical research data that enables users, such as patients, doctors or other health care providers to have integrated and linked view of health-related knowledge. Currently, the system integrates information from PatientsLikeMe, WebMD, and PubMed, and can be used to annotate a wide variety of text based blogs. This system uses ontology-based information extraction and semantic modeling of social health data to integrate informally specified information, which is typical of content written by patients.
bioinformatics	This paper presents CSense - a stream-processing toolkit for developing robust and high-rate mobile sensing application in Java. CSense addresses the needs of these systems by providing a new programming model that supports flexible application configuration, a high-level concurrency model, memory management, and compiler analyses and optimizations. Our compiler includes a novel flow analysis that optimizes the exchange of data across components from an application-wide perspective. A mobile sensing application benchmark indicates that flow analysis may reduce CPU utilization by as much as 45%. Static analysis is used to detect a range of programming errors including application composition errors, improper use of memory management, and data races. We identify that memory management and concurrency limit the scalability of stream processing systems. We incorporate memory pools, frame conversion optimizations, and custom synchronization primitives to develop a scalable run-time. CSense is evaluated on Galaxy Nexus phones running Android. Empirical results indicate that our run-time achieves 19 times higher steam processing rate compared to a realistic baseline implementation. We demonstrate the versatility of CSense by developing three mobile sensing applications.
bioinformatics	Can one trade sensor quality for quantity? While larger networks with greater sensor density promise to allow us to use noisier sensors yet measure subtler phenomena, aggregating data and designing decision rules is challenging. Motivated by dense, participatory seismic networks, we seek efficient aggregation methods for event detection. We propose to perform aggregation by sparsification: roughly, a sparsifying basis is a linear transformation that aggregates measurements from groups of sensors that tend to co-activate, and each event is observed by only a few groups of sensors. We show how a simple class of sparsifying bases provably improves detection with noisy binary sensors, even when only qualitative information about the network is available. We then describe how detection can be further improved by learning a better sparsifying basis from network observations or simulations. Learning can be done offline, and makes use of powerful off-the-shelf optimization packages. Our approach outperforms state of the art detectors on real measurements from seismic networks with hundreds of sensors, and on simulated epidemics in the Gnutella P2P communication network.
bioinformatics	We propose a kernel-level energy profiling tool KLEP that can work with diverse APIs of Android. KLEP addresses the challenges of the tail energy problem and the complex interrelation between hardware components in the device energy consumption profile. KLEP collects energy-sensitive events in the kernel and measures real energy consumption of the device at the same time, and employs a LSTM neural-network-based model for energy profiling. The preliminary results show that the curves profiled by KLEP can match the actual energy consumption with low error and overhead.
bioinformatics	Accurate detection of human intention is always challenging for the effective control of assistive and rehabilitation exoskeletons. In this paper, a human arm motion detection system is presented to recognize movements including elbow flexion, elbow extension, pronation and supination. Force sensing resistors (FSR) based sensor bands are developed to monitor the upper arm and forearm muscles activity. The bands are able to read the muscle deformation for different motions. Support Vector Machine (SVM) is implemented to recognize the motions in real time. The results have shown that the sensing method can detect the intended motions with high accuracy.
bioinformatics	The Fourth Workshop on Hot Topics in Software Upgrades (HotSWUp 2012) was held on June 3, 2012 in Zurich, Switzerland. The workshop was co-located with ICSE 2012. The goal of HotSWUp is to identify, through interdisciplinary collaboration, cutting-edge research ideas for implementing software upgrades. The workshop combined presentations of peer-reviewed research papers with a keynote speech on how empirical software engineering can help reduce update-induced failures. The audience included researchers and practitioners from academia and industry. In addition to the technical presentations, the program allowed ample time for discussions, which were driven by debate questions provided in advance by the presenters. HotSWUp provides a premier forum for discussing problems that are often considered niche topics in the established research communities. For example, the technical discussions at HotSWUp'12 covered dynamic software updates, package management tools, using model-checking and verification to verify updates, empirical software engineering and repository mining, and highlighted many synergies among these and other topics.
bioinformatics	Energy efficiency is one of the major challenges in big datacenters. To facilitate processing of large data sets in a distributed fashion, the MapReduce programming model is employed in these datacenters. Hadoop is an open-source implementation of MapReduce which contains a distributed file system. Hadoop Distributed File System provides a data block replication scheme to preserve reliability and data availability. The distribution of the data block replicas over the nodes is performed randomly by meeting some constraints (e.g., preventing storage of two replicas of a data block on a single node). This study makes use of flexibility in the data block placement policy to increase energy efficiency in datacenters. Furthermore, inspired by Zaharia et al.'s delay scheduling algorithm, a scheduling algorithm is introduced, which takes into account energy efficiency in addition to fairness and data locality properties. Computer simulations of the proposed method suggest its superiority over Hadoop's standard settings.
bioinformatics	Internet research often assumes users may connect devices without consent by their service providers. However, in many networks the service provider only allows use of devices obtained directly from the provider. We review how United States communications law addresses the rights of users to connect devices of their choice. We explicate a set of user and service provider rights. We propose legal requirements for attachment and management of devices. We illustrate how these proposed regulations would affect the services currently offered on telephone, cable, satellite, video networks, and cellular networks, as well as on the Internet.
bioinformatics	Access control policy is typically defined in terms of attributes, but in many applications it is more natural to define permissions in terms of relationships that resources, systems, and contexts may enjoy. The paradigm of relationship-based access control has been proposed to address this issue, and modal logic has been used as a technical foundation. We argue here that hybrid logic -- a natural and well-established extension of modal logic -- addresses limitations in the ability of modal logic to express certain relationships. We identify a fragment of hybrid logic to be used for expressing relationship-based access-control policies, show that this fragment supports important policy idioms, and demonstrate that it removes an exponential penalty in existing attempts of specifying complex relationships such as "at least three friends". We also capture the previously studied notion of relational policies in a static type system.
bioinformatics	The telephone speech corpus is the basis of developing a Human-machine interaction system designed for communication and mobile internet. The main problem nowadays for constructing a qualified speech corpus is lack of a standard scheme. This research tries to find a standardization program which can make the corpus be established more efficiently and be used or shared easier. The specifications of constructing a speech corpus are also introduced in the paper. Finally, a telephone speech corpus, TSC973, be exemplified to illuminate the standardization program.
bioinformatics	Most recommender systems assume user ratings accurately represent user preferences. However, prior research shows that user ratings are imperfect and noisy. Moreover, this noise limits the measurable predictive power of any recommender system. We propose an information theoretic framework for quantifying the preference information contained in ratings and predictions. We computationally explore the properties of our model and apply our framework to estimate the efficiency of different rating scales for real world datasets. We then estimate how the amount of information predictions give to users is related to the scale ratings are collected on. Our findings suggest a tradeoff in rating scale granularity: while previous research indicates that coarse scales (such as thumbs up / thumbs down) take less time, we find that ratings with these scales provide less predictive value to users. We introduce a new measure, preference bits per second, to quantitatively reconcile this tradeoff.
bioinformatics	Bone strength is dependent on both its mass and architecture. In this study, a tool was developed that incorporates metrics associated with both of these features. To accomplish this, textural features of trabecular bone were extracted from stained bone images using Gabor wavelets. Gabor wavelets are 2-D spatial filters that are both frequency and orientation tunable. A texture feature vector was constructed that consists of localized texture energies along different orientations at different scales. The texture feature characterizes the spatial (regional) distributions of the constituent bone lattice in terms of their size, shape and orientation. Results indicated that wavelet analysis provides the insight of the frequency composition that can be localized to the pizel level. Bone mass can be discriminated by the averaged texture energy across all orientations. Dominant bone lattice orientation can be determined by the orientation with the maximal value of the averaged texture energy across all scales. A measure of anisotropy can be quantified by the span between the maximum texture energy and the minimum texture energy. This methodology has the potential to provide a tool for quantifying both bone mass and bone structural anisotropy.
bioinformatics	In this paper we propose an algorithm for counting moving objects in outdoor environments. Similar to other approaches the proposed algorithm uses a traditional surveillance system approach: background subtraction followed by noise removing, tracking and object labeling. The novelty of the proposed algorithm is that each processing step uses a stereo camera. In fact, the algorithms use both rectified and depth images obtained with a stereo camera. Moving objects are extracted from the scene using a novel background subtraction approach. In outdoor environment the images can be degraded by noise. We considered two types of noise, namely periodic movements due to the wind and the cast shadow. In this paper novel algorithms for detecting these types of noise are proposed. Using them, the noisy pixels are removed. The resulting segmented pixels are grouped together by clustering connected regions and then tracked during their movements. The obtained blobs, which correspond to the moving objects, are thus obtained with high accuracy. The final part of the algorithm is blob classification, which identifies the moving objects. The knowledge of the moving objects can be used to build more complex counting applications than that based on just counting blobs. Two simple applications based on the proposed algorithm are worked out and discussed in this paper, namely one that counts the people moving on the left and on the right of the video scene and one that counts the cars moving in the same way.
bioinformatics	We propose a method for obtaining 3D gaze information using inside-out camera. Such information on 3D gaze points can be useful not only to clarify higher cognitive processes in humans but also to reproduce the 3D shape of an object from eyeball movement simply by gazing at the object as an extension of the visual function. Using half-mirrors, an inside-out camera can capture a person's eyeball head-on and can capture the person's visual field from a position equivalent to that of the eyeball. Here, the relationship between the gaze vector obtained from images of the eyeball and the gaze point in images capturing the visual field is expressed by a conversion equation. The 3D position of the gaze point can then be estimated by using stereo constraints in two scene cameras. In an evaluation experiment, the gaze point could be estimated with an average error of about 15 pixels, and we also showed the 3D scan path obtained by the proposed method from eyeball movement by gazing at the object.
bioinformatics	In this paper, a vision-based real-time exercise instruction system is proposed. The purposes of the system are to increase the exercise efficiency and to prevent users from being injured. This system uses a stereo-vision method to compute the 3D user information, and a vision-based pose estimation method to analyze the user's postures. The response time of the system is short enough such that the interaction between the user and the system can be done in real-time.
bioinformatics	Sensor misplacement is a common obstacle that prevents inertial-based technology from providing reliable motion inference. Traditional approaches require certain calibration postures or activities to be performed. However, this may not be feasible for patients with mobility impairments. We propose a system that uses the Kinect's measurement as the ground truth to opportunistically detect and compensate for such errors. The goal of this study is to provide reliable motion data without the requirement of calibration activities or careful placement of the wearable sensors. First, we identified the instances where the Kinect had an unobstructed view of the limb of interest, and collected data for calibration. Then, we applied double exponential smoothing on the Kinect's position data and performed differentiation twice to generate virtual accelerations. By examining the acceleration vectors from the Kinect and inertial measurement unit (IMU) sensor, the misplacement of IMU sensors can be identified and thus compensated. Our results showed that the calibration algorithms successfully detected orientation error and provided accurate compensation. We also present an example of trajectory reconstruction with misplaced sensors and applied the proposed method. We obtained good agreement of reconstructed trajectories between the rectified sensor and the correctly placed sensor. The outcomes of this research will simplify ground-truth collection in the clinic, and provide reliable inference of motion data in the community.
bioinformatics	Wearable computer vision systems provide plenty of opportunities to develop human assistive devices. This work contributes on visual scene understanding techniques using a helmet-mounted omnidirectional vision system. The goal is to extract semantic information of the environment, such as the type of environment being traversed or the basic 3D layout of the place, to build assistive navigation systems. We propose a novel line-based image global descriptor that encloses the structure of the scene observed. This descriptor is designed with omnidirectional imagery in mind, where observed lines are longer than in conventional images. Our experiments show that the proposed descriptor can be used for indoor scene recognition comparing its results to state-of-the-art global descriptors. Besides, we demonstrate additional advantages of particular interest for wearable vision systems: higher robustness to rotation, compactness, and easier integration with other scene understanding steps.
bioinformatics	Deep-web crawl is concerned with the problem of surfacing hidden content behind search interfaces on the Web. While many deep-web sites maintain document-oriented textual content (e.g., Wikipedia, PubMed, Twitter, etc.), which has traditionally been the focus of the deep-web literature, we observe that a significant portion of deep-web sites, including almost all online shopping sites, curate structured entities as opposed to text documents. Although crawling such entity-oriented content is clearly useful for a variety of purposes, existing crawling techniques optimized for document oriented content are not best suited for entity-oriented sites. In this work, we describe a prototype system we have built that specializes in crawling entity-oriented deep-web sites. We propose techniques tailored to tackle important subproblems including query generation, empty page filtering and URL deduplication in the specific context of entity oriented deep-web sites. These techniques are experimentally evaluated and shown to be effective.
bioinformatics	This work presents the use of graph-based approaches to discovering anomalous instances of structural patterns in data that represent entities, relationships and actions. Using the minimum description length (MDL) principle to first identify the normative pattern, the algorithms presented in this paper identify the three possible changes to a graph: modifications, insertions and deletions. Each algorithm discovers those substructures that match the closest to the normative pattern without matching exactly. As a result, this proposed approach searches for those activities that appear to match normal (or legitimate) transactions, but in fact are structurally different. After briefly presenting the three algorithms, we then show the usefulness of applying these graph theoretic approaches to discovering illegal activity for a simulated insider threat within a passport processing scenario.
bioinformatics	Huge volumes of streaming data have been generated by sensors for applications such as environment surveillance. Partially due to the inherited limitation of sensors, these continuous streaming data can be uncertain. Over the past few years, algorithms have been proposed to apply the sliding window or time-fading window model to mine frequent patterns from streams of uncertain data. However, there are also other models to process data streams. In this paper, we propose a landmark-model based system for mining frequent patterns from streams of uncertain data.
bioinformatics	The differential evolution (DE) is a very powerful search method for solving many optimization problems. In this paper we present a new scheme (DESAX) based on the differential evolution to localize the breakpoints utilized with the symbolic aggregate approximation method; one of the most important symbolic representation techniques for times series data. We compare the new scheme with a previous one (GASAX), which is based on the genetic algorithms, and we show how the new scheme outperforms the original one. We also show how (DESAX) can be used for the symbolic aggregate approximation of non-normalized time series.
bioinformatics	Clustering is a fundamental technique in data mining to identify essential group structures in a given data matrix. Traditional clustering methods are one-way clustering, which has however limitations for high-dimensional matrices or matrices with missing values. One possible solution is co-clustering, which does clustering both columns and rows simultaneously. Also auxiliary information over columns or rows is helpful to stabilize/improve the performance of clustering. We propose a new co-clustering approach, which can incorporate auxiliary information on both columns and rows. Our approach is based on a probabilistic model, for which we present an efficient method for estimating parameters, based on variational Bayesian learning. Our problem setting can be semi-supervised, by which our approach can be applied to various data mining applications. We evaluated the performance of the proposed approach by using both synthetic and real datasets, confirming the clear advantage of incorporating auxiliary information as well as of our method over two competing methods.
bioinformatics	We consider the problem of fuzzy full-text search in large text collections, that is, full-text search which is robust against errors both on the side of the query as well as on the side of the documents. Standard inverted-index techniques work extremely well for ordinary full-text search but fail to achieve interactive query times (below 100 milliseconds) for fuzzy full-text search even on moderately-sized text collections (above 10 GBs of text). We present new preprocessing techniques that achieve interactive query times on large text collections (100 GB of text, served by a single machine). We consider two similarity measures, one where the query terms match similar terms in the collection (e.g., algorithm matches algoritm or vice versa) and one where the query terms match terms with a similar prefix in the collection (e.g., alori matches algorithm). The latter is important when we want to display results instantly after each keystroke (search as you type). All algorithms have been fully integrated into the CompleteSearch engine.
bioinformatics	The MiRo robot is a new pet-sized mobile platform with an emotionally-engaging personality and appearance that has been developed for research on companion robotics and robot-assisted therapy. MiRo has six senses and eight degrees of freedom that are designed to promote human-robot interaction. A distinctive feature is the use of a biomimetic brain-based control system consisting of a layered control architecture alongside centralized mechanisms for integration and action selection. MiRo has been developed by Consequential Robotics, a spin-out of the University of Sheffield, and aims to provide the HRI community with a flexible platform for research and education.
bioinformatics	An increasing number of human-robot interaction (HRI) studies are now taking place in applied settings with children. These interactions often hinge on verbal interaction to effectively achieve their goals. Great advances have been made in adult speech recognition and it is often assumed that these advances will carry over to the HRI domain and to interactions with children. In this paper, we evaluate a number of automatic speech recognition (ASR) engines under a variety of conditions, inspired by real-world social HRI conditions. Using the data collected we demonstrate that there is still much work to be done in ASR for child speech, with interactions relying solely on this modality still out of reach. However, we also make recommendations for child-robot interaction design in order to maximise the capability that does currently exist.
bioinformatics	The advent of ROS, the Robot Operating System, has finally made it possible to implement and use state-of-the-art navigation and manipulation algorithms on widely-available, inexpensive standard robot platforms. With the addition of the Rosbridge application programming interface, interface designers and applications programmers can create robot interfaces and behaviors without venturing into the specialized world of robotics engineers. This tutorial introduces ROS and Rosbridge, and shows how quickly and easily these tools can be used to design and conduct large-scale online HRI experiments, access algorithms for autonomous robot behavior, and leverage the huge ecosystem of general-purpose web-based and application-oriented software engineering for robotics and HRI research. Tutorial attendees will learn the basics of autonomous and teleoperated navigation and manipulation, as well as interface design for online interaction with robots. During the tutorial they will design and write their own remote presence application, as well as develop strategies for incorporating autonomy and dealing with data collection.
bioinformatics	With the advent of robotic rehabilitation, we set out to test people's preferences when playing an interactive game with a robotic arm. Twenty two young participants played the mirror game with the robotic arm, where one player (person or robot) follows the movements of the other. The robotic arm performed a set of sharp and smooth movements, which the participants were asked to rate. The greatest preference was given to smooth movements. Half of the participants preferred to lead, and half to follow. Our results highlight the importance of personalized human-robot interactions.
bioinformatics	In this paper, we present an asynchronous display method, coined image queue, which allows operators to search through a large amount of data gathered by autonomous robot teams. We discuss and investigate the advantages of an asynchronous display for foraging tasks with emphasis on Urban Search and Rescue. The image queue approach mines video data to present the operator with a relevant and comprehensive view of the environment in order to identify targets of interest such as injured victims. It fills the gap for comprehensive and scalable displays to obtain a network-centric perspective for UGVs. We compared the image queue to a traditional synchronous display with live video feeds and found that the image queue reduces errors and operator's workload. Furthermore, it disentangles target detection from concurrent system operations and enables a call center approach to target detection. With such an approach we can scale up to very large multi-robot systems gathering huge amounts of data that is then distributed to multiple operators.
bioinformatics	Speech enhancement aims to improve the speech quality by using various techniques. Spectral Subtraction Technique is one earliest and longer standing, popular approaches to noise compensation and speech enhancement. It reduces stationary noise but the non stationary noise still passes through it. Further, it also introduces a musical noise which is very annoying to human ears. Beamforming is another possible method of speech enhancement that can be used. Beamforming by itself, however, does not appear to provide enough improvement. Further, the performance of Beamforming becomes worse if the noise source comes from many directions or the speech has strong reverberation. A combined technique using the Spectral Subtraction Technique followed by Beamforming Technique reduces stationary as well as residual, musical noise. It can be observed that the Spectral Subtraction followed by Beamforming gives better SNR value as compared to that of individual techniques, thereby improving the quality of speech. Numerous simulation results are used to illustrate the reasoning.
bioinformatics	High temperature Tribology is an interesting and challenging area which deserves more attention on wear resistance. The present investigation relates to understanding the surface roughness and hardness by the effect of different operational conditions like wear pressure, sliding speed and ambient temperature on AISI SAE 8630, 3140, and 9310. Dry sliding wear tests have been carried out on the materials AISI SAE 8630, 3140, and 9310 under room temperature and also at high temperatures like 200�C, 400�C and 600�C on pin on disc type wear testing machine for 10,000 m sliding distance. The wear tests were carried out under the operational conditions of three sliding speeds like 1, 3 and 5 m/s, under the wear pressures of 0.125 MPa, 0.375 MPa and 0.625 MPa. Weight loss was recorded after each wear test. Hardness after wear was measured by micro hardness tester under 100 gm wear pressure. Roughness of the worn-out surface along the parallel & perpendicular was measured by roughness instrument. The present investigation was undertaken keeping in view the wear behaviour of metallic material influenced by the frictional force, which is turn, is governed by the hardness & oxidation kinetics of the mating surface on the high temperature wear. Nickel Chromium based alloys have been reported to be widely used as they combine several advantages such as abrasion, erosion & resistance to high temperature corrosive atmospheres. In this article a brief review of the performance of Nickel Chromium based alloy at different speeds, loads and temperatures has been made and compared with the room temperature. It is observed that the hardness of the worn out surface increased with an increase in the load and sliding speed due to work hardening at room temperature. Under low operational conditions, two body abrasive, under high operational conditions oxidative and under moderate operational conditions adhesive wear mechanisms are observed.
bioinformatics	The direct memory access (DMA) I/O technique provides direct access to the memory while the microprocessor is temporarily disabled. A DMA Controller temporarily borrows the address bus, data bus and control bus from the microprocessor and transfers the data bytes directly between an I/O port and a series of memory locations. The proposed model of a Universal DMA Controller is of generic type and supports much functionality. This DMA Controller can be plugged into any SoC system for the required data transfer operation. Support transactions such as Port 1 IO/Memory to Port1 IO/Memory, Port1 IO/Memory to Port2 IO/Memory, Port2 IO/Memory to Port1 IO/Memory and Port2 IO/Memory to Port2 IO/Memory and many more.
bioinformatics	Today's large-scale services generally exploit loosely-coupled architectures that restrict functionality requiring tight cooperation (e.g., leader election, synchronization, and reconfiguration) to a small subset of nodes. In contrast, this work presents a way to scalably deploy tightly-coupled distributed systems that require significant coordination among a large number of nodes in the wide area. Our design relies on a new group membership abstraction, circuit breaker, to preserve efficient pairwise communication despite transient link failures. We characterize our abstraction in the context of a distributed rate limiting (DRL) service's deployment on a global testbed infrastructure. Unlike most distributed services, DRL can safely operate in separate partitions simultaneously, but it requires timely snapshots of global state within each. Our DRL deployment leverages the circuit breaker abstraction along with a robust gossipbased communication protocol to meet its demanding communication requirements. Through local and widearea experiments, we illustrate that DRL remains accurate and fair in the face of a variety of failure scenarios.
bioinformatics	Can one trade sensor quality for quantity? While larger networks with greater sensor density promise to allow us to use noisier sensors yet measure subtler phenomena, aggregating data and designing decision rules is challenging. Motivated by dense, participatory seismic networks, we seek efficient aggregation methods for event detection. We propose to perform aggregation by sparsification: roughly, a sparsifying basis is a linear transformation that aggregates measurements from groups of sensors that tend to co-activate, and each event is observed by only a few groups of sensors. We show how a simple class of sparsifying bases provably improves detection with noisy binary sensors, even when only qualitative information about the network is available. We then describe how detection can be further improved by learning a better sparsifying basis from network observations or simulations. Learning can be done offline, and makes use of powerful off-the-shelf optimization packages. Our approach outperforms state of the art detectors on real measurements from seismic networks with hundreds of sensors, and on simulated epidemics in the Gnutella P2P communication network.
bioinformatics	The paper presents a smartphone-based shooter localization system. As muzzle blasts are difficult to detect at longer distances and consequently present higher false detection rates, the system relies on shockwaves only. Each sensor uses four microphones to detect the Angle of Arrival and the length of the shockwave. This information, along with the sensor's own GPS coordinates, are shared among nearby smartphones. Assuming a known weapon type, it then proceeds to estimate the two possible projectile trajectory candidates for each sensor that are consistent with the observations in the horizontal plane of the sensors. A simple clustering algorithm identifies the correct projectile trajectory relying on as few as two sensors. The trajectory is then used to estimate the bearing to the shooter relative to each sensor. The paper presents the overall system architecture, the design of the sensor node that interfaces with the smartphone, the trajectory and bearing estimation algorithms, and the evaluation of the system based on a field experiment.
bioinformatics	We propose a SDN-like architecture based WSN and improve the existing EC-CKN Sleep Scheduling mechanism to implement more efficient energy management. A SDN-like architecture is adopted instead of traditional WSN architecture and EC-CKN algorithm is applied as the fundamental algorithm. This paper presents the design, implementation and evaluation of the proposed SDN-ECCKN on the WSN with SDN-like architecture.
bioinformatics	Modern day smart phones are powerful connected sensory and computation nodes for crowd-sensing, urban-sensing and personal-sensing applications. We have developed an Internet of Things (IoT) platform that can seamlessly handle data from the wide variety of sensors available on mobile phones. It can store and run aggregated analysis on the data in real-time. However, mobile phones themselves are a very heterogeneous set of devices. Each phone comes with a different array of sensors with varying sensitivity and control functions. Also, there are multiple development environments and programming languages. A final problem is seamless prototyping of applications offline and then seamless partitioning of the algorithm between phone and the cloud. In this paper we present early design elements of a framework aimed at addressing these issues.
bioinformatics	Datacenter networking has brought high-performance storage systems' research to the foreground once again. Many modern storage systems are built with commodity hardware and TCP/IP networking to save costs. In this paper, we highlight a group of problems that are present in such storage systems and which are all related to the use of TCP. As an alternative, we explore Trevi: a fountain coding-based approach for distributing I/O requests that overcomes these problems while still efficiently scheduling resources across both networking and storage layers. We also discuss how receiver-driven flow and congestion control, in combination with fountain coding, can guide the design of Trevi and provide a viable alternative to TCP for datacenter storage.
bioinformatics	When investigating uncanny feelings towards robots, most researchers focus solely on the dimension of human-likeness as a potential cause. In our research, we aim to broaden the understanding of the so-called uncanny valley effect by exploring the potential multidimensionality of the phenomenon: By using a mixed-embodied robot head, we are able to alter visual and auditory cues in the robot and study the influence and interplay with the robot's interaction strategies and the perceived uncanny feelings in different age groups.
bioinformatics	There are several challenges in applying conversational social robots to Technology Enhanced Learning and Serious Gaming. In this paper, we focus in particular on the dialogue management issues in building an empathic robotic tutor that plays a multi-person serious game with students to help them learn and understand the underlying educational concepts.
bioinformatics	Recently, autonomous agents with emotional behavior are developed to improve the human-agent interaction. Users' perceived "believability" of autonomous agents is considered to affect interactions with agents. While human-like emotional behavior is considered to increase believability, actual emotional behaviors differ across cultures. Therefore culture-specific emotional behaviors of agent might increase believability. In this paper, two kinds of emotional behaviors specific in Japanese culture, namely amae and Japanese way of agency appraisal, were implemented in agents. Influences of these behaviors on believability were investigated experimentally. Results showed that both Japanese behaviors increased believability. Especially amae increased personality factor, which is one of the factors of believability.
bioinformatics	Activity recognition is a key capability for a smart environment to offer timely services and intelligent interactions with people, especially with the growing number of connected devices. While logging data from connected sensors is no longer beyond reach, it is still quite difficult to collect the labels required by machine learning approaches to activity recognition. In this research, crowdsourcing agents are designed to acquire status labels from people situated in the environment. Experiments on crowdsourcing in a typical building on campus have been conducted to improve air conditioning and space utilization. In particular, we will discuss how crowdsourcing agents in the form of simple physical objects can significantly improve user engagement as well as data quality. Collaboration among cyber-physical agents can lead to better user experience and overall performance.
bioinformatics	This study explores presentation techniques for a 3D animated chat-based virtual human that communicates engagingly with users. Interactions with the virtual human occur via a smartphone outside of the lab in natural settings. Our work compares the responses of users who interact with no image or a static image of a virtual character as opposed to the animated visage of a virtual human capable of displaying appropriate nonverbal behavior. We further investigate users' responses to the animated character's gaze aversion which displayed the character's act of looking away from users and was presented as a listening behavior. The findings of our study demonstrate that people tend to engage in conversation more by talking for a longer amount of time when they interact with a 3D animated virtual human that averts its gaze, compared to an animated virtual human that does not avert its gaze, a static image of a virtual character, or an audio-only interface.
bioinformatics	We are investigating the potential use of trial transcripts as sources of social knowledge for epistemic agents. But we are immediately faced with the reality that not all transcripts are equal. The quality of the transcripts will be partially related to the knowledge, consistency, and integrity of the individuals that testify during the course of the trial, and related to the nature and sophistication of the questions. Before we can determine whether a transcript will be useful as a knowledge source for an epistemic agent, we have to identify the consistency and quality of the knowledge present in the transcript. Coherence clusters demarcate the network of positively and negatively related propositions in the transcript. The justification clusters define the subcluster of propositions that support or justify other propositions in a coherence cluster. These clusters can be used to determine the nature of the consistency of the knowledge potentially present in the transcript. In this paper, we show how these clusters are identified using epistemic analysis. Our goals is to use these clusters as the basis for an epistemic metric used to determines the quality propositional knowledge present in a transcript.
bioinformatics	The general goal of the paper is to show the normative/deontic nature of conventions. Conventions are traditionally defined as regularity of behavior based on expectations evolved to solve coordination problems [14]. The thesis we defend is that the cognitive attitude of expectations is not only characterized by an anticipatory representation (belief) of a future state of affairs but is coupled with a motivational component (a goal on this state). The possible convergence between beliefs and corresponding goals allows the identification of positive and negative expectations. We argue that in positive expectations (differently from the negative ones) lies implicitly an influencing act aimed at prescribing that the expected event will be realized. We consider conventions as analyzed in Game Theory as regularity of behavior based on positive expectations. These conventions entail the deontic component of prescription. Each agent prescribes (and is subject to prescription) conformity to the convention to the others (prescription to do). This is a possible route to the spontaneous emergence of Social Norms. However we hypothesize, differently, that negative expectations too can sustain conventions. Even "bad habits" share a deontic component but is characterized by the socio-cognitive structure of permission (entitlement to do). We argue that with this analysis is possible to explain the self-organizing and stabilizing effect of conventions that create an equilibrium noxious for all the participants and individually more costly than the individual benefit.
bioinformatics	Drug discovery is a critical but complex and costly endeavor. The rate of approval of new therapeutics by the FDA has been in decline while costs are rising. Increasingly, pharmaceutical companies desire to translate pharmaceutical discovery from academic research in order to decrease risk. Although many researchers have identified very compelling targets, most researchers do not have access to drug discovery resources due to the high cost and complex infrastructure needed to launch a discovery campaign. Long-term objective of this research is to integrate drug interaction simulation software to identify new bioactive molecules and speed drug development with minimum cost and time. This technology is a highly feasible way to rapidly close the therapeutic gap and potentially dramatically improve public health. Initially research was conducted using typical clusters and it took 3 months to perform one run with one conformation of the protein using 1.5 million small molecules. But researchers are interested in working with many proteins with multiple conformations per protein related to entire disease related pathways. At this rate this computational research by itself would take 6 to 7 years of computation on institutional clusters. This resulted in PI applying for the XSEDE allocation with Extended Collaborative Support Services (ECSS) support, which resulted in generation of optimized and scaled the drug interaction workflow on XSEDE supercomputers that reduced computation time for single run from months to 40 minutes using 8000 cores. The results were generated for 5 proteins with 5 conformations with 1.5 million compounds in an afternoon (wall clock time)on Kraken supercomputer which would have taken 5 years of computation on typical cluster. This presentation will discuss about the process from project inception to generating results for publications and proposals for various funding agencies. PI quotes "I thought the computation might not be finished in my life span, this collaboration takes my research to new heights".
bioinformatics	SDN provides a way to manage complex networks by introducing programmability and abstraction of the control plane. All networks suffer from attacks to critical infrastructure and services such as DDoS attacks. We make use of the programmability provided by the SDN environment to provide a game theoretic attack analysis and countermeasure selection model in this research work. The model is based on reward and punishment in a dynamic game with multiple players. The network bandwidth of attackers is downgraded for a certain period of time, and restored to normal when the player resumes cooperation. The presented solution is based on Nash Folk Theorem, which is used to implement a punishment mechanism for attackers who are part of DDoS traffic, and reward for players who cooperate, in effect enforcing desired outcome for the network administrator.
computer_architecture	We bring to the fore of the recommender system research community, an inconvenient truth about the current state of understanding how recommender system algorithms and humans influence one another, both computationally and cognitively. Unlike the great variety of supervised machine learning algorithms which traditionally rely on expert input labels and are typically used for decision making by an expert, recommender systems specifically rely on data input from non-expert or casual users and are meant to be used directly by these same non-expert users on an every day basis. Furthermore, the advances in online machine learning, data generation, and predictive model learning have become increasingly interdependent, such that each one feeds on the other in an iterative cycle. Research in psychology suggests that people's choices are (1) contextually dependent, and (2) dependent on interaction history. Thus, while standard methods of training and assessing performance of recommender systems rely on benchmark datasets, we suggest that a critical step in the evolution of recommender systems is the development of benchmark models of human behavior that capture contextual and dynamic aspects of human behavior. It is important to emphasize that even extensive real life user-tests may not be sufficient to make up for this gap in benchmarking validity because user tests are typically done with either a focus on user satisfaction or engagement (clicks, sales, likes, etc) with whatever the recommender algorithm suggests to the user, and thus ignore the human cognitive aspect. We conclude by highlighting the interdisciplinary implications of this endeavor.
computer_architecture	One important challenge in the field of recommender systems is the sparsity of available data. This problem limits the ability of recommender systems to provide accurate predictions of user ratings. We overcome this problem by using the publicly available user generated information contained in Wikipedia. We identify similarities between items by mapping them to Wikipedia pages and finding similarities in the text and commonalities in the links and categories of each page. These similarities can be used in the recommendation process and improve ranking predictions. We find that this method is most effective in cases where ratings are extremely sparse or nonexistent. Preliminary experimental results on the MovieLens dataset are encouraging.
computer_architecture	In this demo, we propose to showcase a mobile application for personalized and geolocated movie recommendations. While there is a wide range of cinema and movies applications for mobile platforms such as iPhone, ours includes a Recommender System based on Expert Collaborative Filtering that provides recommendations tailored to the user's preferences.
computer_architecture	In this paper, we design a recommender system for the post-purchase stage, i.e., after a user purchases a product. Our method combines both behavioral and content aspects of recommendations. We first find the most related categories for the active product in the post-purchase stage. Among these related categories, products with high behavioral relevance and content relevance are recommended to the user. In addition, our algorithm considers the temporal factor, i.e., the purchase time of the active product and the recommendation time. We apply our algorithm on a random sample of the purchase data from eBay. Comparing to the baseline item-based collaborative filtering approach, our hybrid recommender system achieves significant coverage and purchase rate gain for different time windows.
computer_architecture	Falls are a common source of serious injury for elderly people. The harm can be mitigated by using a fall detector. However they are far from ideal and produce a large number of false alarms for every real fall detected. This paper describes an experiment to examine whether the reliability of fall detectors might be improved by using photoplethysmography, which evaluates heart rate and blood flow in microvascular tissue, to make inferences about the body position. The results show a correlation between body position and pulse shape. However the effect is small and is of similar size to the effect of the position of the arm on which the sensor is mounted. Further work is needed to better understand how arm position affects pulse shape and the extent to which these results may be applicable to elderly people.
computer_architecture	Artificial neural networks are suitable for many tasks in pattern recognition and machine learning. In this paper we present an APL system for forecasting univariate time series with artificial neural networks. Unlike conventional techniques for time series analysis, an artificial neural network needs little information about the time series data and can be applied to a broad range of problems. However, the problem of network ?tuning? remains: parameters of the backpropagation algorithm as well as the network topology need to be adjusted for optimal performances. For our application, we conducted experiments to find the right parameters for a forecasting network. The artificial neural networks that were found delivered a better forecasting performance than results obtained by the well known ARIMA technique.
computer_architecture	In 802.11, all devices are uniquely identified by a Media Access Control (MAC) address. However, legitimate MAC addresses can be easily spoofed to launch various forms of attacks, such as Denial of Service attacks. Impersonating the MAC address of a legitimate user poses a big challenge for cyber crime investigators. Indeed, MAC spoofing makes the task of identifying the source of the attack very difficult. Sequence number analysis is a common technique used to detect MAC spoofing attack. Existing solutions relying on sequence number analysis, adopt a threshold-based approach where the gap between consecutive sequence numbers is compared to a threshold to decide the presence of a MAC spoofing attack. Nevertheless, threshold-based approach may lead to a high rate of false alerts due to lost or duplicated frames. To overcome the limitations of threshold-based approach, this paper proposes a detection method that relies on a machine learning approach, namely Artificial Neural Network (ANN). ANNs provide the potential to identify and classify network behavior from limited, noisy, incomplete and non-linear data sources. The experimentation results showed the effectiveness of the proposed detection technique. Moreover, we proposed a user-friendly graphical representation of information to support the interpretation of quantitative results.
computer_architecture	Identity resolution aims at identifying the newly presented facts and linking them to their previous mentions. Our main hypothesis is that variations of one and the same fact can be recognised, duplications removed and their aggregation actually increases the correctness of fact extraction. Our approach to the identity problem has been implemented as Identity Resolution Framework (IdRF). The framework provides a general solution identifying known and new facts in specific domains, and it can be used in different applications for processing of different types of entity. It uses an ontology for internal and resulting knowledge representational formalism. The ontology not only contains the representation of the domain, but also known entities and properties. Apart from extracting information from textual sources, we also exploit structured information available in databases mapping the database schema to the ontology and populating the ontology with existing knowledge. Our main goal is not to advocate one criterion among the others, but to introduce widely applicable solution of the identity resolution problem, we present a set of customisable criteria as well as a mechanism new criteria to be added. We have carried two series of experiments in two different business intelligence domains - company profiling and recruitment - achieving rather encouraging result.
computer_architecture	The significant improvements and falling costs of photovoltaic (PV) technology make solar energy a promising resource, yet the cloud induced variability of surface solar irradiance inhibits its effective use in grid-tied PV generation. Short-term irradiance forecasting, especially on the minute scale, is critically important for grid system stability and auxiliary power source management. Compared to the trending sky imaging devices, irradiance sensors are inexpensive and easy to deploy but related forecasting methods have not been well researched. The prominent challenge of applying classic time series models on a network of irradiance sensors is to address their varying spatio-temporal correlations due to local changes in cloud conditions. We propose a local vector autoregressive framework with ridge regularization to forecast irradiance without explicitly determining the wind field or cloud movement. By using local training data, our learned forecast model is adaptive to local cloud conditions and by using regularization, we overcome the risk of overfitting from the limited training data. Our systematic experimental results showed an average of 19.7% RMSE and 20.2% MAE improvement over the benchmark Persistent Model for 1-5 minute forecasts on a comprehensive 25-day dataset.
computer_architecture	In recent years, there has been considerable interest in the potential for graphics processing units (GPUs) to speed up the performance of sparse direct linear solvers. Efforts have focused on symmetric positive-definite systems for which no pivoting is required, while little progress has been reported for the much harder indefinite case. We address this challenge by designing and developing a sparse symmetric indefinite solver SSIDS. This new library-quality LDLT factorization is designed for use on GPU architectures and incorporates threshold partial pivoting within a multifrontal approach. Both the factorize and the solve phases are performed using the GPU. Another important feature is that the solver produces bit-compatible results. Numerical results for indefinite problems arising from a range of practical applications demonstrate that, for large problems, SSIDS achieves performance improvements of up to a factor of 4.6 � compared with a state-of-the-art multifrontal solver on a multicore CPU.
computer_architecture	Dantzig--Wolfe Decomposition is recognized as a powerful, algorithmic tool for solving linear programs of block-angular form. While use of the approach has been reported in a wide variety of domains, there has not been a general implementation of Dantzig--Wolfe decomposition available. This article describes an open-source implementation of the algorithm. It is general in the sense that any properly decomposed linear program can be provided to the software for solving. While the original description of the algorithm was motivated by its reduced memory usage, modern computers can also take advantage of the algorithm's inherent parallelism. This implementation is parallel and built upon the POSIX threads (pthreads) library. Some computational results are provided to motivate use of such parallel solvers, as this implementation outperforms state-of-the-art commercial solvers in terms of wall-clock runtime by an order of magnitude or more on several problem instances.
computer_architecture	We present Talbot Suite, a C parallel software collection for the numerical inversion of Laplace Transforms, based on Talbot's method. It is designed to fit both single and multiple Laplace inversion problems, which arise in several application and research fields. In our software, we achieve high accuracy and efficiency, making full use of modern architectures and introducing two different levels of parallelism: coarse and fine grained parallelism. They offer a reasonable tradeoff between accuracy, the main aspect for a few inversions, and efficiency, the main aspect for multiple inversions. To take into account modern high-performance computing architectures, Talbot Suite provides different software versions: an OpenMP-based version for shared memory machines and a MPI-based version for distributed memory machines. Moreover, oriented to hybrid architectures, a combined MPI/OpenMP-based implementation is provided too. We describe our parallel algorithms and the software organization. We also report some performance results. Our software includes sample programs to call the Talbot Suite functions from C and from MATLAB.
computer_architecture	In this article we study the decidability of termination of several variants of simple integer loops, without branching in the loop body and with affine constraints as the loop guard (and possibly a precondition). We show that termination of such loops is undecidable in some cases, in particular, when the body of the loop is expressed by a set of linear inequalities where the coefficients are from Z ? {r} with r an arbitrary irrational; when the loop is a sequence of instructions, that compute either linear expressions or the step function; and when the loop body is a piecewise linear deterministic update with two pieces. The undecidability result is proven by a reduction from counter programs, whose termination is known to be undecidable. For the common case of integer linear-constraint loops with rational coefficients we have not succeeded in proving either decidability or undecidability of termination, but we show that a Petri net can be simulated with such a loop; this implies some interesting lower bounds. For example, termination for a partially specified input is at least EXPSPACE-hard.
computer_architecture	Search engine ad auctions typically have a significant fraction of advertisers who are budget constrained, i.e., if allowed to participate in every auction that they bid on, they would spend more than their budget. This yields an important problem: selecting the ad auctions which these advertisers participate, in order to optimize different system objectives such as the return on investment for advertisers, and the quality of ads shown to users. We present a system and algorithms for optimizing budget constrained spend. The system is designed be deployed in a large search engine, with hundreds of thousands of advertisers, millions of searches per hour, and with the query stream being only partially predictable. We have validated the system design by implementing it in the Google ads serving system and running experiments on live traffic. We have also compared our algorithm to previous work that casts this problem as a large linear programming problem limited to popular queries, and show that our algorithms yield substantially better results.
computer_architecture	We consider shared-object systems that require their threads to fulfill the system jobs by first acquiring sequentially the objects needed for the jobs and then holding on to them until the job completion. Such systems are in the core of a variety of shared-resource allocation and synchronization systems. This work opens a new perspective to study the expected job delay and throughput analytically, given the possible set of jobs that may join the system dynamically. We identify the system dependencies that cause contention among the threads as they try to acquire the job objects. We use these observations to define the shared-object system equilibria. We note that the system is in equilibrium whenever the rate in which jobs arrive at the system matches the job completion rate. These equilibria consider not only the job delay but also the job throughput, as well as the time in which each thread blocks other threads in order to complete its job. We then further study in detail the thread work cycles and, by using a graph representation of the problem, we are able to propose procedures for estimating equilibria, i.e., discovering the job delay and throughput, as well as the blocking time. To the best of our knowledge, this is a new perspective, that can provide better analytical tools for the problem, in order to estimate performance measures similar to ones that can be acquired through experimentation on working systems and simulations.
computer_architecture	Existing compact routing schemes, e.g., Thorup and Zwick [SPAA 2001] and Chechik [PODC 2013], often have no means to tolerate failures, once the system has been setup and started. This paper presents, to our knowledge, the first self-healing compact routing scheme. Besides, our schemes are developed for low memory nodes, i.e., nodes need only O(log2 n) memory, and are thus, compact schemes. We introduce two algorithms of independent interest: The first is CompactFT, a novel compact version (using only O(log n) local memory) of the self-healing algorithm Forgiving Tree of Hayes et al. [PODC 2008]. The second algorithm (CompactFTZ) combines CompactFT with Thorup-Zwick's tree-based compact routing scheme [SPAA 2001] to produce a fully compact self-healing routing scheme. In the self-healing model, the adversary deletes nodes one at a time with the affected nodes self-healing locally by adding few edges. CompactFT recovers from each attack in only O(1) time and ? messages, with only +3 degree increase and O(log?) graph diameter increase, over any sequence of deletions (? is the initial maximum degree). Additionally, CompactFTZ guarantees delivery of a packet sent from sender s as long as the receiver t has not been deleted, with only an additional O(y log?) latency, where y is the number of nodes that have been deleted on the path between s and t. If t has been deleted, s gets informed and the packet removed from the network.
computer_architecture	Automated tools for discourse analysis process tremendous amounts of computer-mediated discourses in a fast way. Assigning receivers to each message is an important step to answer the question "Who is communicating with whom" at any time. Direct addressing helps but is not used in every message. The focus of this paper is mainly split into two parts. First, automated detection and mapping of written receivers (or parts of them) to logged-in users. Second, if no receiver is written, automated receiver guessing without semantics. The architecture of the automated software is described in detail. As an applied example, the well-known and text-based chat system Internet Relay Chat (IRC) is used. An IRC discourse with 5605 messages is manually and automatically analyzed for comparing both approaches. Both - detection and guessing - are similarly well done as by hand.
computer_architecture	This paper describes an open source framework for analysing large volume social media content, which comprises semantic annotation, Linked Open Data, semantic search, dynamic result aggregation, and information visualisation. In particular, exploratory search and sense-making are supported through information visualisation interfaces, such as co-occurrence matrices, term clouds, treemaps, and choropleths. There is also an interactive semantic search interface (Prospector), where users can save, refine, and analyse the results of semantic search queries over time. These functionalities are presented in more detail in the context of analysing tweets from UK politicians and party candidates in the run up to the 2015 UK general election.
computer_architecture	To publish information extracted from multilingual pages of Wikipedia in a structured way, the Semantic Web community has started an effort of internationalization of DBpedia. Multilingual chapters of DBpedia can in fact contain different information with respect to the English version, in particular they provide more specificity on certain topics, or fill information gaps. DBpedia multilingual chapters are well connected through instance interlinking, extracted from Wikipedia. An alignment between properties is also carried out by DBpedia contributors as a mapping from the terms used in Wikipedia to a common ontology, enabling the exploitation of information coming from the multilingual chapters of DBpedia. However, the mapping process is currently incomplete, it is time consuming since it is manually performed, and may lead to the introduction of redundant terms in the ontology, as it becomes difficult to navigate through the existing vocabulary. In this paper we propose an approach to automatically extend the existing alignments, and we integrate it in a question answering system over linked data. We report on experiments carried out applying the QAKiS (Question Answering wiKiframework-based) system on the English and French DBpedia chapters, and we show that the use of such approach broadens its coverage.
computer_architecture	Interaction with a robot has been an active area of research since the inception of robotics. Talking to a robot has always been considered the most natural way to communicate with it. But it is not always possible to have a full-fledged, standalone speech processing engine to be present on a robot or on a single machine. A dedicated system to convert the commands from audio to text is needed. However, as the number of commands and robots increases, it becomes necessary to eliminate all the single-point failure points in the system. Thus, distributed speech engine comes into picture. Also users may want to talk to the robot in different languages. The approach proposed in this paper is distributed, fault tolerant and scalable, such that any new recognition algorithm or language support can be added and used without any changes to the existing system. The work has been demonstrated on a freely available mobile agents based Internet of Things platform. However, any platform can be used.
computer_architecture	In news stories verbatim quotes of persons play a very important role, as they carry reliable information about the opinion of that person concerning specific aspects. As thousands of new quotes are published every hour it is very difficult to keep track of them. In this paper we describe a set of algorithms to solve the knowledge management problem of identifying, storing and accessing verbatim quotes. We handle the verbatim quote task as a relation extraction problem from unstructured text. Using a workflow of knowledge extraction algorithms we provide the required features for the relation extraction algorithm. The central relation extraction procedures is trained using manually annotated documents. It turns out that structural grammatical information is able to improve the F-vale for verbatim quote detection to 84.1%, which is sufficient for many exploratory applications. We present the results in a smartphone app connected to a web server, which employs a number of algorithms like linkage to Wikipedia, topics extraction and search engine indices to provide a flexible access to the extracted verbatim quotes.
computer_architecture	Recognizing, identifying and extracting entities, like Person, Location and Organization are useful for information mining from unstructured texts. Currently, it is a typical way of establishing the content for future use such as filtering, indexing or search. This paper presents the results obtained for the benchmark test on our Text Annotation Engine (T-ANNE) that we have developed against several similar systems. Precision, Recall and F-Measure will be used to measure the results for this evaluation.
computer_architecture	The sudden fall of blood pressure (hypotension) is a common complication in medical care. In critical care patients, hypotension (HT) may cause serious heart, endocrine or neurological disorders, inducing severe or even lethal events. Moreover, recent studies report an increase of mortality in HT prone hemodialysis patients in need of critical care. If HT could be predicted in advance, medical staff could take action to minimize its effects, or even avoid its occurrence. Typically, most medical systems have focused on monitoring and detecting current patient status, rather than determining biosignal trends or predicting a patient's future status. Therefore, predicting HT episodes in advance remains a challenge. Furthermore, since critical care actions such as hemodialysis are oftenly inconvenient and uncomfortable procedures, HT prediction or detection methods should be non-invasive, whenever possible. In this paper, we present a solution for continuous monitorization and prediction of HT episodes, using heart rate (HR) and mean blood pressure (BP) non-invasive measured biosignals. We propose an architecture for a HT Predictor (HTP) Tool, presenting a set of tools and a real-time database capable of continuously storing and real-time monitoring all patient's historical HR and BP biosignal data, and efficiently alerting both probable and detected occurrences of HT episodes for each patient for the following 60 minutes. Additionally, the system promotes medical staff mobility, by taking advantage of using mobile personal devices such as mobile phones and PDA's, optimizing human resources. Finally, an experimental evaluation on real-life data from the well known Physionet database shows the efficiency of the tool, outperforming the winning proposal of the Physionet 2009 Challenge.
computer_architecture	Automatic text segmentation, which is the task of breaking a text into topically-consistent segments, is a fundamental problem in Natural Language Processing, Document Classification and Information Retrieval. Text segmentation can significantly improve the performance of various text mining algorithms, by splitting heterogeneous documents into homogeneous fragments and thus facilitating subsequent processing. Applications range from screening of radio communication transcripts to document summarization, from automatic document classification to information visualization, from automatic filtering to security policy enforcement - all rely on, or can largely benefit from, automatic document segmentation. In this article, a novel approach for automatic text and data stream segmentation is presented and studied. The proposed automatic segmentation algorithm takes advantage of feature extraction and unusual behaviour detection algorithms developed in [4, 5]. It is entirely unsupervised and flexible to allow segmentation at different scales, such as short paragraphs and large sections. We also briefly review the most popular and important algorithms for automatic text segmentation and present detailed comparisons of our approach with several of those state-of-the-art algorithms.
computer_architecture	Fully automatic machine translation cannot produce high quality translation; Dialog-Based Machine Translation (DB-MT) is the only way to provide authors with a means of translating documents in languages they have not mastered, or do not even know. With such environment, the author must help the system to "understand" the document by means of an interactive disambiguation step. In this paper we study the consequences of integrating the DBMT services within a structured document editor (Amaya). The source document (named edited document) needs a companion document enriched with different data produced during the interactive translation process (question trees, answers of the author, translations). The edited document also needs to be enriched (annotated) in order to enable access to the question trees. The enriched edited document and the companion document have to be synchronized in case the edited document is further updated.
computer_architecture	A new global optimization algorithm for functions of continuous variables is presented, derived from the �Simulated Annealing� algorithm recently introduced in combinatorial optimization. The algorithm is essentially an iterative random search procedure with adaptive moves along the coordinate directions. It permits uphill moves under the control of a probabilistic criterion, thus tending to avoid the first local minima encountered. The algorithm has been tested against the Nelder and Mead simplex method and against a version of Adaptive Random Search. The test functions were Rosenbrock valleys and multiminima functions in 2,4, and 10 dimensions. The new method proved to be more reliable than the others, being always able to find the optimum, or at least a point very close to it. It is quite costly in term of function evaluations, but its cost can be predicted in advance, depending only slightly on the starting point.
computer_architecture	This paper describes a lightweight software library to solve the challenges [5, 3, 1, 4, 2] of programming storage class memory (SCM). It provides primitives to demarcate failure-atomic code regions. SCM loads and stores within each de-marcated code region (called a "wrap") are routed through the library, which buffers updates and transmits them to SCM locations asynchronously while allowing their speedy propagation from writers to readers through CPU caches.
computer_architecture	UNIX is a general-purpose, multi-user, interactive operating system for the Digital Equipment Corporation PDP-1 1/40 and 11/45 computers. It offers a number of features seldom found even in larger operating systems, including 1. A hierarchical file system incorporating demountable volumes, 2. Compatible file, device, and inter-process I/O, 3. The ability to initiate asynchronous processes, 4. System command language selectable on a per-user basis, 5. Over 100 subsystems including a dozen languages. This paper discusses the usage and implementation of the file system and of the user command interface.
computer_architecture	This paper describes a drum space allocation and accessing strategy called ?folding?, whereby effective drum storage capacity can be traded off for reduced drum page fetch time. A model for the ?folded drum? is developed and an expression is derived for the mean page fetch time of the drum as a function of the degree of folding. In a hypothetical three-level memory system of primary (directly addressable), drum, and tertiary (usually disk) memories, the tradeoffs among drum storage capacity, drum page fetch time, and page fetch traffic to tertiary memory are explored. An expression is derived for the mean page fetch time of the combined drum-tertiary memory system as a function of the degree of folding. Measurements of the MULTICS three-level memory system are presented as examples of improving multi-level memory performance through drum folding. A methodology is suggested for choosing the degree of folding most appropriate to a particular memory configuration.
computer_architecture	Applications frequently request file system operations that traverse the file system directory tree, such as opening a file or reading a file's metadata. As a result, caching file system directory structure and metadata in memory is an important performance optimization for an OS kernel. This paper identifies several design principles that can substantially improve hit rate and reduce hit cost transparently to applications and file systems. Specifically, our directory cache design can look up a directory in a constant number of hash table operations, separates finding paths from permission checking, memoizes the results of access control checks, uses signatures to accelerate lookup, and reduces miss rates through caching directory completeness. This design can meet a range of idiosyncratic requirements imposed by POSIX, Linux Security Modules, namespaces, and mount aliases. These optimizations are a significant net improvement for real-world applications, such as improving the throughput of the Dovecot IMAP server by up to 12% and the updatedb utility by up to 29%.
computer_architecture	Monitoring and troubleshooting distributed systems is notoriously difficult; potential problems are complex, varied, and unpredictable. The monitoring and diagnosis tools commonly used today -- logs, counters, and metrics -- have two important limitations: what gets recorded is defined a priori, and the information is recorded in a component- or machine-centric way, making it extremely hard to correlate events that cross these boundaries. This paper presents Pivot Tracing, a monitoring framework for distributed systems that addresses both limitations by combining dynamic instrumentation with a novel relational operator: the happened-before join. Pivot Tracing gives users, at runtime, the ability to define arbitrary metrics at one point of the system, while being able to select, filter, and group by events meaningful at other parts of the system, even when crossing component or machine boundaries. We have implemented a prototype of Pivot Tracing for Java-based systems and evaluate it on a heterogeneous Hadoop cluster comprising HDFS, HBase, MapReduce, and YARN. We show that Pivot Tracing can effectively identify a diverse range of root causes such as software bugs, misconfiguration, and limping hardware. We show that Pivot Tracing is dynamic, extensible, and enables cross-tier analysis between inter-operating applications, with low execution overhead.
computer_architecture	Multicore platforms are moving from small numbers of homogeneous cores to 'scale out' designs with multiple tiles or 'islands' of cores residing on a single chip, each with different resources and potentially controlled by their own resource managers. Applications running on such machines, however, operate across multiple such resource islands, and this also holds for global properties like platform power caps. The inTune software architecture meets the consequent need to support platform-level application requirements and properties. It (i) provides the base coordination abstractions needed for realizing platform-global resource management and (ii) offers management overlays that make it easy to implement diverse per-application and platform-centric management policies. A Xen hypervisor-level implementation of inTune supports policies that can (i) pro-actively prepare for increased or decreased resource usage when the inter-island dependencies of applications are known, or (ii) re-actively respond to monitored overloads, threshold violations or similar. Experimental evaluations on a larger-scale multi-core platform demonstrate that its use leads to notable performance and resource utilization gains: such as a reduction in the variability across request response times for a three-tier web server by up to 40%, and completion time gains of 15% for parallel benchmarks.
computer_architecture	We analyze the I/O behavior of iBench, a new collection of productivity and multimedia application workloads. Our analysis reveals a number of differences between iBench and typical file-system workload studies, including the complex organization of modern files, the lack of pure sequential access, the influence of underlying frameworks on I/O patterns, the widespread use of file synchronization and atomic operations, and the prevalence of threads. Our results have strong ramifications for the design of next generation local and cloud-based storage systems.
computer_architecture	In recent years, many high-level synchronization constructs have been proposed. Each claims to satisfy criteria such as expressive power, ease of use, and modifiability. Because these terms are so imprecise, we have no good methods for evaluating how well these mechanisms actually meet such requirements. This paper presents a methodology for performing such an evaluation. Synchronization problems are categorized according to some basic properties, and this categorization is used in formulating more precise definitions of the criteria mentioned, and in devising techniques for assessing how well those criteria are met.
computer_architecture	Functional annotation of newly sequenced genomes is one of the major challenges in modern biology. With modern sequencing technologies, the PSU (Protein Sequence Universe) expands exponentially. Newly sequenced bacterial genomes alone contain over 7.5 million proteins. The rate of data generation has far surpassed that of protein annotation. The volume of protein data makes manual curation infeasible whereas a high compute cost limits the utility of existing automated approaches. In this study, we built an automated workflow to enable large-scale protein annotation into existing orthologous groups using HPC (High Performance Computing) architectures. We developed a low complexity classification algorithm to assign proteins into bacterial COGs (Clusters of Orthologous Groups of proteins). Based on the PSI-BLAST (Position-Specific Iterative Basic Local Alignment Search Tool), the algorithm was validated on simulated and archaeal data to ensure at least 80% specificity and sensitivity. The workflow with highly scalable parallel applications for classification and sequence alignment was developed on XSEDE (Extreme Science and Engineering Discovery Environment) supercomputers. Using the workflow, we have classified one million newly sequenced bacterial proteins. With the rapid expansion of the PSU, the proposed workflow will enable scientists to annotate big genome data.
computer_architecture	We present a study involving 160 participants investigating the effect of associating professional status and ethnicity with an agent by manipulating its appearance, language, and level of education. We aim to discern perceptions of status and ethnicity with respect to participants' cultural background by inviting participants from two different cultural groups (Middle Eastern and Western) to take part in our study. Results revealed that participants' cultural background had a strong impact on their ratings of the agent and its message. However, neither the agent's portrayed status nor its ethnicity appeared to have an effect on participants' perceptions of the agent. We further found that participants from both cultural backgrounds holding a negative attitude towards robots in general tend to perceive the presented message by the agent more negatively. Middle Eastern participants had a more positive attitude towards robotic agents than Western participants, which might have been the main influence on their perception of the message presented by the agent. In addition, participants who identified the agent as a member of their own cultural group perceived the presented message more positively than those from the other cultural group. We discuss our results with an intention to inform design implications for agents in a cross-cultural context.
computer_architecture	We propose a model of vicarious reinforcement in rule-based learning agents. The influence of this reinforcement is investigated in a population where a law is enforced ex ante. The norm-governed population of learning agents is formalised and simulated in an executable probabilistic rule-based argumentation framework. Vicarious experiences are expressed with rules and their learning effects are integrated into reinforcement learning. So, agents learn not only from their own experiences but also by taking into account the experiences of others. We show that simulation results differ from traditional calculus based on expected utilities.
computer_architecture	NOTE FROM ACM: It has been determined that this paper plagiarized earlier works. Therefore ACM has shut off access to this paper.
computer_architecture	This paper compares and analyzes different Segmentation techniques that are used in pattern analysis and machine intelligence. Comparing motion segmentation, script-independent text line segmentation in freestyle handwritten documents and combined top-down and bottom up segmentation based on density estimation and state of the art image segmentation technique. It presents experimental results and quantitative evaluation, demonstrating the resulting approach is effective for very challenging data. The main novel aspects of this work are the fragment learning phase, which efficiently learns the figure-ground labeling of segmentation fragments, even in training sets with high object and background variability; combining the resulting top-down segmentation with bottom-up criteria; and the use of segmentation to improve recognition.
computer_architecture	The IP Multimedia Subsystem (IMS) defines a network that has an all-IP core. This enables it to provide convergence across multimedia applications. Legacy networks like PSTN, PLMN are being rapidly phased out and an all-IP based flat network is close to realization. Nevertheless, legacy networks stil form an important part of Telecommunication domain. The IP Multimedia Subsystem supports such legacy networks and provides enhanced data and voice services based on an IP network. It has been conceived for telecom operators willing to provide advanced services on top of mobile and fixed networks. It aims to achieve the converged communications paradigm by merging the Internet and Telecommunication domains. IMS not only offers rich multimedia content along with voice and data services but also has provisions for service integration to provide a seamless and user-centric experience. Session Initiation Protocol (SIP) has been standardized as the signalling protocol tobe used in the IMS. SIP allows for ubiquitous access to many useful Internet services as well as several new and Innovative ones such as Presence, Push-to-Talk etc. This paper is a brief review of IMS and its core components. It also presents a high level overview of the IMS services and the security aspect in IMS.
computer_architecture	This paper explores the principles of a valuable educational tool - Intelligent Tutoring System (ITS) -- an application of Artificial Intelligence for the education domain. The paper discusses principles and state-of-the-art models of few ITS, the problems associated with Knowledge Engineering based ITS and provides a possible architecture to blend concepts of reciprocal tutoring with ITS. Some empirical work is necessary as proof-of-concept for the proposed architecture.
computer_architecture	The demand and usage of web application are increasing exponentially. Generally, these web applications are dynamic in nature. These applications retrieve the data from databases based on user's requests. The high demand servers receive thousands of requests in a second. This overloads the database and degrades the overall performance of the web applications. If the database and web server are implemented on the same server, the database may become the bottleneck. Hence, these applications are implemented as multitier application. In multitier web applications and web services the web based frontend, business logic and databases are deployed over the different servers. Although this will introduce the network delay as web server, application server and database server are connected through network. To avoid the network delay and to improve the performance of multitier applications a new distributed hash-based database cache implementation is presented. Centralized control of the distributed cache is implemented for overall improved performance.
computer_architecture	There has been a great interest in publish/subscribe systems in recent years. This interest, coupled with the pervasiveness of lightweight electronic devices, such as cellular phones and PDAs, have opened a new arena in publish/subscribe networks. Currently, many broker overlay networks are static and never change in structure. Often, a network overlay structure is predefined or manually manipulated. We present a dynamic broker network in the context of disseminating critical labs and patient information in a Healthcare Information System (HIS). Our work builds upon previous network optimization research on ad-hoc publish/subscribe networks. Our framework utilizes user-defined heuristic cost functions to satisfy QoS constraints. We also address certain reliability issues by providing a scheduling algorithm to selectively retransmit information.
computer_architecture	Trusted resource dissemination over Internet is still an open problem. However, it is crucial to the success of Internet-based or Internetware systems. We propose an approach that supports trusted resource dissemination between Internetware nodes. In this paper, we focus on an evidence-based trustworthiness-assurance mechanism in the approach. An architecture and a protocol that enforces this approach in Internetware systems are also described.
computer_architecture	After decades of engineering development and infrastructural investment, Internet connections have become a commodity product in many countries, and Internetscale "cloud computing" has started to compete with traditional software business through its technological advantages and economy of scale. Cloud computing is a promising enabling technology of Internetware. One distinct characteristic of cloud computing is the global integration of data, logic, and users, but such integration magnifies a sharp concern about privacy, which is one of the most frequently cited reasons by enterprises for not migrating to cloud-based solutions. We argue that cloud-based systems should include privacy as a fundamental design goal, and that privacy in a cloud environment is bidirectional, covering both end users and application providers. End users need privacy-aware software services that prevent their private data from being exposed to other users or the cloud providers. Application providers need a privacy-protected testing methodology to prevent the companies' internal activities and product features from leaking to external users. Focusing on privacy protection, we discuss the research challenges in this unique design space, and explore potential solutions for enhancing privacy protection in several important components of the system.
computer_architecture	With Graphics Processing Units (GPUs) becoming more and more popular in general purpose computing, more attentions have been paid on building a framework to provide convenient interfaces for GPU programming. MapReduce can greatly simplify the programming for data-parallel applications in cloud computing environment, and it is also naturally suitable for GPUs. However, there are some problems in recent reduction-based MapReduce implementation on GPUs. Its performance is dramatically degraded when handling massive distinct keys because the massive data cannot be stored in tiny shared memory entirely. A new MapReduce framework on GPUs, called Jupiter, is proposed with continuous reduction structure. Two improvements are supported in Jupiter, a multi-level reduction scheme tailored for GPU memory hierarchy and a frequency-based cache policy on key-value pairs in shared memory. Shared memories are utilized efficiently for various data-parallel applications whether involving little or abundant distinct keys. Experiments show that Jupiter can achieve up to 3x speedup over the original reduction-based GPU MapReduce framework on the applications with lots of distinct keys.
computer_architecture	In this paper is described Olimpo: a retrieval system for UN Security Council (SC) resolutions developed using a methodology called Contextual Structured Search ?CSS [3], a process model for automated representation and extraction of the knowledge.
computer_architecture	Database fragmentation is a promising approach that can be used in combination with encryption to achieve secure data outsourcing which allows clients to securely outsource their data to remote untrusted server(s) while enabling query support using the outsourced data. Given a set of confidentiality constraints, it vertically partitions the database into fragments such that the set of attributes in each constraint do not appear together in any one fragment. The optimal fragmentation problem is to find a fragmentation with minimum cost for query support. In this paper, we propose an efficient graph search based approach which obtains near optimal fragmentation. We model the fragmentation search space as a graph and propose efficient search algorithms on the graph. We present static and dynamic search strategies as well as a novel level-wise graph expansion technique which dramatically reduces the search time. Extensive experiments showed that our method significantly outperforms other state-of-the-art methods.
computer_architecture	One of the most natural forms of communication tool used by human-being is their voice or speech. Hence it is natural that a lot of research has been devoted to analyzing and understanding human speech for various applications. This paper describes hardware based speech recognition system. For speech recognition systems, speech is an important form of man-machine interface. The proposed system design is used to control room appliances through speech. The system is designed using voice IC HM2007, the microcontroller 89s51, HY 6264 (CMOS SRAM, ROM, latches, relays and display. The designed system is completely assembled and easy to train and use.
computer_architecture	Recent developments in embedded systems open up a new realm of computer vision applications in surveillance and healthcare delivery, etc. However, such applications require high computational resources, which is challenging for embedded devices that are characterized by constrained processing power and limited memory capacity. In this paper we present a novel intelligent framework that enables building computer vision applications with optimal throughput and maintaining the performance of such applications at run-time.
computer_architecture	The possibility to run into a deadlock is an annoying and commonly occurring hazard associated with the concurrent execution of programs. In this paper we present a polymorphic type and effect system that can be used to dynamically avoid deadlocks, guided by information about the order of lock and unlock operations which is computed statically. In contrast to most other type-based approaches to deadlock freedom, our system does not insist that programs adhere to a strict lock acquisition order or use locking primitives in a block-structured way. Lifting these restrictions is primarily motivated by our desire to target low-level languages, such as C with pthreads, but it also allows our system to be directly applicable in optimizing compilers for high-level languages, such as Java. To show the effectiveness of our approach, we have also developed a tool that uses static analysis to instrument concurrent programs written in C/pthreads and then links these programs with a run-time system that avoids possible deadlocks. Although our tool is still in an early development stage, in the sense that currently its analysis only handles a limited class of programs, our benchmark results are very promising: they show that it is not only possible to avoid all deadlocks with a small run-time overhead, but also often achieve better throughput in highly concurrent programs by naturally reducing lock contention.
computer_architecture	In this paper we present Singleton, a dependently typed assembly language. Based upon the calculus of inductive constructions, Singleton's type system allows procedures abstracting over terms, types, propositions, and proof terms. Furthermore, Singleton includes generalised singleton types. In addition to the primitive singleton types of other languages, these generalised singleton types allow the values from arbitrary inductive types to be associated with the contents of registers and memory locations. Along with Singleton's facility for term and proof abstraction, generalised singleton types allow strong statements to be made about the functional behaviour of Singleton programs. We have formalised basic properties of Singleton's type system, namely type safety and a type erasure property, using the Coq proof assistant.
computer_architecture	Reasoning about the correctness of multithreaded programs is complicated by the potential for unexpected interference between threads. Previous work on controlling thread interference focused on verifying race freedom and/or atomicity. Unfortunately, race freedom is insufficient to prevent unintended thread interference. The notion of atomic blocks provides more semantic guarantees, but offers limited benefits for non-atomic code and it requires bi-modal sequential/multithreaded reasoning (depending on whether code is inside or outside an atomic block). This paper proposes an alternative strategy that uses yield annotations to control thread interference, and we present an effect system for verifying the correctness of these yield annotations. The effect system guarantees that for any preemptively-scheduled execution of a well-formed program, there is a corresponding cooperative execution with equivalent behavior in which context switches happen only at yield annotations. This effect system enables cooperative reasoning: the programmer can adopt the simplifying assumption of cooperative scheduling, even though the program still executes with preemptive scheduling and/or true concurrency on multicore processors. Unlike bimodal sequential/multithreaded reasoning, cooperative reasoning can be applied to all program code.
computer_architecture	We propose a dynamic way to model task structures from multi viewpoints at different abstraction levels. For this, we provide a multi-view task modeling framework that defines a two-layered approach: at conceptual-level specific framework concepts for providing a conceptual foundation to model and structure tasks at different abstraction levels; and at representation-level through a formal task modeling language. The motivation behind this is decoupling the complexity of the underlying system behavior and business logic, and giving a comprehensive picture from all perspectives. The framework concepts and the language are customizable and extendible, thus enabling the framework to be used for creating task models for different purposes, from system analysis to performing usability evaluation. We provide details of a case study in which we successfully used the framework for conducting task model-based usability evaluation.
computer_architecture	Roles represent a powerful means to enable software agents to act in open environments. They can be implemented in different ways, and in this talk I will show two directions exploiting Java. The former one is quite traditional and exploits composition; the latter one adds the capabilities of roles to agents' classes in form of injected bytecode. I will compare the two approaches trying to generalize the considerations.
computer_architecture	Remote physiological monitoring of first responders can become instrumental in the quick and timely detection of the onset of harmful cardiac events. This application finds use not only for first responders but for the physically susceptible senior population and recovering patients. Fusion of multiple physiological parameters has the potential to improve the overall performance. In this demonstration, we demonstrate the improved performance achieved through the use of a novel sensor fusion software integrated with a commercial physiological monitoring system.
computer_architecture	In embedded systems, controlling a shared resource like the bus, or improving a property like power consumption, may be hard to achieve when programming device drivers individually. There is a need for global resource control, taking decisions based on a centralized view of the devices' states. In this paper, we study power consumption in sensor networks, where the nodes are small embedded systems powered by batteries. We concentrate on the hardware/software architecture of a node, where significant gains can be achieved by controlling the consumption modes of the various devices globally. The architecture we propose involves a simple adaptation of the application level, to communicate with the hardware via a control layer. The control layer itself is built from a set of simple automata: the drivers of the devices, whose states correspond to power consumption modes, and a controller that enforces global properties. All these automata are programmed using a synchronous language, whose compiler performs static scheduling and produces a single piece of C code. We explain the approach in details, demonstrate its use with either Contiki or a traditional multithreading operating system, and report on our experiments.
computer_architecture	The generation of efficient sequential code for synchronous data-flow languages raises two intertwined issues: control and memory optimization. While the former has been extensively studied, for instance in the compilation of Lustre and Signal, the latter has only been addressed in a restricted manner. Yet, memory optimization becomes a pressing issue when arrays are added to such languages. This article presents a two-level solution to the memory optimization problem. It combines a compile-time optimization algorithm, reminiscent of register allocation, paired with language annotations on the source given by the designer. Annotations express in-place modifications and control where allocation is performed. Moreover, they allow external functions performing in-place modifications to be safely imported. Soundness of annotations is guaranteed by a semilinear type system and additional scheduling constraints. A key feature is that annotations for well-typed programs do not change the semantics of the language: removing them may lead to less efficient code but will not alter the semantics. The method has been implemented in a new compiler for a LUSTRE-like synchronous language extended with hierarchical automata and arrays. Experiments show that the proposed approach removes most of the unnecessary array copies, resulting in faster code that uses less memory.
computer_architecture	Compiler controlled memories or scratchpad memories offer more predictable program execution times than cache memories. Scratchpad memories are often employed in multi-processor system-on-chip (MPSoC) platforms which seek to meet the performance needs of embedded applications while limiting power consumption and timing unpredictability. Scratchpad allocation schemes optimize performance while ensuring predictable execution times (as compared to caches). In this work, we develop a compile-time scratchpad allocation framework for multi-processor platforms, where the processors (virtually) share on-chip scratchpad space and external memory is accessed through a shared bus. Our allocation method considers the waiting time for bus access while deciding which memory blocks to load into the shared scratchpad memory space. Incorporating the bus schedule into our scratchpad allocation method leads to a global optimization of an application, as compared to employing local scratchpad allocation schemes in individual processors which locally optimize the per-processor execution time. We evaluate the efficacy, sensitivity and efficiency of our memory allocation scheme on two real-world embedded applications - an application controlling an Unmanned Aerial Vehicle (UAV), and a (fragment of) an in-orbit spacecraft software.
computer_architecture	In previous works, we have proposed a Web-based sensor network system or "Field Server" for agricultural use that has a monitoring module with a Web server, a wireless communication module to the Internet, and a camera module for image monitoring. In this paper, we have focused on providing effective, individualized and practical monitoring to provide the information farmers need, and developed an Open Field Server that enables easy and flexible monitoring of various types of agricultural fields and contexts. Then, we deployed the Open Field Server in five different practical agricultural applications: weather monitoring for growth prediction, insect counting for pest damage forecasting, crop growth monitoring with image data, wild animal surveillance, and farm operation recognition. Through these monitoring experiments, we demonstrated the potential and effectiveness of the system in a wide variety of practical agricultural applications while our experience has pointed to areas in which the system could be further refined to achieve advanced monitoring in a wide variety of situations.
computer_architecture	Systems-on-Chip (SoCs) typically implement complex applications, each consisting of multiple tasks. Several applications share the SoC cores, to reduce cost. Applications have mixed time-criticality, i.e., real-time or not, and are typically developed together with their schedulers, by different parties. Composability, i.e., complete functional and temporal isolation between applications, is an SoC property required to enable fast integration and verification of applications. To achieve composability, an Operating System (OS) allocates processor time in quanta of constant duration. The OS executes first the application scheduler, then the corresponding task scheduler, to determine which task runs next. As the OS should be a trusted code base, both inter- and intra-application schedulers should be thoroughly analysed and verified. This is required anyway for real-time intra-application schedulers. But for non-real-time applications, a costly effort is required to achieve the desired confidence level in their intra-application schedulers. In this paper we propose a light-weight, real-time OS implementation that overcomes these limitations. It separates the two arbitration levels, and requires only the inter-application scheduler to run in OS time. The intra-application scheduler runs in user time, and is therefore not trusted code. This approach allows each application to execute its own specialised task scheduler. We evaluated the practical implications of our proposal on an SoC modelled in FPGA, running an H264 and a JPEG decoder and we found that composability is preserved and performance is improved with up to 37%.
computer_architecture	Tracing is an inevitable concept for assessing system behavior and optimizing resource utilization in parallel embedded real-time architectures. In the last decades, tracing and trace-recording analysis gained increasing importance due to more complex, computation intense and especially parallel applications spread across a variety of domains. Safety requirements, load balancing, effective memory utilization, product line engineering, heterogeneous targets or demands of common standards are just some of the occurring challenges. Our work addresses iterative system augmentation among models, performance-optimizing features and tracing in order to increase system performance and allow advanced analysis processes. Along with this automatic approach, development results benefit from lower costs and efforts, optimized parallelization, more effective resource utilization and reduced time-to-markets. By replacing a chain of different tools and corresponding manual data transformation and data adaptation with our iterative and comprehensive concept, we provide an advanced, integrated and highly adaptable solution for embedded system development. Furthermore, we illustrate the utilization of vector clocks instead of timestamps for process synchronization and trace analysis. Causal order determination as well as constraint derivation are described among examples and benefits are outlined correspondingly.
computer_architecture	As the distributed resources required for the processing of High Performance Computing (HPC) applications are becoming larger in scale and computational capacity, their energy consumption has become a major concern. Therefore, there is a growing focus from both the academia and the industry on the minimization of the carbon footprint of the computational resources, especially through the efficient scheduling of the workload. In this paper, a technique is proposed for the energy-aware scheduling of bag-of-tasks applications with time constraints in a large-scale heterogeneous distributed system. Its performance is evaluated by simulation and compared with a baseline algorithm. The simulation results show that the proposed heuristic not only reduces the energy consumption of the system, but also improves its performance.
computer_architecture	Data mining is a new field of computer science with a wide range of applications. Its goal is to extract knowledge from massive datasets in a human-understandable structure, for example, the decision trees. In this article we present an innovative, high-performance, system-level architecture for the Classification And Regression Tree (CART) algorithm, one of the most important and widely used algorithms in the data mining area. Our proposed architecture exploits parallelism at the decision variable level, and was fully implemented and evaluated on a modern high-performance reconfigurable platform, the Convey HC-1 server, that features four FPGAs and a multicore processor. Our FPGA-based implementation was integrated with the widely used ?rpart? software library of the R project in order to provide the first fully functional reconfigurable system that can handle real-world large databases. The proposed system, named HC-CART system, achieves a performance speedup of up to two orders of magnitude compared to well-known single-threaded data mining software platforms, such as WEKA and the R platform. It also outperforms similar hardware systems which implement parts of the complete application by an order of magnitude. Finally, we show that the HC-CART system offers higher performance speedup than some other proposed parallel software implementations of decision tree construction algorithms.
computer_architecture	In this article, we are interested in the fact that relevance and trustworthiness of information acquired by an agent X from a source F strictly depends and derives from X's trust in F with respect to the kind of information. In particular, we are interested in analyzing the relevance of F's category as indicator for its trustworthiness with respect to the specific informative goals of X. In this article, we analyze an interactive cognitive model for searching information in a world in which each agent can be considered as belonging to a specific agent's category. We also consider variability within the canonical categorical behavior and consequent influence on the trustworthiness of provided information. The introduced interactive cognitive model also allows evaluation of the trustworthiness of a source both on the basis of its category and on past direct experience with it, thus selecting the more adequate source with respect to the informative goals to achieve. We present a computational approach based on fuzzy sets and some selected simulation scenarios together with the discussion of their more interesting results.
computer_architecture	Healthcare information systems play an important role in improving healthcare quality. As providing healthcare increasingly changes from isolated treatment episodes towards a continuous medical process involving multiple healthcare professionals and institutions, there is an obvious need for an information system to support processes and span the whole healthcare network. A suitable architecture for such an information system must take into account that it has to work as an integral part of a complex socio-technical system with changing conditions and requirements. We have surveyed the core requirements of healthcare professionals and analysed the literature for known problems and information needs. We consolidated the results to define use cases for an integrated information system as communication patterns, from which general implications on the required properties of a helathcare network information system could be derived. Key issues are flexibility, adaptability, robustness, integration of existing systems and standards, semantic compatibility, security and process orientation. Based on these results an IT architecture is being designed that is capable of addressing the requirements mostly on the basis of well-established standards and concepts.
computer_architecture	Web services promise to revolutionize the way computational resources and business processes are offered and invoked in open, distributed systems, such as the Internet. These services are described using machine-readable metadata, which enables consumer applications to automatically discover and provision suitable services for their workflows at run-time. However, current approaches have typically assumed service descriptions are accurate and deterministic, and so have neglected to account for the fact that services in these open systems are inherently unreliable and uncertain. Specifically, network failures, software bugs and competition for services may regularly lead to execution delays or even service failures. To address this problem, the process of provisioning services needs to be performed in a more flexible manner than has so far been considered, in order to proactively deal with failures and to recover workflows that have partially failed. To this end, we devise and present a heuristic strategy that varies the provisioning of services according to their predicted performance. Using simulation, we then benchmark our algorithm and show that it leads to a 700% improvement in average utility, while successfully completing up to eight times as many workflows as approaches that do not consider service failures.
computer_architecture	Localization of sensor nodes is important for wireless sensor networks (WSNs) especially for underwater WSNs (UWSNs). Among the existing UWSN localization approaches, reverse localization scheme (RLS) is an event-driven method suitable for underwater surveillance. RLS adopts the strongest arrival or the first arrival as the direct path, which is however not accurate enough due to the severe multipath effect in underwater acoustic channels. We propose median RLS (MRLS) to select the median path and weighted RLS (WRLS) by assigning each path with a possibility to be the direct path for UWSNs. Numerical results have validated the proposed schemes and further suggested that WRLS outperforms MRLS and traditional approaches and is capable to diminish the multipath effect.
computer_architecture	Inter-Component Communication (ICC) enables useful interactions between mobile apps. However, misuse of ICC exposes users to serious threats such as intent hijacking/spoofing and app collusions, allowing malicious apps to access privileged user data via another app. Unfortunately, existing ICC analyses are largely incompetent in both accuracy and scale. This poster points out the need and technical challenges of prioritized analysis of inter-app ICC risks. In this poster, we propose MR-Droid, a MapReduce-based computing framework for accurate and scalable inter-app ICC analysis in Android. MR-Droid extracts data-flow features between multiple communicating apps and the target apps to build a large-scale ICC graph. Our approach is to leverage the ICC graph to provide contexts for inter-app communications to produce precise alerts and prioritize risk assessments. This process requires large app-pair data, which is enabled by our MapReduce-based program analysis. Our initial extensive experiments on 11,996 apps from 24 app categories (13 million pairs) demonstrate the scalability of our approach.
computer_architecture	Reputation is a primary mechanism for trust management in decentralized systems. Many reputation-based trust functions have been proposed in the literature. However, picking the right trust function for a given decentralized system is a non-trivial task. One has to consider and balance a variety of factors, including computation and communication costs, scalability and resilience to manipulations by attackers. Although the former two are relatively easy to evaluate, the evaluation of resilience of trust functions is challenging. Most existing work bases evaluation on static attack models, which is unrealistic as it fails to reflect the adaptive nature of adversaries (who are often real human users rather than simple computing agents). In this paper, we highlight the importance of the modeling of adaptive attackers when evaluating reputation-based trust functions, and propose an adaptive framework - called COMPARS - for the evaluation of resilience of reputation systems. Given the complexity of reputation systems, it is often difficult, if not impossible, to exactly derive the optimal strategy of an attacker. Therefore, COMPARS takes a practical approach that attempts to capture the reasoning process of an attacker as it decides its next action in a reputation system. Specifically, given a trust function and an attack goal, COMPARS generates an attack tree to estimate the possible outcomes of an attacker's action sequences up to certain points in the future. Through attack trees, COMPARS simulates the optimal attack strategy for a specific reputation function f, which will be used to evaluate the resilience of f. By doing so, COMPARS allows one to conduct a fair and consistent comparison of different reputation functions.
computer_architecture	Both security and efficiency are crucial to the success of cloud storage. So far, security and efficiency of cloud storage have been separately investigated as follows: On one hand, security notions such as Proof of Data Possession (PDP) and Proof of Retrievability (POR) have been introduced for detecting that the data stored in the cloud has been tampered with. On the other hand, the notion of Proof of Ownership (POW) has also been proposed to alleviate the cloud server from storing multiple copies of the same data, which could substantially reduce the consumption of both network bandwidth and server storage space. These two aspects are seemingly quite to the opposite of each other. In this paper, we show, somewhat surprisingly, that the two aspects can actually co-exist within the same framework. This is possible fundamentally because of the following insight: The public verifiability offered by PDP/POR schemes can be naturally exploited to achieve POW. This "one stone, two birds" phenomenon not only inspired us to propose the novel notion of Proof of Storage with Deduplication (POSD), but also guided us to design a concrete scheme that is provably secure in the Random Oracle model based on the Computational Diffie-Hellman (CDH) assumption.
computer_architecture	Reputation mechanisms represent a major class of techniques for managing trust in decentralized systems. Quite a few reputation-based trust functions have been proposed in the literature for use in many different application domains. However, in many situations, one cannot always obtain all of the information required by the trust evaluation process. For example, access control restrictions or high collection costs might limit one's ability to gather every possible feedback that could be aggregated. Thus, one key question is how to analytically quantify the quality of reputation scores computed using incomplete information. In this paper, we start a first effort towards answering the above question by studying the following problem: given the existence of certain missing information, what are the worst and best trust scores (i.e., the bounds of trust) a target entity can be assigned by a given reputation function? We formulate this problem based on a general model of reputation systems, and then examine the ability to bound a collection representative trust functions in the literature. We show that most existing trust functions are monotonic in terms of direct missing information about the target of a trust evaluation, which greatly simplifies this process. The problem of trust bounding with the presence of indirect missing information is much more complicated. We show that many well-known trust functions are not monotonic regarding indirect missing information, which means that a case-by-case analysis needs to be conducted for each trust function in order to bound an entity's trust.
computer_architecture	Existing approaches to replication based middleware suffers from performance limitations with the additional traffic in the Grid. Hence the proposed approach provides a replication middleware which offers highly efficient, consistent and fully replicated system for the Grid environment. This paper proposes a highly available fault tolerant Grid based replication middleware, which achieves replication of data dynamically upon incoming transaction request from the clients. The Grid based Replication Middleware (GRM) offers: Semi active replication strategy based on Total Ordering-Multicasting, SOAP based message communication, Dijkstra and Scholten election algorithm, improved Membership service and Hash based replica Location service to achieve high availability, heterogeneity, Fault tolerance, improved scalability, efficient data management services respectively and security features like authentication, authorization, message confidentiality are also addressed. Proposed GRM reduces the system overhead by reducing the traffic in the Grid and thereby improving the performance of data management for large scale complex Grid based applications.
computer_architecture	Cloud computing these days is here. Running applications on machines in an internet accessible data center can bring plenty of advantages. Yet to run an application we always need a platform. For many applications this platform usually includes an operating system, some way to store data and more. Applications running in the cloud also need some foundation. Cloud Computing is a foundation for running applications and storing data in the cloud. Cloud Computing applications run on machines in the data centers. Rather than providing software that customers can install and run themselves on their own computers, thus it is a service; customers use it to run applications and store data on internet accessible machines owned by the service provider. Those applications might provide services to businesses, to consumers, or both. The applications that could be built like, If an independent software vendor (ISV) could create an application that targets business users, an approach that's often referred to as Software as a Service (SaaS). ISV's can use Cloud computing framework as a foundation for a variety of business-oriented SaaS applications. An ISV might create a SaaS application that targets consumers. Cloud computing framework is designed to support very scalable software, and so a firm that plans to target a large consumer market might well choose it as a platform for a new application. Enterprises might use Cloud computing framework to build and run applications that are used by their own employees.
computer_architecture	Advancements in communication technologies have given rise to Mobile Database Systems (MDS). MDS allows a mobile user to initiate transactions from anywhere and at any time and guarantees their consistency preserving execution. Transaction processing in MDS has been suggested by modifying the traditional two and three phase commit (2PC) to accommodate the limitations of a Mobile Host and wireless network like intermitted connectivity, limited power, limited memory etc. A timeout based approach "Transaction Commit on Timeout", TCOT, has been suggested which shows better performance compared to 2PC and 3PC. However, this scheme does not explicitly consider a distributed database system. We propose a scheme Distributed Transaction Commit on Timeout, DTCOT, to commit the transaction in a MDS environment with distributed database. This scheme uses a timeout to decide a commit or abort. DTCOT requires two coordinators: - one for coordinating the entire transaction execution and one, which acts as the interface to the distributed database (i.e. database coordinator). The database coordinator is required because data distribution must be transparent the end user (i.e. transaction coordinator). This is a one-phase protocol due to which the time and message complexity are much lower compared to the conventional 2 phases commit and 3 phase commit protocols. We will compare the performance with the modified 2PC and show that it performs better w.r.t. Message and time complexity.
computer_architecture	Multi-Agent Systems provide a secure environment for gathering information about customers by effective communication between agents for effective sharing of resources. In modern days, due to the enormous volume of data in applications, extraction of data becomes very difficult. A Multi-Agent System helps users to find relevant information from varied data sources. A multi agent system that fills the gap between a customer and resource provider through agent communications in the banking domain has been proposed which has a service requestor module which can access the services offered by the Multi-Agent by making request with Interface Agents. Also the Multi-Agent system is made up of a number of role based agents which provide services such as user authorization, negotiation, accessing and extracts the relevant information from databases to present them to the users. During the information access process, the role agent negotiates with the storage repository about the user access. After successful validation, the role based multi-agent system provides information to the user. The Multi-Agent System provides all the bank facilities to its customers when their authentications match, including viewing account information, performing transfer, viewing transactions and the branch information.
computer_architecture	Increasing complexity of large scale distributed systems is becoming unmanageable because of the manual style adopted for the management today. The manual management techniques are time-consuming, un-secure and more prone to errors. A new paradigm for self-management is pervading over the old manual system to begin the next generation of computing. In this paper we have, proposed and implemented the automated multiagent based approach to self-heal the grid system from faults. This approach automatically monitor and handle the faults occurred in grid environment while executing the job.
computer_architecture	Cloud computing is driving a fundamental shift in the way organizations build, deploy and use applications, and it's raising expectations on how quickly and cost-effectively new IT functionality can be made available to the business. And even though the delivery chain for these "borderless applications" now crosses organizational and geographic boundaries, users will still expect the applications to perform well, and they will hold IT accountable if they don't. For its part, IT is faced with managing an increasingly complex and diverse delivery chain, consisting of perhaps dozens of service and content providers spread around the world. Cloud computing is the set of technologies and infrastructure capabilities being offered in a utility based consumption model. Windows Azure is Microsoft's Cloud Computing offering to help its customers realize the benefits of cloud computing. This paper is an introduction on Windows Azure and provides insights into different aspects of Azure based development, especially for those who are interested in adopting Windows Azure within their Enterprise IT landscape. The key aspects we shall be discussing in this paper include overview of cloud computing, Windows Azure platform components, features, implementation, creating a cloud project in Windows Azure and developer's perspective about Azure.
computer_architecture	Routing protocols are the binding force in mobile ad hoc network (MANETs) since they facilitate communication beyond the wireless transmission range of the nodes. However, the infrastructure-less, pervasive, and distributed nature of MANETs renders them vulnerable to security threats. In this paper, we propose a novel opinion based trust-aware routing protocol (OBTRP) for MANETs to protect forwarded packets from intermediary malicious nodes. In the proposed model, each node determines the trustworthiness of the other nodes with respect to behavior observed. It calculated the direct trust by the information obtained independently of other nodes and indirect trust information obtained via opinion of other nodes. Our trust model exploits information sharing among nodes to accelerate the convergence of trust establishment procedures, yet is robust against the propagation of false trust information by malicious nodes. We present simulation results which demonstrate the effectiveness of the proposed trust model scheme in a variety of scenarios involving nodes that are malicious with respect to both packet forwarding and trust propagation.
computer_architecture	An adhoc wireless network is a self-organizing and self-configuring network with the capability of rapid deployment in response to application needs. Each host is equipped with a CSMA/CA (carrier sense multiple access with collision avoidance) transceiver. All the nodes are free to move independently and randomly without any problem. This creates the scenario of multihop, where the packets originated from the source host are relayed by several intermediate hosts before reaching the destination. Routing is the process of finding a path from a source to destination among randomly distributed routers. In this paper hybrid routing protocol called Zone Routing Protocol (ZRP) and Fisheye State Routing Protocol (FSR) are examined. The performance analysis is done using performance metrics throughput, end-to-end delay packet delivery ratio is presented using network simulator Qualnet 5.0.2. The characteristic summery is also presented.
computer_architecture	Security in infrastructure-less networks like MANETs (Mobile Ad Hoc Networks) has proven to be a challenging task. Inherently secure routing protocols is a must for operational continuity of such networks. A number of secure routing protocols based on trust have recently been proposed. These protocols use the traditional route discovery model where along with normal nodes misbehaving nodes also participate in the route discovery process to incapacitate the network. To avoid the participation of misbehaving nodes, a secure and efficient route to destination can be calculated using weighted average of the number of nodes in the route and their trust values. However, the challenge is to minimize overhead of the above weighted average based routing technique while ensuring good throughput. In this paper we propose efficient trust based multi-path dynamic source routing protocol (TMDSR). Unlike previous approaches which are based on broadcast and hence ignore the path from one hop neighbor of destination, the protocol proposed in this paper consider such path as it uses unicasting of route discovery packet from one hop neighbor of destination. Results show that our method increases the throughput of the network while minimizing the routing overhead.
computer_architecture	This paper addresses, assigning cells to switches, given that each cell is connected to only one switch and performs signal handoff between cells that are connected to same switch, particularly know as single homed simple handoff assignment problem of Cellular Mobile Network. The objective function is minimization of cost while assigning cells to switches. This assignment problem falls under the same category of Complex Integer Programming Problem, which is proved to be a NP hard problem. In this paper a nondeterministic heuristic approach based on efficient Iterative Local search has been proposed. 2-opt and 3-opt local search techniques are used. Empirical results show that the proposed heuristic produces results nearer to optimal solution as expected.
computer_architecture	Traditional packaged desktop applications, proprietary enterprise and custom built enterprise applications are soon to be replaced by web based outsourced and on-demand products and services that put the onus of building of such services and their maintenance on companies with their core competency in developing such products. Software as a Service form a very important and fast growing segment of the information technology industry, and is based on the concept of cloud computing. This paper reviews the concept of software as service, its implication in the e-Business environment, the cost effectiveness of applications built on the cloud. Also this paper contains a sample application developed using Windows� Azure� to view in detail the underlying operations in the services provided in a cloud computing environment.
computer_architecture	The content-aware load balancing strategy uses the services or content requested for the scheduling of a request. This paper considers a special scenario where the application server and database server are deployed over a Distributed Computing Environment (DCE). The content aware algorithms developed till now are specially designed for the clusters computing, but this paper considers all possible DCE like cluster, grid and cloud computing. There are two types of users in a DCE, namely the service providers and consumers. Service providers provide the services to its consumers' requests. Generally the services providers offer the computing nodes for processing of the users requests. This paper proposes a new content aware load balancing policy called workload and client aware policy (WCAP). This method is a hybrid approach of client aware policy and workload aware request distribution policy discussed below. A parameter unique and special property (USP) is defined to specify the property of the service provider's computing nodes and the consumer's requests for data. This parameter specifies the unique and special property of the requests as well as computing nodes. Based on this parameter, the scheduler decides the node that is best suitable for the processing of the requests. This load balancing strategy is implemented in a decentralized manner with low overhead. Preliminary investigation proves that the proposed method is a promising content aware load balancing.
computer_architecture	Crowdsourcing has emerged as an attractive paradigm in recent years for information collection for disaster response, which utilizes data received from the human crowd, to provide critical information collection and dissemination during emergency situations and visualize this data to generate emergency maps for the human crowd. In this paper we investigate the use of crowdsourcing mechanisms for real-time emergency response and describe our approach for developing a crowdsourcing tool that can be effectively used to formulate questions and seek answers from the human crowd using a MapReduce programming model, and integrate this information into a novel spatiotemporal data structure and create a visual emergency map. Our experimental evaluation shows that our approach is practical, efficient and can be used for applications with real-time demands.
computer_architecture	Data management in ad-hoc scenarios is an essential problem at any pervasive scenario based on cooperative device ensembles. At MuSAMA (Multimodal Smart Appliance Ensembles for Mobile Applications [6]), a project dealing with the fact of assistance in smart scenarios we believe that pervasive technologies are essential for our future everyday environments to assist its users e.g. elderlies proactive. Members (devices) of such ensembles need to be able to cooperate spontaneously and without human guidance in order to achieve their joint goal of assisting the user. One part of assistance is data provision regarding the challenges of ad-hoc dynamics. In cooperation with the Marika project (Mobile Assistance for Route Information and Electronic Health Record) as outlined in [10] we have to focus on care relative information exchange. Following Franklin [4] we believe that data management is essential to reach the goal of pervasive assistance in smart scenarios. There is one major problem we have to focus on: data loss in ad-hoc scenrios because of leaving ensemble members. To overcome this problem caching solutions like the approach of semantic caching (SC) as introduced in [2, 3] can be used. Here SCs are outlined to bridge the gap between local data management for data reuse and communication efforts. In this paper we observe the network overhead and query performance of a standard information retrieval system versus our implementation of semantic caching.
computer_architecture	HIV, tuberculosis and histoplasmosis are infectious diseases that affect millions of people world-wide. We describe our efforts to find cures for these diseases using the technique of virtual screening to identify possible inhibitors for essential proteins in these organisms using one of the XSEDE supercomputers. We have completed the virtual screens and have found promising compounds for each disease. Cell culture experiments have supported the likelihood of a number of the compounds being effective for treating both histoplasmosis and tuberculosis.
computer_architecture	Researches on Internetware have gained daily expanding attentions and interests. Internetware intends to be a framework of Web-based software development. This paper we model an Agent oriented Internetware Framework. Four principles are followed when the modeling approach is developed. They are the autonomy principle, the abstract principle, the explicitness principle and the competence principle. For conducting the Agent-oriented Internetware computing, three types of agents are needed based on these principles. They are the capability providing agents, the task planning agents and the task request agents. Different types of agents have different responsibilities in the computing framework. Base on the framework, we define the assignment problem in the collaboration of the autonomous Internetware entities, and show that its complexity is NP-complete. Then we model it as a Kripke structure with normative systems on it. This, which will help us to discuss some absorbing issues like the robustness or applying power indices on it in the future, builds the bridge between our work and researches on normative systems, games, mechanisms, etc. An approach based on negotiation is given to solve the problem.
computer_architecture	With the rapid advances of wearable sensors and wireless networks. There is a growing research interest in building Body Area Sensor Network (BSN) systems to support applications such as human activity recognition and daily health monitoring. The main challenges of building a BSN system include: First, the wearable sensor nodes are highly constrained in resources; Second, the sensor nodes are prong to failures; Third, the applications requires the sensors to work with high sampling rate which results in the heavy load on each sensor node. Beside the technical challenges, the usability of the system is also a critical issue when deployed into the the real-world environment. We present in this paper the lessons we learnt while building a real-world BSN system which aims at supporting real-time human activity recognition. We analyzed the challenges and problems in depth and proposed possible solutions. The conclusions we made in this paper can serve as the guidelines for building a BSN system in the future.
computer_architecture	The open, dynamic and uncertain internet platform requires software systems running on it should be able to probe the changes of their running environments and adapt themselves accordingly. The Internetware paradigm and the environment driven application model open a promising way to cope with these requirements. A uniform middleware for environmental context information handling is essential to achieve the environment driven application model. In this paper, we argue that mobile agent technology is competent for building such a middleware and propose the Probe system, implemented based on mobile agent technology, with respects to the environment driven application model. The Probe system provides a uniform, hierarchical, and dynamic configurable platform for environmental context information collection, dissemination and processing for Internetware applications.
computer_architecture	Online social networking platforms regularly support hundreds of millions of users, who in aggregate generate substantially more data than can be stored on any single physical server. As such, user data are distributed, or sharded, across many machines. A key requirement in this setting is rapid retrieval not only of a given user's information, but also of all data associated with his or her social contacts, suggesting that one should consider the topology of the social network in selecting a sharding policy. In this paper we formalize the problem of efficiently sharding large social network databases, and evaluate several sharding strategies, both analytically and empirically. We find that random sharding---the de facto standard---results in provably poor performance even when frequently accessed nodes are replicated to many shards. By contrast, we demonstrate that one can substantially reduce querying costs by identifying and assigning tightly knit communities to shards. In particular, our theoretical analysis motivates a novel, scalable sharding algorithm that outperforms both random and location-based sharding schemes.
computer_architecture	Wireless Community Networks (WCNs) are metropolitan-area networks with nodes owned and managed by volunteers. These networks can be used to build large scale public infrastructures for providing ubiquitous wireless broadband access through the private contributions of individual community members who use their hotspots to forward foreign traffic from and to nearby low-mobility users. We have designed and developed a prototype aggregation scheme that (1) assumes that community members are selfish and do not trust each other and uses a secure incentive technique to encourage their contribution; (2) protects the real-world identities of community providers and clients by relying only on disposable opaque identifiers (public/private key pairs); (3) is fully distributed, open to all, and does not rely on any authority to resolve disputes or to control membership; (4) applies a Quality-of-Service mechanism to protect the resources of hotspot owners and punish or reward users with different QoS levels according to their contribution; (5) is automated, using standard, widely available hardware and software that we have developed for some of the main available platforms (Linux-based WLAN access points and Windows Mobile-based cell phones). Thus, it can easily complement cellular networks in metropolitan areas where some WCNs provide wide coverage.
computer_architecture	With the recent increase of the number of Fiber-to-the-Home deployments worldwide, and the corresponding huge investment in infrastructure, there is a need to devise a migration path that assures the full future usability and enhanced performance of the installed fiber plant, possibly using emerging opto-electronic technologies. With this aim, within the IST ePhoton/One Network of Excellence, different technological solutions have been extensively analyzed and discussed. In this paper, a brief summary on access applications is presented.
computer_architecture	We describe a new architecture designed to outsource the complexity of configuring, deploying and operating network security monitoring systems. Our architecture is designed to allow the concurrent operation of best-of-breed network security applications which include flow analysis, Intrusion Detection Systems (IDS), passive Operating System (OS) fingerprinting and application-level service discovery. In addition, we introduce a new framework for the inclusion of network security intelligence data into the security event correlation. We also provide some preliminary quantification of the effectiveness of this new framework.
computer_architecture	Evaluating IDS algorithms and systems is often an ad-hoc process and makes it hard to compare evaluation results and performance of IDS systems. There is a need for divers and realistic test traffic and for developing metrics to be able to judge whether some generated traffic is a representative sample of observed traffic. In this paper, the authors propose a framework for a network traffic generator which creates diverse traffic through a variety of traffic sources and describe a working implementation of it. The lessons learned from this experience can serve as the basis to create a detailed specification for an open-source implementation of the framework.
computer_architecture	The use of wandering, mobile agents can provide a robust approach for managing, monitoring, and securing electrical distribution networks. However, the topological structure of an electrical network can affect the agents' ability to monitor the network. For example, if the multi-agent system relies on a regular inspection rate (on average, points of interest are inspected with equal frequency), then locations that are not well connected within the network will on average be inspected less frequently. This results in lower observability into network operations and it also allows issues, either simple faults or adversarial actions, to persist longer in the network before they can be identified and addressed. This paper discusses creation and use of overlay networks that create a virtual grid graph that can provide faster coverage and a more uniform average agent sampling rate. Using overlays, agents wander a virtual neighborhood consisting of points of interest that are interconnected in a regular fashion (each point has the same number of neighbors). Experimental results show that using an overlay can often provide better network coverage and a uniform inspection rate, which can improve cyber security by providing a faster detection of threats.
computer_architecture	Healthcare network and computing infrastructure is rapidly changing from closed environments to open environments that incorporate new devices and new application scenarios. Home-based healthcare is such an example of leveraging pervasive sensors and analyzing sensor data (often in real-time) to guide therapy or intervene. In this paper, we address the challenges in regulatory compliance when designing and deploying healthcare applications on a heterogeneous cloud environment. We propose CareNet framework, consisting of a set of abstraction and APIs, to allow the specification of compliance requirements. This work is a collaboration among computer scientists, medical researchers, healthcare IT and healthcare providers, and its goal is to reduce the gap between the availability of software defined infrastructure and meeting regulatory compliance in healthcare applications.
computer_architecture	Reed--Solomon (RS) codes are non-binary cyclic error correcting codes. They are block-based error correcting codes with a wide range of applications in digital communications and storage and are used in various applications that required robust and energy efficient transmissions. The proposed project model is FPGA implementation and performance analysis of the RS (n, k) codec architecture. The proposed model is to design RS codec to occupy the least amount of logic blocks, be fast and parameterizable. In the proposed model both encoder and decoder will be synthesized to other FPGA architectures.
computer_architecture	Biodiesel is a cleaner burning fuel than diesel and a suitable replacement. It is made from non-toxic, biodegradable, renewable resources. Biodiesel can be produced in many ways. The method used in the laboratory was transesterification which is actually replacement of alcohol group from an ester by another alcohol. The reaction was carried out by varying different parameters to find the best conversion of oil to biodiesel. Alkali catalysed transesterification is considered to be the best amongst all methods available for the production of biodiesel from fresh oil. It was found out that the rate of transesterification reaction in a batch reactor increases with temperature. Higher temperatures do not reduce the time to reach maximal conversion. The conversion of triglyceride, diglyceride and monoglyceride appears to be second order upto 10 minutes of reaction time. Our aim is to establish the kinetics of the reaction during transesterification and to do the modeling and predict rate equation for the reaction
computer_architecture	The mathematical model (Interval Plant) of the web guide in rolling mill is controlled using PID controller. The given interval plant is approximated to first order plus time delay with integrator (FOPTDI) system. The model reference control (MRC) method proposed by Chidambaram for design PI controllers for FOPTD systems is extended to FOPTDI systems. The performance of the closed loop system is evaluated for both the original and the approximated model is compared by simulation.
computer_security	Factorized collaborative models show a promising accuracy and scalability in recommendation systems. They employ the latent collaborative information of users and items to achieve higher accuracy of recommendation. In this paper, we propose a new approach to improve the accuracy of two well-known, highly scalable factorized models: SVD++ and Asymmetric-SVD++. These are cutting-edge factorized models that have played a key role in the Netflix prize winner's solution. We first employ collaborative information to categorize the users and items. We then discover the shared interests between these categories. Including this new information, we extend these cutting-edge models regarding two main goals: 1) to improve their recommendation accuracies; 2) to keep the extended models still scalable. Finally, we evaluate our proposed models on two recommendation datasets: MovieLens100k, and Netflix. Our experiment shows that adding the shared interests among categories into these models improves their accuracy while maintaining scalability.
computer_security	Collaborative filtering, being a popular method for generating recommendations, produces satisfying results for users by providing extremely relevant items. Despite being popular, however, this method is prone to many problems. One of these problems is popularity bias, in which the system becomes skewed towards items that are popular amongst the general user population. These 'obvious' items are, technically, extremely relevant items but fail to be novel. In this paper, we maintain using collaborative filtering methods while still managing to produce novel yet relevant items. This is achieved by utilizing the long-tailed distribution of listening behavior of users, in which their playlists are biased towards a few songs while the rest of the songs, those in the long tail, have relatively low play counts. In addition, we also apply a link analysis method to users and define links between them to create an increasingly fine-grained approach in calculating weights for the recommended items. The proposed recommendation method was available online as a user study in order to measure the relevancy and novelty of the recommended items. Results show that the algorithm manages to include novel recommendations that are still relevant, and shows the potential for a new way of generating novel recommendations.
computer_security	This workshop presents the current status related to the design, development and evaluation of recommender systems in educational settings. It emphasizes the importance of recommender systems for Technology Enhanced Learning (TEL) to support learners with personalized learning resources and suitable peer learners to improve their learning process. Moreover, it proposes a dataTEL challenge to obtain data sets from TEL applications that can be used to benchmark algorithms specifically for the TEL context.
computer_security	We present a matrix factorization model inspired by challenges we encountered while working on the Xbox movies recommendation system. The item catalog in a recommender system is typically equipped with meta-data features in the form of labels. However, only part of these features are informative or useful with regard to collaborative filtering. By incorporating a novel sparsity prior on feature parameters, the model automatically discerns and utilizes informative features while simultaneously pruning non-informative features. The model is designed for binary feedback, which is common in many real-world systems where numeric rating data is scarce or non-existent. However, the overall framework is applicable to any likelihood function. Model parameters are estimated with a Variational Bayes inference algorithm, which is robust to over-fitting and does not require cross-validation and fine tuning of regularization coefficients. The efficacy of our method is illustrated on a sample from the Xbox movies dataset as well as on the publicly available MovieLens dataset. In both cases, the proposed solution provides superior predictive accuracy, especially for long-tail items. We then demonstrate the feature selection capabilities and compare against the common case of simple Gaussian priors. Finally, we show that even without features, our model performs better than a baseline model trained with the popular stochastic gradient descent approach.
computer_security	This paper presents an interactive hybrid recommendation system that generates item predictions from multiple social and semantic web resources, such as Wikipedia, Facebook, and Twitter. The system employs hybrid techniques from traditional recommender system literature, in addition to a novel interactive interface which serves to explain the recommendation process and elicit preferences from the end user. We present an evaluation that compares different interactive and non-interactive hybrid strategies for computing recommendations across diverse social and semantic web APIs. Results of the study indicate that explanation and interaction with a visual representation of the hybrid system increase user satisfaction and relevance of predicted content.
computer_security	Social media sites accumulate a wide variety of information about users: likes and ratings, friend and follower links, annotations, posts, media uploads, just to name a few. Key challenges for recommender systems research are (a) to synthesize of all of this data into an integrated recommendation model and (b) to support a wide variety of recommendation types simultaneously (items, friends, tags, etc.) One approach that has been explored in recent research is to view this multi-faceted data as a heterogeneous network and use network-based methods of generating recommendations. However, most such approaches involve computationally-intensive model generation resulting in a single-purpose recommender system. Our approach is to create a component-based hybrid model whose components can be reused for multiple recommendation tasks. In this paper, we show how this model can be applied to heterogeneous networks.
computer_security	This paper describes and evaluates a method for presenting recommendations that will increase the efficiency of the social activity stream while preserving the users' accurate awareness of the activity within their own social networks. With the help of a content-based recommender system, the application displays the user's home timeline in Twitter as three visually distinct tiers by emphasizing more strongly those Tweets predicted to be more interesting. Pilot study participants reported that they were able to read the interesting Tweets while ignoring the others with relative ease and that the recommender accurately categorized their Tweets into three tiers.
computer_security	The emergence of what is called the social web and the continuing stream of new applications and community-based platforms including Facebook, Twitter, LinkedIn and others had a substantial impact on recommender systems research and practice over the last years in different ways. First, today's web users are more willing to share more about themselves than before the Web 2.0, thus providing more information sources that can be leveraged in the user modeling and recommendation process. Furthermore, the newly available information sources can not only be used to optimize the recommendations for an individual user, but can also help to identify more general patterns and trends in the behavior of the community that can be exploited by other applications. Second, personalization, information filtering and recommendation are often the central functionality of many of these social web based applications. On typical social networks, users for example get recommendations for news to read, songs to listen to, groups to join, friends to follow, people to connect or jobs that might be interesting. These developments lead to different challenges to be addressed in recommender systems research. On the one hand, for example, the question arises of how to effectively combine the huge variety of information sources for improved recommendations. On the other hand, regarding the new opportunities for applying recommender systems in social web environments, in many cases new techniques are required to address the particularities of the domain or to deal with scalability issues. The ACM RecSys 2014 Workshop on Recommender Systems and the Social Web aims to be a platform for researchers from academia and industry as well as for practitioners to present and discuss the various challenges and possible solutions related to all aspects of social web recommendations. The call for papers correspondingly covered a variety of topics in this area including all sorts of applications of recommender systems technology and their interfaces; collective knowledge creation and topic emergence;context-aware and group recommendation approaches; and case studies and empirical evaluations. This year's workshop was already the sixth in a series of successful workshops co-located with the ACM Conference on Recommender Systems since 2009. Again, we received several submissions from researchers from academia and industry which were thoroughly reviewed and selected for presentation at the workshop by a program committee of international experts in the field. The papers submitted to the workshop addressed a number of different topics and put forward novel proposals to build social web recommender system. In the context of applying recommendation technology to information personalization and resource ranking problems in Social Web environments, the submitted papers for example dealt with the problem of ranking community-provided product reviews based on opinion mining or with the recommendation of friends on social networks. As an example of how to leverage Social Web information to build better systems, one of the works proposed to analyze the characteristics of publicly shared music playlists to better understand how future music recommendation systems should be designed. Finally, another contribution from industry addressed challenges and lessons learned when building large-scale collaborative filtering solutions on Social Web platforms in a real-world environment.
computer_security	Recommender systems provide users with products or content intended to satisfy their information needs. The primary evaluation measures for recommender systems emphasize either the perceived relevance of the recommendations or the actions driven by those recommendations (e.g., purchases on ecommerce sites or clicks on news and social networking sites). Unfortunately, this transactional emphasis neglects the inherently interactive nature of the user experience. This tutorial explores recommendations as part of a conversation between users and systems. A conversational approach should provide transparency, control, and guidance. Transparency means that users understand why systems offer particular recommendations. Control means that users can explicitly manipulate the behavior of recommender systems based on personal needs and preferences. Guidance means that systems offers plausible and predictable next steps rather than requiring users to guess the consequences of their interactions. Finally, there are psychological factors -- in particular, the faith that users place in the recommender system's effectiveness. Since users develop mental models of recommender systems, the system should become more predictable with repeated use. The tutorial does not require any special background in interfaces or usability. Rather, it summarizes the best lessons from research and industry, offering concrete examples and practical techniques to make recommender systems most effective for users.
computer_security	In this paper, we present a system which is able to interact through natural dialogue, with PTSD patients, as well as to guide the conversation aiming to elicit enough information to make an assessment of their condition, in a manner similar to a self assessment test. Our system is able to adapt to each individual patient and can operate in two modes: one that stores information about previous sessions with a patient to provide a sense of trust and relationship; and one that does not store information to preserve anonymity.
computer_security	Context-aware music recommender systems suggest music items taking into consideration contextual conditions, such as the user mood or location, that may influence the user preferences at a particular moment. In this paper we consider a particular kind of context-aware recommendation task: selecting music suited for a place of interest (POI), which the user is visiting, and that is illustrated in a mobile travel guide. We have designed an approach for this novel recommendation task by matching music to POIs using emotional tags. In order to test our approach, we have developed a mobile application that suggests an itinerary and plays recommended music for each visited POI. The results of the study show that users judge the recommended music suited for the POIs, and the music is rated higher when it is played in this usage scenario.
computer_security	A recommender system has to collect users' preference data. To collect such data, rating or scoring methods that use rating scales, such as good-fair-poor or a five-point-scale, have been employed. We replaced such collection methods with a ranking method, in which objects are sorted according to the degree of a user's preference. We developed a technique to convert the rankings to scores based on order statistics theory. This technique successfully improved the accuracy of ranking recommended items. However, we targeted only memory-based recommendation algorithms. To test whether or not the use of ranking methods and our conversion technique are effective for wide variety of recommenders, we apply our conversion technique to model-based algorithms.
computer_security	A group may appreciate recommendations on items that fit their joint preferences. When the members' actual preferences are unknown, a recommendation can be made with the aid of collaborative filtering methods. We offer to narrow down the recommended list of items by eliciting the users' actual preferences. Our final goal is to output top-k preferred items to the group out of the top-N recommendations provided by the recommender system (k), where one of the items is a necessary winner. We propose an iterative preference elicitation method, where users are required to provide item ratings per request. We suggest a heuristic that attempts to minimize the preference elicitation effort under two aggregation strategies. We evaluate our methods on real-world Netflix data as well as on simulated data which allows us to study different cases. We show that preference elicitation effort can be cut in up to 90% while preserving the most preferred items in the narrowed list.
computer_security	Online Social Rating Networks (SRNs) such as Epinions and Flixter, allow users to form several implicit social networks, through their daily interactions like co-commenting on the same products, or similarly co-rating products. The majority of earlier work in Rating Prediction and Recommendation of products (e.g. Collaborative Filtering) mainly takes into account ratings of users on products. However, in SRNs users can also built their explicit social network by adding each other as friends. In this paper, we propose Social-Union, a method which combines similarity matrices derived from heterogeneous (unipartite and bipartite) explicit or implicit SRNs. Moreover, we propose an effective weighting strategy of SRNs influence based on their structured density. We also generalize our model for combining multiple social networks. We perform an extensive experimental comparison of the proposed method against existing rating prediction and product recommendation algorithms, using synthetic and two real data sets (Epinions and Flixter). Our experimental results show that our Social-Union algorithm is more effective in predicting rating and recommending products in SRNs.
computer_security	In this paper, we propose a novel Bayesian approach for personalized recommendations. In our approach, we model both user preferences and items under recommendation as multivariate Gaussian distributions; and make use of Normal-Inverse Wishart priors to model the recommendation agent beliefs about user types. We employ a lightweight agent-user interaction process, during which the user is presented with and asked to rate a small number of items. We then interpret these ratings in an innovative way, using them to guide a Bayesian updating process that helps us both capture a user's current mood, and maintain her overall user type. We produced several variants of our approach, and applied them in the movie recommendations domain, evaluating them on data from the MovieLens dataset. Our algorithms are shown to be competitive against a state-of-the-art method, which nevertheless requires a minimum set of ratings from various users to provide recommendations---unlike our entirely personalized approach.
computer_security	The evaluation of recommender systems in terms of ranking has recently gained attention, as it seems to better fit the top-k recommendation task than the usual ratings prediction task. In that context, several authors have proposed to consider missing ratings as some form of negative feedback to compensate for the skewed distribution of observed ratings when users choose the items they rate. In this work, we study two major biases of the selection of items: the first one is that some items obtain more ratings than others (popularity effect), and the second one is that positive ratings are observed more frequently than negative ratings (positivity effect). We present a theoretical analysis and experiments on the Yahoo! dataset with randomly selected items, which show that considering missing data as a form of negative feedback during training may improve performances, but also that it can be misleading when testing, favoring models of popularity more than models of user preferences.
computer_security	This work aims to visually describe the important concepts of a dependable computing system and the relationships between the concepts. The concept map here for dependable computing system concepts would help us for easier and meaningful understanding of this emerging important research topic of computer science and engineering. Readers here won't feel tired of reading long text horizontally lines after lines for conceptualizing this much research and interesting topic.
computer_security	Alzheimer's patients need a large amount of varied types of attention by carers, due to the disorders of memory and orientation they present. Knowledge about the particular state of each individual day-care patient is sometimes deficient, however, as assistants have no time to supervise patients' records because they do not wish to have their attention distracted from care. In this work we present a proposal to solve this problem. A complement to support the information management routine by adapting Near Field Communication Technology is offered here... By means of touching tags with mobile phones, day center assistants carry out this management. This is done via tags placed throughout the context, which means on patients, places, devices and applications. With this simple interaction, some aspects of the routine in this kind of context, such as information about the patient's basic status, orientation, door security, therapy and other activities can be dealt with. In addition, this proposal concludes with an information-filtering process whose purpose is to achieve an easy interaction method with mobile phones and provide recommendations to assistants and family. Finally, our process offers appropriate information to physicians.
computer_security	We consider the problem of jointly training structured models for extraction from multiple web sources whose records enjoy partial content overlap. This has important applications in open-domain extraction, e.g. a user materializing a table of interest from multiple relevant unstructured sources; or a site like Freebase augmenting an incomplete relation by extracting more rows from web sources. Such applications require extraction over arbitrary domains, so one cannot use a pre-trained extractor or demand a huge labeled dataset. We propose to overcome this lack of supervision by using content overlap across the related web sources. Existing methods of exploiting overlap have been developed under settings that do not generalize easily to the scale and diversity of overlap seen on Web sources. We present an agreement-based learning framework that jointly trains the models by biasing them to agree on the agreement regions, i.e. shared text segments. We present alternatives within our framework to trade-off tractability, robustness to noise, and extent of agreement enforced; and propose a scheme of partitioning agreement regions that leads to efficient training while maximizing overall accuracy. Further, we present a principled scheme to discover low-noise agreement regions in unlabeled data across multiple sources. Through extensive experiments over 58 different extraction domains, we establish that our framework provides significant boosts over uncoupled training, and scores over alternatives such as collective inference, staged training, and multi-view learning.
computer_security	With the popularity of smart phones and mobile devices, the number of mobile applications (a.k.a. "apps") has been growing rapidly. Detecting semantically similar apps from a large pool of apps is a basic and important problem, as it is beneficial for various applications, such as app recommendation, app search, etc. However, there is no systematic and comprehensive work so far that focuses on addressing this problem. In order to fill this gap, in this paper, we explore multi-modal heterogeneous data in app markets (e.g., description text, images, user reviews, etc.), and present "SimApp" -- a novel framework for detecting similar apps using machine learning. Specifically, it consists of two stages: (i) a variety of kernel functions are constructed to measure app similarity for each modality of data; and (ii) an online kernel learning algorithm is proposed to learn the optimal combination of similarity functions of multiple modalities. We conduct an extensive set of experiments on a real-world dataset crawled from Google Play to evaluate SimApp, from which the encouraging results demonstrate that SimApp is effective and promising.
computer_security	Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of data in human-interactive systems. While implicit feedback has many advantages (e.g., it is inexpensive to collect, user centric, and timely), its inherent biases are a key obstacle to its effective use. For example, position bias in search rankings strongly influences how many clicks a result receives, so that directly using click data as a training signal in Learning-to-Rank (LTR) methods yields sub-optimal results. To overcome this bias problem, we present a counterfactual inference framework that provides the theoretical basis for unbiased LTR via Empirical Risk Minimization despite biased data. Using this framework, we derive a Propensity-Weighted Ranking SVM for discriminative learning from implicit feedback, where click models take the role of the propensity estimator. In contrast to most conventional approaches to de-biasing the data using click models, this allows training of ranking functions even in settings where queries do not repeat. Beyond the theoretical support, we show empirically that the proposed learning method is highly effective in dealing with biases, that it is robust to noise and propensity model misspecification, and that it scales efficiently. We also demonstrate the real-world applicability of our approach on an operational search engine, where it substantially improves retrieval performance.
computer_security	Current intrusion detection systems (IDS) generate a large number of specific alerts, but typically do not provide actionable information. Compounding this problem is the fact that many alerts are false positive alerts. This paper applies the Cross Industry Standard Process for Data Mining (CRISP-DM) to develop an understanding of a host environment under attack. Data is generated by launching scans and exploits at a machine outfitted with a set of host-based forensic data collectors. Through knowledge discovery, features are selected to project human understanding of the attack process into the IDS model. By discovering relationships between the data collected and controlled events, false positive alerts were reduced by over 91% when compared to a leading open source IDS. This method of searching for hidden forensic evidence relationships enhances understanding of novel attacks and vulnerabilities, bolstering ones ability to defend the cyberspace domain. The methodology presented can be used to further host-based intrusion detection research.
computer_security	Finding a parking spot in a busy indoor parking lot is a daunting task. Retracing a parked vehicle can be equally frustrating. We present BluePark, a collaborative sensing mechanism using smartphone sensors to solve these problems in real-time, without any input from user. We propose a novel technique of combining accelerometer and WiFi data to detect and localize parking and un-parking events in indoor parking lot. We validate our approach at the basement parking of a popular shopping mall. The proposed method outperforms Google Activity Recognition API by 20% in detecting drive state in indoor parking lot. Our experiments show 100% precision and recall for parking and un-parking detection events at low accelerometer sampling rate of 15Hz, irrespective of phone?s position. It has a low detection latency of 20s with probability of 0.9 and good location accuracy of 10m.
computer_security	Evaluation is crucial in Information Retrieval. The Cranfield paradigm allows reproducible system evaluation by fostering the construction of standard and reusable benchmarks. Each benchmark or test collection comprises a set of queries, a collection of documents and a set of relevance judgements. Relevance judgements are often done by humans and thus expensive to obtain. Consequently, relevance judgements are customarily incomplete. Only a subset of the collection, the pool, is judged for relevance. In TREC-like campaigns, the pool is formed by the top retrieved documents supplied by systems participating in a certain evaluation task. With multiple retrieval systems contributing to the pool, an exploration/exploitation trade-off arises naturally. Exploiting effective systems could find more relevant documents, but exploring weaker systems might also be valuable for the overall judgement process. In this paper, we cast document judging as a multi-armed bandit problem. This formal modelling leads to theoretically grounded adjudication strategies that improve over the state of the art. We show that simple instantiations of multi-armed bandit models are superior to all previous adjudication strategies.
computer_security	Many data stream clustering algorithms operate in two well-defined steps: (i) online statistical data collection stage; and (ii) offline macro-clustering stage. The well-known k-means algorithm is often employed for performing the offline macro-clustering step. The conventional k-means algorithm assumes that the number of clusters (k) is defined a priori by the user. Given the difficulty of defining the value of k a priori in real-world problems, we describe a new approach that allows estimating k dynamically from streams with variable number of clusters, which is a common scenario in data with a non-stationary distribution. In addition, we combine our dynamic approach with two different strategies for initializing the centroids during the offline clustering. Analysis of results suggest that, using the dynamic approach, the method k-means++ for centroids initialization present better results.
computer_security	Recent advances have allowed Deep Spiking Neural Networks (SNNs) to perform at the same accuracy levels as Artificial Neural Networks (ANNs), but have also highlighted a unique property of SNNs: whereas in ANNs, every neuron needs to update once before an output can be created, the computational effort in an SNN depends on the number of spikes created in the network. While higher spike rates and longer computing times typically improve classification performance, very good results can already be achieved earlier. Here we investigate how Deep SNNs can be optimized to reach desired high accuracy levels as quickly as possible. Different approaches are compared which either minimize the number of spikes created, or aim at rapid classification by enforcing the learning of feature detectors that respond to few input spikes. A variety of networks with different optimization approaches are trained on the MNIST benchmark to perform at an accuracy level of at least 98%, while monitoring the average number of input spikes and spikes created within the network to reach this level of accuracy. The majority of SNNs required significantly fewer computations than frame-based ANN approaches. The most efficient SNN achieves an answer in less than 42% of the computational steps necessary for the ANN, and the fastest SNN requires only 25% of the original number of input spikes to achieve equal classification accuracy. Our results suggest that SNNs can be optimized to dramatically decrease the latency as well as the computation requirements for Deep Neural Networks, making them particularly attractive for applications like robotics, where real-time restrictions to produce outputs and low energy budgets are common.
computer_security	Many real life networks have an underlying bipartite structure based on which similarity between two nodes or data instances can be defined. For example, in the case of a document corpus, the similarity between a pair of documents can be assumed to arise from the words that co-occur in them, and this document-word co-occurrence relationship can be modeled as a bipartite graph. A document similarity graph can be obtained by taking a one-mode projection of the bipartite graph, which is a popular technique for studying similar networks which arise from bipartite structure. A graph-based clustering algorithm can then be applied to this projection graph to obtain clusters of documents. In this paper we study the use of one-mode projection of the document-word bipartite graph and the subsequent application of a modularity optimization algorithm to cluster the documents. In particular, we propose an alternative and faster algorithm, which works in two-steps: first, finding the documents that are easy to cluster, and then, assigning the remaining documents to the existing or new clusters. We show that the algorithms based on one-mode projections perform significantly better than traditional clustering approaches. In addition, our method has similar or better clustering performance than the most popular algorithm for modularity optimization, while also running four times faster.
computer_security	We present a simple method to solve spherical harmonics moment systems, such as the the time-dependent PN and SPN equations, of radiative transfer. The method, which works for arbitrary moment order N, makes use of the specific coupling between the moments in the PN equations. This coupling naturally induces staggered grids in space and time, which in turn give rise to a canonical, second-order accurate finite difference scheme. While the scheme does not possess TVD or realizability limiters, its simplicity allows for a very efficient implementation in Matlab. We present several test cases, some of which demonstrate that the code solves problems with ten million degrees of freedom in space, angle, and time within a few seconds. The code for the numerical scheme, called StaRMAP (Staggered grid Radiation Moment Approximation), along with files for all presented test cases, can be downloaded so that all results can be reproduced by the reader.
computer_security	The application of Differential Equation Interpolants (DEIs) to the visualization of the solutions to Partial Differential Equations (PDEs) is investigated. In particular, we describe how a DEI can be used to generate a fine mesh approximation from a coarse mesh approximation; this fine mesh approximation can then be used by a standard contouring function to render an accurate contour plot of the surface. However, the standard approach has a time complexity equivalent to that of rendering a surface plot, O(fm2) for each element of the coarse mesh, (where fm is the ratio of the width of the coarse mesh to the fine mesh). To address this concern three fast contouring algorithms are proposed that compute accurate contour lines directly from the DEI, and have time complexity at most O(fm) for each coarse mesh element.
computer_security	L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables. It is intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems. L-BFGS-B can also be used for unconstrained problems and in this case performs similarly to its predessor, algorithm L-BFGS (Harwell routine VA15). The algorithm is implemented in Fortran 77.
computer_security	New approximations for the inverse of the incomplete gamma function are derived, which are used to develop efficient evaluations of the inverse Poisson cumulative distribution function. An asymptotic approximation based on the standard Normal approximation is particularly good for CPUs with MIMD cores, while for GPUs and other hardware with vector units, a second asymptotic approximation based on Temme's approximation of the incomplete gamma function is more efficient due to conditional branching within each vector. The accuracy and efficiency of the software implementations is assessed on both CPUs and GPUs.
computer_security	Gossip algorithms for aggregation have recently received significant attention for sensor network applications because of their simplicity and robustness in noisy and uncertain environments. However, gossip algorithms can waste significant energy by essentially passing around redundant information multiple times. For realistic sensor network model topologies like grids and random geometric graphs, the inefficiency of gossip schemes is caused by slow mixing times of random walks on those graphs. We propose and analyze an alternative gossiping scheme that exploits geographic information. By utilizing a simple resampling method, we can demonstrate substantial gains over previously proposed gossip protocols. In particular, for random geometric graphs, our algorithm computes the true average to accuracy 1/nausing O(n1.5?log n) radio transmissions, which reduces the energy consumption by a ?nover log n factor over standard gossip algorithms.
computer_security	Needs and studies on service robot are growing with rapid market extension. Generating human-like motions on robot is one research area of human-robot interaction. There are several studies about motion generation and humanoid design. However, kinematic analysis of humanoid design for service tasks should also performed. This paper focuses on motor alignment of humanoid shoulder. After we measure human greeting gestures with 3D capture camera and predefine workspace, simulation is performed to find motor alignment on humanoid shoulder to minimize total required motor power. Result shows that there are specific alignment for designing shoulder to perform greeting tasks on small motor powers.
computer_security	Invalidity search poses different challenges when compared to conventional Information Retrieval problems. Presently, the success of invalidity search relies on the queries created from a patent application by the patent examiner. Since a lot of time is spent in constructing relevant queries, automatically creating them from a patent would save the examiner a lot of effort. In this paper, we address the problem of automatically creating queries from an input patent. An optimal query can be formed by extracting important keywords or phrases from a patent by using Key Phrase Extraction (KPE) techniques. Several KPE algorithms have been proposed in the literature but their performance on query construction for patents has not yet been explored. We systematically evaluate and analyze the performance of queries created by using state-of-the-art KPE techniques for invalidity search task. Our experiments show that queries formed by KPE approaches perform better than those formed by selecting phrases based on tf or tf-idf scores.
computer_security	The decentralized transshipment problem is a two-stage decision making problem where the companies first choose their individual production levels in anticipation of random demands and after demand realizations they pool residuals via transshipment. The coordination will be achieved if at optimality all the decision variables, i.e. production levels and transshipment patterns, in the decentralized system are the same as those of centralized system. In this paper, we study the coordination via transshipment prices. We propose a procedure for deriving the transshipment prices based on the coordinating allocation rule introduced by Anupindi et al. [1]. With the transshipment prices being set, the companies are free to match their residuals based on their individual preferences. We draw upon the concept of pair-wise stability to capture the dynamics of corresponding matching process. As the main result of this paper, we show that with the derived transshipment prices, the optimum transshipment patterns are always pair-wise stable, i.e. there are no pairs of companies that can be jointly better off by unilaterally deviating from the optimum transshipment patterns.
computer_security	Networks are a fundamental tool for modeling complex systems in a variety of domains including social and communication networks as well as biology and neuroscience. The counts of small subgraph patterns in networks, called network motifs, are crucial to understanding the structure and function of these systems. However, the role of network motifs for temporal networks, which contain many timestamped links between nodes, is not well understood. Here we develop a notion of a temporal network motif as an elementary unit of temporal networks and provide a general methodology for counting such motifs. We define temporal network motifs as induced subgraphs on sequences of edges, design several fast algorithms for counting temporal network motifs, and prove their runtime complexity. We also show that our fast algorithms achieve 1.3x to 56.5x speedups compared to a baseline method. We use our algorithms to count temporal network motifs in a variety of real-world datasets. Results show that networks from different domains have significantly different motif frequencies, whereas networks from the same domain tend to have similar motif frequencies. We also find that measuring motif counts at various time scales reveals different behavior.
computer_security	Vast amounts of data are collected and stored every day, as part of corporate knowledge bases and as a response to legislative compliance requirements. To reduce the cost of retaining such data, compression tools are often applied. But simply seeking the best compression ratio is not necessarily the most economical choice, and other factors also come in to play, including compression and decompression throughput, the main memory required to support a given level of on-going access to the stored data, and the types of storage available. Here we develop a model for the total retention cost (TRC) of a data archiving regime, and by applying the charging rates associated with a cloud computing provider, are able to derive dollar amounts for a range of compression options, and hence guide the development of new approaches that are more cost-effective than current mechanisms. In particular, we describe an enhancement to the Relative Lempel Ziv (RLZ) compression scheme, and show that in terms of TRC, it outperforms previous approaches in terms of providing economical long-term data retention.
computer_security	In the future, patients may be able to receive health care through the use of pervasive medical devices, sensors, and applications, even outside of hospitals. These data sources monitor and assist patients, aid in treatment, and notify doctors of problems as they develop, allowing them to send help and prepare for emergency treatment faster than otherwise possible. But such data sources are subject to attack or failure. Current trust models guarantee control over access to patient data, but not to determine the trustworthiness of the sources of that data, or of the data itself. This paper shows how the Solar Trust Model can be used to evaluate the trustworthiness of data and data sources in networks of pervasive healthcare devices, sensors, and applications.
computer_security	In this paper, I describe a framework for spectral transforms called P3DFFT, and its extended features and applications. I discuss the scaling seen on petascale platforms, and directions and some results of the ongoing work on improving performance, such as overlap of communication with computation and hybrid MPI/OpenMP implementation.
computer_security	This paper proposes the first implementation of a self-stabilizing atomic register that is tolerant to both Mobile Byzantine Agents and transient failures. The register is maintained by n servers and our algorithm tolerates (i) any number of transient failures and (ii) up to f Mobile Byzantine Failures. In the Mobile Byzantine Failure model, faulty agents move from one server to another and when they are affecting a server, it behaves arbitrarily. Our implementation is designed for the round-based synchronous model where agents are moved from round to round. The paper considers four Mobile Byzantine Failure models differing for the diagnosis capabilities at server side i.e., when servers can diagnose their failure state (that is, be aware that the mobile Byzantine agent has left the server), and when servers cannot self-diagnose. We first prove lower bounds on the number of servers n necessary to construct a register tolerant to the presence of f Mobile Byzantine Failures for each of the Mobile Byzantine Failure models considered and then we propose a parametric algorithm working in all the models and matching the lower bounds.
computer_security	This paper underlines a methodology for translating text from English into the Dravidian language, Malayalam using statistical models. By using a monolingual Malayalam corpus and a bilingual English/Malayalam corpus in the training phase, the machine automatically generates Malayalam translations of English sentences. This paper also discusses a technique to improve the alignment model by incorporating the parts of speech information into the bilingual corpus. Removing the insignificant alignments from the sentence pairs by this approach has ensured better training results. Pre-processing techniques like suffix separation from the Malayalam corpus and stop word elimination from the bilingual corpus also proved to be effective in training. Various handcrafted rules designed for the suffix separation process which can be used as a guideline in implementing suffix separation in Malayalam language are also presented in this paper. The structural difference between the English Malayalam pair is resolved in the decoder by applying the order conversion rules. Experiments conducted on a sample corpus have generated reasonably good Malayalam translations and the results are verified with F measure, BLEU and WER evaluation metrics.
computer_security	With recent research interest in the confounding roles of homophily and contagion in studies of social influence, there is a strong need for reliable content-based measures of the similarity between people. In this paper, we investigate the use of text similarity measures as a way of predicting the similarity of prolific weblog authors. We describe a novel method of collecting human judgments of overall similarity between two authors, as well as demographic, political, cultural, religious, values, hobbies/interests, personality, and writing style similarity. We then apply a range of automated textual similarity measures based on word frequency counts, and calculate their statistical correlation with human judgments. Our findings indicate that commonly used text similarity measures do not correlate well with human judgments of author similarity. However, various measures that pay special attention to personal pronouns and their context correlate significantly with different facets of similarity.
computer_security	Wikipedia has become the most frequently viewed online encyclopaedia website. Some sentences in Wikipedia articles have direct and obvious impact on people's opinions towards the mentioned named entities. This paper defines and tackles the problem of reputation-influential sentence detection in Wikipedia articles from various domains. We leverage multiple lexicons, to generate domain independent features. We generate topical features and word embedding features from unlabelled dataset, to boost the classification performance. We conduct several experiments, to prove the effectiveness of these features. We further adapt a two-step binary classification method, to perform multi-classification. Our evaluation results show that this method outperforms the state-of-the-art one-vs-one multi-classification method for this problem.
computer_security	In a competitive market like the App Store, high user perceived quality is paramount, especially due to the public review system offered. These reviews give developers feedback on their own apps, as well as help provide data for competitor analysis. However, given the size of the data set, manual analysis of reviews is unrealistic, especially given the need for a rapid response to changing market dynamics. Current research into mobile app reviews has provided an insight into the statistical distributions, but there is minimal knowledge about the content in these reviews. In particular, we do not know if the aggregated numerical rating is a reliable indicator of the information within a review. This work reports on an analysis of reviews to determine how closely aligned the numerical ratings are to the textual description. We observed that short user reviews mostly contain a small set of words, and the corresponding numerical rating matches the underlying sentiment.
computer_security	In the last years, the vision of the Semantic Web has led to many approaches that aim to automatically derive knowledge bases from Wikipedia. These approaches rely mostly on the English Wikipedia as it is the largest Wikipedia version and have lead to valuable knowledge bases. However, each Wikipedia version contains socio-cultural knowledge, i.e. knowledge with specific relevance for a culture or language. One difficulty of the application of existing approaches to multiple Wikipedia versions is the use of additional corpora. In this paper, we describe the adaptation of existing heuristics that make the extraction of large sets of hyponymy relations from multiple Wikipedia versions with little information about each language possible. Further, we evaluate our approach with Wikipedia versions in four different languages and compare results with GermaNet for German and WordNet for English.
computer_security	This study investigates how politicians and citizens cooperate to create an e-democracy based on information dissemination and political awareness in an ICT environment, especially during elections. This research is conducted in the Indian context, where new ICT channels, such as Facebook and Twitter, were extensively used for campaigning and citizen engagement prior to elections. We downloaded 98,396 tweets posted by the official Twitter accounts of the top ten political parties during a two month period prior to election and conducted a three-level analysis to identify the overall trend in usage, the interactive characteristics of tweets and the functions driving the Twitter usage of political parties. Our findings show that the more successful parties used Twitter to push timely updates on online and offline campaign activities, to their followers. The exemplary use of Twitter for campaigns was by the Bharatiya Janta Party (BJP) Twitter account for interacting with the public, and by the Aam Aadmi Party (AAP) account for self-promotion and highlighting its party manifesto. Further, we identify the new paradigms created by political parties to engage and inform voters, driven on modern ICT. Our study is the first in analyzing Twitter usage by Indian political parties, and its findings corroborate seminal research in other developing countries.
computer_security	The need for automatic generation of summaries gained importance with the unprecedented volume of information available in the Internet. Automatic systems based on extractive summarization techniques select the most significant sentences of one or more texts to generate a summary. This article makes use of Machine Learning techniques to assess the quality of the twenty most referenced strategies used in extractive summarization, integrating them in a tool. Quantitative and qualitative aspects were considered in such assessment demonstrating the validity of the proposed scheme. The experiments were performed on the CNN-corpus, possibly the largest and most suitable test corpus today for benchmarking extractive summarization strategies.
computer_security	Over centuries texts of all genres have been connected by quotes, allusions, idioms, stylistic imitations and many more. Understanding literature means understanding these kinds of intertextual relations. The goal of the G�ttingen sub-project of eTRACES, an interdisciplinary project of humanists and computer scientists, is to enable research on this essential part of literary studies. We are creating a digital working environment, a tool called GERTRUDE (G�ttingen E-Research: Text Re-Use for Digital Editions), in an effort to determine whether and to what degree such a tool can support a researcher in finding, marking and annotating intertextual relations: - Especially in big text corpora, looking for intertextual relations can be very time-consuming. So Text-Mining-algorithms are integrated to determine so called "textual re-use", hoping also to find interesting textual relations not yet known or expected (serendipity effect). - For referencing an exact text passage, the TextGrid Citation Schema is used, because it enables us to mark up a segment of text down to the granularity of letters and to address different editions of a text. - The possibilities and limitations of annotating or even evaluating a text passage in its relation to others, its form, function and"degree"of intertextuality will be researched by creating this tool as a crowd-sourcing environment: It is usable by everyone interested and it is also integrated in university courses, where students are encouraged to use it. By this means it is possible to compare and discuss the results as well as the usability, possibilities and limitations of the tool. Our approach is based on German literature from 1500s to 1900s and is part of a BMBF-sponsored text corpus available under a Creative Commons License online. In the future it will be possible to use other corpora, even in languages other than German, if the algorithms are adapted. Because of its great influence, we chose the Luther-Bible and its re-use in German literature as a use case.
computer_security	A key difference between traditional humanities research and the emerging field of digital humanities is that the latter aims to complement qualitative methods with quantitative data. In linguistics, this means the use of large corpora of text, which are usually annotated automatically using natural language processing tools. However, these tools do not exist for historical texts, so scholars have to work with unannotated data. We have developed a system for systematic, iterative exploration and annotation of historical text corpora, which relies on an XML database (BaseX) and in particular on the Full Text and Update facilities of XQuery.
computer_security	Sentiment Analysis is the task of automatically identifying whether a text or a single sentence is intended to carry a positive or negative connotation. The commonly used Bag-of-Words approach that relies on counting positive and negative words, whose connotation is indicated by specially crafted sentiment dictionaries, is not ideal because it does not take into account the relations between words and how the connotation of single words changes according to the context. This paper proposes a way of identifying and analysing the targets of the opinions and their modifiers, along with their linkage (appraisal group) through an annotation schema called SentiML. Such schema has been developed in order to facilitate the identification of these elements and the annotation of their sentiment, along with advanced linguistic features such as their appraisal type according to the Appraisal Framework. The schema is XML-based and has been also designed to be language-independent. Preliminary results show that the schema allows more coverage than a sentiment dictionary, while achieving reasonably fast and reliable annotation in spite of its fine granularity.
computer_security	Google has millions of users around the world...but occasionally we mess up. We'll explore some of Google's international "bloopers" and show how to avoid similar mistakes in other applications. We'll also highlight how we solved a persistent blooper, namely having UI strings like "Alice has added 3 contact to his address book." Our Plural/Gender API allows complicated UI strings to change appropriately, based on numbers and personal gender. We'll look at the document formats used to express plural and gendered messages, and explore the impact on the translation process.
computer_security	Among all the Internet applications, microblog is warmly welcomed by the users for its concise content, flexible form, rapid spread and real-time update. The total number of microblogs registered in China's main platforms, such as Sina, Tencent, Netease, Sohu and so on, exceeds 1 billion. Because of the huge number of the user, microblog seems to have become an independent route of information transmission, which can work independently in the guidance of public opinion. How to regulate microblog and locate public opinion hazards, and make use of microblog for better planning and guiding the public opinion, and create elegant and pleasant cultural life should be our new concern. In this paper, according to the characteristics of microblog content and form, we propose a technology based on the analysis of the fingerprint information. We use network spiders crawling microblog content with limited public opinion keywords, then extract the text content and use simhash algorithm creating the unique fingerprint. Fingerprints are clusterd base on hamming distance. In the same category, we analyze the data, discovering the sensitive topic, looking for key information disseminators, thus control the public opinion. After many times of experiments, this method has a higher accuracy and credibility.
computer_security	We employ a method to mine social networks of person entities from Wikipedia in this paper. A person entity is represented as a vector by anchor text set and content text set of his page in Wikipedia using Improved Vector Space Model (IVSM). We use cosine similarity of the vectors to present the similarity of person entities, and at last we get the similarity array of all the person entities. Finally, we extract the social network from the array which shows the relations of person entities. On Wikipedia data, we conduct some experiments on social network analysis, and the experimental results show our social network mining approaches are effective.
computer_security	Employee appraisal systems are widely regarded as fundamental to enhancing the success of organizations. Yet, there is evidence that employees doubt their benefits, organizations find them difficult to implement and their value is questioned. Although computing systems that support appraisals have been developed, their focus remains on recording information, thereby not providing the kind of meaningful and deeper support that could help appraisers and appraises achieve organizational goals. Hence, this paper explores the potential for utilizing sentiment analysis for developing a novel system for supporting appraisals. The paper reviews the challenges facing appraisal processes, summarizes some features of sentiment analysis and then proposes a new semantic framework for appraisal systems. The first steps in developing part of the framework using the GATE system are illustrated with an example showing the potential for using sentiment analysis for assessing whether objectives are SMART.
computer_security	Heart disorders are one of the most problematic issues of human health. There are currently many efforts to reduce the time for first assistance based on electronic systems that continuously records the electric heart activity (ECG), for further inspection and anomalies detection. However, an efficient automatic detection of heart problems still being a big challenge mainly due to difficulty of accessing some specific cardiac problems, signal variations between different patients and the level of uncertainty caused by the absence of significant data or medical elements from patient. This research explores the use of several health condition signals and indicators for heart monitoring and injuries detection. This paper presents the first approach of a portable solution running decision support algorithms to produce medical diagnosis based on electrical and sound heart signals, ambient conditions, patient mobility, patient personal data and clinical history.
computer_security	Storage devices based on Phase Change Memory (PCM) devices are beginning to generate considerable attention in both industry and academic communities. But whether the technology in its current state will be a commercially and technically viable alternative to entrenched technologies such as flash-based SSDs still remains unanswered. To address this it is important to consider PCM SSD devices not just from a device standpoint, but also from a holistic perspective. This paper presents the results of our performance measurement study of a recent all-PCM SSD prototype. The average latency for 4 KB random read is 6.7 ?s, which is about 16x faster than a comparable eMLC flash SSD. The distribution of I/O response times is also much narrower than the flash SSD for both reads and writes. Based on real-world workload traces, we model a hypothetical storage device which consists of flash, HDD, and PCM to identify the combinations of device types that offer the best performance within cost constraints. Our results show that - even at current price points - PCM storage devices show promise as a new component in multi-tiered enterprise storage systems.
computer_security	The Synthesis operating system kernel combines several techniques to provide high performance, including kernel code synthesis, fine-grain scheduling, and optimistic synchronization. Kernel code synthesis reduces the execution path for frequently used kernel calls. Optimistic synchronization increases concurrency within the kernel. Their combination results in significant performance improvement over traditional operating system implementations. Using hardware and software emulating a SUN 3/160 running SUNOS, Synthesis achieves several times to several dozen times speedup for UNIX kernel calls and context switch times of 21 microseconds or faster.
computer_security	Real-world data-parallel programs commonly suffer from great memory pressure, especially when they are executed to process large datasets. Memory problems lead to excessive GC effort and out-of-memory errors, significantly hurting system performance and scalability. This paper proposes a systematic approach that can help data-parallel tasks survive memory pressure, improving their performance and scalability without needing any manual effort to tune system parameters. Our approach advocates interruptible task (ITask), a new type of data-parallel tasks that can be interrupted upon memory pressure---with part or all of their used memory reclaimed---and resumed when the pressure goes away. To support ITasks, we propose a novel programming model and a runtime system, and have instantiated them on two state-of-the-art platforms Hadoop and Hyracks. A thorough evaluation demonstrates the effectiveness of ITask: it has helped real-world Hadoop programs survive 13 out-of-memory problems reported on StackOverflow; a second set of experiments with 5 already well-tuned programs in Hyracks on datasets of different sizes shows that the ITask-based versions are 1.5--3x faster and scale to 3--24x larger datasets than their regular counterparts.
computer_security	This paper appears in the March, 1972, issue of the Communications of the ACM. Its abstract is reproduced below. Formalization of a well-defined synchronization mechanism can be used to prove that concurrently running processes of a system communicate correctly. This is demonstrated for a system consisting of many sending processes which deposit messages in a buffer and many receiving processes which remove messages from that buffer. The formal description makes it very easy to prove that the buffer will neither overflow nor underflow, that senders and receivers will never operate on the same message frame in the buffer nor will they run into a deadlock.
computer_security	A formal model of hardware/software architectures is developed and applied to Virtual Machine Systems. Results are derived on the sufficient conditions that a machine architecture must verify in order to support VM systems. The model deals explicitly with resource mappings (protection) and with I/O devices. Some already published results are retrieved and other ones, more general, are obtained.
computer_security	Browser caches are typically designed to maximize hit rates---if all other factors are held equal, then the highest hit rate will result in the highest performance. However, if the performance of the underlying cache storage (i.e. the file system) varies with differing workloads, then these other factors may in fact not be equal when comparing different cache strategies. Mobile systems such as smart phones are typically equipped with low-speed flash storage, and suffer severe degradation in file system performance under sufficiently random write workloads. A cache implementation which performs random writes will thus spend more time reading and writing its cache, possibly resulting in lower overall system performance than a lower-hit-rate implementation which achieves higher storage performance. We present a log-structured browser cache, generating almost purely sequential writes, and in which cleaning is efficiently performed by cache eviction. An implementation of this cache for the Chromium browser on Android was developed; using captured user browsing traces we test the log-structured cache and compare its performance to the existing Chromium implementation. We achieve a ten-fold performance improvement in basic cache operations (as measured on a Nexus 7 tablet), while in the worst case increasing miss rate by less than 3% (from 65% to 68%). For network bandwidths of 1Mb/s or higher the increased cache performance more than makes up for the decrease in hit rate; the effect is more pronounced when examining 95th percentile delays.
computer_security	We propose an I/O classification architecture to close the widening semantic gap between computer systems and storage systems. By classifying I/O, a computer system can request that different classes of data be handled with different storage system policies. Specifically, when a storage system is first initialized, we assign performance policies to predefined classes, such as the filesystem journal. Then, online, we include a classifier with each I/O command (e.g., SCSI), thereby allowing the storage system to enforce the associated policy for each I/O that it receives. Our immediate application is caching. We present filesystem prototypes and a database proof-of-concept that classify all disk I/O --- with very little modification to the filesystem, database, and operating system. We associate caching policies with various classes (e.g., large files shall be evicted before metadata and small files), and we show that end-to-end file system performance can be improved by over a factor of two, relative to conventional caches like LRU. And caching is simply one of many possible applications. As part of our ongoing work, we are exploring other classes, policies and storage system mechanisms that can be used to improve end-to-end performance, reliability and security.
computer_security	Today, systems software is too complex to be bug-free. To find bugs in systems software, developers often rely on code checkers, like Linux's Sparse. However, the capability of existing tools used in commodity, large-scale systems is limited to finding only shallow bugs that tend to be introduced by simple programmer mistakes, and so do not require a deep understanding of code to find them. Unfortunately, the majority of bugs as well as those that are difficult to find are semantic ones, which violate high-level rules or invariants (e.g., missing a permission check). Thus, it is difficult for code checkers lacking the understanding of a programmer's true intention to reason about semantic correctness. To solve this problem, we present Juxta, a tool that automatically infers high-level semantics directly from source code. The key idea in Juxta is to compare and contrast multiple existing implementations that obey latent yet implicit high-level semantics. For example, the implementation of open() at the file system layer expects to handle an out-of-space error from the disk in all file systems. We applied Juxta to 54 file systems in the stock Linux kernel (680K LoC), found 118 previously unknown semantic bugs (one bug per 5.8K LoC), and provided corresponding patches to 39 different file systems, including mature, popular ones like ext4, btrfs, XFS, and NFS. These semantic bugs are not easy to locate, as all the ones found by Juxta have existed for over 6.2 years on average. Not only do our empirical results look promising, but the design of Juxta is generic enough to be extended easily beyond file systems to any software that has multiple implementations, like Web browsers or protocols at the same layer of a network stack.
computer_security	As the number of processes and resources increases within a computer system, does the probability of that system's being in deadlock increase or decrease? This paper introduces Probabilistic Automata as a model of computer systems. This allows investigation of the above question for various scheduling algorithms. A theorem is proven which indicates that, within the types of systems considered, the probability of deadlock increases.
computer_security	This paper reports on the effects of using hardware virtual memory assists in managing file buffer caches in UNIX. A controlled experimental environment was constructed from two systems whose only difference was that one of them (XMF) used the virtual memory hardware to assist file buffer cache search and retrieval. An extensive series of performance characterizations was used to study the effects of varying the buffer cache size (from 3 Megabytes to 70 MB); I\O transfer sizes (from 4 bytes to 64 KB); cache-resident and non-cache-resident data; READs and WRITEs; and a range of application programs. The results: small READ/WRITE transfers from the cache (?1 KB) were 5O% faster under XMF, while larger transfers (?8 KB) were 20% faster. Retrieving data from disk, the XMF improvement was 25% and 1O% respectively, although OPEN/CLOSE system calls took slightly longer in XMF. Some individual programs ran as much as 40% faster on XMF, while an application benchmark suite showed a 7-15% improvement in overall execution time. Perhaps surprisingly. XMF had fewer translation lookaside buffer misses.
computer_security	Solid-state drives are becoming increasingly popular in enterprise storage systems, playing the role of large caches and permanent storage. Although SSDs provide faster random access than hard-drives, their performance under read/write workloads is highly variable often exceeding that of hard-drives (e.g., taking 100ms for a single read). Many systems with mixed workloads have low latency requirements, or require predictable performance and guarantees. In such cases, the performance variance of SSDs becomes a problem for both predictability and raw performance. In this paper, we propose a design based on redundancy, which provides high performance and low latency for reads under read/write workloads by physically separating reads from writes. More specifically, reads achieve read-only performance while writes perform at least as good as before. We evaluate our design using micro-benchmarks and real traces, illustrating the performance benefits of read/write separation in solid-state drives.
computer_security	Irony has been proven to be pervasive in social media, posing a challenge to sentiment analysis systems. It is a creative linguistic phenomenon where affect-related aspects play a key role. In this work, we address the problem of detecting irony in tweets, casting it as a classification problem. We propose a novel model that explores the use of affective features based on a wide range of lexical resources available for English, reflecting different facets of affect. Classification experiments over different corpora show that affective information helps in distinguishing among ironic and nonironic tweets. Our model outperforms the state of the art in almost all cases.
computer_security	File caching is essential to good performance in a distributed system, especially as processor speeds and memory sizes continue to improve rapidly while disk latencies do not. Stateless-server systems, such as NFS, cannot properly manage client file caches. Stateful systems, such as Sprite, can use explicit cache consistency protocols to improve both cache consistency and overall performance. By modifying NFS to use the Sprite cache consistency protocols, we isolate the effects of the consistency mechanism from the other features of Sprite. We find dramatic improvements on some, although not all, benchmarks, suggesting that an explicit cache consistency protocol is necessary for both correctness and good performance.
computer_security	The efficacy of data aggregation in sensor networks is a function of the degree of spatial correlation in the sensed phenomenon. While several data aggregation (i.e., routing with data compression) techniques have been proposed in the literature, an understanding of the performance of various data aggregation schemes across the range of spatial correlations is lacking. We analyze the performance of routing with compression in wireless sensor networks using an application-independent measure of data compression (an empirically obtained approximation for the joint entropy of sources as a function of the distance between them) to quantify the size of compressed information, and a bit-hop metric to quantify the total cost of joint routing with compression. Analytical modeling and simulations reveal that while the nature of optimal routing with compression does depend on the correlation level, surprisingly, there exists a practical static clustering scheme which can provide near-optimal performance for a wide range of spatial correlations. This result is of great practical significance as it shows that a simple cluster-based system design can perform as well as sophisticated adaptive schemes for joint routing and compression.
computer_security	The primal problem with the Internet of Things is the lack of interoperability at various levels, and more predominately at the device level. While there exists multitude of platforms from multiple manufacturers, the existing ecosystem still remains highly closed. In this work, we propose SNaaS or Sensor/Network as a Service: a service layer that enables the creation of the plug-n-play infrastructure, across platforms from multiple vendors, necessary for interoperability and successful deployment of large-scale systems. We present the design and implementation of SNaaS, along with preliminary microbenchmarks.
computer_security	Free web services often face growing pains. In the current client-server access model, the cost of providing a service increases with its popularity. This leads organizations that want to provide services free-of-charge to rely to donations, advertisements, or mergers with larger companies to cope with operational costs. This paper proposes an alternative architecture for deploying services that allows more web services to be offered for free. We leverage recent developments in web technologies to combine the portability of the existing web with the user-powered scalability of distributed P2P solutions. We show how this solution addresses issues of user security, data sharing, and application distribution. By employing an easily composable communication interface and rich storage permissions, the FreeDOM architecture encourages flexible interactions between applications while enforcing privacy controls. We demonstrate the applicability of this architecture by presenting a SQL database and a community-supported Wiki as case studies.
computer_security	There is a growing recognition among researchers, industry practitioners, and service providers of the need to optimize user-perceived application experience. Network infrastructure owners (i.e., ISPs) have traditionally been left out of this equation, leading to repeated tussles between content providers and ISPs. In parallel, application providers have to deploy complex workarounds that reverse engineer the network's impact on application-level metrics. In this work, we make the case for EONA, a new network paradigm where application providers and network providers can collaborate meaningfully to improve application experience. We observe a confluence of technology trends that are enablers for EONA: the ability to collect large volumes of client-side application measurements, the emergence of novel "big data" platforms for real-time analytics, and new control plane capabilities for ISPs (e.g., SDN, IXPs, NFV). We highlight the challenges and opportunities in designing suitable EONA interfaces between infrastructure and application providers and EONA-enhanced control loops that leverage these interfaces to optimize user experience.
computer_security	The current political problematic of electoral corruption in both Brazil and the world opens new challenges not only in social aspects, but it also can be a base for technological developments. The objective of this IoT project is to develop a kiosk system for electoral and political consciousness, using a Raspberry Pi board and other components as its hardware, to provide accurate information to electors about political parties and candidates. The hardware and software infrastructure was arranged integrating the necessary modules to the board and developing algorithms for its adequate functioning; also, it was defined the layout and presentation of data, in order to facilitate the usability, accomplish accessibility requirements and improve user experience. The result is a functional kiosk system that accomplishes the established architectural design and contributes to a social and political issue. In conclusion, techniques were developed to optimize the use of Raspberry Pi in IoT projects, and it is a viable solution to be adopted on a wider scale for political consciousness and at a world level.
computer_security	A master of ceremonies (MC) plays an important role to ensure all events progress smoothly because unexpected interruption make them unsuccessful. MCs must have various abilities such as being able to memorize the content of given scenarios and manage problems that occur unexpectedly. Moreover, since unskilled MCs cannot intuit the atmosphere in the audiences during an event, they cannot control this smoothly. Therefore, we propose a wearable system that solves these problems for MCs achieved through wearable computing technologies. Our system has functions to support MCs in carrying out their duties smoothly, such as a robust voice-tracking function for them to read scripts, a user interface that does not interrupt other tasks, and a function that enables MCs intuit grasp the atmosphere of the audience. We implemented a prototype of the wearable MC system and actually used it at several events. The results we obtained from actually using it confirmed that it worked well and helped MCs to carry out their official duties smoothly.
computer_security	In the context of fast adoption and deployment of recent video compression standard and thanks to recent high performance embedded processors, software video decoding can be performed in real time. But, it becomes among the most energy-intensive applications. Current embedded processors are based on multi-core architecture with advanced convenient features such as Dynamic Voltage Frequency Scaling (DVFS) in order to reduce their power consumption, allowing low power video decoding when no hardware decoding support is available for a given device. This paper deals with energy efficiency impact of different parallelization strategies of a software High Efficiency Video Coding (HEVC) decoder on multi-core ARM big.LITTLE processor. These strategies include the exploitation of data and task-level parallelism, as well as the use of different available DVFS policies.
computer_security	In this paper we analyze the robustness of multi-tasking applications when mapped on an on-chip multiprocessor platform. We assume a multiprocesso structure which embeds a hierarchical cache organization with two levels. The first one is private to the processor cores while the second one is shared among the processors. To enable compositionality, i.e, to be able to evaluate the system's performance out of the individual task's performance, the second level of cache (L2) is partitioned per task basis. Two robustness aspects are relevant in this context: internal (performance deviations are caused by the tasks comprising the application) and external (performance variations are caused by external stimuli). First we introduce two metrics to quantify the robustness. The internal robustness is estimated by a sensitivity function which measures the performance variations induced by the %private inter-task cache interference. The external robustness is quantified by a stability function which reflects the variations induced by different input data on the partitioned L2 behavior. Subsequently, we exercise our method on two applications (H.264 and picture-in-picture TV) running on a CAKE multi-processor platform. Our experiments indicate that, if the cache is partitioned, the sensitivity is 8% and 5% for the H.264 and PiPTV, respectively. For the shared cache scenario the sensitivity is 40% and 50% for the H.264 and PiPTV, respectively. The variations induced in the L2 behavior by various input data sets are at most 4% for the PiPTV application, respectively 9% for the H.264 decoder. This accounts for a stability of at least 96%, respectively 91%, therefore, for the investigated applications, we can conclude that the static cache partitioning is quite robust to input stimuli.
computer_security	It is widely accepted that existing knowledge about the structure of many biological pathways is incomplete, and that uncovering missing proteins in a biological pathway can help guide targeted therapy, drug design, and drug discovery. Current approaches address the complex/pathway membership problem through identifying potentially missing proteins using probabilistic protein-protein interaction (PPI) networks. In this paper, we extend the idea of the pathway membership problem and define the pathway completion problem. In addition to finding possible protein candidates, this problem requires predicting the locations and connections of these proteins within a given incomplete pathway. We propose the use of network motifs to tackle the pathway completion problem. We present an algorithm which breaks down an incomplete pathway into a set of constituent motifs. The algorithm utilizes the proteins retrieved from a probabilistic PPI network to improve the motifs. Furthermore, our approach has the potential to improve solutions to the membership problem by better exploiting the local structures represented by network motifs. These new ideas are illustrated with a set of preliminary experiments.
computer_security	Modern multimedia systems must support a variety of different use-cases. Multi-processors Systems-on-Chip (MPSoCs) are used to realize these systems. A system designer has to dimension the size of an MPSoC such that the performance constraints of the applications are satisfied in all use-cases. In this paper, we present an approach to design MPSoCs that can meet the throughput constraints of a set of applications while minimizing the resource requirements.
computer_security	Mobile systems are executing applications with increasingly large memory footprints on more processor cores. New execution paradigms for quickly suspending and resuming an application have also become common. Energy consumption remains a paramount concern. Consequently, phase-change memory (PCM) has been suggested for main memory to increase capacity, provide non-volatility for suspend/resume and decrease energy consumption. Because it has limitations for writes, a large PCM is often used along with a small DRAM for good performance. The two memory types may be managed by the operating system, which selects where to allocate pages and schedules background migrations between memory types to move data. To ensure correctness, an application that writes to a migrating page must be paused until the migration completes. Because PCM has long write latency, this situation happens frequently in hybrid memory, leading to long pauses that hurt application responsiveness and performance. This paper describes concurrent page migration (CPM) to alleviate the pauses by buffering writes to migrating pages through the last-level cache. CPM improves performance by up to 22% for single-programmed workloads (17% average) and 13% for multi-programmed workloads (8% average). The technique also preserves the energy and non-volatility benefits of hybrid main memory.
computer_security	In this paper, we present MOBI-COG which is an application that runs on a mobile device, such as a tablet or a smartphone, and provides an automated and instant dementia screening service. The MOBI-COG App is a complete automation of a widely used 3-minute dementia screening test called the Mini-Cog test, which is administered by primary caregivers for a quick screening of dementia in elderly. Besides asking the patient to remember and then recall a set of three words, the test involves a free-hand clock drawing test. The MOBI-COG App automates all these steps -- including the automatic assessment of the correctness of a clock drawn on the touch screen of a mobile device. We train the MOBI-COG App with over 1000 touch-drawn clocks and show that the system is capable of detecting and recognizing digits in less than 100 ms, in-situ (i.e. without the help of any back-end server), with 99.53% accuracy, and is robust to changes in people, sizes of the drawn digits, and screen sizes of the mobile devices. We perform a usability study of MOBI-COG involving eight healthy human subjects and show that the system is capable of performing all three steps of the test effectively. We also provide a summary of the users' comments on the application.
computer_security	Wearable sensing systems are becoming widely used for a variety of applications, including sports, entertainment, and military. These systems have recently enabled a variety of medical monitoring and diagnostic applications in Wireless Health. The need for multiple sensors and constant monitoring lead these systems to be power hungry and expensive, with short operating lifetimes. In this paper, we introduce a novel methodology that takes advantage of the influence of human behavior on signal properties and reduces those three metrics from the data size point of view. This, in turn, directly influences the wireless communication and local processing power consumption. We exploit intrinsic space and temporal correlations between sensor data while considering both user and system behavior. Our goal is to select a small subset of sensors to accurately capture and/or predict all possible signals of a fully instrumented wearable sensing system. Our approach leverages novel modeling, partitioning, and behavioral optimization, which consists of signal characterization, segmentation and time shifting, mutual signal prediction, and subset sensor selection. We demonstrate the effectiveness of the technique on an insole instrumented with 99 pressure sensors placed in each shoe, which cover the bottom of the entire foot, resulting in energy reduction of 56% to 96% for error rates of 5% to 17.5%.
computer_security	Ubiquitous physiological monitoring will be a key driving force in the upcoming wireless health revolution. Cardiac and brain signals in the form of ECG and EEG are two critical health indicators that directly benefit from long-term monitoring. Despite advancements in wireless technology and electronics miniaturization, however, the use of wireless home ECG/EEG monitoring is still limited by the inconvenience and discomfort of wet adhesive electrodes. We have developed a wireless biopotential instrumentation system using non-contact capacitive electrodes that operate without skin contact. The sensors can be embedded within comfortable layers of fabric for unobtrusive use. All of the issues relating to the design of low noise, high performance capacitive sensors are discussed along with full technical details, circuit schematics and construction techniques. The non-contact electrode has been integrated into both a wearable ECG chest harness as well a EEG headband. We have also designed a compact, battery-powered, wireless data acquisition system to interface with multiple electrodes and monitor patient cardiac and neural signals in real time. Experimental data shows that the non-contact capacitive electrode perform comparable to Ag/AgCl electrodes using our special chest harness and head bands to ensure tight, movement-free electrode positioning.
computer_security	This material is based on work in progress. Over the last decade, precision agriculture has grown in importance in order to meet the increasing food demand and ensure sustainability of farming. Today, advances in the Internet of Things (IoT) paradigm have promoted the use of Wireless Sensor Networks (WSN) for precision farming. However, recent technological developments suggest that use of Nanotechnology has immense potential to further improve the farming productivity. In this paper, we present some use-cases for the application of Internet of Nano Things (IoNT) in dairy farming. Although the use of IoNT involves several challenges, we envisage a multitude of benefits associated with its implementation.
computer_security	In this paper, a novel energy-efficient concurrency control protocol, called conditional abortable stack resource policy (CA-SRP), is proposed for scheduling of dependent real-time tasks with abortable critical sections. Under the CA-SRP, a critical section is allowed to be aborted only if its re-execution is more energy-efficient than the blocking of higher priority tasks. The CA-SRP dynamically determines the appropriate processor speed for executing tasks on a DVS processor so that the energy consumption can be reduced. The capabilities of the CA-SRP were evaluated by a series of experiments, for which we have some encouraging results.
computer_security	Embedded system architecture is developed in a mobile-like handset device for controlling machineries in Industries through wireless mode of communication. Speech recognition is the ability of machines to respond to spoken commands. This mechanism is activated in two ways: 1. Speech 2. Manually by hand The speech implementation is performed when speech input is given to the device. A software program is developed for recognizing the commands. It performs according to the conditions specified in the code and corresponding machinery actions are controlled. The manual implementation is executed when the user presses on the button, which is designed to control all necessary actions in the Machine. This method of automating the machines in the Industry serves as a communication & controlling aid to the differently-abled people working in the Industries.
computer_security	Locking cache lines in hard real-time systems is a common means of achieving predictability of cache access behavior and tightening as well as reducing worst case execution time, especially in a multitasking environment. However, cache locking poses a challenge for multi-core hard real-time systems since theoretically optimal scheduling techniques on multi-core architectures assume zero cost for task migration. Tasks with locked cache lines need to proactively migrate these lines before the next invocation of the task. Otherwise, cache locking on multi-core architectures becomes useless as predictability is compromised. This paper proposes hardware-based push-assisted cache migration as a means to retain locks on cache lines across migrations. We extend the push-assisted migration model with several cache migration techniques to efficiently retain locked cache lines on a bus-based chip multi-processor architecture. We also provide deterministic migration delay bounds that help the scheduler decide which migration technique(s) to utilize to relocate a single or multiple tasks. This information also allows the scheduler to determine feasibility of task migrations, which is critical for the safety of any hard real-time system. Such proactive migration of locked cache lines in multi-cores is unprecedented to our knowledge.
computer_security	The disparity in performance between processors and main memories has led computer architects to incorporate large cache hierarchies in modern computers. Because these cache hierarchies are designed to be general-purpose, they may not provide the best possible performance for a given application. In this paper, we determine a memory subsystem well suited for a given application and main memory by discovering a memory subsystem comprised of caches,scratchpads, and other components that are combined to provide better performance. We draw motivation from the superoptimization of instruction sequences, which successfully finds unusually clever instruction sequences for programs. Targeting both ASIC and FPGA devices, we show that it is possible to discover unusual memory subsystems that provide performance improvements over a typical memory subsystem.
computer_security	In embedded systems, SPM (scratchpad memory) is an attractive alternative to cache memory due to its lower energy consumption and higher predictability of program execution. This paper studies the problem of placing variables of a program into an SPM such that its WCET (worst-case execution time) is minimized. We propose an efficient dynamic approach that comprises two novel heuristics. The first heuristic iteratively selects a most beneficial variable as an SPM resident candidate based on its impact on the k longest paths of the program. The second heuristic incrementally allocates each SPM resident candidate to the SPM based on graph coloring and acyclic graph orientation. We have evaluated our approach by comparing with an ILP-based approach and a longest-path-based greedy approach using the eight benchmarks selected from Powerstone and M?lardalen WCET Benchmark suites under three different SPM configurations. Our approach achieves up to 21% and 43% improvements in WCET reduction over the ILP-based approach and the greedy approach, respectively.
computer_security	Affine Control Loops (ACLs) occur frequently in data- and computeintensive applications. Implementing ACLs directly on dedicated hardware has the potential for spectacular performance improvement in area, time and energy. An important challenge for such direct hardware compilation of ACLs is the interconnection between the different processing elements, which may be non-local as well as dynamic. We propose a generic, reconfigurable interconnection fabric which can realize the data-path of any ACL and be dynamically reconfigured in constant time. We have applied for a patent for this technology.
computer_security	Within the domain of embedded systems, hardware architectures are commonly characterised by application-specific heterogeneity. Systems may contain multiple dissimilar processing elements, non-standard memory architectures, and custom hardware elements. The programming of such systems is a considerable challenge, not only because of the need to exploit large degrees of parallelism but also because hardware architectures change from system to system. To solve this problem, this paper proposes the novel combination of a new industry standard for communication across multicore architectures (MCAPI), with a minimal-overhead technique for targeting complex architectures with standard programming languages (Compile-Time Virtualisation). The Multicore Association have proposed MCAPI as an industry standard for on-chip communications. MCAPI abstracts the on-chip physical communication to provide the application with logical point-to-point unidirectional channels between nodes (software thread, hardware core, etc.). Compile-Time Virtualisation is used to provide an extremely lightweight implementation of MCAPI, that supports a much wider range of architectures than its specification normally considers. Overall, this unique combination enhances programmability by abstracting on-chip communication whilst also exposing critical parts of the target architecture to the programming language.
computer_security	Real-time and embedded applications often need to satisfy several non-functional properties such as timing. Consequently, performance validation is a crucial stage before the deployment of real-time and embedded software. Cache memories are often used to bridge the performance gap between a processor and memory subsystems. As a result, the analysis of caches plays a key role in the performance validation of real-time, embedded software. In this paper, we propose a novel approach to compute the cache performance signature of an entire program. Our technique is based on exploring the input domain through different path programs. Two paths belong to the same path program if they follow the same set of control flow edges but may vary in the iterations of loops encountered. Our experiments with several subject programs show that the different paths grouped into a path program have very similar and often exactly same cache performance. Our path program exploration can be viewed as partitioning the input domain of the program. Each partition is associated with its cache performance and a symbolic formula capturing the set of program inputs which constitutes the partition. We show that such a partitioning technique has wide spread usages in performance prediction, testing, debugging and design space exploration.
computer_security	This paper proposes a new QoS control scheme that is suited for embedded real-time systems. Our scheme focused on real-time systems where both device control and multimedia processing are required. Such systems needs to keep timing constraints of control tasks while providing the highest possible quality of service(QoS) to multimedia processing tasks. Although many QoS control schemes are proposed and used in distributed multimedia systems, they are not suited for such real-time systems. Their QoS control policies cannot exactly keep the timing constraints of control tasks.To overcome this problem we chose a scheme which uses a table describing resource requirements of all tasks. Resource allocations for tasks and total resource utilization in a system can be calculated from the table. Using our scheme, any QoS control policies, such as a fair-share policy or a priority based policy, can be implemented. In other words, it has become possible for the first time to allow the intention of system designers to be directly reflected on QoS control.We have implemented a CPU time QoS control mechanism based on our proposed scheme and evaluated it on a &mu;-ITRON Ver. 3.0 based real-time OS. The evaluation results demonstrate that the QoS control scheme can keep deadline misses low and CPU utilization high under an overloaded state. The results also show that overhead of the QoS control mechanism is small enough to support both multimedia and control applications.
computer_security	Low-end embedded systems are still programmed in C and assembly, and adopting high-level languages such as C# should reduce the length of their development cycles. For these systems, code size is a major concern, but run-time efficiency should also be reasonable --- programmers will not migrate to C# unless the overhead compared with C is insignificant. In this paper, we propose a static approach based on whole program optimization for implementing .Net generics in such systems. Indeed, the implementation of run-time generics involves a tradeoff between size and run-time efficiency. In this proposal, generic instances are detected through a generalization of RTA to parametric polymorphism. Also, we propose an implementation scheme which employs code sharing and more effective coercions than boxing. Unlike existing implementation schemes, it is scalable in the number of generic instances without involving boxing and unboxing in a systematic way.
computer_security	With the increasing bandwidth demand in system technologies, several generations of memory have been optimized in order to guarantee higher QoS and better performance. Dynamic memory has various advantages in terms of frequency and bandwidth but the periodic refresh operation remains its principle weakness which reduces the chance of its use in reliable hardware real time systems. In spite of the refreshment important role to prevent data loss, this operation decreases predictability and bandwidth by about 3% in dynamic memories. We propose a memory controller which guarantees the refreshment of our DDR3 memory during the write operation. We choose to write in predefined address following a barrel shifter technique. This idea represents the key solution to visit all the rows within the maximum required refresh time. This technique is an original solution to preserve memory write, read and refresh reliability.
computer_security	For the exploitation of the available parallelism clustered Very Long Instruction Word (VLIW) processors rely on highly optimizing compilers. Aiming this parallelism, many advanced compiler concepts have been developed and proposed in the past. Many of them concentrate on loops only as most of the execution time is usually spent executing repeating patterns of code. Software pipelining techniques, such as modulo scheduling, try to speed up the execution of loops by simultaneous initiation of multiple iterations, thus additionally exploiting parallelism across loop iteration boundaries. This increases processor utilization at the cost of higher complexity which is especially true for architectures featuring multiple clusters and distributed register files. Additional scheduling constraints need to be considered in order to produce valid schedules. Targeting TI's TMS320C64x+ clustered VLIW architecture, we describe a code generation approach that adapts an iterative modulo scheduling scheme, and also propose two heuristics for cluster assignment, all together implemented within the popular LLVM compiler framework. We cover implementation of developed algorithms, present evaluation results for a selection of benchmarks popular for embedded system development and discuss gained insights on the topics of integrated modulo scheduling and cluster assignment in this paper.
computer_security	Static analysis tools that are used for worst-case execution time (WCET) analysis of real-time software just provide partial information on an analyzed program. Only the longest-executing path, which currently determines the WCET bound is indicated to the programmer. This limited view can prevent a programmer (or compiler) from targeting optimizations the right way. A possible resort is to use a metric that targets WCET and which can be efficiently computed for all code parts of a program. Similar to dynamic profiling techniques, which execute code with input that is typically expected for the application, based on WCET analysis we can indicate how critical a code fragment is, in relation to the worst-case bound. Computing such a metric on top of static analysis, incurs a certain overhead though, which increases with the complexity of the underlying WCET analysis. We present our approach to estimate the Criticality metric, by relaxing the precision of WCET analysis. Through this, we can reduce analysis time by orders of magnitude, while only introducing minor error. To evaluate our estimation approach and share our garnered experience using the metric, we evaluate real-time programs, which are considered as standard WCET benchmarks. We furthermore demonstrate how the visualization of a Criticality-based profile can aid in the understanding of a program's worst-case behavior.
computer_security	This paper proposes an observer-assisted (indirect) adaptive trajectory tracking control scheme for an under-actuated autonomous underwater robotic vehicle. The proposed control algorithm was based on feedback linearization control, using the estimated vehicle (hydrodynamic) parameters and external disturbance parameters (e.g., underwater current, buoyancy variations, etc.). These parameters were estimated online by employing a well-known non-linear observer extended Kalman filter. Using these estimated parameters and estimated disturbance vector, a feedback linearization control system was constructed for an under-actuated underwater vehicle. The effectiveness of the proposed system demonstrated and discusses the robustness using simulation results. The simulation results were compared with those of a control scheme that employed true parameters and without compensation, as well and results show the proposed scheme works well.
computer_security	Long-term monitoring of cultivation environments and a scientific understanding of tree vigor and grape quality are needed for advanced viticultural management. Cultivation environments, such as micrometeorological data and soil moisture, should be continuously monitored. In addition, we have to get a good grasp of grapevine vigor in association with the surrounding climate and soil water stress, in order to grow high-quality grapes. In this paper, we discuss a wireless sensor network for climate and soil moisture measurement in the vineyard. As regards tree vigor, grapevine sap flow was measured to evaluate gas exchange capacity at grapevine canopies. We also made the color image measurement of grape berries in the vineyard for high quality cultivation.
computer_security	In this demo, we present a platform for a wireless underground sensor network (WUSN). For a large-scale monitoring in places sensitive to disturbances, wireless sensor nodes with a very long battery life time are required. The cost of these nodes can be reduced by using relatively cheap standard components. With optimizations, a low-power communication and operation can be achieved. With our extremely low-power prototype, an underground communication over a distance of 7.5 m (both nodes in a depth of 20 cm) was achieved, while the communication distance underground to above-ground ranged up to 80 m.
computer_security	To ensure that the University of Delaware is doing its best to reduce the chance of identity theft and to help the University meet the obligations imposed by laws regarding personal information, the University developed a campaign to educate faculty and staff about the proper use of Social Security Numbers (SSNs). A team from the Information Technology Department was assigned to work on this project.The first step in this campaign was to ask each department to provide us with information about their use of SSNs. The team designed a web survey to help determine: How and why different University units acquire SSNs. How SSNs are being stored. How SSNs are being guarded.The SSN is currently the student ID, but will be replaced with another identifier in the summer of 2006 when the new Student Information System is implemented.Data from the survey was loaded into an online database for the IT staff members. . The committee used the database as a tool to follow up with departments who responded to the survey. The paper will discuss the project timeline and campaign process. It will also detail the development and use of the online database as a tool to record information and track progress.
computer_vision	The Web has grown into one of the most important channels to communicate social events nowadays. However, the sheer volume of events available in event-based social networks (EBSNs) often undermines the users' ability to choose the events that best fit their interests. Recommender systems appear as a natural solution for this problem, but differently from classic recommendation scenarios (e.g. movies, books), the event recommendation problem is intrinsically cold-start. Indeed, events published in EBSNs are typically short-lived and, by definition, are always in the future, having little or no trace of historical attendance. To overcome this limitation, we propose to exploit several contextual signals available from EBSNs. In particular, besides content-based signals based on the events' description and collaborative signals derived from users' RSVPs, we exploit social signals based on group memberships, location signals based on the users' geographical preferences, and temporal signals derived from the users' time preferences. Moreover, we combine the proposed signals for learning to rank events for personalized recommendation. Thorough experiments using a large crawl of Meetup.com demonstrate the effectiveness of our proposed contextual learning approach in contrast to state-of-the-art event recommenders from the literature.
computer_vision	Activity streams have emerged as a means to syndicate updates about a user or a group of users within a social network site or a set of sites. As the flood of updates becomes highly intensive and noisy, users are faced with a "needle in a haystack" challenge when they wish to read the news most interesting to them. In this work, we study activity stream personalization as a means of coping with this challenge. We experiment with an enterprise activity stream that includes status updates and news across a variety of social media applications. We examine an entity-based user profile and a stream-based profile across three dimensions: people, terms, and places, and provide a rich set of results through a user study that combines direct rating of the objects in the profile with rating of the news items it produces.
computer_vision	We present two ongoing projects aimed at learning from health care records. The first project, DADEL, is focusing on high-performance data mining for detrecting adverse drug events in healthcare, and uses electronic patient records covering seven years of patient record data from the Stockholm region in Sweden. The second project is focusing on heart failure and on understanding the differences in treatment between various groups of patients. It uses a Swedish administrative health register containing health care data for over two million patients.
computer_vision	A common scenario considered in recommender systems is to predict a user's preferences on unseen items based on his/her preferences on observed items. A major limitation of this scenario is that a user might be interested in different things each time when using the system, but there is no way to allow the user to actively alter or adjust the recommended results. To address this issue, we propose the idea of "query-based recommendation" that allows a user to specify his/her search intention while exploring new items, thereby incorporating the concept of information retrieval into recommendation systems. Moreover, the idea is more desirable when the user intention can be expressed in different ways. Take music recommendation as an example: the proposed system allows a user to explore new song tracks by specifying either a track, an album, or an artist. To enable such heterogeneous queries in a recommender system, we present a novel technique called "Heterogeneous Preference Embedding" to encode user preference and query intention into low-dimensional vector spaces. Then, with simple search methods or similarity calculations, we can use the encoded representation of queries to generate recommendations. This method is fairly flexible and it is easy to add other types of information when available. Evaluations on three music listening datasets confirm the effectiveness of the proposed method over the state-of-the-art matrix factorization and network embedding methods.
computer_vision	As users browse a recommender system, they systematically consider or skip over much of the displayed content. It seems obvious that these eye gaze patterns contain a rich signal concerning these users' preferences. However, because eye tracking data is not available to most recommender systems, these signals are not widely incorporated into personalization models. In this work, we show that it is possible to predict gaze by combining easily-collected user browsing data with eye tracking data from a small number of users in a grid-based recommender interface. Our technique is able to leverage a small amount of eye tracking data to infer gaze patterns for other users. We evaluate our prediction models in MovieLens -- an online movie recommender system. Our results show that incorporating eye tracking data from a small number of users significantly boosts accuracy as compared with only using browsing data, even though the eye-tracked users are different from the testing users (e.g. AUC=0.823 vs. 0.693 in predicting whether a user will fixate on an item). We also demonstrate that Hidden Markov Models (HMMs) can be applied in this setting; they are better than linear models in predicting fixation probability and capturing the interface regularity through Bayesian inference (AUC=0.823 vs. 0.757).
computer_vision	Many recommendation techniques rely on the knowledge of preferences data in the form of ratings for items. In this paper, we focus on pairwise preferences as an alternative way for acquiring user preferences and building recommendations. In our scenario, users provide pairwise preference scores for a set of item pairs, indicating how much one item in each pair is preferred to the other. We propose a matrix factorization (MF) and a nearest neighbor (NN) prediction techniques for pairwise preference scores. Our MF solution maps users and items pairs to a joint latent features vector space, while the proposed NN algorithm leverages specific user-to-user similarity functions well suited for comparing users preferences of that type. We compare our approaches to state of the art solutions and show that our solutions produce more accurate pairwise preferences and ranking predictions.
computer_vision	The availability of a huge amount of interconnected data in the so called Web of Data (WoD) paves the way to a new generation of applications able to exploit the information encoded in it. In this paper we present a model-based recommender system leveraging the datasets publicly available in the Linked Open Data (LOD) cloud as DBpedia and LinkedMDB. The proposed approach adapts support vector machine (SVM) to deal with RDF triples. We tested our system and showed its effectiveness by a comparison with different recommender systems techniques -- both content-based and collaborative filtering ones.
computer_vision	An ever increasing number of social services offer thousands of diverse events per day. Users tend to be overwhelmed by the massive amount of information available, especially with limited browsing options perceived in many event web services. To alleviate this information overload, a recommender system becomes a vital component for assisting users selecting relevant events. However, such system faces a number of challenges owed to the the inherent complex nature of an event. In this paper, we propose a novel hybrid approach built on top of Semantic Web. On the one hand, we use a content-based system enriched with Linked Data to overcome the data sparsity, a problem induced by the transiency of events. On the other hand, we incorporate a collaborative filtering to involve the social aspect, an influential feature in decision making. This hybrid system is enhanced by the integration of a user diversity model designed to detect user propensity towards specific topics. We show how the hybridization of CB+CF systems and the integration of interest diversity features are important to improve predictions. Experimental results demonstrate the effectiveness of our approach using precision and recall measures.
computer_vision	Document network is a kind of intriguing dataset which provides both topical (texts) and topological (links) information. Most previous work assumes that documents closely linked with each other share common topics. However, the associations among documents are usually complex, which are not limited to the homophily (i.e., tendency to link to similar others). Actually, the heterophily (i.e., tendency to link to different others) is another pervasive phenomenon in social networks. In this paper, we introduce a new tool, called copula, to separately model the documents and links, so that different copula functions can be applied to capture different correlation patterns. In statistics, a copula is a powerful framework for explicitly modeling the dependence of random variables by separating the marginals and their correlations. Though widely used in Economics, copulas have not been paid enough attention to by researchers in machine learning field. Besides, to further capture the potential associations among the unconnected documents, we apply the tree-averaged copula instead of a single copula function. This improvement makes our model achieve better expressive power, and also more elegant in algebra. We derive efficient EM algorithms to estimate the model parameters, and evaluate the performance of our model on three different datasets. Experimental results show that our approach achieves significant improvements on both topic and link modeling compared with the current state of the art.
computer_vision	Inferring movement trajectories can be a challenging task, in particular when detailed tracking information is not available due to privacy and data collection constraints. In this paper we present a complete and computationally tractable model for estimating and predicting trajectories based on sparsely sampled, anonymous GPS land-marks that we call GPS snippets. To combat data sparsity we use mapping data as side information to constrain the inference process. We show the efficacy of our approach on a set of prediction tasks over data collected from different cities in the US.
computer_vision	In this paper, we present our plan for constructing a platform for collecting, mining, and utilizing behavior data for detecting students with depression risks. Unipolar depression makes a large contribution to the burden of disease, being at the first place in middle- and high-income countries. We survey descriptors of depressions and then design a data collection platform in a classroom based on the assumption that such descriptors are also effective to students with depression risks. Visual, acoustic, and e-learning data are chosen for collection and various issues including devices, preprocessing, and consent agreements are investigated. We also show two kinds of utilization scenarios of the collected data and introduce several techniques and methods we developed for feature extraction and early detection.
computer_vision	This paper addresses an open challenge in educational data mining, i.e., the problem of using observed prerequisite relations among courses to learn a directed universal concept graph, and using the induced graph to predict unobserved prerequisite relations among a broader range of courses. This is particularly useful to induce prerequisite relations among courses from different providers (universities, MOOCs, etc.). We propose a new framework for inference within and across two graphs---at the course level and at the induced concept level---which we call Concept Graph Learning (CGL). In the training phase, our system projects the course-level links onto the concept space to induce directed concept links; in the testing phase, the concept links are used to predict (unobserved) prerequisite links for test-set courses within the same institution or across institutions. The dual mappings enable our system to perform an interlingua-style transfer learning, e.g. treating the concept graph as the interlingua, and inducing prerequisite links in a transferable manner across different universities. Experiments on our newly collected data sets of courses from MIT, Caltech, Princeton and CMU show promising results, including the viability of CGL for transfer learning.
computer_vision	PageRank has been the signature unsupervised ranking model for ranking node importance in a graph. One potential drawback of PageRank is that its computation depends only on input graph structures, not considering external information such as the attributes of nodes. This work proposes AttriRank, an unsupervised ranking model that considers not only graph structure but also the attributes of nodes. AttriRank is unsupervised and domain-independent, which is different from most of the existing works requiring either ground-truth labels or specific domain knowledge. Combining two reasonable assumptions about PageRank and node attributes, AttriRank transfers extra node information into a Markov chain model to obtain the ranking. We further develop approximation for AttriRank and reduce its complexity to be linear to the number of nodes or links in the graph, which makes it feasible for large network data. The experiments show that AttriRank outperforms competing models in diverse graph ranking applications.
computer_vision	We describe and evaluate methods for learning to forecast forthcoming events of interest from a corpus containing 22 years of news stories. We consider the examples of identifying significant increases in the likelihood of disease outbreaks, deaths, and riots in advance of the occurrence of these events in the world. We provide details of methods and studies, including the automated extraction and generalization of sequences of events from news corpora and multiple web resources. We evaluate the predictive power of the approach on real-world events withheld from the system.
computer_vision	We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs.
computer_vision	Due to the exponentially increasing multimedia traffic on the Internet, steganography, hiding messages within a seemingly normal cover object (e.g., an image), becomes potentially a serious national security threat as images and other media files can be exploited to communicate secret messages without even being noticed, a unique advantage of steganography over encryption. Therefore, steganalysis, the detection of the presence of steganographic embedding in media files, has become an active area of research. It appears that the most state of the art steganalysis routines are based on a learning strategy, where a classifier is first learned based on a training set with known cover and stego objects and then the learned classifier is used to detect the presence of new media files; among classifiers, support vector machines (SVMs) are the most commonly used due to their reported effectiveness. In this paper, based on the observation that a stego object file is typically very close to its cover object (so that it will not attract attention), we argue that the generalization performance of support vector machines and other classifiers are inherently limited, unless the steganographic routine has a detectable intrinsic signature. The claim is supported by systematic investigation on the effectiveness of detecting perturbed quantization (PQ), a method of minimal distortion steganography using JPEG images. Experimental results suggest interesting alternatives to the generic machine learning paradigm, which should lead to next generation steganalysis methods.
computer_vision	Uncertain time series analysis is important in applications such as wireless sensor networks and location-based services. This has been the subject of some recent studies, and a number of solution techniques have been proposed for similarity search problems. We classify the proposed similarity measures into deterministic, which returns a value, and probabilistic, which returns a random variable. By means of our classification, we present an overview of the proposed similarity measures and evaluate them experimentally. We conducted a comprehensive performance evaluation of these techniques through numerous experiments using the well-known real-life UCR benchmark data. As the computational complexity of some of these similarity measures was very high, we devised an effective sampling-based heuristic method to complete the experiments which could not be done before. The results of our experimental evaluation and comparison provide useful insights and guidelines for researchers and practitioners in similarity search and analysis of uncertain time series data.
computer_vision	In this paper we describe an approach to classifying heart sounds (classes Normal, Murmur and Extra-systole) that is based on the discretization of sound signals using the SAX (Symbolic Aggregate Approximation) representation. The ability of automatically classifying heart sounds or at least support human decision in this task is socially relevant to spread the reach of medical care using simple mobile devices or digital stethoscopes. In our approach, sounds are first pre-processed using signal processing techniques (decimate, low-pass filter, normalize, Shannon envelope). Then the pre-processed symbols are transformed into sequences of discrete SAX symbols. These sequences are subject to a process of motif discovery. Frequent sequences of symbols (motifs) are adopted as features. Each sound is then characterized by the frequent motifs that occur in it and their respective frequency. This is similar to the term frequency (TF) model used in text mining. In this paper we compare the TF model with the application of the TFIDF (Term frequency - Inverse Document Frequency) and the use of bi-grams (frequent size two sequences of motifs). Results show the ability of the motifs based TF approach to separate classes and the relative value of the TFIDF and the bi-grams variants. The separation of the Extra-systole class is overly difficult and much better results are obtained for separating the Murmur class. Empirical validation is conducted using real data collected in noisy environments. We have also assessed the cost-reduction potential of the proposed methods by considering a fixed cost model and using a cost sensitive meta algorithm.
computer_vision	Decision making in the business sector is considered one of the sensitive tasks. The business managers always makes decisions one thing in mind the power selling of items. The objective of the study is to develop a mechanism for the identification of Dead Stock, Slow and Fast Moving items and analyze the trends of the certain products of certain properties. In general practice, Lot of items are available for selling, in which different variations are found according to their sales i.e. some will be high sells, few may be moderate sells and few may be low sales items but the problem is to find the selling power of a product in the market. The problem is the evaluation of stock data to identify the reasons of dead stock, slow moving and fast moving. The problem which will be investigated is finding out the selling power of products in the market based on various scenarios. We have to investigate how to distinguish the selling frequency of items on the basis the known attributes. Form these known attributes we have to predict that certain products of certain properties have what type of sales trends in various locations
computer_vision	Cartesian Genetic Programming based Neuroevolutionary algorithm is proposed. It encodes the neural network attributes namely weights, topology and functions and then evolves them for best possible weight, topology and function. The architecture generated are both feedforward and recurrent. The proposed algorithm is applied on the standard benchmark control problem: balancing single and double pole at both markovian and non-markovian states. Results demonstrate that CGPANN has the potential to generate neural architecture and parameters in substantially fewer number of evaluations in comparison to earlier neuroevolutionary techniques. The power of CGPANN is its representation which leads to a thorough evolutionary search producing generalized networks. This opens new avenues of applying the proposed technique to any non-linear and dynamic problem.
computer_vision	This paper describes a feature extraction technique from human face image sequences using model based approach. We study two different models with our proposed approach towards multifeature extraction. These features are efficiently used for human face information extraction for different applications. The approach follows in fitting a model to face image using robust objective function and extracting textural and temporal features for three major applications naming 1) face recognition, 2) facial expressions recognition and 3) gender classification. For experimentation and comparative study of our multi-features over two models, we use same set of features with two different classifiers generating promising results to explain that extracted features are strong enough to be used for face image analysis. Features goodness has been investigated on Cohn Kanade Facial Expressions Database (CKFED). The proposed multi-features approach is automatic and real time.
computer_vision	Background: Since the introduction of the systematic review process to Software Engineering in 2004, researchers have investigated a number of ways to mitigate the amount of effort and time taken to filter through large volumes of literature. Aim: This study aims to provide a critical analysis of text mining techniques used to support the citation screening stage of the systematic review process. Method: We critically re-reviewed papers included in a previous systematic review which addressed the use of text mining methods to support the screening of papers for inclusion in a review. The previous review did not provide a detailed analysis of the text mining methods used. We focus on the availability in the papers of information about the text mining methods employed, including the description and explanation of the methods, parameter settings, assessment of the appropriateness of their application given the size and dimensionality of the data used, performance on training, testing and validation data sets, and further information that may support the reproducibility of the included studies. Results: Support Vector Machines (SVM), Na?ve Bayes (NB) and Committee of classifiers (Ensemble) are the most used classification algorithms. In all of the studies, features were represented with Bag-of-Words (BOW) using both binary features (28%) and term frequency (66%). Five studies experimented with n-grams with n between 2 and 4, but mostly the unigram was used. ?2, information gain and tf-idf were the most commonly used feature selection techniques. Feature extraction was rarely used although LDA and topic modelling were used. Recall, precision, F and AUC were the most used metrics and cross validation was also well used. More than half of the studies used a corpus size of below 1,000 documents for their experiments while corpus size for around 80% of the studies was 3,000 or fewer documents. The major common ground we found for comparing performance assessment based on independent replication of studies was the use of the same dataset but a sound performance comparison could not be established because the studies had little else in common. In most of the studies, insufficient information was reported to enable independent replication. The studies analysed generally did not include any discussion of the statistical appropriateness of the text mining method that they applied. In the case of applications of SVM, none of the studies report the number of support vectors that they found to indicate the complexity of the prediction engine that they use, making it impossible to judge the extent to which over-fitting might account for the good performance results. Conclusions: There is yet to be concrete evidence about the effectiveness of text mining algorithms regarding their use in the automation of citation screening in systematic reviews. The studies indicate that options are still being explored, but there is a need for better reporting as well as more explicit process details and access to datasets to facilitate study replication for evidence strengthening. In general, the reader often gets the impression that text mining algorithms were applied as magic tools in the reviewed papers, relying on default settings or default optimization of available machine learning toolboxes without an in-depth understanding of the statistical validity and appropriateness of such tools for text mining purposes.
computer_vision	Smart environments have emerged as a thriving field of research. Understanding user's context is one of the core areas of smart home research. In this study we present a smart home living experiment in which a test subject lived two weeks in the EleHome lab. During this time sensor data was gathered along with annotation data. We used na?ve Bayes classifier in a supervised learning setting to recognize a few of the basic contexts and to validate the gathered data. The first results were good for simpler contexts, but further analysis is needed for more complex activities. All in all the study gives us a good baseline for further development of context recognition methods.
computer_vision	Multi-objective optimization has been widely used in evolutionary computation for solving problems in which two or more conflicting objectives need to be optimized in a simultaneous fashion. This paper presents a multi-objective hyper-heuristic based on evolutionary algorithms that automatically designs complete decision-tree induction algorithms. Such algorithms are designed to generate decision trees that present an interesting trade-off between predictive performance and complexity. The proposed approach is tested over 20 UCI datasets, and it is compared with a single-objective hyper-heuristic as well as with traditional decision-tree induction algorithms. Experimental results show that the proposed approach can match the top predictive performance achieved by the baseline methods, without significant loss in model comprehensibility.
computer_vision	In this paper we investigate a method for classifying microarray data using association rules. Associative classifiers, classification systems based on association rules, show good performance level while being easy to read and understand. This feature is especially attractive for biological data where experts can read and validate the association rules. Relevant features are selected using Support Vector Machines with Recursive Feature Elimination. These features are discretized according to their relative expression levels (upregulated, downregulated or no change) and then they are used to build an associative classifier. The proposed combination proves highly accurate for the studied microarray data collection. In addition the classification rules discovered and employed in the classification process prove to be biologically relevant.
computer_vision	Keyword spotting is the task of retrieving all instances of a given keyword in a set of documents. In the current paper we consider the problem of keyword spotting in handwritten text. This is a difficult problem due to the great variety of different writing styles. Recently, learning based keyword spotting systems have been shown to outperform traditional approaches, at the cost of requiring large amounts of training data. The training data need to be manually labeled, which is tedious and time-consuming. In this paper we propose to exploit unlabeled data via semi-supervised learning to reduce the need for labeled data when training a keyword spotting system. We demonstrate, on historic as well as modern handwritten text, that the performance of a learning based keyword spotting system can be dramatically increased using this approach.
computer_vision	In this work, we propose an approach based on analyzing the spatio-temporal partitions of a system log, generated by supercomputers consisting of several nodes, for alert detection without employing semantic analysis. In this case, "Spatial" refers to the source of the log event and "Temporal" refers to the time the log event was reported. Our research shows that these spatio-temporal partitions can be clustered to separate normal activity from anomalous activity, with high accuracy. Therefore, our proposed method provides an effective alert detection mechanism.
computer_vision	In character recognition, multiple prototype classifiers, where multiple patterns are prepared as representative patterns of each class, have often been employed to improve recognition accuracy. Our question is how we can improve the recognition accuracy by increasing prototypes massively in the multiple prototype classifier. In this paper, we will answer this question through several experimental analyses, using a simple 1-nearest neighbor (1-NN) classifier and about 550,000 manually labeled handwritten numeral patterns. The analysis results under the leave-one-out evaluation showed not only a simple fact that more prototypes provide fewer recognition errors, but also a more important fact that the error rate decreases approximately to 40% by increasing the prototypes 10 times. The analysis results also showed other phenomena in massive character recognition, such that the NN prototypes become visually closer to the input pattern by increasing the prototypes.
computer_vision	This paper presents False Positive-Critical Classifiers Comparison a new technique for pairwise comparison of classifiers that allow the control of bias. An evaluation of Na?ve Bayes, k-Nearest Neighbour and Support Vector Machine classifiers has been carried out on five datasets containing unsolicited and legitimate e-mail messages to confirm the advantage of the technique over Receiver Operating Characteristic curves. The evaluation results suggest that the technique may be useful for choosing the better classifier when the ROC curves do not show comprehensive differences, as well as to prove that the difference between two classifiers is not significant, when ROC suggests that it might be. Spam filtering is a typical application for such a comparison tool, as it requires a classifier to be biased toward negative prediction and to have some upper limit on the rate of false positives. Finally the particular evaluation summary is presented, which confirms that Support Vector Machines out-perform other methods in most cases, while the Na?ve Bayes classifier works well in a narrow, but relevant range of false positive rate.
computer_vision	Clustering is a fundamental task in Knowledge Discovery and Data mining. It aims to discover the unknown nature of data by grouping together data objects that are more similar. While hundreds of clustering algorithms have been proposed, many are complex and do not scale well as more data become available, making then inadequate to analyze very large datasets. In addition, many clustering algorithms are sequential, thus inherently difficult to parallelize. We propose PatchWork, a novel clustering algorithm to address those issues. PatchWork is a distributed density clustering algorithm with linear computational complexity and linear horizontal scalability. It presents several desirable characteristics in knowledge discovery, in particular, it does not require a priori the number of clusters to identify, and offers a natural protection against outliers and noise. In addition, PatchWork makes it possible to discover spatially large clusters instead of dense clusters only. PatchWork relies on the map/reduce paradigm to parallelize computations and was implemented using Apache Spark, the distributed computation framework. As a result, PatchWork can cluster a billion points in a few minutes only, a 40x improvement over the distributed implementation of k-means in Spark MLLib.
computer_vision	Software product lines (SPL) aim at facing the increasing costs of software products by reusing core assets of existing products in a given domain. They are often described using feature models which, as we proposed in a previous work, can be built from possibly incomplete, documented UML use case diagrams assets using the Formal Concept Analysis method, semantic model and trigger model. In order to evaluate this approach, we present in this paper the UC2FM-tool which automates all its steps. In addition, we report on a comparison of the values of quality metrics of feature models produced by our approach with those of existing feature models built by experts for five different domains.
computer_vision	The goal of this paper is to improve the Named Entity Recognition for automatic information extraction related to record based data in text documents.
computer_vision	Existing works suggest that random inputs and random features produce good results in classification. In this paper we study the problem of generating random rule sets from data streams. One of the most interpretable and flexible models for data stream mining prediction tasks is the Very Fast Decision Rules learner (VFDR). In this work we extend the VFDR algorithm using random rules from data streams. The proposed algorithm generates several sets of rules. Each rule set is associated with a set of Natt attributes. The proposed algorithm maintains all properties required when learning from stationary data streams: online and any-time classification, processing each example once.
computer_vision	Due to the growing number of Web shops, aggregating product data from the Web is growing in importance. One of the problems encountered in product aggregation is duplicate detection. In this paper, we extend and significantly improve an existing state-of-the-art product duplicate detection method. Our approach employs a novel method for combining the titles' and the attributes' similarities into a final product similarity. We use q-grams to handle partial matching of words, such as abbreviations. Where existing methods cluster products of only two Web shops, we propose a hierarchical clustering method to handle multiple Web shops. Applying our new method to a dataset of TV's from four Web shops reveals that it significantly outperforms the Hybrid Similarity Method, the Title Model Words Method, and the well-known TF-IDF method, with an F1 score of 0.475 compared to 0.287, 0.298, and 0.335, respectively.
computer_vision	Prior works on traffic classification mainly focus their attentions on dividing Internet traffic into different categories based on their application layer protocols (such as BitTorrent, eDonkey etc.). Making traffic classification from another point of view, we divide Internet traffic into different content types. Our technology is an attempt to solve the classification problem of unknown and proprietary protocols. In this paper, we design a classifier which can distinguish Internet traffic into different content types using machine learning techniques, and features are the entropy of consecutive bytes and frequency of characters. The chief features of our classifier are high classification accuracy (about 81%) and small computing space (about 1K Bytes).
computer_vision	The recent explosion in available SNP data requires exploration of satisfactory methods for efficiently clustering a set of individuals into their respective populations. As a practical matter, we describe a modified euclidean distance-based approach for successfully clustering 525 HapMap individuals into their 4 original populations; European, African, Japanese and Chinese. Our approach relies on the computation of the Fst estimator using 4 distinct methods, and shows that that the k-means clustering of the 10 highest Fst scoring SNPs is sufficient for producing an error-free description of the underlying population structure. A generalization of our approach represents a more "faithful" way of selecting the SNPs having the highest discriminating power and thus generating the most accurate population specific assignments of individuals, using the smallest possible SNP panel.
computer_vision	This paper presents a powerful, practical, and essentially language-independent syntactic error diagnosis and recovery method that is applicable within the frameworks of LR and LL parsing. The method generally issues accurate diagnoses even where multiple errors occur within close proximity, yet seldom issues spurious error messages. It employs a new technique, parse action deferral, that allows the most appropriate recovery in cases where this would ordinarily be precluded by late detection of the error. The method is practical in that it does not impose substantial space or time overhead on the parsing of correct programs, and in that its time efficiency in processing an error allows for its incorporation in a production compiler. The method is language independent, but it does allow for tuning with respect to particular languages and implementations through the setting of language-specific parameters.
computer_vision	This paper proposes a data-driven method for constructing materials to be used in a probabilistic knowledge base for human activity recognition. The utilized dataset, challenge subset of Opportunity, is a publicly available dataset. It consists of a set of daily activities, which has been manually labeled as modes of locomotion and gestures. We applied several methods to extract proper features from sensors on bodies of subjects, then, chosen features are fed into two different classifiers. Finally, predicted labels for modes of locomotion and hand gestures are calculated. To evaluate the method, the recognition rates are benchmarked against the results of the competitors who have participated in Opportunity challenge as well as the baseline results provided by the Opportunity group. For modes of locomotion, our results surpass all of the available results and in some cases the recognition rate of our model is very close to the highest recognition rate. For gestures, regular or noisy data, in some cases our method is still higher than baseline or challenge participants but unlike locomotion, it is not capable to beat them all.
computer_vision	Allocation of independent jobs to available resources in a computational grid environment is a combinatorial optimization problem which comes under category of NP hard, NP complete problems. The main objective of this study is to maximize the resource utilization in computational grid environment. The second objective is user satisfaction by minimizing job completion time. This paper presents genetic algorithm with overlapping populations and parallel genetic algorithm to the solution of the problem of job scheduling with resource and timing constraints. The scheduling problem is tackled in preemptive and non-preemptive mode. Two dimensional chromosome representation is proposed for the job assignment problem which reduces the calculation in fitness evaluations. Proposed Genetic algorithms are applied to three problem instances and the results obtained are compared with the results from literature.
computer_vision	The goal of this research is to develop a process, using current imaging hardware and without human intervention, that provides an accurate and timely detection alert of a concealed weapon and its location in the image of the luggage. There are several processes in existence that are able to highlight or otherwise outline a concealed weapon in baggage but so far those processes still require a highly trained operator to observe the resulting image and draw the correct conclusions. We attempted three different approaches in this project. The first approach uses edge detection combined with pattern matching to determine the existence of a concealed pistol. Rather than use the whole body of the weapon which varies significantly, the trigger guard was used since it is fairly consistent in dimensions. While the processes were reliable in detecting a pistol's presence, on any but the simplest of images, the computational time was excessive and a substantial number of false positives were generated. The second approach employed Daubechie wavelet transforms but the results have so far been inconclusive. A third approach involving an algorithm based on the scale invariant feature transform (SIFT) is proposed.
computer_vision	In this paper we present a workflow for the automated creation of parallel domain-specific corpora, i.e. multilingual translated text collections of a certain domain, in which the text pieces are aligned at sentence level. The source for the text extraction are Wikipedia articles. This workflow will be adaptable to any language pair, though the first implementation is targeting English/Japanese. The workflow consists of intelligent text acquisition, text alignment including novel techniques, and large scale quality evaluation by human experts. This will enable us to create an adaptable, fine-tuned system as well as high quality corpora, which will be compiled during the implementation.
computer_vision	Deal with Call for Paper (CFP) is a time consuming task, find if a conference suit to your requirement, check deadlines, specifications, etc... In order to improve this matter, this paper proposes a web-based system to automatically extract useful data from CFP emails using six different text mining methods and organizes them in an ontology model which advises user about interesting conferences according to his personal data. Focusing on both extraction method and ontology model, we will also slightly describe future work and improvement in a large-scale.
computer_vision	The goal of our two year NSF National Science Digital Library-funded project is to develop Natural Language Processing technology that will automatically produce metadata values that correlate individual educational resources in digital libraries to content standards. The goal is to assign this metadata to the descriptive metadata records for resources in support of standards-based discovery and retrieval. The project will utilize the Achieve/McREL Compendix, a comprehensive knowledgebase of K-12 content standards derived from over 137 state, national and international content standards documents. The test collection of educational resources being analyzed is drawn from the more than 400 Web-based collections represented in the Gateway to Educational Materials catalog.The significance of this project in terms of the Digital Library movement is that high-quality automatic correlation of educational resources to content standards is essential to meet the demands for searching and retrieving such resources based on those correlations. This demand will increase as the national focus on greater accountability in our K-12 institutions increases. While human correlations of resources to content standards characterize current practice, it is clear that the scale of the need for such correlations calls for sophisticated means for automatic mapping. This project is intended to provide an NLP-based solution to the problem.Briefly, our NLP approach in this project is to analyze language utilizing all the levels through which humans extract meaning-morphological, lexical, syntactic, semantic, discourse, and pragmatic. The extent to which an individual technology includes these levels, particularly the higher-level ones determines the capability and sophistication of the resultant application. Having incorporated each of these levels into our baseline NLP document-processing module, we are extending the system's capabilities in this project to the task of learning the linguistic features that can be relied on to indicate what content standard an educational resource supports.We are applying a sublanguage analysis framework to automatically identify clues that can be recognized in the mathematics and science educational materials to indicate to which standards the resources apply. Based on the discourse model, the system learns from recognizing these linguistic clues in the training set. The system will then be able to process new resources as they are added to the digital library and appropriately assign to the metadata for those resources the learning standards to which they are applicable.This work is a continuation of our NSF NSDL project "Breaking the Metadata Generation Bottleneck" where we were successful in processing text to automatically assign metadata tags for the descriptive and subject aspects of educational resources.
computer_vision	This paper addresses the problem of determining the best answer in Community-based Question Answering websites by focussing on the content. Previous research on this topic relies on the exploitation of community feedback on the answers, which involves rating of either users (e.g., reputation) or answers (e.g. scores manually assigned to answers). We propose a new technique that leverages the content/textual features of answers in a novel way. Our approach delivers better results than related linguistics-based solutions and manages to match rating-based approaches. More specifically, the gain in performance is achieved by rendering the values of these features into a discretised form. We also show how our technique manages to deliver equally good results in real-time settings, as opposed to having to rely on information not always readily available, such as user ratings and answer scores. We ran an evaluation on 21 StackExchange websites covering around 4 million questions and more than 8 million answers. We obtain 84% average precision and 70% recall, which shows that our technique is robust, effective, and widely applicable.
computer_vision	In this work we propose a Web-centric approach for estimating legislative bill tendency. Our main assumption is that the current state of the Web represents a complex system that reflects human thinking and behavior. Today's Web services are characterized by user generated content, allowing everyone to interact and share their view about almost any topic, in the form of posts, comments, reviews, etc. If that data is extracted and efficiently aggregated, it could be possible to obtain a general estimation or view of a given phenomenon or event. We perform semi supervised classification of legislative bill by generating vector representations using three methods, Term Frequency vectors, Topic Models and Word Embeddings with the goal of estimating the tendency of a bill to favor corporations and industries over common good. The output, which can be seen as an estimation of the ideology of a bill, is then used to support political analysis, specifically, to study the relationship between campaign funding and voting behavior.
computer_vision	Text mining on a lexical basis is quite well developed for the English language. In compounding languages, however, lexicalized words are often a combination of two or more semantic units. New words can be built easily by concatenating existing ones, without putting any white spaces in between. That poses a problem to existing search algorithms: Such compounds could be of high interest for a search request, but how can be examined whether a compound comprises a given lexeme? A string match can be considered as an indication, but does not prove semantic relation. The same problem is faced when using lexicon based approaches where signal words are defined as lexemes only and need to be identified in all forms of appearance, and hence also as component of a compound. This paper explores the characteristics of compounds and their constituent elements for German, and compares seven algorithms with regard to runtime and error rates. The results of this study are relevant to query analysis and term weighting approaches in information retrieval system design.
computer_vision	Research on differencing and versioning in computer science, writing research, and scholarly editing are all concerned with the evolution of texts and documents through various drafts, versions, and changes between different stages. While the different disciplines all have their own perspectives, on closer inspection, there is a large overlap of concerns. However, there are currently no points of contacts between these disciplines. In this vision paper, we propose a working definition of "version" to reconcile the different views and open the way to closer collaboration. In particular, we propose to model the semantics of changes in what seems to be unstructured text as differences between syntactic parse trees.
computer_vision	The DH-CASE II Workshop, held in conjunction with ACM Document Engineering 2014, focused on the tools and environments that support annotation, broadly defined, including modeling, authoring, analysis, publication and sharing. Participants explored shared challenges and differing approaches, seeking to identify emerging best practices, as well as those approaches that may have potential for wider application or influence.
computer_vision	The present paper deals with strong-monotonic, monotonic and weak-monotonic language learning from positive data as well as from positive and negative examples. The three notions of monotonicity reflect different formalizations of the requirement that the learner has to produce always better and better generalizations when fed more and more data on the concept to be learnt. We characterize strong-monotonic, monotonic, weak-monotonic and finite language learning from positive data in terms of recursively generable finite sets, thereby solving a problem of Angluin (1980). Moreover, we study monotonic inference with iteratively working learning devices which are of special interest in applications. In particular, it is proved that strong-monotonic inference can be performed with iteratively learning devices without limiting the inference capabilities, while monotonic and weak-monotonic inference cannot.
computer_vision	This paper proposes a personal pronoun resolution algorithm based on multi-level linguistic knowledge for Chinese topic-oriented microblogs. Experimental result achieves 91.37% in F-measure, significantly outperforming the baseline. It indicates the multi-level linguistic features can work efficiently in the resolution and the algorithm is suitable for the domain of Chinese topic-oriented microblogs.
computer_vision	Microblogging is becoming a popular social media in recent years. Observations show that a large part of posts in microblogging were talking about public events occurred in the real world. Public concerns reflect interests and expectations of the mass for an event. Therefore, to understand and analyze of public concerns will help us to grasp an event, and predict its trend. This paper presents an evolution analysis method of public concerns for a special kind of post in microblogging, which can provides sufficient background information about an event by its attachments, e.g. a URL for details, a picture, or a video, etc. we called it expandable post. We use expandable posts to reconstruct the topic space. Their reposts are regarded as public concerns, and are located on the space. Thus, the task of tracking public concerns is transformed into tracking the movement of those reposts, and analyzing the relationships between them and their corresponding expandable posts on the topic space. The preliminary experiments on our dataset about H7N9 bird flu collected from Weibo, shows the effectiveness of our method.
computer_vision	Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.
computer_vision	Monitoring social media in critical disaster situations can potentially assist emergency and media personnel to deal with events as they unfold, and focus their resources where they are most needed. We address the issue of filtering massive amounts of Twitter data to identify high-value messages related to disasters, and to further classify disaster-related messages into those pertaining to particular disaster types, such as earthquake, flooding, fire, or storm. Unlike post-hoc analysis that most previous studies have done, we focus on building a classification model on past incidents to detect tweets about current incidents. Our experimental results demonstrate the feasibility of using classification methods to identify disaster-related tweets. We analyse the effect of different features in classifying tweets and show that using generic features rather than incident-specific ones leads to better generalisation on the effectiveness of classifying unseen incidents.
computer_vision	We describe a plan to create an auditable version of Multics. The engineering experiments of that plan are now complete. Type extension as a design discipline has been demonstrated feasible, even for the internal workings of an operating system, where many subtle intermodule dependencies were discovered and controlled. Insight was gained into several tradeoffs between kernel complexity and user semantics. The performance and size effects of this work are encouraging. We conclude that verifiable operating system kernels may someday be feasible.
computer_vision	The efficiency of replacement algorithms in paged virtual-storage systems depends on the locality of memory references. The restructuring of the blocks which compose the program may improve this locality. [HATFIELD and GERALD 71] [MASUDA SHIOTA NOGUCHI and OHKI 74] [FERRARI 76]. In confining this restructuring to the link-editing operation, a general and completely automatic solution has been implemented, in the form of a self-adaptative system, on the SIRIS 8 operating system. A reduction of 40 to 70% in the page fault rate has been obtained.
computer_vision	There is a growing need to develop effective interaction methods that enable a single operator to manage a team of multiple robots. This paper presents a novel approach that involves treating the team as a moldable volume, in which deformations of the volume correspond to changes in team shape. The team possesses a level of autonomy that allows the team to travel to and surround buildings of interest in a patrol and cordon scenario. During surround mode, the operator explores or manipulates the team shape to create desired formations around a building. A spacing interaction method also allows the operator to adjust how robots are spaced within the current shape. Separate haptic feedback is developed for each method to allow the operator to "feel" the shape or spacing manipulation. During travel mode, the operator chooses desired travel locations and receives feedback to help identify how and where the team travels. Results from a user study suggest that haptic feedback significantly improves operator performance in a reconnaissance task when task demand is higher, but may slightly increase operator workload. In the context of the experimental setup, these results suggest that haptic feedback may contribute to heads-up control of a team of autonomous robots. There were no significant differences in levels of situation awareness due to haptic feedback in this study.
computer_vision	Cloud computing is getting increasingly popular, but has yet to be widely adopted arguably because there are many security and privacy problems that have not been adequately addressed. A specific problem encountered in the context of cloud storage, where clients outsource their data (files) to untrusted cloud storage servers, is to convince the clients that their data are kept intact at the storage servers. An important approach to achieve this goal is called Proof of Retrievability (POR), by which a storage server can convince a client --- via a concise proof --- that its data can be recovered. However, existing POR solutions can only deal with static data (i.e., data items must be fixed), and actually are not secure when used to deal with dynamic data (i.e., data items need be inserted, deleted, and modified). Motivated by the need to securely deal with dynamic data, we propose the first dynamic POR scheme for this purpose. Moreover, we introduce a new property, called fairness, which is necessary and also inherent to the setting of dynamic data because, without ensuring it, a dishonest client could legitimately accuse an honest cloud storage server of manipulating its data. Our solution is based on two new tools, one is an authenticated data structure we call range-based 2-3 trees (rb23Tree for short), and the other is an incremental signature scheme we call hash-compress-and-sign (HCS for short). These tools might be of independent value as well.
computer_vision	Over the past few years, researchers have been striving for a Utility Computing Model. Cloud computing allows delivering information technology on demand. Cloud Computing service providers provides resources by means of virtualization. Applying these techniques however goes along with handling over ultimate control of data to a third party. This paper discusses several issues in virtualization security, summery of various solutions, risk prevention in VMM (virtual machine monitor). Here an attempt is made to perform the analysis of various issues pertaining to virtualization and security.
computer_vision	About 38% faults of IM are stator faults. This paper present an ANN based MCSA method for detection of incipient stator winding faults in IM, mainly, inter-turn and phase-to-phase faults. An experimental setup implemented in the Electrical Machines Laboratory of K. K. Wagh Institute of Engineering Education and Research, Nashik (India) consists of a specially designed test motor, having loading facility. The test motor is a three phase 415 V, 50 Hz, 2 HP, 4 pole squirrel cage IM with loading arrangement. In order to create the stator faults, four tappings on the stator windings of each phase are taken out. The inter-turn and phase-to-phase faults in the stator windings are created by short circuiting the appropriate tappings. The data acquisition is done through the ordinary sound card of PC; using data acquisition tool box in MATLAB. The signal processing and implementation of ANN is also done in MATLAB. The results obtained are presented; based on these results it is concluded that, using ANN based MCSA method the stator faults in IM can be detected in their developing stage. Therefore, preventive maintenance can be carried out well before the occurrence of the probable failure of IM, thereby resulting in savings in terms of post failure maintenance time and maintenance cost.
computer_vision	The Digital Watermark technology and the Mobile Agent technology play significant roles in the industrial applications in tandem with each other. In recent years, the union of the mobile agent technology and the watermark technology has been intensively investigated. A new architecture is proposed using this integration technology to protect copyright by detecting the watermark on the internet. This architecture comprises four integral blocks: Author Block (AB), User Block (UB), Watermark Agent Block (WAB) and Mobile Agent Block (MAB). In AB, author embeds his ownership watermark in his work and then registers the same in Copyright Management Server (CMS). Then CMS embeds the second watermark in the digital work produced by watermark agent. This is actually a hidden agent. In UB, user submits detail information about personality and digital work to CMS. CMS gives user the digital work and user key according to the input information. WAB creates the custom-built watermark agents and designs the route strategy. In MAB, the watermark agents are dispatched to the suspicious hosts by WAB and are examined for their copyright.
computer_vision	This paper presents the Hefestos Smart Wheelchair, a wheelchair with sensing capability, designed to provide ubiquitous accessibility. The project aims at supporting accessibility for wheelchair users in various situations of their everyday life. Offering context awareness, user profiles, and trails management. We also present the evaluation of the prototype based on an academic experiment. This assessment used the smart wheelchair, which is under development at University of the Rio dos Sinos Valley (Unisinos). The prototype was operated by ten wheelchair users.
computer_vision	This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.
computer_vision	The detection of malicious files is an important component of any intrusion detection system. Due to increase in network speeds and new worms being discovered frequently, there arises a need to detect worms on the fly without disassembling or running them. Signature-based detection is considered to be an important technique in anti-virus applications because of its accuracy and lack of false positives. by checking the signatures stored in the database. As the database of signatures increase, the time required to perform the pattern matching operation in software increases. FPGAs(Field Programmable Gate Arrays) present us with ideal platforms for these kinds of pattern matching applications since they are characterized by low cost and short application development cycles. They also provide a right compromise between flexibility of re-programming and capability in operating at high speeds. In this paper we present a design where FPGA is used as a co-processor to help out with signature matching. They can also be extended to perform inline matching where the packets can be inspected for protocol analysis.
computer_vision	Coordinated data structures are sets of (perhaps unbounded) data structures where the nodes of each structure may share abstract types with the corresponding nodes of the other structures. For example, consider a list of arguments, and a separate list of functions, where the n-th function of the second list should be applied only to the n-th argument of the first list. We can guarantee that this invariant is obeyed by coordinating the two lists, such that the type of the n-th argument is existentially Quantified and identical to the argument type of the n-th function. In this paper, we describe a minimal set of features sufficient for a type system to support coordinated data structures. We also demonstrate that two known type systems (Crary and Weirich's LX [6] and Xi, Chen and Chen's guarded recursive datatypes [24]) have these features, even though the systems were developed for other purposes. We illustrate the power of coordinated data structures as a programming idiom with three examples: (1) a list of function closures stored as a list of environments and a separate list of code pointers, (2) a "tagless" list, and (3) a red-black tree where the values and colors are stored in separate trees that are guaranteed to have the same shape.
computer_vision	This extended abstract presents the techniques to identify C++ data structures in binary executables. With respect to automated tools, this is a largely open problem and generally requires significant manual intervention, inspection, and tracing to establish. The techniques for manual evaluation of C++ data structures are well known, but tedious. Because of this manual handling, the results are error prone and sensitive to the time available and experience of the analyst. All of our work is accomplished using the ROSE compiler infrastructure. ROSE is an open source compiler infrastructure that handles source code, and also binary executables. Uniquely ROSE handles binary executables much like source code, parsing them to identify and represent their internal parts in an intermediate representation (IR), disassembling the appropriate segments containing instructions, defining a number of standard forms of program analysis, and permitting users to define there own specialized forms of analysis. The work to reconstruct C++ data structures is part of larger work that reconstructs all the data used in the binary more generally.
computer_vision	The cloud services invocation from the handset enables the next generation of mobile applications that are not limited by storage space and processing power. Bakabs is one such application for Android and iOS devices, which makes use of Google Analytics cloud services to track the traffic of websites, replicated on multiple instances in different locations. Bakabs also suggests the number and type of instances that are required to handle the loads, based on the linear programming model. The prediction is performed at the mobile cloud middleware, which facilitates invocation of multiple cloud services from mobiles. Based on the suggestions, the user can decide to turn on/off instances, thus saving costs taking advantage of the pay-as-you-go model and the elasticity of the cloud. The performance analysis of the application shows that Bakabs can utilize cloud services with significant ease and reasonable performance latencies on the devices, with the considered technological choices.
computer_vision	We have seen the rise of smartphones and highly capable mobile computing platforms over a relatively short period of time. These devices are quickly becoming ubiquitous and already enable access to basic sensors to augment the excellent communication and computing capabilities. However, advanced sensor interaction has been limited. Readily available environment and health sensors cannot effectively leverage the smartphone yet. One such health application that holds much promise is breath analysis through laser absorption spectroscopy (LAS). Breath analysis can help augment existing diagnostic techniques with early detection. Technology advances have led to the development of portable LAS sensors that can monitor the user's health and the environment. Our demonstration will show such a portable laser sensor along with an event-driven data collection architecture to enable future personalized health applications.
computer_vision	Hybrid modelers such as Simulink have become corner stones of embedded systems development. They allow both discrete controllers and their continuous environments to be expressed in a single language. Despite the availability of such tools, there remain a number of issues related to the lack of reproducibility of simulations and to the separation of the continuous part, which has to be exercised by a numerical solver, from the discrete part, which must be guaranteed not to evolve during a step. Starting from a minimal, yet full-featured, Lustre-like synchronous language, this paper presents a conservative extension where data-flow equations can be mixed with ordinary differential equations (ODEs) with possible reset. A type system is proposed to statically distinguish discrete computations from continuous ones and to ensure that signals are used in their proper domains. We propose a semantics based on non-standard analysis which gives a synchronous interpretation to the whole language, clarifies the discrete/continuous interaction and the treatment of zero-crossings, and also allows the correctness of the type system to be established. The extended data-flow language is realized through a source-to-source transformation into a synchronous subset, which can then be compiled using existing tools into routines that are both efficient and bounded in their use of memory. These routines are orchestrated with a single off-the-shelf numerical solver using a simple but precise algorithm which treats causally-related cascades of zero-crossings. We have validated the viability of the approach through experiments with the Sundials library.
computer_vision	Symbolic schedulability analysis of dataflow graphs is the process of synthesizing the timing parameters (i.e. periods, phases, and deadlines) of actors so that the task system is schedulable and achieves a high throughput when using a specific scheduling policy. Furthermore, the resulted schedule must ensure that communication buffers are underflow- and overflow-free. This paper describes a (partitioned) earliest-deadline first symbolic schedulability analysis of dataflow graphs that minimizes the buffering requirements. Our scheduling analysis consists of three major steps. (1) The construction of an abstract affine schedule of the graph that excludes overflow and underflow exceptions and minimizes the buffering requirements assuming some precedences between jobs. (2) Symbolic deadlines adjustment that guarantees precedences without the need for lock-based synchronizations. (3) The concretization of the affine schedule using a symbolic, fast-converging, processor-demand analysis for both uniprocessor and multiprocessor systems. Experimental results show that our technique improves the buffering requirements in many cases.
computer_vision	A large number of embedded systems include 8-bit microcontrollers for their energy efficiency and low cost. Multi-bank memory architecture is commonly applied in 8-bit microcontrollers to increase the size of memory without extending address buses. To switch among different memory banks, a special instruction, Bank Selection, is used. How to minimize the number of bank selection instructions inserted is important to reduce code size for embedded systems. In this paper, we consider how to insert the minimum number of bank selection instructions in a program to achieve feasibility. A program can be represented by a control flow graph (CFG). We prove that it is NP-Hard to insert the minimum number of bank selection instructions if all the variables are pre-assigned to memory banks. Therefore, we introduce a 2-approximation algorithm using a rounding method. When the CFG is a tree or the out-degree of each node in the CFG is at most two, we show that we can insert the bank selection instructions optimally in polynomial time. We then consider the case when there are some nodes that do not access any memory bank and design a dynamic programming method to compute the optimal insertion strategy when the CFG is a tree. Experimental result shows the proposed techniques can reduce bank selection instructions significantly on partitioned memory architecture.
computer_vision	In practice, time critical portions of hard real-time systems are still implemented in low-level programming languages and manually tuned to meet all the timing requirements. Without a real-time language that supports an appropriate way of specifying timing constraints for a generic hard real-time systems, and high precision timing analysis that is transparent to users, the users will ever suffer from the complex coding and analysis, particularly for systems requiring fast turnaround responses.In this paper, we propose novel language constructs that can be added to any imperative programming language so that the extended language provides users a way to specify relative timing constraints between arbitrary operations at instruction-level. The compilation techniques unique to transformation of the proposed language are also presented as a part of CHaRTS, the Compiler for Hard Real-Time Systems, which generates a valid instruction sequence for a target execution model.
computer_vision	Surgery is an important option in the treatment of patients with medically intractable epilepsy. Traditional techniques for the localization of the epileptogenic zone (EZ), e.g. surface electroencephalography (EEG) and magnetic resonance (MR) imaging, allow accurate localization in a significant number of epileptic patients. Besides, in many situations, the single photon emission computed tomography (SPECT) images have played a very important role in EZ localization. However, the identification of the EZ based on the visual interpretation side-by-side of the ictal and periictal images is in general a difficult task. To improve SPECT analysis, computational techniques have been developed focusing on the registration and fusion of ictal and periictal SPECT images. The combination of image registration and subtraction of SPECT images can improve the selective detection of brain regions with functional activation during epileptic seizures. The proposed methodology shown high sensitivity to detect hyperperfusion higher than 10%, in this article we apresents the techniques implemented, the methodology used to test and some initial results obtained with our own developed software B.R.A.S.I.L. ("Brain Registration And Subtraction: Improved Localization for SPECT Analysis").
computer_vision	The same advances in computer hardware that have revolutionized computing now promise to make a substantial improvement in the way resources can be accessed over a network. The most significant development is the availability of powerful but low-cost microcomputers. For approximately $2,500 it is now possible to make available a microcomputer that: 1) greatly simplifies access to a given network resource, especially for the untrained, non-technical person; 2) can substantially reduce the cost of communications; and 3) provides valuable local computing services on its own, without resorting to a remote network computer. The topic of this paper is a hardware/software system that functions both as a stand-alone microcomputer system and, using special access software, as an intelligent terminal useful in accessing the large computer systems that supply services through EDUNET. The current implementation of EASy is based on a 48K AppleTM II Plus system with the ?Language Systems? card that supports UCSD PascalTM, a Disk II floppy disk subsystem, and a communications card for network access or connection of external devices.
computer_vision	This paper presents a performance comparison of four classification techniques (i.e., Decision Tree, Na?ve Bayes, Neuron Network, and Support Vector Machine) for appliance classification by analyzing each appliance's electricity usage sent via a wireless sensor network. To measure and collect the actual electrical power consumed by each device, we designed sensor circuits, each of which is deployed inside each power outlet. The measured data are sent to a centralized system via a wireless sensor network (which can also be used to deliver control commands to turn on/off each appliance). The system uses the data to classify a type of each appliance connected to each of the outlet. Since this research is to be detecting electrical usage at each outlet (instead of at the main circuit as in previous works), the system can be developed further to help identifying the abnormal operation of each appliance, and to automatically recognize the device when it is moved to another outlet, making possible automatic appliance on/off control. As a result, it could promote home safety and energy savings without affecting users' normal behaviors. Comparing the accuracies of classifying 40 electric devices using the four techniques, we found that 1) standard deviation of measured electricity usage is one of necessary attributes for accurately classifying appliance operating states, and 2) the decision tree algorithm (i.e., C4.5) performs best (with the error of 5.73%).
computer_vision	Worldwide, traffic accidents cause over a million fatalities every year. Thus, improving road safety and saving people's lives is an international priority. One major challenge faced by researchers is to design an ideal system that is able to predict road accidents and implement efficient prevention actions. Context-aware systems are those systems that are able to sense, reason and react upon the current contextual information. Utilising those systems in intelligent transportation systems (ITS) might improve road safety and enhance traffic efficiency. This paper introduces a context-aware accidents prediction and prevention system taking into account the most contributory factors that cause road accidents including factors related to the driver, the environment, the vehicle and other vehicles on the road. A context-aware architecture based on VANET's On Board Unit (OBU) is presented. The architecture is divided into three phases: physical phase, thinking phase and application phase, which represent the three main subsystems of context-aware systems: the sensing, the reasoning and the acting subsystem respectively. In the thinking phase, a Dynamic Bayesian Networks (DBN) model has been proposed to predict the accident likelihood and the severity level. The evaluation of the proposed system showed good results in predicting accidents and their levels of severity.
computer_vision	A trend in automotive infotainment software is to create a separation of components based on different domains (e.g. Navigation, Radio, etc.). This intends to limit susceptibility to errors, simplify maintainability and to organize development based on domains. Multi-OS environments create another layer of separation through hardware/software virtualization. Using a hypervisor for virtualization allows the development of mixed critical systems. However, we see a contradiction in current architectures, which on one side aim to separate everything into virtual machines (VMs), while on the other side allow inter-VM-connectivity. In the end all applications are composited into one homogeneous UI and the previous intent of separation is disregarded. In this paper we investigate current architectures for in-vehicle infotainment systems (IVIS), i.e. mixed critical systems for automotive purposes, and show that regulations and/or requirements break the previous intents of the architecture.
computer_vision	Distributed Adaptive Real-Time (DART) systems are interconnected and collaborating systems that continuously must satisfy guaranteed and highly critical requirements (e.g., collision avoidance), while at the same time adapt, smartly, to achieve best-effort and low-critical application requirements (e.g., protection coverage) when operating in dynamic and uncertain environments. This short paper introduces our architecture and approach to engineering a DART system so that we achieve high assurance in its runtime behavior against a set of formally specified requirements. It describes our technique to: (i) ensure asymmetric timing protection between high- and low-critical threads on each node in the DART system, and (ii) verify that the self-adaptation within, and coordination between, the nodes and their interaction with the physical environment do not violate high and low criticality requirements. We present our ongoing research to integrate advances in model-based engineering with compositional analysis techniques to formally verify safety-critical properties demanded in safety-conscious domains such as aviation and automotive; and introduce our DART model problem to demonstrate of our engineering approach.
computer_vision	The Embedded Machine is a virtual machine that mediates in real time the interaction between software processes and physical processes. It separates the compilation of embedded programs into two phases. The first, platform-independent compiler phase generates E code (code executed by the Embedded Machine), which supervises the timing ---not the scheduling--- of application tasks relative to external events, such as clock ticks and sensor interrupts. E~code is portable and exhibits, given an input behavior, predictable (i.e., deterministic) timing and output behavior. The second, platform-dependent compiler phase checks the time safety of the E code, that is, whether platform performance (determined by the hardware) and platform utilization (determined by the scheduler of the operating system) enable its timely execution. We have used the Embedded Machine to compile and execute high-performance control applications written in Giotto, such as the flight control system of an autonomous model helicopter.
computer_vision	Energy harvesting enables novel devices and applications without batteries, but intermittent operation under energy harvesting poses new challenges to memory consistency that threaten to leave applications in failed states not reachable in continuous execution. This paper presents analytical models that aid in reasoning about intermittence. Using these, we develop DINO (Death Is Not an Option), a programming and execution model that simplifies programming for intermittent systems and ensures volatile and nonvolatile data consistency despite near-constant interruptions. DINO is the first system to address these consistency problems in the context of intermittent execution. We evaluate DINO on three energy-harvesting hardware platforms running different applications. The applications fail and exhibit error without DINO, but run correctly with DINO?s modest 1.8?2.7? run-time overhead. DINO also dramatically simplifies programming, reducing the set of possible failure- related control transfers by 5?9?.
computer_vision	This work presents an approach for virtualization-driven mapping and switching of software tasks for embedded multi-processor System-on-Chips (MPSoCs). We exploit a dedicated Virtualization Middleware between an array of processors and independent software tasks. Thus, the usually strict and static processor-to-task binding is resolved. By introducing a dynamically reconfigurable interconnection network based on permutation networks inside this Virtualization Middleware, an easy mapping and scheduling of software task groups may be achieved. Emphasis is put on the scalability as well as on the generic structure of the proposed solution. Sharing processor resources among software tasks and the ability to realize load distribution between processors are built-in features. Neither the software tasks nor the processors employed have to be modified in order to be used with the proposed procedure. The approach is demonstrated by a state-of-the-art cryptographic computation example, where the process of eight parallel AES-128 encryptions and decryptions is dynamically mapped and (re-)scheduled on a parallel array of embedded soft-core processors.
computer_vision	In this work, we propose a new approach for protein classification based on Bayesian classifiers. Our goal is to predict the functional family of novel protein sequences based on their motif composition. For this purpose, datasets extracted from Prosite, a curated protein family database, are used as training datasets. In the conducted experiments, the performance of our classifier is compared to other known data mining approaches. The computational results have shown that the proposed method outperforms the other ones and looks very promising for problems with characteristics similar to the problem addressed here.
computer_vision	As their complexity grows, the architectures of embedded systems are becoming increasingly parallel. However, the frameworks used to assist development on highly-parallel general-purpose systems (such as CORBA or MPI) are too heavyweight for use on the non-standard architectures of embedded systems. They introduce significant overheads due to the lack of architectural and structural information contained within most programming languages. Specifically, thread migration across irregular architectures can lead to very poor memory access times, and unconstrained cache coherency cannot scale to cope with large systems. This paper introduces an approach to solving these problems in a scalable way with minimal run-time overhead by using the concept of 'Islands of Coherency'. Cooperating threads are grouped into clusters along with the data that they use. These clusters can then be efficiently mapped to the target architecture, utilising migration only in the areas where the programmer explicitly declares it. This is supported through the use of an existing technique called Compile-Time Virtualisation (CTV). CTV does not support run-time dynamism, so it is extended to allow the implementation of Islands of Coherency. The presented system is evaluated experimentally through implementation on an FPGA platform. Simulation-based results are also presented that show the potential that this approach has for increasing the performance of future embedded systems.
computer_vision	SIGNAL belongs to the synchronous languages family. Such languages are widely used in the design of safety-critical real-time systems such as avionics, space systems, and nuclear power plants. This paper reports a key step of a verified SIGNAL compiler prototype, that is the transformation from a subset of SIGNAL to S-CGA (a variant of clocked guarded actions) and the proof of semantics preservation. Compared with the existing SIGNAL compiler, we use clocked guarded actions as the intermediate representation, to integrate more synchronous programs into our verified compiler prototype in the future. However, in contrast to the SIGNAL language, clocked guarded actions can evaluate a variable even if its clock does not hold. Thus, we propose a variant of clocked guarded actions, namely S-CGA, which constrains variable accesses as done by SIGNAL. To conform with the revised semantics of clocked guarded actions, we also do some adjustments on the existing translation rules from SIGNAL to clocked guarded actions. Finally, the verified transformation is mechanized in the theorem prover Coq.
computer_vision	One of the most critical challenges for today's and future data-intensive and big-data problems (ranging from economics and business activities to public administration, from national security to many scientific research areas) is data storage and analysis. The primary goal is to increase the understanding of processes by extracting highly useful values hidden in the huge volumes of data. The increase of the data size has already surpassed the capabilities of today's computation architectures which suffer from the limited bandwidth (due to communication and memory-access bottlenecks), energy inefficiency and limited scalability (due to CMOS technology). This talk will first address the CMOS scaling and its impact on different aspects of IC and electronics; the major limitations the scaling is facing (such as leakage, yield, reliability, etc) will be shown and the need of a new technology will be motivated. Thereafter, an overview of computing systems, developed since the introduction of Stored program computers by John von Neumann in the forties, will be given. Shortcomings of today's architectures to deal with data-intensive applications will be discussed. It will be shown that the speed at which data is growing has already surpassed the capabilities of today's computation architectures suffering from communication bottleneck and energy inefficiency; hence the need for a new architecture. Finally, the talk will introduce a new architecture paradigm for big data problems; it is based on the integration of the storage and computation in the same physical location (using a cross-bar topology) and the use of non-volatile resistive-switching technology, based on memristors, instead of CMOS technology. The huge potential of such architecture in realizing order of magnitude improvement will be illustrated by comparing it with the state-of-the art architectures (multi-core, GPUs, FPGAs) for different data-intensive applications.
computer_vision	Exploiting effectively massively parallel architectures is a major challenge that stream programming can help to face. We investigate the problem of generating energy-optimal code for a collection of streaming tasks that include parallelizable or moldable tasks on a generic manycore processor with dynamic discrete frequency scaling. In this paper we consider crown scheduling, a novel technique for the combined optimization of resource allocation, mapping and discrete voltage/frequency scaling for moldable streaming task collections in order to optimize energy efficiency given a throughput constraint. We present optimal off-line algorithms for separate and integrated crown scheduling based on integer linear programming (ILP) and heuristics able to compute solution faster and for bigger problems. We make no restricting assumption about speedup behavior. Our experimental evaluation of the ILP models for a generic manycore architecture shows that at least for small and medium sized streaming task collections even the integrated variant of crown scheduling can be solved to optimality by a state-of-the-art ILP solver within a few seconds. Our heuristics produce makespan and energy consumption close to optimality within the limits of the phase-separated crown scheduling technique and the crown structure. Their optimization time is longer than the one of other algorithms we test, but our heuristics consistently produce better solutions. This is an extended abstract of Melot et al., ACM Trans. Arch. Code Opt. 11(4) 2015.
computer_vision	Virtualized runtime environments like Java Virtual Machine (JVM) or Microsoft .NET's Common Language Runtime (CLR) introduce additional challenges to real-time software development. Since applications for such environments are usually deployed in platform independent intermediate code, one issue is the timing of code transformation from intermediate code into native code. We have developed a solution for this problem, so that code transformation is suitable for real-time systems. It combines pre-compilation of intermediate code with the elimination of indirect references in native code. The gain of determinism comes with an increased application startup time. In this paper we present an optimization that utilizes an Ahead-of-Time compiler to reduce the startup time while keeping the real-time suitable timing behaviour. In an experiment we compare our approach with existing ones and demonstrate its benefits for certain application cases.
computer_vision	Computer simulation of binding a small molecule (ligand) to the protein receptor is one of the most important issues in present drug design research. The goal of this procedure is to find the best protein-ligand complex by in silico methods. Among different types of approaches that have been developed, metaheuristic algorithms have a major contribution to solve docking problem. In this paper, a population based iterative search algorithm is used for finding the best docking pose. This algorithm is an extension of the differential evolution (DE) algorithm called opposition-based differential evolution (ODE). Also ODE is enhanced by a local search algorithm and a pseudo-elitism operator. The scoring function which is used in this paper is the AutoDock scoring function. Six different protein-ligand complexes are used to verify the efficiency of the proposed algorithm. The experimental results show that the modified ODE (mODE) is more robust and reliable than the other algorithms such as simulated annealing and Lamarckian genetic algorithm.
computer_vision	Cyclo-Static DataFlow (CSDF) is a powerful model for the specification of DSP applications. However, as in any asynchronous model, the synchronization of the different communicating tasks (processes) is made through buffers that have to be sized such that timing constraints are met. In this paper, we want to determine buffer sizes such that the throughput constraint is satisfied. This problem has been proved to be of exponential complexity. Exact techniques to solve this problem are too time and/or space consuming because of the self-timed schedule needed to evaluate the maximum throughput. Therefore, a periodic schedule is used. Each CSDF actor is associated with a period that satisfies the throughput constraint and sufficient buffer sizes are derived in polynomial time. However, within a period, an actor phases can be scheduled in different manners which impacts the evaluation of sufficient buffer sizes. This paper presents a Min-Max Linear Program that derives an optimized periodic phases scheduling per CSDF actor in order to minimize buffer sizes. It is shown through different applications that this Min-Max Linear Program allows to obtain close to optimal values while running in polynomial time.
computer_vision	Different applications concurrently running on modern MPSoCs can interfere with each other when they use shared resources. This interference can cause side channels, i.e., sources of unintended information flow between applications. To prevent such side channels, we propose a hybrid mapping methodology that attempts to ensure spatial isolation, i.e., a mutually-exclusive allocation of resources to applications in the MPSoC. At design time and as a first step, we compute compact and connected application mappings (called shapes). In a second step, run-time management uses this information to map multiple spatially segregated shapes to the architecture. We present and evaluate a (fast) heuristic and an (exact) SAT-based mapper, demonstrating the viability of the approach.
computer_vision	Safety-critical Java (SCJ) is designed to enable development of applications that are amenable to certification under safety-critical standards. However, its shared-memory concurrency model causes several problems such as data races, deadlocks, and priority inversion. We propose therefore a dataflow design model of SCJ applications in which periodic and aperiodic tasks communicate only through lock-free channels. We provide the necessary tools that compute scheduling parameters of tasks (i.e. periods, phases, priorities, etc) so that uniprocessor/multiprocessor preemptive fixed-priority schedulability is ensured and the throughput is maximized. Furthermore, the resulted schedule together with the computed channel sizes ensure underflow/overflow-free communications. The scheduling approach consists in constructing an abstract affine schedule of the dataflow graph and then concretizing it.
computer_vision	This presentation demonstrates a scalable, modular, refinable methodology for translation validation applied to a mature (20 years old), large (500k lines of C), open source (Eclipse/Polarsys IWG project POP) code generation suite, all by using off-the-shelf, open-source, SAT/SMT verification tools (Yices), by adapting and optimizing the translation validation principle introduced by Pnueli et al. in 1998. This methodology results from the ANR project VERISYNC, in which we aimed at revisiting Pnueli's seminal work on translation validation using off-the-shelf, up-to-date, verification technology. In face of the enormous task at hand, the verification of a compiler infrastructure comprising around 500 000 lines of C code, we devised to narrow down and isolate the problem to the very data-structures manipulated by the infrastructure at the successive steps of code generation, in order to both optimize the whole verification process and make the implementation of a working prototype at all doable. Our presentation outlines the successive steps of this endeavour, from clock synthesis, static scheduling to target code production.
computer_vision	Designing time-predictable architectures to support the requirements of hard real-time systems is the goal of several research projects. In this paper we assume that such platforms exist and we focus on the timing analysis of parallel real-time applications. One of the main challenges is to determine how much the delays induced by software constructs such as synchronisations can impact the worst-case execution times (WCETs) of parallel threads. In this paper, we refine state-of-the-art analysis: first, we derive more accurate estimations of stalls at critical sections; second, we introduce new locking primitives that minimise stall times on the worst-case path. Experimental results show noticeable improvements on the WCETs of benchmarks.
computer_vision	Systems continue to comprise a rapidly growing number of cores on a single chip to gain performance benefits from parallel processing. A key challenge is how their computational resources can be used efficiently, which depends to a large degree on how their resources are allocated to the applications. In this paper, we describe our current research for addressing this challenge and highlight current and upcoming hurdles that need to be addressed.
computer_vision	Today's embedded systems demand increasing computing power to accommodate the ever-growing software functionality. Automotive and avionic systems aim to leverage the high performance capabilities of multicore platforms, but are faced with challenges with respect to temporal predictability. Multicore designers have achieved much progress on improvement of memory-dependent performance in caching systems and shared memories in general. However, having applications running simultaneously and requesting the access to the shared memories concurrently leads to interference. The performance unpredictability resulting from interference at any shared memory level may lead to violation of the timing properties in safety-critical real-time systems. In this paper, we introduce a formal analysis framework for the schedulability and memory interference of multicore systems with shared caches and DRAM. We build a multicore system model with a fine grained application behavior given in terms of periodic preemptible tasks, described with explicit read and write access numbers for shared caches and DRAM. We also provide a method to analyze and recommend candidates for task-to-core reallocation with the goal to find schedulable configurations if a given system is not schedulable. Our model-based framework is realized using Uppaal and has been used to analyze a case study.
computer_vision	DNA microarrays are one of the most used technologies for gene expression measurement. However, there are several distinct microarray platforms, from different manufacturers, each with its own measurement protocol, resulting in data that can hardly be compared or directly integrated. Data integration from multiple sources aims to improve the assertiveness of statistical tests, reducing the data dimensionality problem. This work intends to establish a basis for the integration of gene expression measurements from several manufacturers, a problem that can be addressed at different levels. We will focus on the reannotation process, a cornerstone of multi-platform integration. The proposed approach is based on a reannotation from probesets to transcripts, preserving valuable information for further analysis. Gene expression data from glioblastoma studies will be used as case studies, considering data from Agilent and Affymetrix platforms.
computer_vision	Recent advances in networks and computing systems have led many aspects of our daily life to depend on distributed interconnected computing resources. Large scale distributed systems such as computational and data grids and clouds are used for serving large and complex applications [1]. Grids and clouds performance became more important due to the increase of users and computationally intensive applications. However, the usage of energy has become a major source of concern for these systems due to the price of electricity and the impact on the environment. Energy efficiency in large scale distributed systems reduces energy consumption and operational costs [2]. However, energy conservation should be considered together with users' satisfaction regarding QoS. Complex multiple-task applications may have precedence constraints and specific deadlines and may impose several restrictions and QoS requirements [3, 4], therefore energy-efficient job scheduling is a difficult task in grids and clouds where there are many alternative heterogeneous computers. Advanced modelling and simulation techniques are a basic aspect of performance evaluation that is needed before the costly prototyping actions required for large scale distributed systems [5]. In this talk we will present state-of-the-art research covering a variety of concepts on resource allocation and job scheduling in large scale existing or simulated distributed systems that provide insight into energy conservation problems solving. We will also provide future directions in the area of energy efficiency in grids and clouds.
computer_vision	This paper presents a method to synchronize the data streams from multiple sensors, including wearables and sensors in the environment. Our approach exploits common events observed by the sensors as they interact. We detect physical and cyber couplings between the sensor data streams and determine which couplings will minimize the overall clock drift. We present a graph model to represent the event couplings between sensors and the drift in the sensor timing and propose a solution that employs a shortest path algorithm to minimize the overall clock drift in the system based on the graph model. Experimental results over two trials show an improvement of 21.5% and 43.7% for total drift and 59.4% and 60.7% for average drift.
computer_vision	A major challenge in the future of traffic is to understand how "socially-aware vehicles" could be making use of their social habitus, formed by any information that can be inferred from past and present social relations, social interactions, and a driver's social state when exposed to other participants in real, live traffic. The aim of this workshop in recognition of this challenge is to advance on a common understanding of the symbiosis between drivers, cars, and the infrastructure. The central objective of the workshop is to provoke an active debate on the adequacy of the concept of social, natural, and peripheral interaction, addressing questions such as "who can communicate what", "when", "how", and "why"? To tackle these questions, we would like to collect different, radical, innovative, versatile, and engaging works that challenge or re-imagine human interactions in the near future automobile space.
computer_vision	Drivers' cognitive workload can be difficult to assess. Here one method based on drivers' visual behavior is examined and various approaches are presented.
computer_vision	Traffic is a social system in which road users have their own personality and steer their cars based on learned behavior, experience, and familiarity with situations or street sections. The assumption of this work is, that traffic efficiency and safety could be enhanced when the more competent road users support the less competent ones by sharing data about specific road characteristics and providing steering recommendations. This information should help the latter to move its vehicle in a more efficient and safe way. We thus designed, prototyped, and tested a "Social driving app" that allows experienced drivers to collect and share driving data (speed, gear, brake force, etc.) and that generates, based on the aggregated driving profiles of experts, steering recommendations for the lay drivers. By introducing a ranking system to motivate the individual drivers to follow the instructions from the system, the project further examined the influence of social pressure on driving performance.
database	Data streams are endless flow of data produced in high speed, large size and usually non-stationary environments. The main property of these streams is the occurrence of concept drifts. Using decision trees is shown to be a powerful approach for accurate and fast learning of data streams. In this paper, we present an incremental regression tree that can predict the target variable of newly incoming instances. The tree is updated in the case of occurring concept drifts either by altering its structure or updating its embedded models. Experimental results show the effectiveness of our algorithm in speed and accuracy aspects in comparison to the best state-of-the-art methods.
database	In many applications, one is interested to detect certain patterns in random process signals. We consider a class of random process signals which contain sub-similarities at random positions representing the texture of an object. Those repetitive parts may occur in speech, musical pieces and sonar signals. We suggest a warped time-resolved spectrum kernel for extracting the subsequence similarity in time series in general, and as an example in biosonar signals. Having a set of those kernels for similarity extraction in different size of subsequences, we propose a new method to find an optimal linear combination of those kernels. We formulate the optimal kernel selection via maximizing the kernel Fisher discriminant (KFD) criterion and use Mesh Adaptive Direct Search (MADS) method to solve the optimization problem. Our method is used for biosonar landmark classification with promising results.
database	This paper is concerned with the problem of global exponential stability analysis for a class of cellular neural networks with time-varying discrete and distributed delays (DDCNNs). A new delay-dependent sufficient condition is derived for the global exponential stability of the DDCNNs by using the integral inequality method and the newly proposed Lyapunov-Krasovskii functional. The obtained stability condition is less conservative than some of the existing results in the literature. Numerical examples are given to demonstrate the effectiveness and superiority of the proposed results.
database	In this paper, the problem of delay-dependent robust stability for Hopfield neural networks of neutral-type is investigated. The neural networks system considered is different from the others, which has time-varying delays in neutral and discrete terms. A new class of Lyapunov-Krasovskii functional combines with the descriptor model transformation to ensure a large upper bound for time delay. The delay-dependent robust stability criterion is formulated in terms of linear matrix inequalities, in which the restriction of the derivative of time-varying delay in discrete terms is removed. Since both the neutral-delays and discrete-delays are taken into account, so the obtained criterion is more general than some existing ones. Numerical examples are given to illustrate the effectiveness of the proposed methods.
database	To achieve high-performance on processors featuring ILP, most compilers apply locally a set of heuristics. This leads to a potentially high-performance on separate code fragments. Unfortunately, most optimizations also increase code size, which may lead to a global net performance loss. In this paper, we propose a Global Constraints-Driven Strategy (GCDS) for guiding code optimization. When using GCDS, the final code optimization decision is taken according to global criteria rather than local criteria. For instance, such criteria might be performance, code size, instruction cache behavior, etc. The performance/code size trade-off is a particularly important problem for embedded systems. We show how GCDS can be used to master code size while optimizing performance.
database	Information resources in digital libraries are usually described, along with their context, by structured data records, commonly referred as metadata. Those records often contain unstructured information in natural language text, since they typically follow a data model which defines generic semantics for its data elements, or includes data elements modeled to contain free text. The information contained in these data elements, although machine readable, resides in unstructured natural language texts that are difficult to process by computers. This paper addresses a particular task of information extraction, typically called named entity recognition, which deals with the references to entities made by names occurring in the texts. This paper presents the results of a study of how the named entity recognition problem manifests itself in digital library metadata. In particular, we present the main differences between performing named entity recognition in natural language and in the text within metadata. The paper finalizes with a novel approach for named entity recognition in metadata.
database	A significant portion of the LOD cloud consists of Life Sciences data sets, which together contain billions of clinical facts that interlink to form a "Web of Clinical Data". However, tools for new publishers to find relevant datasets that could potentially be linked to are missing, particularly in specialist domain-specific settings. Based on a set of domain-specific keywords extracted from a local dataset, this paper proposes methods to automatically identify relevant public SPARQL endpoints from a list of candidates.
database	A variety of extremely challenging biological sequence analyses were conducted on the XSEDE large shared memory resource Blacklight, using current bioinformatics tools and encompassing a wide range of scientific applications. These include genomic sequence assembly [6,12], very large metagenomic sequence assembly, transcriptome assembly [3], and sequencing error correction. The datasets used in these analyses included uncategorized fungal species, reference microbial data, very large soil and human gut microbiome sequence data, and primate transcriptomes, comprised of both short- and long-read sequence data. A new parallel command execution program was developed on the Blacklight resource to handle some of these analyses. These results represent significant advances for their respective scientific communities. The breadth and depth of the results achieved demonstrate the ease of use, versatility, and unique capabilities of the Blacklight XSEDE resource for scientific analysis of genomic and transcriptomic sequence data, and the power of these resources, together with XSEDE support, in meeting the most challenging scientific problems.
database	Researches on Internetware have gained daily expanding attentions and interests. Internetware intends to be a framework of Web-based software development. A key issue in this framework is how these Internetware entities aggregate to form a coalition to fulfill the newly occurred requirements. This paper assumes that the Internetware entities distributed in Internet are autonomous and builds a mechanism for the aggregation of autonomous Internetware entities driven by requirements. A function ontology has been constructed for allowing the requirements to be understandable by these Interenetware entities so that these entities can recognize the requirements and realize that they are able to contribute for the realization of the requirements. After that, the requester and the contributors will negotiate to form an effective coalition.
database	A powerful and easy-to-use querying environment is certainly one of the most important components in a multidimensional database, and its effectiveness is influenced by many other aspects, both logical (data model, integration, policy of view materialization, etc.) and physical (multidimensional or relational storage, indexes, etc.). As is evident, multidimensional querying is often based on the metaphor of the data cube and on the concepts of facts, measures, and dimensions. In contrast to conventional transactional environments, multidimensional querying is often an exploratory process, performed by navigating along the dimensions and measures, increasing/decreasing the level of detail and focusing on specific subparts of the cube that appear to be "promising" for the required information.In this chapter we focus on the main languages proposed in the literature to express multidimensional queries, particularly those based on: (i) an algebraic approach, (ii) a declarative paradigm (calculus), and (iii) visual constructs and syntax. We analyze the problem of evaluation, i.e., the issues related to the efficient data retrieval and calculation, possibly (often necessarily) using some pre-computed data, a problem known in the literature as the problem of rewriting a query using views. We also illustrate the use of particular index structures to speed up the query evaluation process.
database	Integrity types can help detect information flow vulnerabilities in web applications and Android apps. We study DFlow, a context-sensitive integrity type system and we give an interpretation of DFlow in terms of CFL-reachability. We propose DFlowCFL, a new, more precise integrity type system, and DFlowCFL-Infer, the corresponding type inference analysis, which is equivalent to CFL-reachability. DFlowCFL-Infer is an effective taint analysis for Android. It scales well and detects numerous privacy leaks in popular Android apps.
database	The Malay language has two types of writing script, known as Rumi and Jawi. Most previous stemmer results have reported on Malay Rumi characters and only a few have tested Jawi characters. In this article, a new Jawi stemmer has been proposed and tested for document retrieval. A total of 36 queries and datasets from the transliterated Jawi Quran were used. The experiment shows that the mean average precision for a �stemmed Jawi� document is 8.43%. At the same time, the mean average precision for a �nonstemmed Jawi� document is 5.14%. The result from a paired sample t-test showed that the use of a �stemmed Jawi� document increased the precision in document retrieval. Further experiments were performed to examine the precision of the relevant documents that were retrieved at various cutoff points for all 36 queries. The results for the �stemmed Jawi� document showed a significantly different start, at a cutoff of 40, compared with the �nonstemmed Jawi� documents. This result shows the usefulness of a Jawi stemmer for retrieving relevant documents in the Jawi script.
database	Compressed sensing MRI (CS-MRI) has shown great potential in reducing data acquisition time in MRI. Sparsity or compressibility plays an important role to reduce the image reconstruction error. Conventional CS-MRI typically uses a pre-defined sparsifying transform such as wavelet or finite difference, which sometimes does not lead to a sufficient sparse representation for the image to be reconstructed. In this paper, we design a patch-based nonlocal operator (PANO) to sparsify magnetic resonance images by making use of the similarity of image patches. The definition of PANO results in sparse representation for similar patches and allows us to establish a general formulation to trade the sparsity of these patches with the data consistency. It also provides feasibility to inc orporate prior information learnt from undersampled data or another contrast image, which leads to optimized sparse representation of images to be reconstructed. Simulation results on in vivo data demonstrate that the proposed method achieves lower reconstruction error and higher visual quality than conventional CS-MRI methods. Elsevier B.V.
database	Dr. Raffaello D'Andrea speaks at length about what it takes to build commercially viable robotic systems, the future of autonomous machines, the role humans will play in this future, and how we can best prepare for it.
database	There is growing evidence that calcified arterial deposits play a crucial role in the pathogenesis of cardiovascular disease. This paper investigates the challenging problem of unsupervised calcified lesion classification. We propose an algorithm, US-CALC (UnSupervised Calcified Arterial Lesion Classification), that discriminates arterial lesions from non-arterial lesions. The proposed method first mines the characteristics of calcified lesions using a novel optimization criterion and then identifies a subset of lesion features which is optimal for classification. Second, a two stage clustering is deployed to discriminate between arterial and non-arterial lesions. A histogram intersection distance measure is incorporated to determine cluster proximity. The clustering hierarchies are carefully validated and the final clusters are determined by a new intra-cluster compactness measure. Experimental results indicate an average accuracy of approximately 80% on a database of electron beam CT heart scans.
database	In this paper, we propose a set of new generic automated processing tools to characterise the local asymmetries of anatomical structures (represented by surfaces) at an individual level, and within/between populations. The building bricks of this toolbox are: 1) a new algorithm for robust, accurate, and fast estimation of the symmetry plane of grossly symmetrical surfaces, and 2) a new algorithm for the fast, dense, non-linear matching of surfaces. This last algorithm is used both to compute dense individual asymmetry maps on surfaces, and to register these maps to a common template for population studies. We show these two algorithms to be mathematically well-grounded, and provide some validation experiments. Then we propose a pipeline for the statistical evaluation of local asymmetries within and between populations. Finally we present some results on real data.
database	To quantify the complex relationships between (1) the temperature, and temperature differences, on the surface of the breast as recorded by infrared thermal imaging and (2) the underlying physiological and pathological factors, we have developed a dynamic finite element method for comprehensive modeling of both the thermal and elastic properties of normal and tumorous breast tissues. In the steady state, the gravity-induced deformation is found to cause markedly asymmetric surface temperatures even though all thermal-elastic properties are symmetrical. In the dynamic state, the time course of breast thermal imaging in cold-stress and thermal-recovery procedures is found to be useful in characterizing the origins of the thermal contrast on the breast surface. The tumor-induced thermal contrast has slower temporal behavior than the deformation-induced thermal contrast on the breast surface, which may lead to improvements in breast-tumor diagnosis.
database	In this paper, we propose a fully automatic method for the coupled 3D localization and segmentation of lower abdomen structures. We apply it to the joint segmentation of the prostate and bladder in a database of CT scans of the lower abdomen of male patients. A flexible approach on the bladder allows the process to easily adapt to high shape variation and to intensity inhomogeneities that would be hard to characterize (due, for example, to the level of contrast agent that is present). On the other hand, a statistical shape prior is enforced on the prostate. We also propose an adaptive non-overlapping constraint that arbitrates the evolution of both structures based on the availability of strong image data at their common boundary. The method has been tested on a database of 16 volumetric images, and the validation process includes an assessment of inter-expert variability in prostate delineation, with promising results.
database	This paper presents a hybrid (geometry- & image-based) technique suitable for providing interactive walkthroughs of large, complex outdoor scenes. Motion is restricted along a smooth predefined path and the input to the system is a sparse set of stereoscopic views at certain points (key-positions) along that path (one view per position). An approximate local 3D model is constructed from each view, capable of capturing photometric and geometric properties of the scene only locally. Then during the rendering process, a continuous morphing (both photometric & geometric) takes place between successive local 3D models, using what we call a "morphable 3D-model". The morphing proceeds in a physically-valid way. For this reason, a wide-baseline image matching technique is proposed, handling cases where the wide baseline between the two images is mainly due to a looming of the camera. Our system can be extended in the event of multiple stereoscopic views (and therefore multiple local models) per key-position of the path (related by a camera rotation). In that case one local 3D-mosaic (per key-position) is constructed comprising all local 3D models therein and a "morphable 3D-mosaic" is used during the rendering process. A partial-differential equation is adopted to handle the problem of geometric consistency of each 3D-mosaic.
database	In this paper, we propose a novel method of merging a series of range images with a minimal overlap between any two consecutive range images. We rigidly mount a parabolic catadioptric camera to the range scanner. Using two omniviews we are able to accurately estimate the relative displacement between two range views. The resultant motion is used for the registration of all range data to the same coordinate system. An additional perspective camera calibrated with respect to the scanner is used for texture mapping.
database	Since any organizational environment is typically resource constrained, especially in terms of human capital, organization managers would like to maximize the utilization of available human resources. However, tasks cannot simply be assigned to arbitrary employees since the employee needs to have the necessary capabilities for executing a task. Furthermore, security policies constrain the assignment of tasks to employees, especially given the other tasks assigned to the same employee. Since role-based access control (RBAC) is the most commonly used access control model for commercial information systems, we limit our attention to consider constraints in RBAC. In this article, we define the Employee Assignment Problem (EAP), which aims to identify an employee to role assignment such that it permits the maximal flexibility in assigning tasks to employees while ensuring that the required security constraints are met. We prove that finding an optimal solution is NP-complete and therefore provide a greedy solution. Experimental evaluation of the proposed approach shows that it is both efficient and effective.
database	In this article, we propose a new micropayment model for nonspecialized commodity web-services based on microcomputations. In our model, a user that wishes to access online content (offered by a website) does not need to register or pay to access the website; instead, he will accept to run microcomputations on behalf of the service provider in exchange for access to the content. These microcomputations can, for example, support ongoing computing projects that have clear social benefits (e.g., projects relating to medical research) or can contribute towards commercial computing projects. We analyze the security and privacy of our proposal and we show that it preserves the privacy of users. We argue that this micropayment model is economically and technically viable and that it can be integrated in existing distributed computing frameworks (e.g., the BOINC platform). In this respect, we implement a prototype of a system based on our model and we deploy our prototype on Amazon Mechanical Turk to evaluate its performance and usability given a large number of users. Our results show that our proposed scheme does not affect the browsing experience of users and is likely to be used by a non-trivial proportion of users. Finally, we empirically show that our scheme incurs comparable bandwidth and CPU consumption to the resource usage incurred by online advertisements featured in popular websites.
database	In this paper we consider one aspect of the problem of automatically building shape models of articulating objects from example range images. Central to the model construction problem is the registration of range data, taken from different vantage points, into a common coordinate frame. This involves determining a transformation for each set of range data which aligns overlapping surface points in the common frame. Current registration algorithms have been developed specifically for rigid objects, but it is not obvious how these can be extended to articulated or more generally deformable objects. Here, we propose that range images of articulated objects are first segmented into their rigid subcomponents. Each subcomponent can then be registered in isolation using the existing algorithms designed specifically for rigid parts and the final model formed by reassembling all of the submodels. This has motivated the development of a rigid part segmentation algorithm which is described and demonstrated here. The algorithm is currently limited to non-umbilic surfaces, but in this more restricted domain is shown to work well.
database	Owing to dense deployment of light fixtures and multipath-free propagation, visible light localization technology holds potential to overcome the reliability issue of radio localization. However, existing visible light localization systems require customized light hardware, which increases deployment cost and hinders near term adoption. In this paper, we propose LiTell, a simple and robust localization scheme that employs unmodified fluorescent lights (FLs) as location landmarks and commodity smartphones as light sensors. LiTell builds on the key observation that each FL has an inherent characteristic frequency which can serve as a discriminative feature. It incorporates a set of sampling, signal amplification and camera optimization mechanisms, that enable a smartphone to capture the extremely weak and high frequency ( > 80 kHz) features. We have implemented LiTell as a real-time localization and navigation system on Android. Our experiments demonstrate LiTell's high reliability in discriminating different FLs, and its potential to achieve sub-meter location granularity. Our user study in a multi-storey office building, parking lot and grocery store further validates LiTell as an accurate, robust and ready-to-use indoor localization system.
database	In this work we propose ParkMaster, a low-cost crowdsourcing architecture which exploits machine learning techniques and vision algorithms to evaluate parking availability in cities. While the user is normally driving ParkMaster enables off the shelf smartphones to collect information about the presence of parked vehicles by running image recognition techniques on the phones camera video streaming. The paper describes the design of ParkMaster's architecture and shows the feasibility of deploying such mobile sensor system in nowadays smartphones, in particular focusing on the practicability of running vision algorithms on phones.
database	Business intelligence and analytics (BIA) is about the development of technologies, systems, practices, and applications to analyze critical business data so as to gain new insights about business and markets. The new insights can be used for improving products and services, achieving better operational efficiency, and fostering customer relationships. In this article, we will categorize BIA research activities into three broad research directions: (a) big data analytics, (b) text analytics, and (c) network analytics. The article aims to review the state-of-the-art techniques and models and to summarize their use in BIA applications. For each research direction, we will also determine a few important questions to be addressed in future research.
database	Proactive recommender systems push recommendations to users without their explicit request whenever a recommendation that suits a user is available. These systems strive to optimize the match between recommended items and users' preferences. We assume that recommendations might be reflected with low accuracy not only due to the recommended items' suitability to the user, but also because of the recommendations' timings. We therefore claim that it is possible to learn a model of good and bad contexts for recommendations that can later be integrated in a recommender system. Using mobile data collected during a three week user study, we suggest a two-phase model that is able to classify whether a certain context is at all suitable for any recommendation, regardless of its content. Results reveal that a hybrid model that first decides whether it should use a personal or a non-personal timing model, and then classifies accordingly whether the timing is proper for recommendations, is superior to both the personal or non-personal timing models.
database	In prior work we addressed a major problem faced by media sites with popularity based recommender systems such as the top-10 list of most liked or most clicked posts. We showed that the hard cutoff used in these systems to generate the "Top N"lists is prone to unduly penalizing good articles that may have just missed the cutoff. A solution to this was to generate recommendations probabilistically, which is an approach that has been shown to be robust against some manipulation techniques as well. The aim of this research is to introduce a class of probabilistic news recommender systems that incorporates widely practiced recommendation techniques as a special case. We establish our results in a special case of two articles using the urn models with feedback mechanism from probability theory.
database	Managing fine-grained provenance is a critical requirement for data stream management systems (DSMS), not only for addressing complex applications that require diagnostic capabilities and assurance, but also for providing advanced functionality, such as revision processing or query debugging. This article introduces a novel approach that uses operator instrumentation, that is, modifying the behavior of operators, to generate and propagate fine-grained provenance through several operators of a query network. In addition to applying this technique to compute provenance eagerly during query execution, we also study how to decouple provenance computation from query processing to reduce runtime overhead and avoid unnecessary provenance retrieval. Our proposals include computing a concise superset of the provenance (to allow lazily replaying a query and reconstruct its provenance) as well as lazy retrieval (to avoid unnecessary reconstruction of provenance). We develop stream-specific compression methods to reduce the computational and storage overhead of provenance generation and retrieval. Ariadne, our provenance-aware extension of the Borealis DSMS implements these techniques. Our experiments confirm that Ariadne manages provenance with minor overhead and clearly outperforms query rewrite, the current state of the art.
database	We consider the challenge of providing privacy-preserving access to data outsourced to an untrusted cloud provider. Even if data blocks are encrypted, access patterns may leak valuable information. Oblivious RAM (ORAM) protocols guarantee full access pattern privacy, but even the most efficient ORAMs to date require roughly L log2 N block transfers to satisfy an L-block query, for block store capacity N. We propose a generalized form of ORAM called Tunably-Oblivious Memory (lambda-TOM) that allows a query's public access pattern to assume any of lambda possible lengths. Increasing lambda yields improved efficiency at the cost of weaker privacy guarantees. 1-TOM protocols are as secure as ORAM. We also propose a novel, special-purpose TOM protocol called Staggered-Bin TOM (SBT), which efficiently handles large queries that are not cache-friendly. We also propose a read-only SBT variant called Multi-SBT that can satisfy such queries with only O(L + log N) block transfers in the best case, and only O(L log N) transfers in the worst case, while leaking only O(log log log N) bits of information per query. Our experiments show that for N = 2^24 blocks, Multi-SBT achieves practical bandwidth costs as low as 6X those of an unprotected protocol for large queries, while leaking at most 3 bits of information per query.
database	The unauthorized propagation of information is an important problem in the Internet, especially because of the increasing popularity of On-line Social Networks. To address this issue, many access control mechanisms have been proposed so far, but there is still a lack of techniques to evaluate the risk of unauthorized flow of information within social networks. This paper introduces a probability-based approach to modeling the likelihood that information propagates from one social network user to users who are not authorized to access it. The approach is demonstrated via an example, to show how it can be applied in practical cases.
database	To achieve a trustworthy cloud data service, there is a need to both provide the right services from a security engineering perspective, as well as to allows specific types of computations to be carried out on encrypted cloud data. However, traditional encryption solutions can't be used to process outsourcing encrypted data hosting to an untrusted cloud provider. A novel encryption scheme, called fully homomorphic encryption (FHE), could afford the circuit ability over encrypted data without decrypting it. In this paper, we deliver a universal construction framework for fully homomorphic encryption schemes. At first, this framework initializes a somewhat homomorphic encryption scheme based on the concept of metric space in abstract algebra which encodes the plaintext into a offset vector and generates a ciphertext by adding the offset vector to a random eigenvector in the metric space. As an abelian group, the ring is closed under addition and multiplication, this abstract algebra assume the metric space could forma ring and the eigenvectors belong to an ideal of this ring, then this framework could achieve homomorphism by having the scheme live in rings. We also deduce some well-known fully homomorphic schemes from the construction framework, and propose a prototype with an FHE encryption proxy to solve confidentiality problems in cloud systems. At last, we show the performance of FHE with some experiments, and speed the performance of fully homomorphic encryption up with cloud computing (parallel computing, distribute computing, etc.). We also discuss some opening issues and directions for future fully homomorphic encryption researches.
database	SPARQL is a powerful query language for Semantic Web data sources but it is quite complex to master. The jigsaw puzzle methaphor has been succesfully used in Blockly to teach programming to kids. We discuss its applicability to the problem of building SPARQL queries, through the presentation of a dedicated Blockly-based visual user interface.
database	To achieve scalable data intensive analytics, we investigate methods to integrate general purpose analytic computation into a query pipeline using User Defined Functions (UDFs). However, an existing UDF cannot act as a block operator with chunk-wise input along the tuple-wise query processing pipeline, therefore unable to deal with the application semantics definable on the set of incoming tuples representing a single object or falling in a time window, and unable to leverage external computation engines for efficient batch processing. To enable the data intensive computation pipeline, we introduce a new kind of UDFs called Set-In Set-Out (SISO) UDFs. A SISO UDF is a block operator for processing the input tuples and returning the resulting tuples chunk by chunk. Operated in the query processing pipeline, a SISO UDF pools a chunk of input tuples, dispatches them to GPUs or an analytic engine in batch, materializes and then streams out the results. This behavior differentiates SISO UDF from all the existing ones, and makes efficient integration of analytic computation and data management feasible. We have implemented the SISO UDF framework by extending the PostgreSQL query engine, and further demonstrated the use of SISO UDF with GPU-enabled analytical query evaluation. Our experiments show that the proposed approach is scalable and efficient.
database	Provenance that records the derivation history of data is useful for a wide variety of applications, including those where an audit trail needs to be provided, where the sources and the trust-level attributed to the sources contribute to determining the trust-level in results etc. There have been different efforts in the past for representing provenance information, the most notable being the Open Provenance Model (OPM). OPM defines structures for representing the provenance information as a graph with nodes and edges, and also specifies inference queries. Our work builds on these by proposing query language constructs, that the users will find useful for manipulating the provenance information. Rather than specifying a query language, we define two classes of algebraic constructs: content-based operators that operate on the content of nodes and edges, and structure-based operators that operate on the graph structure of the provenance graph. These content-based and the structure-based constructs can be combined to express a wide variety of interesting queries on the provenance data that go much beyond simple inference queries as expressible using Datalog/SQL.
database	Semi-stream join algorithms join a continuous stream with a large disk-based relation. While there are efficient semi-stream equijoins for exact matches in the joined data, there are currently no semi-stream similarity joins for approximate matches. The existing similarity join algorithms work either offline (on datasets that are fully known) or on several streams (using a join window), and are less suitable for applications where continuous, immediate and complete similarity join results are required. To address this gap we propose S3J, the first semi-stream similarity join algorithm. To utilize disk and CPU optimally, S3J combines a disk-intensive queue-based semi-stream join approach with a CPU-intensive similarity matching algorithm. The similarity matching algorithm is based on tries to minimize the memory footprint. Moreover, it supports parallel execution to utilize modern multicore CPUs. We provide a cost model for S3J and evaluate its performance empirically.
database	Memory and time optimization is a key task of Stream Data Warehouses (SDWs). StrETL processes in those systems are similar to queries in Data Stream Management Systems (DSMSs). This fact allows us to migrate some methods from DSMS to SDW. We have observed that schedulers and algorithms introduced to create operator partitions are analyzed separately either in StrETL processes or in stream queries. The fact is, those two mechanisms affect each other and it is justified to study potential benefits of combining them together. In the paper we introduce a solution which cooperates with a scheduler in order to create more efficient operator partitions. Another noteworthy issue is that this algorithm is able to optimize a wider range of operator topologies. Finally, experimental evaluation show that our solution allows achieving a smaller memory consumption or a shorter response time in comparison with the competing strategies.
database	Cloud business intelligence is an increasingly popular choice to deliver decision support capabilities via elastic, pay-per-use resources. However, data security issues are one of the top concerns when dealing with sensitive data. In this paper, we propose a novel approach for securing cloud data warehouses by flexible verifiable secret sharing, fVSS. Secret sharing encrypts and distributes data over several cloud service providers, thus enforcing data privacy and availability. fVSS addresses four shortcomings in existing secret sharing-based approaches. First, it allows refreshing the data warehouse when some service providers fail. Second, it allows on-line analysis processing. Third, it enforces data integrity with the help of both inner and outer signatures. Fourth, it helps users control the cost of cloud warehousing by balancing the load among service providers with respect to their pricing policies. To illustrate fVSS' efficiency, we thoroughly compare it with existing secret sharing-based approaches with respect to security features, querying power and data storage and computing costs.
database	Building scalable back-end infrastructures for data-centric applications is becoming important. Applications used in data-centres have complex, multilayer software stacks and are required to scale to a large number of nodes. Today, there is increased interest in improving the efficiency of such software stacks. In this paper, we examine the efficiency of such a stack used for distributed stream processing, an important application domain. We use a specific streaming system, Borealis [10], and extensively hand-tune the end-to-end data path. We focus on parts of the stack that are related to intra- and inter-node communication and data exchange, a central component of many software stacks. We find that application-independent code in stream processing middleware employs operations for communication that consume significant amount of CPU cycles and are not strictly necessary. We first categorize these operations based on the protocol function they support. We then proceed to remove these operations by producing a functionally equivalent software stack in terms of application processing. Our results show that restructuring the data path achieves up to 5x higher throughput, reduces energy consumption by up to 60% and saves infrastructure cost by up to 40%. Finally, we project that with 1024-core processors per node, stream processing applications will demand up to 2 TBits/s/node of networking throughput.
database	In this paper, we present a three-level ontology-based framework for effectively designing GAV data integration systems. In our approach, the mediated schema is represented by a domain ontology, which provides a conceptual representation of the application. Each local source is described by an application ontology, whose vocabulary is restricted to be a subset of the vocabulary of domain ontology. The three-level architecture permits dividing the mapping definition in two stages: local mappings and mediated mappings. Due to this architecture the problem of query answering can also be broken into two steps. First, the query is decomposed, using the mediated mappings, into a set of elementary sub-queries expressed in terms of the application ontologies. Then, these sub-queries are rewritten, using the local mappings, in terms of their local sources schemas. This paper focus on a method for query processing that addresses the problem of efficient query answering. Our approach is illustrated by an example of a virtual store mediating access to online booksellers.
database	We study the power of four query models in the context of property testing in general graphs, where our main case study is the problem of testing k-colorability. Two query types, which have been studied extensively in the past, are pair queries and neighbor queries. The former corresponds to asking whether there is an edge between any particular pair of vertices, and the latter to asking for the i'th neighbor of a particular vertex. We show that while for pair queries, testing k-colorability requires a number of queries that is a monotone decreasing function in the average degree d, the query complexity in the case of neighbor queries remains roughly the same for every density and for large values of k. We also consider a combined model that allows both types of queries, and we propose a new, stronger, query model, which is related to the field of Group Testing. We give one-sided error upper and lower bounds for all the models, where the bounds are nearly tight for three of the models. In some of the cases our lower bounds extend to two-sided error algorithms. The problem of testing k-colorability was previously studied in the contexts of dense and sparse graphs, and in our proofs we unify approaches from those cases, and also provide some new tools and techniques which may be of independent interest.
database	We propose to design data structures called succinct geometric indexes of negligible space (more precisely, o(n) bits) that support geometric queries in optimal time, by taking advantage of the n points in the data set permuted and stored elsewhere as a sequence. Our first and main result is a succinct geometric index that can answer point location queries, a fundamental problem in computational geometry, on planar triangulations in O(lg n) time. We also design three variants of this index. The first supports point location using lg n + 2 ?lg n + O(lg1/4n) point-line comparisons. The second supports point location in o(lg n) time when the coordinates are integers bounded by U. The last variant can answer point location queries in O(H + 1) expected time, where H is the entropy of the query distribution. These results match the query efficiency of previous point location structures that occupy O(n) words or O(n lg n) bits, while saving drastic amounts of space. We generalize our succinct geometric index to planar subdivisions, and design indexes for other types of queries. Finally, we apply our techniques to design the first implicit data structures that support point location in O(lg2 n) time.
database	The problems of query containment, equivalence, and minimization are fundamental problems in the context of query processing and optimization. In their classic work [2] published in 1977, Chandra and Merlin solved the three problems for the language of conjunctive queries (CQ queries) on relational data, under the "set-semantics" assumption for query evaluation. While the results of [2] have been very influential in database research, it was recognized long ago that the set semantics does not correspond to the semantics of the standard commercial query language SQL. Alternative semantics, called bag and bag-set semantics, have been studied since 1993; Chaudhuri and Vardi in [5] outlined necessary and sufficient conditions for equivalence of CQ queries under these semantics. (The problems of containment of CQ bag and bag-set queries remain open to this day.) More recently, Cohen [7, 8] introduced a formalism for treating (generalizations of) CQ queries evaluated under each of set, bag, and bag-set semantics uniformly as special cases of the more general combined semantics. This formalism provides tools for studying broader classes of practical SQL queries, specifically important types of queries that arise in on-line analytical processing (OLAP). Cohen in [8] provides a sufficient condition for equivalence of (generalizations of) combined-semantics CQ queries, as well as sufficient and necessary equivalence conditions for several proper sublanguages of the query language of [8]. To the best of our knowledge, no results on minimization of CQ queries beyond set-semantics queries have been reported in the literature. Our goal in this paper is to continue the study of equivalence and minimization of CQ queries. We consider the problems of (i) finding minimized versions of combined-semantics CQ queries, and of (ii) determining whether two CQ queries are combined-semantics equivalent. We continue the tradition of [2, 5, 8] of studying these problems using the tool of containment between queries. We extend the containment, equivalence, and minimization results of [2] to general combined-semantics CQ queries, and show the limitations of each extension. We show that the minimization approach of [2] can be extended to general CQ queries without limitations. We also propose a necessary and sufficient condition for equivalence of queries belonging to a large natural sublanguage of combined-semantics CQ queries; this sublanguage encompasses (but is not limited to) all set, bag, and bag-set queries. Our equivalence and minimization results, as well as our general sufficient condition for containment of combined-semantics CQ queries, reduce correctly to the special cases reported in [5] for bag and bag-set semantics. Our containment and equivalence conditions also properly generalize the results of [8], provided the latter are restricted to the language of (combined-semantics) CQ queries.
database	We address the problem of comparing the expressiveness of workflow specification formalisms using a notion of view of a workflow. Views allow to compare widely different workflow systems by mapping them to a common representation capturing the observables relevant to the comparison. Using this framework, we compare the expressiveness of several workflow specification mechanisms, including automata, temporal constraints, and pre-and-post conditions, with XML and relational databases as underlying data models. One surprising result shows the considerable power of static constraints to simulate apparently much richer workflow control mechanisms.
database	This paper considers the problem of providing security to statistical databases against disclosure of confidential information. Security-control methods suggested in the literature are classified into four general approaches: conceptual, query restriction, data perturbation, and output perturbation. Criteria for evaluating the performance of the various security-control methods are identified. Security-control methods that are based on each of the four approaches are discussed, together with their performance with respect to the identified evaluation criteria. A detailed comparative analysis of the most promising methods for protecting dynamic-online statistical databases is also presented. To date no single security-control method prevents both exact and partial disclosures. There are, however, a few perturbation-based methods that prevent exact disclosure and enable the database administrator to exercise "statistical disclosure control." Some of these methods, however introduce bias into query responses or suffer from the 0/1 query-set-size problem (i.e., partial disclosure is possible in case of null query set or a query set of size 1). We recommend directing future research efforts toward developing new methods that prevent exact disclosure and provide statistical-disclosure control, while at the same time do not suffer from the bias problem and the 0/1 query-set-size problem. Furthermore, efforts directed toward developing a bias-correction mechanism and solving the general problem of small query-set-size would help salvage a few of the current perturbation-based methods.
database	In this paper we develop a new data structure for implementing heaps (priority queues). Our structure, Fibonacci heaps (abbreviated F-heaps), extends the binomial queues proposed by Vuillemin and studied further by Brown. F-heaps support arbitrary deletion from an n-item heap in O(log n) amortized time and all other standard heap operations in O(1) amortized time. Using F-heaps we are able to obtain improved running times for several network optimization algorithms. In particular, we obtain the following worst-case bounds, where n is the number of vertices and m the number of edges in the problem graph: O(n log n + m) for the single-source shortest path problem with nonnegative edge lengths, improved from O(mlog(m/n+2)n); O(n2log n + nm) for the all-pairs shortest path problem, improved from O(nm log(m/n+2)n); O(n2log n + nm) for the assignment problem (weighted bipartite matching), improved from O(nmlog(m/n+2)n); O(m&bgr;(m, n)) for the minimum spanning tree problem, improved from O(mlog log(m/n+2)n); where &bgr;(m, n) = min {i ? log(i)n ? m/n}. Note that &bgr;(m, n) ? log*n if m ? n. Of these results, the improved bound for minimum spanning trees is the most striking, although all the results give asymptotic improvements for graphs of appropriate densities.
database	A simple variant of a priority queue, called a soft heap, is introduced. The data structure supports the usual operations: insert, delete, meld, and findmin. Its novelty is to beat the logarithmic bound on the complexity of a heap in a comparison-based model. To break this information-theoretic barrier, the entropy of the data structure is reduced by artifically raising the values of certain keys. Given any mixed sequence of n operations, a soft heap with error rate &egr; (for any 0 < &egr; ? 1/2) ensures that, at any time, at most &egr;n of its items have their keys raised. The amortized complexity of each operation is constant, except for insert, which takes 0(log 1/&egr;)time. The soft heap is optimal for any value of &egr; in a comparison-based model. The data structure is purely pointer-based. No arrays are move items across the data structure not individually, as is customary, but in groups, in a data-structuring equivalent of ?car pooling.? Keys must be raised as a result, in order to preserve the heap ordering of the data structure. The soft heap can be used to compute exact or approximate medians and percentiles optimally. It is also useful for approximate sorting and for computing minimum spanning trees of general graphs.
database	The advent of computers with high processing power has led to the generation of large, multidimensional collections of data. Visualization lends itself well to the challenge of exploring and analyzing these information spaces by harnessing the strengths of the human visual system. Most visualization techniques are based on the assumption that the display device has sufficient resolution, and that our visual acuity is adequate for completing the analysis tasks. However, this may not be true, particularly for specialized display devices (e.g., PDAs or large-format projection walls). In this article, we propose to: (1) determine the amount of information a particular display environment can encode; (2) design visualizations that maximize the information they represent relative to this upper-limit; and (3) dynamically update a visualization when the display environment changes to continue to maintain high levels of information content. To our knowledge, there are no visualization systems that do this type of information addition/removal based on perceptual guidelines. However, there are systems that attempt to increase or decrease the amount of information based on some level-of-detail or zooming rules. For example, semantic zooming tags objects with "details" and adds or removes them as the user zooms in and out. Furnas's original fisheye lens system [9] used semantic details to determine how much zoom was necessary to include certain details. Thus, while zooming for detail, you see not only a more detailed graphic representation, but also more text details (e.g., more street names on the zoomed-in portion of a map). Level-of-detail hierarchies have also been used in computer graphics to reduce geometric complexity where full resolution models are unnecessary and can be replaced with low-detail models where the resulting error cannot be easily recognized. Our approach is motivated by all these ideas, but our key contribution is that we use human perception constraints to define when to add or remove information.
database	Wireless spectrum is increasingly fragmented due to the growing proliferation of unlicensed wireless devices and piecemeal licensed spectrum allocations. Current radios are ill-equipped to exploit such fragmented spectrum since they expect large contiguous chunks of spectrum to operate on. In this paper we argue that future radios should provide full duplex signal shaping to the higher layers to systematically exploit fragmented spectrum. Such an architectural design would allow the radio to decouple the use of different spcetrum fragments. We present the design and implementation of Picasso, a system that provides such a general signal shaping abstraction. Picasso has two novel components: a self-interference cancellation technique and a programmable filter engine that enables it to simultaneously send and receive over different spectrum fragments. We provide an initial design and empirically evaluate the feasibility of both components.
database	Database Management Systems (DBMSs) provide access control mechanisms that allow database administrators (DBA) to grant application programs access privileges to databases. However, securing the database alone is not enough, as attackers aiming at stealing data can take advantage of vulnerabilities in the privileged applications and make applications to issue malicious database queries. Therefore, even though the access control mechanism can prevent application programs from accessing the data to which the programs are not authorized, it is unable to prevent misuse of the data to which application programs are authorized for access. Hence, we need a mechanism able to detect malicious behavior resulting from previously authorized applications. In this paper, we design and implement an anomaly detection mechanism, DetAnom, that creates a profile of the application program which can succinctly represent the application's normal behavior in terms of its interaction (i.e., submission of SQL queries) with the database. For each query, the profile keeps a signature and also the corresponding constraints that the application program must satisfy to submit that query. Later in the detection phase, whenever the application issues a query, the corresponding signature and constraints are checked against the current context of the application. If there is a mismatch, the query is marked as anomalous. The main advantage of our anomaly detection mechanism is that we need neither any previous knowledge of application vulnerabilities nor any example of possible attacks to build the application profiles. As a result, our DetAnom mechanism is able to protect the data from attacks tailored to database applications such as code modification attacks, SQL injections, and also from other data-centric attacks as well. We have implemented our mechanism with a software testing technique called concolic testing and the PostgreSQL DBMS. Experimental results show that our profiling technique is close to accurate, and requires acceptable amount of time, and that the detection mechanism incurs low run-time overhead.
database	Organizations need to monitor their business processes to ensure that what actually happens in the system is compliant with the prescribed behavior. Deviations from the prescribed behavior may correspond to violations of security requirements and expose organizations to severe risks. Thus, it is crucial for organizations to detect and address nonconforming behavior as early as possible. In this paper, we present an auditing framework that facilitates the analysis of process executions by detecting nonconforming behaviors and ranking them with respect to their criticality. Our framework employs conformance checking techniques to detect possible explanations of nonconformity. Based on such explanations, the framework assesses the criticality of nonconforming process executions based on historical logging data and context information.
database	Range query is one of the most frequently used queries for online data analytics. Providing such a query service could be expensive for the data owner. With the development of services computing and cloud computing, it has become possible to outsource large databases to database service providers and let the providers maintain the range-query service. With outsourced services, the data owner can greatly reduce the cost in maintaining computing infrastructure and data-rich applications. However, the service provider, although honestly processing queries, may be curious about the hosted data and received queries. Most existing encryption based approaches require linear scan over the entire database, which is inappropriate for online data analytics on large databases. While a few encryption solutions are more focused on efficiency side, they are vulnerable to attackers equipped with certain prior knowledge. We propose the Random Space Encryption (RASP) approach that allows efficient range search with stronger attack resilience than existing efficiency-focused approaches. We use RASP to generate indexable auxiliary data that is resilient to prior knowledge enhanced attacks. Range queries are securely transformed to the encrypted data space and then efficiently processed with a two-stage processing algorithm. We thoroughly studied the potential attacks on the encrypted data and queries at three different levels of prior knowledge available to an attacker. Experimental results on synthetic and real datasets show that this encryption approach allows efficient processing of range queries with high resilience to attacks.
database	We consider the problem of privacy-preserving data aggregation in a star network topology, i.e., several untrusting participants connected to a single aggregator. We require that the participants do not discover each other's data, and the service provider remains oblivious to each participant's individual contribution. Furthermore, the final result is to be published in a differentially private manner, i.e., the result should not reveal the contribution of any single participant to a (possibly external) adversary who knows the contributions of all other participants. In other words, we require a secure multiparty computation protocol that also incorporates a differentially private mechanism. Previous solutions have resorted to caveats such as postulating a trusted dealer to distribute keys to the participants, or introducing additional entities to withhold the decryption key from the aggregator, or relaxing the star topology by allowing pairwise communication amongst the participants. In this paper, we show how to obtain a noisy (differentially private) aggregation result using Shamir secret sharing and additively homomorphic encryption without these mitigating assumptions. More importantly, while we assume semi-honest participants, we allow the aggregator to be stronger than semi-honest, specifically in the sense that he can try to reduce the noise in the differentially private result. To respect the differential privacy requirement, collusions of mutually untrusting entities need to be analyzed differently from traditional secure multiparty computation: It is not sufficient that such collusions do not reveal the data of honest participants; we must also ensure that the colluding entities cannot undermine differential privacy by reducing the amount of noise in the final result. Our protocols avoid this by requiring that no entity -- neither the aggregator nor any participant -- knows how much noise a participant contributes to the final result. We also ensure that if a cheating aggregator tries to influence the noise term in the differentially private output, he can be detected with overwhelming probability.
database	To mitigate security concerns of outsourced databases, quite a few protocols have been proposed that outsource data in encrypted format and allow encrypted query execution on the server side. Among the more practical protocols, the "bucketization" approach facilitates query execution at the cost of reduced efficiency by allowing some false positives in the query results. Precise Query Protocols (PQPs), on the other hand, enable the server to execute queries without incurring any false positives. Even though these protocols do not reveal the underlying data, they reveal query access pattern to an adversary. In this paper, we introduce a general attack on PQPs based on access pattern disclosure in the context of secure range queries. Our empirical analysis on several real world datasets shows that the proposed attack is able to disclose significant amount of sensitive data with high accuracy provided that the attacker has reasonable amount of background knowledge. We further demonstrate that a slight variation of such an attack can also be used on imprecise protocols (e.g., bucketization) to disclose significant amount of sensitive information.
database	We describe a method using a Support Vector Machine (SVM) to classify and diagnose skin biopsies from patients as either melanoma or nevi based on H&E stained histological slides alone. Our method differs from other approaches to digital melanoma diagnoses in using the histology slide, not digital clinical pictures of the patients' skin to make the classification. Using only the histological criterion of irregularities in the nucleus, our best SVM utilizes nucleus perimeter/area ratio and nucleus major/minor axis ratio as features to give a classification accuracy of 90%, sensitivity of 100% and specificity of 75%, (at magnification of 400 times) in our data set. The performance is remarkable given a dermatological pathologist typically examines a plethora of features to make a diagnosis. Our SVM in conjunction with clinical digital diagnoses systems could reduce the number of missed melanoma diagnoses.
database	Human perception is a highly non-linear process, influenced by many more factors that we can measure on a video streaming service. In fact, despite the tremendous advances in video coding, we still don?t understand the intricate relationship between a stream and its delivery systems. We have little clues as to how different network conditions actually affect the quality of a video service perceived by the end user. Today, the most predominant consumer of network capacity (video) transits through a ?video-repellent? network (the Internet), one that has no notion of data delivery deadlines. So what are we getting from modern video services? Can we manage the quality of user experience, instead of trying to monitor or control the quality of network services? In this talk I give a critical perspective on video quality, its measurement and optimization, ending up with a controversial proposition.
database	This paper presents a novel musical performance system named onNote that directly utilizes printed music scores as a musical instrument. This system can make users believe that sound is indeed embedded on the music notes in the scores. The users can play music simply by placing, moving and touching the scores under a desk lamp equipped with a camera and a small projector. By varying the movement, the users can control the playing sound and the tempo of the music. To develop this system, we propose an image processing based framework for retrieving music from a music database by capturing printed music scores. From a captured image, we identify the scores by matching them with the reference music scores, and compute the position and pose of the scores with respect to the camera. By using this framework, we can develop novel types of musical interactions.
database	Henschen and Naqvi described a technique for translating queries on recursively defined relations of a Datalog database into iterative programs that invoke a query processor for conventional select-project-join queries of the relational algebra. Although the technique has been cited as one of the most efficient available, it will in some cases fail to produce all answers defined by the usual semantics for such databases. The technique is reviewed, a recursive query is exhibited where it fails, the cause of failure is noted, and a correction is described. A graphical representation of the computation based on a formal representation of rule expansions is employed.
database	ETL jobs are used to integrate data from distributed and heterogeneous sources into a data warehouse. A well-known challenge in this context is the development of incremental ETL jobs for efficiently maintaining warehouse data in the presence of source data updates. In this paper, we present a new transformation-based approach to automatically derive incremental ETL jobs. To this end, we consider a simplification of the underlying update propagation process based on the computation of so-called safe updates instead of true ones. Additionally, we identify the limitations of already proposed incremental solutions, which are cured by employing Magic Sets leading to dramatic performance gains.
database	Even though an effective cost-based query optimizer is of utmost importance for the efficient evaluation of XQuery expressions in native XML database systems, such a component is currently out of sight, because former approaches do not pay attention to the latest advances in the area of physical operators (e. g., Holistic Twig Joins and advanced indexes) or just focus only on some of them. To support the development of native XML query optimizers, we introduce an extensible cost-based optimization framework that integrates the cutting-edge XML query evaluation operators into a single system. Using the well-known plan generation techniques from the relational world and a novel set of plan equivalences---which allows for the generation of alternative query plans consisting of Structural Joins, Holistic Twig Joins, and numerous indexes (especially path indexes and content-and-structure indexes)---our optimizer can now benefit from the knowledge on native XML query evaluation to speed-up query execution significantly.
database	Many update queries on a database can eventually degrade the structural efficiency of the database and result in lower performance. This phenomenon is called aging. On aged databases, conventional cost-based query optimizers could choose non-optimal query execution plan because they are not aging-aware and could not accurately estimate query execution cost. As a first step to build an aging-aware query optimizer, we experimentally examined the discrepancy between actual and estimated cost in order to investigate aging influence on the accuracy of cost estimation. The experimental result shows that aging could produce non-negligible cost estimation errors, up to 66.3%. The result indicates that a conventional query optimizer can fail to choose the optimal execution plan even for a simple query.
database	Object-relational databases (ORDB) can be created with a mixture of relational and object-oriented features, giving database designers a wide range of options. However, a poor combination of choices can result in problems with storing, retrieving or updating data. This paper shows that decisions about the features to use in ORDB design require a holistic approach, considering the database design as a whole and as part of an information system. We highlight problems that occur with some combinations of features that can cause poor functionality.
database	MapReduce query processing systems translate a query statement into a query plan, consisting of a set of MapReduce jobs to be executed in distributed machines. During query translation, these query systems uniformly allocate computing resources to each job by delegating the same tuning to the entire query plan. However, jobs may implement their own collection of operators, which lead to different usage of computing resources. In this paper we propose an adaptive tuning mechanism that enables setting specific resources to each job within a query plan. Our adaptive mechanism relies on a data structure that maps jobs to tuning codes by analyzing source code and log files. This adaptive mechanism allows delegating specific resources to the query plan at runtime as the data structure hosts specific pre-computed tuning codes.
database	Queries over probabilistic databases lead to probabilistic results. As the process of arriving at these results is based on underlying data probabilities, we believe involving a user in the loop of query processing and leveraging the user's personal knowledge to deal with uncertain data, will enable the system to scrub (correct) and tailor its probabilistic query results towards a better quality from the perspective of the specific user. In this paper, we propose to open the black box of a probabilistic database query engine, and explain to the user how the engine comes up with the probabilistic query result as well as which uncertain tuples in the database the result is derived from. In this way, the user based on his/her knowledge about uncertain information can not only decide how much confidence to be placed on the query engine, but also help clarify some uncertain information so that the query engine can re-generate an improved query result. Two particular issues associated with such a probabilistic database query framework are addressed: (i) how to interact with a user for answer explanation and uncertainty clarification without bringing much burden to the user, and (ii) how to scrub/correct the query result without incurring much computation overhead to the query engine. Our performance study demonstrates the accuracy effectiveness and computational efficiency achieved by the proposed framework.
database	An index in a Multi-Version DBMS (MV-DBMS) has to reflect different tuple versions of a single data item. Existing approaches follow the paradigm of logically separating the tuple version data from the data item, e.g. an index is only allowed to return at most one version of a single data item (while it may return multiple data items that match a search criteria). Hence to determine the valid (and therefore visible) tuple version of a data item, the MV-DBMS first fetches all tuple versions that match the search criteria and subsequently filters visible versions using visibility checks. This involves I/O storage accesses to tuple versions that do not have to be fetched. In this vision paper we present the Multi-Version Index (MV-IDX) approach that allows index-only visibility checks which significantly reduce the amount of I/O storage accesses as well as the index maintenance overhead. The MV-IDX achieves significantly lower response times and higher transactional throughput on OLTP workloads.
database	Integrating and accessing data from the many existing formats is a key challenge for the Semantic Web. In particular, representing and understanding non-RDF/OWL data in terms of ontologies is important for reasoning about this data and its relationships to other data, and for extracting or accessing the data itself. This paper presents a declarative method for mapping and translating spreadsheet data to RDF/OWL. First, a declarative mapping language is presented. Then, for this syntax, the paper presents a declarative semantics in terms of matrix operations as abstractions over spreadsheets and the relations therein. The language supports arbitrary search and indexing functions over spreadsheet data and allows the generation of complex associations and aggregations in the RDF/OWL output. Imperative and functional implementations of the mapping and extraction technology are presented.
database	The federated database architecture has been introduced to maintain the autonomy of individual data sources yet accomplish federated task for diverse applications from traditional enterprises to computational sciences. We identify two challenging problems of query optimization in large-scale database federation systems. First, run-time conditions of data sources have a profound effect on the performance of database federations, yet the distributed environment of database federations makes it prohibitively expensive for the optimizer to gather rapidly fluctuating run-time conditions from remote data sources. Second, large-scale database federation systems are often widely distributed and built on heterogeneous networks, thus efficiently utilizing network resources is of ever increasing importance for query scheduling. In this paper, we propose to exploit the clustered hierarchical structure of database federations to solve these two problems. Our Cluster-and-Conquer strategy coordinates hierarchical clusters of data sources to optimize and process queries cooperatively. Within each cluster we employ an I/O-bound cost model with run-time conditions being accessible with relatively little delay. While among clusters a network-bound cost model is instead utilized to capture the network heterogeneity and optimize the query plans for efficient network utilization. The experimental study on the prototype database federation system with real-world network settings shows the effectiveness of our Cluster-and-Conquer strategy for scheduling data-intensive queries, as well as demonstrates the performance benefits of our proposed strategies over existing state-of-art solutions.
database	A database is an organized collection of data.[1] A relational database, more restrictively, is a collection of schemas, tables, queries, reports, views, and other elements. Database designers typically organize the data to model aspects of reality in a way that supports processes requiring information, such as (for example) modelling the availability of rooms in hotels in a way that supports finding a hotel with vacancies.
database	Formally, a "database" refers to a set of related data and the way it is organized. Access to this data is usually provided by a "database management system" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
database	In this chapter we will focus on the rules of aggregation hierarchies in analysis dimensions of a cube. We give an overview of the related works on the basic concepts of the different types of aggregation hierarchies. We then discuss the hierarchies from two different points of view: mapping between domain values and hierarchical structures. In relation to them, we introduce the characterization of some OLAP operators on hierarchies and give a set of operators that concern the change in the hierarchy structure. Finally, we propose an enlargement of the operator set concerning hierarchies.
database	Bioinformatics tasks may become very complex and usually require to manually integrate both data and results from different knowledge sources and tools. In this scenario, an integrated environment for designing and executing complex biological workflows is a must. Even though several efforts are trying to cope with this aspects, they mostly focus on gene or protein sequence analysis underestimating more complex biological data such as molecular interaction data. The aim of this paper is to present the BioTRON system, which supports biologists in the various steps necessary to perform complex biological tasks such as biological network comparison. BioTRON also features a mechanism to automatically integrate even existing on-line Web services. We present the BioTRON architecture along with a real example, which shows the suitability of the tool.
database	The accurate characterization of collections of bacterial strains is a major scientific challenge, since bacteria are indeed responsible of significant plant diseases and thus subjected to official control procedures (e.g., in Europe, Directive 2000/29/EC). The development of diagnostic tests is therefore an important issue in order to routinely identify strains of these species.
database	On the matter of memory, there is no comparision. Neural networks are potentially faster and more accurate than humans.
database	In underwater wireless sensor networks (USWNs), localizing unknown nodes is essential for most applications while is more complex than that of terrestrial WSNs. In this paper, we propose a range-based localization scheme using deep neural network (DNN). Numerical results suggest that the proposed DNN localization algorithm outperforms traditional schemes using least squares support vector machines (LS-SVM) or generalized least squares (GLS) in terms of localization accuracy and efficiency. Moreover, the proposed algorithm requires a small number of anchor nodes, which is plausible for practical applications.
database	A leading cause of mortality of hospitalized patients are hospital acquired infections (HAI). Unclean hands of healthcare personnel (HCP) are the most common factor contributing to HAI, but their strict compliance to hand hygiene protocols is difficult to supervise. In this work, we propose CleanHands: a simple, low-cost and scalable monitoring and alerting system to ensure adequate thoroughness of disinfection. CleanHands uses a combination of low-cost Bluetooth low energy (BLE) beacon tags and mobile phones for HCP tracking. It integrates infection control models and state-following algorithms for alarming in the event of noncompliance to hand hygiene. Our preliminary experiments in a mockup, small scale intensive care unit (ICU) facility shows promising results with less than 5% false positives.
database	Background subtraction is often the first step in many computer vision applications such as object localisation and tracking. It aims to segment out moving parts of a scene that represent object of interests. In the field of computer vision, researchers have dedicated their efforts to improve the robustness and accuracy of such segmentations but most of their methods are computationally intensive, making them non-viable options for our targeted embedded camera platform whose energy and processing power is significantly more constrained. To address this problem as well as maintain an acceptable level of performance, we introduce Compressive Sensing (CS) to the widely used Mixture of Gaussian to create a new background subtraction method. The results show that our method not only can decrease the computation significantly (a factor of 7 in a DSP setting) but remains comparably accurate.
database	A radio tomographic (RT) system uses the received signal strength (RSS) measurements collected on the links of a wireless mesh network composed of low-power transceivers in order to form real-time images of the attenuation field of the monitored area. These images indicate the position of people, without requiring them to participate in the localization effort by wearing or carrying any electronic device. Accurate localization and tracking of multiple people in real-time is required in several real-world applications, such as ambient-assisted living, tactical operations, and pedestrian traffic analysis in stores. In these scenarios, RT systems must perform reliably also a) when the number of targets is not known a priori and varies over time, and b) when people interact, i.e., have intersecting trajectories, in the monitored area. We demonstrate a RT system which tackles all of these challenges and provides accurate tracking of a varying and unknown number of people (both stationary and mobile) in real-time.
database	Monitoring aquatic environment is of great interest to the ecosystem, marine life, and human health. This paper presents the design and implementation of Samba -- an aquatic surveillance robot that integrates an off-the-shelf Android smartphone and a robotic fish to monitor harmful aquatic processes such as oil spill and harmful algal blooms. Using the built-in camera of on-board smartphone, Samba can detect spatially dispersed aquatic processes in dynamic and complex environments. To reduce the excessive false alarms caused by the non-water area (e.g., trees on the shore), Samba segments the captured images and performs target detection in the identified water area only. However, a major challenge in the design of Samba is the high energy consumption resulted from the continuous image segmentation. We propose a novel approach that leverages the power-efficient inertial sensors on smartphone to assist the image processing. In particular, based on the learned mapping models between inertial and visual features, Samba uses real-time inertial sensor readings to estimate the visual features that guide the image segmentation, significantly reducing energy consumption and computation overhead. Samba also features a set of lightweight and robust computer vision algorithms, which detect harmful aquatic processes based on their distinctive color features. Lastly, Samba employs a feedback-based rotation control algorithm to adapt to spatiotemporal evolution of the target aquatic process. We have implemented a Samba prototype and evaluated it through extensive field experiments, lab experiments, and trace-driven simulations. The results show that Samba can achieve 94% detection rate, 5% false alarm rate, and a lifetime up to nearly two months.
database	Clock synchronization is highly desirable in distributed systems, including many applications in the Internet of Things and Humans (IoTH). For IoTH, Bluetooth Low Energy (BLE) - a subset of the recent Bluetooth v4:0 stack - provides a low-power and loosely coupled mechanism for sensor data collection with ubiquitous units (e.g., smartphones and tablets) carried by humans. This fundamental design paradigm of BLE is enabled by a range of broadcast advertising modes. While its operational benefits are numerous, the lack of a common time reference in the broadcast mode of BLE has been a fundamental limitation. This work presents and describes CheepSync: a time synchronization service architecture for BLE advertisers. We implement CheepSync on custom designed nRF24Cheep beacon platforms (as broadcasters) and commercial off-the-shelf Android ported smartphones (as passive listeners); and show that the average (single hop) time synchronization accuracy is in the 10 ?s range.
database	I present preliminary results of my work on an architecture for Wireless Sensor Networks (nets) that realizes the novel concept of ASFECs (approximately synchronized fetch-and-execute cycles). This architecture extends the classical fetch-and-execute cycles of computers by syncing phases. It guarantees a consistent value of the instruction pointer for all (sensor) nodes and a maximum skew between the starting times of the corresponding instructions on the nodes. In addition, I present my current work. I show how ASFECs can process net-assemblies. So far, I have studied centralized nets, only. I expect that ASFECs can be a important step towards fully deterministic, hard real time measurements fulfilled by nets.
database	Node reprogramming serves as an important service to support post-deployment code management and maintenance for unattended wireless sensor networks or autonomous robotics applications. We present a prototype of a design framework called SHAMPU, a Single chip Host for Autonomous Mote Programming over USB. A SHAMPU device is portable and can be paired with a sensor node for post-deployment reprogramming that is OS independent. Moreover, it is small, lightweight and energy efficient. In the demo, we will present two scenarios, in which SHAMPU-paired Telosbs are integrated with a mobile flying robot and locomotives for local autonomous reprogramming by the flying robot and for network reprogramming with code dissemination via the data links on the tracks, respectively.
database	We describe a method of using a network of thermopile sensors distributed along the walls of a room to locate a person within the room. At any given time, a person would be in the view of at least one of the sensor nodes. An algorithm is used to calculate the distance of a person from each sensor node based on its temperature reading and locate the person based on a combination of the sensor readings from the distributed nodes. This method is immune to lighting changes and works even in total darkness. Furthermore, this method of sensing can also detect events such as a person walking past the doorway to a room, lingering outside, and entering or leaving the room.
database	We present SiCILIA, a hardware platform that extracts physical and personal variables of an individual's thermal environment to infer the amount of clothing insulation and thermal sensation without human intervention. The proposed inference algorithms build upon theories of body heat transfer, and are corroborated by empirical data. Experimental results show the algorithm is capable of accurately predicting an occupant's thermal insulation with a confidence interval of approximately ?0.3 and a mean prediction error of 0.2.
database	In large-scale resource-constrained systems, such as wireless sensor networks, global objectives should be ideally achieved through inexpensive local interactions. A technique satisfying these requirements is information potentials, in which distributed functions disseminate information about the process monitored by the network. Information potentials are usually computed through local aggregation or gossiping. These methods however, do not consider the topological properties of the network, such as node density, which could be exploited to enhance the performance of the system. This paper proposes a novel aggregation method with which a potential becomes sensitive to the network topology. Our method introduces the notion of affinity spaces, which allow us to uncover the deep connections between the aggregation scope (the radius of the extended neighborhood whose information is aggregated) and the network's Laplacian (which captures the topology of the connectivity graph). Our study provides two additional contributions: (i) It characterizes the convergence of information potentials for static and dynamic networks. Our analysis captures the impact of key parameters, such as node density, time-varying information, as well as of the addition (or removal) of links and nodes. (ii) It shows that information potentials are decomposed into wave-like eigenfunctions that depend on the aggregation scope. This result has important implications, for example it prevents greedy routing techniques from getting stuck by eliminating local-maxima. Simulations and experimental evaluation show that our main findings hold under realistic conditions, with unstable links and message loss.
database	This article studies and analyzes three other-condemning moral emotions: anger, contempt, and disgust. We utilize existing psychological theories�appraisal theories of emotion and the CAD triad hypothesis�and incorporate them into a unified framework. A semiformal specification of the elicitation conditions and prototypical coping strategies for the other-condemning emotions are proposed. The appraisal conditions are specified in terms of cognitive and social concepts such as goals, beliefs, actions, control and accountability, while coping strategies are classified as belief-, goal- and intention-affecting strategies, and specified in terms of action specifications. Our conceptual analysis and semiformal specification of the three other-condemning moral emotions are illustrated by means of an example of trolling in the domain of social media.
database	Sleep apnea, which is a common sleep disorder characterized by the repetitive cessation of breathing during sleep, can result in various diseases, including headaches, hypertension, stroke and cardiac arrest, as well as produce severe consequences such as impaired concentration and traffic accidents. A traditional diagnosis method of sleep apnea is polysomnography, which can only be conducted in sleep center with specialized personals, thus is expensive and inconvenient. Moreover, it is only used for understanding the conditions, without treatment function. Some other methods or devices have been developed to alleviate sleep apnea, such as continuous positive airway pressure (CPAP) and intraoral mandibular advancement device and surgery. However, they only provide a treatment method without detection or monitoring function. There is no existing device which can provide both apnea detection and treatment functionality. In this paper, we propose and implement a smart phone-based auto-adjustable pillow system, which enables both sleep apnea detection and treatment. Sleep apnea events can be detected in real-time using the blood oxygen sensor, accordingly, the height and shape of the pillow can be automatically adjusted to terminate the sleep apnea event. On the other hand, after the adjustment, the sensor can continuously monitor the blood oxygen signal to evaluate the effectiveness of the pillow adjustment and to help in selecting a suitable adjustment scheme. Therefore, a real-time feedback control system is formed. Besides, compared with existing diagnosis or treatment devices, our system is non-invasive, inexpensive and portable, which can be used at home or during traveling. In this paper, a real-time sleep apnea detection and classification algorithm is proposed to decide whether the pillow should be adjusted or not. We also design a real-time feedback pillow adjustment algorithm, to decide when and how to adjust the pillow and how to evaluate the effectiveness of the adjustment. We conducted experiments on 40 patients, which demonstrate that using our novel smart pillow system, both the sleep apnea duration and the number of sleep apnea events are dramatically reduced by more than 50%.
database	This study presents TriopusNet, a mobile wireless sensor network system for autonomous sensor deployment in pipeline monitoring. TriopusNet works by automatically releasing sensor nodes from a centralized repository located at the source of the water pipeline. During automated deployment, TriopusNet runs a sensor deploy-ment algorithm to determine node placement. While a node is flowing inside the pipeline, it performs placement by extending its mechanical arms to latch itself onto the pipe's inner surface. By continuously releasing nodes into pipes, the TriopusNet system builds a wireless network of interconnected sensor nodes. When a node runs at a low battery level or experiences a fault, the TriopusNet system releases a fresh node from the repository and performs a node replacement algorithm to replace the failed node with the fresh one. We have evaluated the TriopusNet system by creating and collecting real data from an experimental pipeline testbed. Comparing with the non-automated static deployment, TriopusNet is able to use less sensor nodes to cover a sensing area in the pipes while maintaining network connectivity among nodes with high data collection rate. Experimental results also show that TriopusNet can recover from the network disconnection caused by a battery-depleted node and successfully replace the battery-depleted node with a fresh node.
database	Badly lit roads usually lead to vehicle accidents and encourage crime in the area. Therefore, it is important to detect faulty street lamps rapidly and report them to related authorities to keep roads safe. Currently, communities still mostly depend on electrical inspectors to check street lamps regularly, which may result in long time delays for repair. Recent studies focus on add networking capability into street lamp poles to enable real-time reports on the healthy status of lamps. However, such a smart system increases costs to add sensors and network modules in every street lamp; therefore, it is nearly impossible to realize this kind of system in a short term. We propose a new method to detect faulty lamps. We designed equipment that could be installed on fixed bus routes and collect the lighting intensity along the routes. We created illumination maps in meter-level resolution. The differences between illumination maps created at different times can help identify the changes of lighting intensity in specific locations. We executed a proof-of-concept experiment that shows our method is feasible. This method can be extended to a city-wide scale at low cost. As a result, this would detect faulty street lamps along main roads and prevent accidents and crime by shortening the duration of badly lit streets.
database	Hand Hygiene Duration and Technique The Trip Quality Measurement System (TQMS) is a prototype mobile data participatory system for the efficient collection and management of mobility and survey data. A transport trip quality measurement application has been developed using this system. The target users of this application are the commuters and passengers who travel in buses, taxis, metro trains, etc. This application measures the quality of the users' transport trips based on various factors such as bumpiness of ride, wait/travel time, distance, etc. The system consists of a smartphone client (iOS and Android) app for collecting mobility and survey data from the users, and a data management and analytics framework to manage, process and analyze the participatory data to derive useful insights on the mobility pattern of users and the transport trip quality experienced by the users.
database	Socio-Technical Systems demand an evolution of computing into social computing, with a transition from an individualistic to a societal view. As such, they seem particularly suitable to realize multiparty, cross-organizational systems. Multi-Agent Systems are a natural candidate to realize Socio-Technical Systems. However, while Socio-Technical Systems envisage an explicit layer that contains the regulations that all parties must respect in their interaction, and thus preserve the agents' autonomy, current frameworks and platforms require to hard-code the coordination requirements inside the agents. We propose to explicitly represent the missing layer of Socio-Technical Systems in terms of social relationships among the involved parties, that is, in terms of a set of normatively defined relationships among two or more parties, subject to social control by monitoring the observable behaviour. In our proposal, social relationships are resources, available to agents, who use them in their practical reasoning. Both agents and social relationships are first-class entities of the model. The work also describes 2COMM4JADE, a framework that realizes the proposal by extending the well-known JADE and CArtAgO. The impact of the approach on programming is explained both conceptually and with the help of an example.
database	The paper presents a smartphone-based shooter localization system. As muzzle blasts are difficult to detect at longer distances and consequently present higher false detection rates, the system relies on shockwaves only. Each sensor uses four microphones to detect the Angle of Arrival and the length of the shockwave. This information, along with the sensor's own GPS coordinates, are shared among nearby smartphones. Assuming a known weapon type, it then proceeds to estimate the two possible projectile trajectory candidates for each sensor that are consistent with the observations in the horizontal plane of the sensors. A simple clustering algorithm identifies the correct projectile trajectory relying on as few as two sensors. The trajectory is then used to estimate the bearing to the shooter relative to each sensor. The paper presents the overall system architecture, the design of the sensor node that interfaces with the smartphone, the trajectory and bearing estimation algorithms, and the evaluation of the system based on a field experiment.
database	We demonstrate the design and implementation of a prototype hardware/software architecture for automatic single-word speech recognition on resource-constrained embedded devices. Designed as a voice-activated extension of an existing wireless nurse call system, our prototype device continually listens for a pre-recorded keyword, and uses speech recognition techniques to trigger an alert upon detecting a match. Preliminary experiments show that our prototype achieves a high average detection rate of 96%, while only dissipating 28.5mW for continuous audio sampling and duty-cycled speech recognition.
database	We propose a SDN-like architecture based WSN and improve the existing EC-CKN Sleep Scheduling mechanism to implement more efficient energy management. A SDN-like architecture is adopted instead of traditional WSN architecture and EC-CKN algorithm is applied as the fundamental algorithm. This paper presents the design, implementation and evaluation of the proposed SDN-ECCKN on the WSN with SDN-like architecture.
database	Daylight harvesting is the use of natural sunlight to reduce the need for artificial lighting in buildings. The key challenge of daylight harvesting is to provide stable indoor lighting levels even though natural sunlight is not a stable light source. In this paper, we present a new technique called SunCast that improves lighting stability by predicting changes in future sunlight levels. The system has two parts: 1) it learns predictable sunlight patterns due to trees, nearby buildings, or other environmental factors, and 2) it controls the window transparency based on a quadratic optimization over predicted sunlight levels. To evaluate the system, we record daylight levels at 39 different windows for up to 12 weeks at a time, and apply our control algorithm on the data traces. Our results indicate that SunCast can reduce glare by 59% over a baseline approach with only a marginal increase in artificial lighting energy.
database	In this paper, we present the architecture, design and experiences from a wirelessly managed microgrid deployment in rural Les Anglais, Haiti. The system consists of a three-tiered architecture with a cloud-based monitoring and control service, a local embedded gateway infrastructure and a mesh network of wireless smart meters deployed at 52 buildings. Each smart meter device has an 802.15.4 radio that enables remote monitoring and control of electrical service. The meters communicate over a scalable multi-hop TDMA network back to a central gateway that manages load within the system. The gateway also provides an 802.11 interface for an on-site operator and a cellular modem connection to a cloud-backend that manages and stores billing and usage data. The cloud backend allows occupants in each home to pre-pay for electricity at a particular peak power limit using a text messaging service. The system activates each meter within seconds and locally enforces power limits with provisioning for theft detection. We believe that this fine-grained micro-payment model can enable sustainable power in otherwise unfeasible areas. This paper provides a chronology of our deployment and installation strategy that involved GPS-based site mapping along with various network conditioning actions required as the network evolved. Finally, we summarize key lessons learned and hypothesis about additional hardware that could be used to ease the tracing of faults like short circuits and downed lines within microgrids.
database	Micro-aerial vehicle (MAV) swarms are an emerging class of mobile sensing systems. Designing the next generation of such swarms requires the ability to rapidly test algorithms, sensors, and support infrastructure at scale. Simulation is useful in the early stages of such large-scale system design, when hardware is unavailable or deployment at scale is impractical. To faithfully represent the problem domain, an MAV swarm simulator must be able to model all key aspects of the system: actuation, sensing, and communication. Further, it is important to be able to quickly test swarm behavior using different control algorithms in a varied set of environments, and with a variety of sensors. We demonstrate Simbeeotic, a simulation framework that is capable of modeling large-scale MAV swarms. Simbeeotic enables algorithm development and rapid prototyping through both simulation and hardware-in-the-loop experimentation. We demonstrate Simbeeotic running simulated applications and videos demonstrating hybrid experiments with simulated MAVs as well as helicopters flying in our testbed that show the power and versatility required to assist next generation swarm design.
database	Understanding human drivers' behavior is critical for the self-driving cars, and has been intensively studied in the past decade. We exploit the widely available camera and motion sensor data from car recorders, and propose a hybrid method of recognizing driving events based on the random forest approach. The classification results are analyzed by comparing different features, classifiers and filters. A high accuracy of 98.1% on driving behavior classification is obtained and the robustness is verified on a dataset including 2400 driving events.
database	The efficacy of data aggregation in sensor networks is a function of the degree of spatial correlation in the sensed phenomenon. While several data aggregation (i.e., routing with data compression) techniques have been proposed in the literature, an understanding of the performance of various data aggregation schemes across the range of spatial correlations is lacking. We analyze the performance of routing with compression in wireless sensor networks using an application-independent measure of data compression (an empirically obtained approximation for the joint entropy of sources as a function of the distance between them) to quantify the size of compressed information, and a bit-hop metric to quantify the total cost of joint routing with compression. Analytical modeling and simulations reveal that while the nature of optimal routing with compression does depend on the correlation level, surprisingly, there exists a practical static clustering scheme which can provide near-optimal performance for a wide range of spatial correlations. This result is of great practical significance as it shows that a simple cluster-based system design can perform as well as sophisticated adaptive schemes for joint routing and compression.
database	Using an air purifying system can remove indoor air pollutants, but because it increases electric power utilization, results in broader increases in air pollution. To explore the tradeoff between energy consumption and healthful air, we demonstrate the indoor air sensing and automation (IASA) system, an internet-of-things system. The IASA system uses an air quality sensor, gateway device, and smart thermostat to control the fan in a home's heating and cooling system. When fine particulate matter is high, the system operates the fan to pull air through a furnace filter and remove the pollution from the indoor air. We describe our system design, deployment, and collected data. To date, we have collected 861,000 air quality measurements with IASA.
database	Several concurrent applications running on a sensor network may cause a node to transmit packets at distinct periods, which increases the radio-switching rate and has significant impact in terms of the overall energy consumption. We propose to batch the transmissions together by defining a harmonizing period to align the transmissions from multiple applications at periodic boundaries. This harmonizing period is then leveraged to design a distributed protocol called Network-Harmonized Scheduling (NHS) that coordinates transmissions across nodes and provides real-time guarantees in a multi-hop network.
machine_learning	Collaborative filtering is a widely used technique for rating prediction in recommender systems. Memory based collaborative filtering algorithms assign weights to the users to capture similarities between them. The weighted average of similar users' ratings for the test item is output as prediction. We propose a memory based algorithm that is markedly different from the existing approaches. We use preference relations instead of absolute ratings for similarity calculations, as preference relations between items are generally more consistent than ratings across like-minded users. Each user's ratings are viewed as a preference graph. Similarity weights are learned using an iterative method motivated by online learning. These weights are used to create an aggregate preference graph. Ratings are inferred to maximally agree with this aggregate graph. The use of preference relations allows the rating of an item to be influenced by other items, which is not the case in the weighted-average approaches of the existing techniques. This is very effective when the data is sparse, specially for the items rated by few users. Our experiments show that the our method outperforms other methods in the sparse regions. However, for dense regions, sometimes our results are comparable to the competing approaches, and sometimes worse.
machine_learning	In this paper, we propose Personalized Markov Embedding (PME), a next-song recommendation strategy for online karaoke users. By modeling the sequential singing behavior, we first embed songs and users into a Euclidean space in which distances between songs and users reflect the strength of their relationships. Then, given each user's last song, we can generate personalized recommendations by ranking the candidate songs according to the embedding. Moreover, PME can be trained without any requirement of content information. Finally, we perform an experimental evaluation on a real world data set provided by ihou.com which is an online karaoke website launched by iFLYTEK, and the results clearly demonstrate the effectiveness of PME.
machine_learning	Random walks on graphs are a staple of many ranking and recommendation algorithms. Simulating random walks on a graph which fits in memory is trivial, but massive graphs pose a problem: the latency of following walks across network in a cluster or loading nodes from disk on-demand renders basic random walk simulation unbearably inefficient. In this work we propose DrunkardMob, a new algorithm for simulating hundreds of millions, or even billions, of random walks on massive graphs, on just a single PC or laptop. Instead of simulating one walk a time it processes millions of them in parallel, in a batch. Based on DrunkardMob and GraphChi we further propose a framework for easily expressing scalable algorithms based on graph walks.
machine_learning	The expansion of TV services such as DVR and, more recently, Catch-up have removed the temporal constraint typical of the Linear "appointment" TV enabling users to watch content they love at any time and on-demand. However, the DVR and Catch-up TV libraries, while providing a convenient time-shifted "on-demand" consumption, are indeed composed by content recently aired on a linear channel, so that they have more in common with Linear TV than they have with VOD. In this talk we will present and discuss the main challenges and some possible solutions to personalize the user experience with content from DVR and Catch-up TV, such as: (i) The consumption pattern is strongly affected by the context (e.g., time and device used to access the video service). (ii) Some content is consumed serially and still follows seasonal dynamics (e.g., TV Series). (iii) The system is fed with a massive and very dynamic streams of data (e.g., new content arriving right after broadcast, signals of user interactions). (iv) The same piece of content may coexist across multiple services provided by the same operator (e.g., linear schedule, network-DVR, catch-up TV, subscription VOD, rental VOD).
machine_learning	The proverbial celestial jukebox has become a reality. With today's online music services a music fan is never more than a few clicks away from being able to listen to nearly any song that has ever been recorded. Recommender systems can play a key role in this new music ecosystem, helping listeners explore, discover, organize and share music. However, in many ways music recommendation is very different than recommendation in other well-studied domains such as books and movies. In this talk we explore how recommender systems can be used in the music space, and the particular challenges that the music domain presents to the designers of recommender systems.
machine_learning	Over the recent years, a plethora of recommender systems (RS) have been proposed by academics. The degree of adoptability of these algorithms by industrial e-commerce platforms remains unclear. To get an insight into real-world recommendation engines, we survey more than 30 existing shopping cart solutions and compare the performance of popular recommendation algorithms on proprietary e-commerce datasets. Our results show that deployed systems rarely go beyond trivial "best seller" lists or very basic personalized recommendation algorithms, which nevertheless exhibit superior performance to more elaborate techniques both in our experiments and other related studies. We also perform chronological dataset splits to demonstrate the importance of preserving the sequence of events during evaluation, and the recency of events during training. The second part of our research is still ongoing and focuses on various ethical challenges that complicate the design of recommender systems. We believe that this direction of research remains mostly neglected despite its increasing impact on RS' quality and safety.
machine_learning	In this paper we consider the application of content-based recommendation techniques to web curation services which allow users to curate and share topical collections of content (e.g. images, news, web pages etc.). Curation services like Pinterest are now a mainstay of the modern web and present a range of interesting recommendation challenges. In this paper we consider the task of recommending collections to users and evaluate a range of different content-based techniques across a variety of content signals. We present the results of a large-scale evaluation using data from the Scoop.it web page curation service
machine_learning	Current approaches on collaborative filtering factorize user-item matrices in order to infer latent factors from ratings previously assigned by users. However, they all have to deal with sparseness, whose workarounds are prone to bias and/or overfitting. This paper proposes a recommender algorithm that is based on a factorized matrix composed of user preferences associated to the movies' genres/categories. The advantage of using such user-genre matrix factorization model is that it requires less computational resources, as the matrix will be less sparse and at lower dimension. We present the experimental results with a dataset composed of real users, comparing the performance of different modules of our algorithm.
machine_learning	In this tutorial we first characterize user queries before focusing in two important problems related to search engines: query intention prediction and query recommendation
machine_learning	In this paper we give methods for time-aware music recommendation in a social media service with the potential of exploiting immediate temporal influences between users. We consider events when a user listens to an artist the first time and this event follows some friend listening to the same artist short time before. We train a blend of matrix factorization methods that model the relation of the influencer, the influenced and the artist, both the individual factor decompositions and their weight learned by variants of stochastic gradient descent (SGD). Special care is taken since events of influence form a subset of the positive implicit feedback data and hence we have to cope with two different definitions of the positive and negative implicit training data. In addition, in the time-aware setting we have to use online learning and evaluation methods. While SGD can easily be trained online, evaluation is cumbersome by traditional measures since we will have potentially different top recommendations at different times. Our experiments are carried over the two-year "scrobble" history of 70,000 Last.fm users and show a 5% increase in recommendation quality by predicting temporal influences.
machine_learning	The ubiquity of mobile devices and cloud services has led to an unprecedented growth of online personal photo and video collections. Due to the scarcity of personal media search log data, research to date has mainly focused on searching images and videos on the web. However, in order to manage the exploding amount of personal photos and videos, we raise a fundamental question: what are the differences and similarities when users search their own photos versus the photos on the web? To the best of our knowledge, this paper is the first to study personal media search using large-scale real-world search logs. We analyze different types of search sessions mined from Flickr search logs and discover a number of interesting characteristics of personal media search in terms of information needs and click behaviors. The insightful observations will not only be instrumental in guiding future personal media search methods, but also benefit related tasks such as personal photo browsing and recommendation. Our findings suggest there is a significant gap between personal queries and automatically detected concepts, which is responsible for the low accuracy of many personal media search queries. To bridge the gap, we propose the deep query understanding model to learn a mapping from the personal queries to the concepts in the clicked photos. Experimental results verify the efficacy of the proposed method in improving personal media search, where the proposed method consistently outperforms baseline methods.
machine_learning	In an entity classification task, topic or concept hierarchies are often incomplete. Previous work by Dalvi et al. [12] has showed that in non-hierarchical semi-supervised classification tasks, the presence of such unanticipated classes can cause semantic drift for seeded classes. The Exploratory learning [12] method was proposed to solve this problem; however it is limited to the flat classification task. This paper builds such exploratory learning methods for hierarchical classification tasks. We experimented with subsets of the NELL [8] ontology and text, and HTML table datasets derived from the ClueWeb09 corpus. Our method (OptDAC-ExploreEM) outperforms the existing Exploratory EM method, and its naive extension (DAC-ExploreEM), in terms of seed class F1 on average by 10% and 7% respectively.
machine_learning	In this paper we address the problem of automatically learning to classify the sentiment of short messages/reviews by exploiting information derived from meta-level features i.e., features derived primarily from the original bag-of-words representation. We propose new meta-level features especially designed for the sentiment analysis of short messages such as: (i) information derived from the sentiment distribution among the k nearest neighbors of a given short test document x, (ii) the distribution of distances of x to their neighbors and (iii) the document polarity of these neighbors given by unsupervised lexical-based methods. Our approach is also capable of exploiting information from the neighborhood of document x regarding (highly noisy) data obtained from 1.6 million Twitter messages with emoticons. The set of proposed features is capable of transforming the original feature space into a new one, potentially smaller and more informed. Experiments performed with a substantial number of datasets (nineteen) demonstrate that the effectiveness of the proposed sentiment-based meta-level features is not only superior to the traditional bag-of-word representation (by up to 16%) but is also superior in most cases to state-of-art meta-level features previously proposed in the literature for text classification tasks that do not take into account some idiosyncrasies of sentiment analysis. Our proposal is also largely superior to the best lexicon-based methods as well as to supervised combinations of them. In fact, the proposed approach is the only one to produce the best results in all tested datasets in all scenarios.
machine_learning	For the past five years, the Google Brain team has focused on conducting research in difficult problems in artificial intelligence, on building large-scale computer systems for machine learning research, and, in collaboration with many teams at Google, on applying our research and systems to dozens of Google products. Our group has recently open-sourced the TensorFlow system (tensorflow.org), a system designed to easily express machine ideas, and to quickly train, evaluate and deploy machine learning systems. In this talk, I'll highlight some of the design decisions we made in building TensorFlow, discuss research results produced within our group, and describe ways in which these ideas have been applied to a variety of problems in Google's products, usually in close collaboration with other teams. This talk describes joint work with many people at Google.
machine_learning	Understanding air travel choice behavior of air passengers is of great significance for various purposes such as travel demand prediction and trip recommendation. Existing approaches based on surveys can only provide aggregate level air travel choice behavior of passengers and they fail to provide comprehensive information for personalized services. In this paper we focus on modeling individual level air travel choice behavior of passengers, which is valuable for recommendations and personalized services. We employ a probabilistic model to represent individual level air travel choice behavior based on a large dataset of historical booking records, leveraging several key factors, such as takeoff time, arrival time, elapsed time between reservation and takeoff, price, and seat class. However, each passenger has only a limited number of historical booking records, causing a serious data sparsity problem. To this end, we propose a mixed kernel density estimation (mix-KDE) approach for each passenger with a mixture model that combines probabilistic estimation of both regularity of the individual himself and social conformity of similar passengers. The proposed model is trained and evaluated via the expectation-maximization (EM) algorithm with a huge dataset of booking records of over 10 million air passengers from a popular online travel agency in China. Experimental results demonstrate that our mix-KDE approach outperforms the Gaussian mixture model (GMM) and the simple kernel density estimation in the presence of the sparsity issue.
machine_learning	Recent research has made significant advances in automatically constructing knowledge bases by extracting relational facts (e.g., Bill Clinton-presidentOf-US) from large text corpora. Temporally scoping such relational facts in the knowledge base (i.e., determining that Bill Clinton-presidentOf-US is true only during the period 1993 - 2001) is an important, but relatively unexplored problem. In this paper, we propose a joint inference framework for this task, which leverages fact-specific temporal constraints, and weak supervision in the form of a few labeled examples. Our proposed framework, CoTS (Coupled Temporal Scoping), exploits temporal containment, alignment, succession, and mutual exclusion constraints among facts from within and across relations. Our contribution is multi-fold. Firstly, while most previous research has focused on micro-reading approaches for temporal scoping, we pose it in a macro-reading fashion, as a change detection in a time series of facts' features computed from a large number of documents. Secondly, to the best of our knowledge, there is no other work that has used joint inference for temporal scoping. We show that joint inference is effective compared to doing temporal scoping of individual facts independently. We conduct our experiments on large scale open-domain publicly available time-stamped datasets, such as English Gigaword Corpus and Google Books Ngrams, demonstrating CoTS's effectiveness.
machine_learning	In this paper, we focus on information diffusion through social networks. Based on the well-known Independent Cascade model, we embed users of the social network in a latent space to extract more robust diffusion probabilities than those defined by classical graphical learning approaches. Better generalization abilities provided by the use of such a projection space allows our approach to present good performances on various real-world datasets, for both diffusion prediction and influence relationships inference tasks. Additionally, the use of a projection space enables our model to deal with larger social networks.
machine_learning	The objective of this research is to use satellite images for the classification and identification of settlements. Satellite images are used in this research. A wide area is covered in a single satellite image and it contains enormous information therefore satellite images can be used for many useful purposes, like classification of objects and land cover classes, change detection and population estimation etc. Many small settlements are usually scattered in remote areas. These settlements are very important in many aspects for a country such as; the data can be used developmental planning and the accurate censuses statistics of a country; moreover this data can be used for damage detection in case of any disaster. Furthermore it can help estimate etc. This is very laborious and expensive task to visit each and every place but this work can be carried out very easily and affordably using satellite imagery. To obtain the desired information from satellite images many different classification algorithms are used. The algorithms are broadly categorized into supervised and unsupervised classification. The selection and result of these algorithms depend upon the nature of the problem to be analyzed. In this work Principal Component Analysis (PCA) is used to extract the feature or signature in the shape of Eigen vectors. These Eigen vectors are used for classification on the basis Euclidean distance. Comparing the actual results with the reference map checks the accuracy of the system.
machine_learning	In this paper we present a data mining system, which allows the application of different clustering and cluster validity algorithms for DNA microarray data. This tool may improve the quality of the data analysis results, and may support the prediction of the number of relevant clusters in the microarray datasets. This systematic evaluation approach may significantly aid genome expression analyses for knowledge discovery applications. The developed software system may be effectively used for clustering and validating not only DNA microarray expression analysis applications but also other biomedical and physical data with no limitations. The program is freely available for non-profit use on request at http://www.cs.tcd.ie/Nadia.Bolshakova/Machaon.html
machine_learning	Molecular docking (MD) simulation is one of the four steps of the rational drug design. By the use of a fully-flexible receptor (FFR) model, protein flexibility can be explicitly considered during the drug design process. FFR models are composed of hundreds to several thousands of conformations to simulate the receptor flexibility in cell environments. So, for each conformation in the FFR model a MD simulation is executed and analyzed against a small molecule. However, it presents an important challenge due to small molecules databases, e.g. ZINC, have more than 21 million compounds available. It is computationally very demanding task to perform virtual screening of millions of ligands using an FFR model in a sequential mode. This paper introduces a data-flow pattern to perform massively-parallel MD simulations using FFR models, named Self-adaptive Multiple Instances (P-SaMI). Based on a previously-clustered FFR model, P-SaMI permits to selectively perform experiments by the use of the Free Energy of Binding (FEB) output, used as quality criterion: smaller FEBs mean better results. The main goal achieved on using P-SaMI was the significant reduction of the number of docking experiments comparing with exhaustive ones, for a specific small molecule.
machine_learning	Many linear statistical models have been lately proposed in text classification related literature and evaluated against the Unsolicited Bulk Email filtering problem. Despite their popularity - due both to their simplicity and relative ease of interpretation - the non-linearity assumption of data samples is inappropriate in practice, due to its inability to capture the apparent non-linear relationships, which characterize these samples. In this paper, we propose the SF-HME, a Hierarchical Mixture-of-Experts system, attempting to overcome limitations common to other machine-learning based approaches when applied to spam mail classification. By reducing the dimensionality of data through the usage of the effective Simba algorithm for feature selection, we evaluated our SF-HME system with a publicly available corpus of emails, with very high similarity between legitimate and bulk email - and thus low discriminative potential - where the traditional rule based filtering approaches achieve considerable lower degrees of precision. As a result, we confirm the domination of our SF-HME method against other machine learning approaches, which appeared to present lesser degree of recall.
machine_learning	The labor-intensive and time-consuming process of annotating data is a serious bottleneck in many pattern recognition applications when handling massive datasets. Active learning strategies have been sought to reduce the cost on human annotation, by means of automatically selecting the most informative unlabeled samples for annotation. The critical issue lies on the selection of such samples. As an effective solution, we propose an active learning approach that preprocesses the dataset, efficiently reduces and organizes a learning set of samples and selects the most representative ones for human annotation. Experiments performed on real datasets show that the proposed approach requires only a few iterations to achieve high accuracy, keeping user involvement to a minimum.
machine_learning	Position-based routing, as it is used by protocols like Greedy Perimeter Stateless Routing (GPSR) [5], is very well suited for highly dynamic environments such as inter-vehicle communication on highways. However, it has been discussed that radio obstacles [4], as they are found in urban areas, have a significant negative impact on the performance of position-based routing. In prior work [6] we presented a position-based approach which alleviates this problem and is able to find robust routes within city environments. It is related to the idea of position-based source routing as proposed in [1] for terminode routing. The algorithm needs global knowledge of the city topology as it is provided by a static street map. Given this information the sender determines the junctions that have to be traversed by the packet using the Dijkstra shortest path algorithm. Forwarding between junctions is then done in a position-based fashion. In this short paper we show how position-based routing can be aplied to a city scenario without assuming that nodes have access to a static street map and without using source routing.
machine_learning	Grover's search algorithm is designed to be executed on a quantum-mechanical computer. In this article, the probabilistic wp-calculus is used to model and reason about Grover's algorithm. It is demonstrated that the calculus provides a rigorous programming notation for modeling this and other quantum algorithms and that it also provides a systematic framework of analyzing such algorithms.
machine_learning	Here we describe a parameter-driven solution for generating novel yet similar movements from a sparse example set obtained through observation. In our experiments, a humanoid learns to represent movement trajectories demonstrated by a person with intuitive parameters describing the start and end points of different motion trajectory segments. These segments are automatically produced based on changes in curvature. After rebinning to equate similar segments across the samples, we use a linear approximation framework to build a representation based on relevant task features (segment start and end points) where radial basis functions(RBFs) are used to approximate the unknown non-linear characteristics describing a trajectory. The solution is accomplished on-line and requires no interaction. With this approach a humanoid can learn from only a few examples, and quickly produce new movements.
machine_learning	So far, many studies on the load forecasting have been made to improve the prediction accuracy using various methods such as regression, artificial neural network (ANN) and neural network-fuzzy methods. In order to reduce the load forecasting error, the concept of fuzzy regression analysis is employed to load forecasting problem. The difference between regressed forecasting results and actual results is analyzed using fuzzy concept. Error correction was then applied based on fuzzy sense of error involved in prediction. Forecasted results need expert inference for which fuzzy proves to be beneficial. Results analyzed clearly support the viewpoint. The fuzzy linear regression model is made from the load data of the previous years and the coefficients of the model are found by solving the mixed linear programming problem. The fuzzy correction in predicted results improved the error from 1 to 3 %.
machine_learning	The Internetware system is a complex and distributed self-adaptive system, which executes in an open, uncertain and dynamic environment, and adapts itself to changes in the environment. We hope that Internetware systems have the ability to automatically evolve in respond to changes. An important problem related to the development of Internetware systems is how to formulate proper adaptation rules. Because of the uncertainty of environment, the adaptation rules may not be suitable to the current system. Adaptation rules always need to be evolved to obtain better results. Some traditional methods can decide adaptation actions in different environmental conditions and evolve adaptation rules. But most of these methods bring about huge computation cost, which are not highly-efficient. To resolve these problems, we propose a method for evolving adaptation rules automatically, based on genetic algorithm and linear regression. We apply this method to evolve adaptation rules for a web application based on a widely used prototype --- RUBiS, which is an auction site similar to eBay. Experiments show that our method can evolve adaptation rules and improve the web application's performance in dynamic environment.
machine_learning	Distributed on a unit circle are k exits. Two autonomous mobile robots are placed on the circle. Each robot has a maximum speed of 1 and the robots can communicate wirelessly. The robots have a map of the domain, including exits, but do not have knowledge of their own initial locations on the domain, rather they only know their relative distance. The goal of the evacuation problem is to give an algorithm for the robots which minimizes the time required for both robots to reach an exit, in the worst case. We consider two variations of the problem depending on whether the two robots have control over their initial distance. When the initial distance of the robots is part of the input (i.e. no control), we show that simple algorithms exist which achieve optimal worst case evacuation times for the cases where: the robots begin colocated with an arbitrary distribution of the exits; and when the exits are either colocated or evenly spaced, with arbitrary starting positions of the robots. We also give upper and lower bounds on the problem with arbitrary exit distribution and starting positions of the robots. For the problem where robots can choose their initial distance (with knowledge of, but not control over the distribution of exits), we propose a natural family of algorithms parameterized by the maximum distance between any two exits.
machine_learning	The mixing time of a graph is an important metric, which is not only useful in analyzing connectivity and expansion properties of the network, but also serves as a key parameter in designing efficient algorithms. We present an efficient distributed algorithm for computing the mixing time of undirected graphs. Our algorithm estimates the mixing time ?s (with respect to a source node s) of any n-node undirected graph in O(?s log n) rounds. Our algorithm is based on random walks and require very little memory and use lightweight local computations, and work in the CONGEST model. Hence our algorithm is scalable under bandwidth constraints and can be an helpful building block in the design of topologically aware networks.
machine_learning	InkWell is a writer's assistant---a natural language revision program designed to assist creative writers by producing stylistic variations on texts based on craft-based facets of creative writing and by mimicking aspects of specified writers and their personality traits. It is built on top of an optimization process that produces variations on a supplied text, evaluates those variations quantitatively, and selects variations that best satisfy the goals of writing craft and writer mimicry. We describe the design and capabilities of InkWell, and present an early evaluation of its effectiveness and uses with two established literary writers along with an experiment using InkWell to write haiku on its own.
machine_learning	Considerable amount of research has been carried out in the domain of man-machine interaction. Interaction with machines using hand gestures, eye motion, etc. has already been proposed by researchers all over the world. However, interacting with devices using speech is of particular interest to researchers since speech is the most natural way of interaction and communication for human beings. In this paper, we have tried to develop a client-server based architecture for controlling several robots simultaneously through voice commands. The robots used in the experiment are the LEGO� Mindstorm� NXT robots. The entire architecture is developed using the client-server model of communication which enables each and every component of the architecture to be present on a physically different machine and at a physically different location allowing the user to control multiple robots using speech on the go. The speech recognition server accepts speech inputs from the client, translates them into robot control commands and sends them to the specific server controlling the robot. Thus the user need not be physically present at the same location as the robot and is able to control robots remotely.
machine_learning	DETIboot is an over-the-air, file distribution system that allows a teacher to gain control over students' personal laptops during exams, by distributing a properly hardened Linux operating system image. In this scenario, a student could boot the hardened image over a Virtual Machine, being able to explore the hosting system while doing the exam on the hosted one. In this paper we present Virtual Machine Halt (VMHalt), a solution conceived for the detection of unknown, uncooperative x86_64 Virtual Machines and system emulators. For that purpose, VMHalt uses several strategies to classify an underlying layer as virtual. Our results shows that: although individual strategies have their own weaknesses, leading to a wrong decision, by excluding the ones that could classify a physical system incorrectly as a virtual one, a weighted decision from all strategies correctly detects an underlying virtualization engine with increased probability, while true hardware is always recognised as such.
machine_learning	The file system durability is provided by flushing dirty pages periodically into the non-volatile storage. Since the traditional storage devices such as hard disk and flash memory can be written in the unit of block, the file system writes a whole block even when only a small number of bytes are modified. To resolve such a wasting write traffic problem, we propose a two-level logging scheme by exploiting non-volatile and byte-addressable memories (NVMs). Whereas the previous approach which exploits the NVM device is targeted for EXT4 file system, our scheme uses log-structured file systems in order to guarantee the file system reliability even for sudden system crashes. While the NVM is used for fine-grained logging, the flash memory is used for coarse-grained logging. Experiments with a real NVM device show that the proposed scheme reduces the write traffic on storage by up to 78% and improves the I/O performance significantly.
machine_learning	Many contemporary operating systems utilize a system call interface between the operating system and its clients. Increasing numbers of systems are providing low-level mechanisms for intercepting and handling system calls in user code. Nonetheless, they typically provide no higher-level tools or abstractions for effectively utilizing these mechanisms. Using them has typically required reimplementation of a substantial portion of the system interface from scratch, making the use of such facilities unwieldy at best.This paper presents a toolkit that substantially increases the ease of interposing user code between clients and instances of the system interface by allowing such code to be written in terms of the high-level objects provided by this interface, rather than in terms of the intercepted system calls themselves. This toolkit helps enable new interposition agents to be written, many of which would not otherwise have been attempted.This toolkit has also been used to construct several agents including: system call tracing tools, file reference tracing tools, and customizable filesystem views. Examples of other agents that could be built include: protected environments for running untrusted binaries, logical devices implemented entirely in user space, transparent data compression and/or encryption agents, transactional software environments, and emulators for other operating system environments.
machine_learning	Storage systems rely on maintenance tasks, such as backup and layout optimization, to ensure data availability and good performance. These tasks access large amounts of data and can significantly impact foreground applications. We argue that storage maintenance can be performed more efficiently by prioritizing processing of data that is currently cached in memory. Data can be cached either due to other maintenance tasks requesting it previously, or due to overlapping foreground I/O activity. We present Duet, a framework that provides notifications about page-level events to maintenance tasks, such as a page being added or modified in memory. Tasks use these events as hints to opportunistically process cached data. We show that tasks using Duet can complete maintenance work more efficiently because they perform fewer I/O operations. The I/O reduction depends on the amount of data overlap with other maintenance tasks and foreground applications. Consequently, Duet's efficiency increases with additional tasks because opportunities for synergy appear more often.
machine_learning	Multithreaded programming is notoriously difficult to get right. A key problem is non-determinism, which complicates debugging, testing, and reproducing errors. One way to simplify multithreaded programming is to enforce deterministic execution, but current deterministic systems for C/C++ are incomplete or impractical. These systems require program modification, do not ensure determinism in the presence of data races, do not work with general-purpose multithreaded programs, or run up to 8.4? slower than pthreads. This paper presents Dthreads, an efficient deterministic multithreading system for unmodified C/C++ applications that replaces the pthreads library. Dthreads enforces determinism in the face of data races and deadlocks. Dthreads works by exploding multithreaded applications into multiple processes, with private, copy-on-write mappings to shared memory. It uses standard virtual memory protection to track writes, and deterministically orders updates by each thread. By separating updates from different threads, Dthreads has the additional benefit of eliminating false sharing. Experimental results show that Dthreads substantially outperforms a state-of-the-art deterministic runtime system, and for a majority of the benchmarks evaluated here, matches and occasionally exceeds the performance of pthreads.
machine_learning	Modern Web services rely extensively upon a tier of in-memory caches to reduce request latencies and alleviate load on backend servers. Within a given cache, items are typically partitioned across cache servers via consistent hashing, with the goal of balancing the number of items maintained by each cache server. Effects of consistent hashing vary by associated hashing function and partitioning ratio. Most real-world workloads are also skewed, with some items significantly more popular than others. Inefficiency in addressing both issues can create an imbalance in cache-server loads. We analyze the degree of observed load imbalance, focusing on read-only traffic against Facebook's graph cache tier in TAO. We investigate the principal causes of load imbalance, including data co-location, non-ideal hashing scenarios, and hot-spot temporal effects. We also employ trace-drive analytics to study the benefits and limitations of current load-balancing methods, suggesting areas for future research.
machine_learning	Software Systems running in the Internet are facing an ever-changing environment, which requires constant monitoring and adaptation facilities to be in place, so that user's needs and system requirements are satisfied on-demand. This paper summarizes our general observation and understanding to how user and system goals can drive the adaptation mechanisms of individual software service modules, implemented as internetware components. In particular, a theoretical typology of different levels of adaptation capability is proposed. Their correspondence with the key characteristics of internetware is analyzed. A common architecture to support the run-time adaptation of internetware is also suggested in company with a development method.
machine_learning	Stroke commonly leads to partial or complete paralysis of one side of the body and there is limited availability of therapists to provide rehabilitation. It is a priority therefore to identify the most effective rehabilitation strategies and/or pharmacotherapies. Motor learning, the essential process underpinning rehabilitation, can be assessed more quickly and robustly than outcomes from rehabilitation. In this paper we describe a proof of concept system that utilises a bespoke video game to measure the critical components of motor learning. We demonstrate that it is sensitive enough to detect how simple changes in therapist instruction significantly change motor performance and learning. Although video games have been shown to aid in rehabilitation, this is the first time video games have been used to derive early response markers, based on the measurement of performance and motor learning, for use in the evaluation of the efficacy of a rehabilitation strategy.
machine_learning	Mutual-exclusion locking is the prevailing technique for protecting shared resources in concurrent programs. Fine-grained locking maximizes the opportunities for concurrent execution while preserving correctness, but increases both the number of locks and the frequency of lock operations. Adding to the frequency of these operations is the practice of using locks defensively --- such as in library code designed for use in both concurrent and single-threaded scenarios. If the library does not protect itself with locks, an engineering burden is placed on the library's users; if the library does use locks, it punishes those who use it only from a single thread. Biased locking is a dynamic protocol for eliminating this trade-off, in which the underlying run-time system optimizes lock operations by biasing a lock to a specific thread when the lock is dynamically found to be thread-local. Biased locking protocols are distinguished by how many opportunities for optimization are found, and what performance trade-offs for non-local locks are experienced. Of particular concern is the relatively high cost involved in revoking the bias of a lock, which makes existing biased locking protocols susceptible to performance pathologies for programs with specific patterns of contention. This work presents the biased locking protocol used in Jikes RVM, a high-throughput Java virtual machine. The protocol, dubbed Fable, builds on prior work by adding per-object-instance dynamic adaptation and inexpensive bias revocation. We describe the protocol, detail how it was implemented, and use it in offering the most thorough evaluation of Java locking protocols to date. Fable is shown to provide speed-ups over traditional Java locking across a broad spectrum of benchmarks while being robust to cases previous protocols handled poorly.
machine_learning	Myopia becomes a more and more serious worldwide problem as the number of myopic people (especially young people) grows rapidly. Efficient methods are required to monitoring the deterioration of nearsightness so as to take further treatment. This demo realizes a noval nearsightness monitoring system, called iSee, which utilizes the widely used smartphones to detect the deterioration of nearsightness by monitoring and analysing the the distance between the eyes and the smartphone screen. A prototype of iSee has been developed to evaluated the effectiveness under different environmental conditions.
machine_learning	Greendicator is an indicator system that enables embedded systems to output text to camera-equipped smartphones by blinking an LED. The transmitter emits modulated light pulses using an existing visible-light LED or an IR diode, laser, or light reflector. The receiver uses a camera-equipped smartphone to sense the light pulses and GPU to decode the original message. We demonstrate its use in supporting existing RF-based networks and an aid for pairing and configuration of wireless systems while occupying only a small memory footprint.
machine_learning	We propose a priced options model for solving the exposure problem of bidders with valuation synergies participating in a sequence of online auctions. We consider a setting in which complementary-valued items are offered sequentially by different sellers, who have the choice of either selling their item directly or through a priced option. In our model, the seller fixes the exercise price for this option, and then sells it through a first-price auction. We analyze this model from a decision-theoretic perspective and we show, for a setting where the competition is formed by local bidders (which desire a single item), that using options can increase the expected profit for both sides. Furthermore, we derive the equations that provide minimum and maximum bounds between which the bids of the synergy buyer are expected to fall, in order for both sides of the market to have an incentive to use the options mechanism. Next, we perform an experimental analysis of a market in which multiple synergy buyers are active simultaneously. We show that, despite the extra competition, some synergy buyers may benefit, because sellers are forced to set their exercise prices for options at levels which encourage participation of all buyers.
machine_learning	Energy harvesting sensor nodes based on real nonvolatile processors are demonstrated to show the desirable characteristics of those systems, such as no battery, zero stand-by power, microsecond-scale sleep and wake-up time, high resilience to random power failures and fine-grained power management. Furthermore, we show its applications to a distributed moving object detection system, one of novel nonvolatile computing systems.
machine_learning	Although mobile phones are ideal platforms for continuous human centric sensing, the state of the art phone architectures today have not been designed to support continuous sensing applications. Currently, sampling and processing sensor data on the phone requires the main processor and associated components to be continuously on, creating a large energy overhead that can severely impact the battery lifetime of the phone. We will demonstrate Little Rock, a novel sensing architecture for mobile phones, where sampling and, when possible, processing of sensor data is offloaded to a dedicated low-power processor. This approach enables the phone to perform continuous sensing three orders of magnitude more energy efficiently compared to the normal approaches.
machine_learning	Efficiency and flexibility are among the key requirements for Wireless Sensor and Actuator Networks (WSAN) of Internet of Things (IoT) era. In this work we present and demonstrate a novel WSAN and IoT platform. The new nodes are constructed by stacking together the different hardware modules encapsulating power sources, processing units, wired and wireless transceivers, sensors and actuators, or sets of those. Once a node is built, its processing unit can automatically identify all the connected hardware modules, obtain required software modules and tune node's operation accounting for its structure, available resources and active applications.
machine_learning	Current approaches to learning partial ordering constraints by demonstration require demonstrating all (or almost all) possible completion orders. We have developed an algorithm that, for plans involving relative placement of objects, learns the partial ordering constraints from a single demonstration by letting the user specify naturally conceived reference frame information. This work is an example of a broader research agenda that involves applying principles of human collaboration to robot learning from demonstration.
machine_learning	In a distributed computers network based on a packet-swltched communication subnet, it is possible to describe the system as a hierarchical structure. We will consider here only three levels: -level 0: communication between nodes of the subnets. -level 1: communication between hosts connected to the net. -level 2: communication between users processes (subscribers) in different hosts. At each level, the communication is based on a protocol and the data structures to be exchanged are different. For instance the data structure exchanged at level 2 may be a sequential file. As there is no direct support of communication at this level, it is through the mechanisms of level 1 and 0 that the transfer will take place with data structure at each level not directly related to the upper level one. The complete definition of the system will therefore required not only the level 0, 1 and 2 protocols, but also inter-level protocols. In the following, we will concentrate on the level 1 protocol. It is the basic communication protocol of a network since it will be used by user processes in different hosts and it will use the subnet as a communication support. This level 1 entity is called a �TS� (transport station) in CYCLADES [13] and a �TCP� (transmission control program] in [2].
machine_learning	This study offers a first step toward understanding the extent to which we may be able to predict cyber security incidents (which can be of one of many types) by applying machine learning techniques and using externally observed malicious activities associated with network entities, including spamming, phishing, and scanning, each of which may or may not have direct bearing on a specific attack mechanism or incident type. Our hypothesis is that when viewed collectively, malicious activities originating from a network are indicative of the general cleanness of a network and how well it is run, and that furthermore, collectively they exhibit fairly stable and thus predictive behavior over time. To test this hypothesis, we utilize two datasets in this study: (1) a collection of commonly used IP address-based/host reputation blacklists (RBLs) collected over more than a year, and (2) a set of security incident reports collected over roughly the same period. Specifically, we first aggregate the RBL data at a prefix level and then introduce a set of features that capture the dynamics of this aggregated temporal process. A comparison between the distribution of these feature values taken from the incident dataset and from the general population of prefixes shows distinct differences, suggesting their value in distinguishing between the two while also highlighting the importance of capturing dynamic behavior (second order statistics) in the malicious activities. These features are then used to train a support vector machine (SVM) for prediction. Our preliminary results show that we can achieve reasonably good prediction performance over a forecasting window of a few months.
machine_learning	Android's Inter-Component Communication (ICC) mechanism strongly relies on Intent messages. Unfortunately, due to the lack of message origin verification in Intents, application security completely relies on the programmer's skill and attention. In this paper, we advance the state of the art by developing a method to automatically detect potential vulnerabilities and, most importantly, demonstrate whether they can be exploited or not. To this end, we adopt a formal approach to automatically produce malicious payloads that can trigger dangerous behavior in vulnerable applications. We test our methods on a representative sample of applications, and we find that 29 out of 64 tested applications are potentially vulnerable, while 26 of them are automatically proven to be exploitable.
machine_learning	The basis for all IPv4 network communication is the Address Resolution Protocol (ARP), which maps an IP address to a device's Media Access Control (MAC) identifier. ARP has long been recognized as vulnerable to spoofing and other attacks, and past proposals to secure the protocol have often involved modifying the basic protocol. This paper introduces arpsec, a secure ARP/RARP protocol suite which a) does not require protocol modification, b) enables continual verification of the identity of the tar- get (respondent) machine by introducing an address binding repository derived using a formal logic that bases additions to a host's ARP cache on a set of operational rules and properties, c) utilizes the TPM, a commodity component now present in the vast majority of modern computers, to augment the logic-prover-derived assurance when needed, with TPM-facilitated attestations of system state achieved at viably low processing cost. Using commodity TPMs as our attestation base, we show that arpsec incurs an overhead ranging from 7% to 15.4% over the standard Linux ARP implementation and provides a first step towards a formally secure and trustworthy networking stack.
machine_learning	Recommender systems have emerged as an effective decision tool to help users more easily and quickly find products that they prefer, especially in e-commerce environments. However, few studies have tried to understand how this technology has influenced the way users search for products and make purchase decisions. Our current research aims at examining the impact of recommenders by understanding how recommendation tools integrate the classical economic schemes and how they modify product search patterns. We report our work in employing an eye tracking system and collecting users' interaction behaviors as they browsed and selected products to buy from an online product retail website offering over 3,500 items. This in-depth user study has enabled us to collect over 48,000 fixation data points and 7,720 areas of interest from eighteen users, each spending more than one hour on our site. Our study shows that while users still use traditional product search tools to examine alternatives, recommenders definitely provide users with new opportunities in their decision process. More specifically, users actively click and gaze at products recommended to them, up to 40% of the time. In addition, recommendation areas are highly attractive, drawing users to add 50% more items to their baskets as a traditional tool does. Observing that users consult the recommendation area more as they are close to the end of their search process, it seems that recommenders enhance users' decision confidence by satisfying their need for diversity. Based on these results, we derive several interaction design guidelines that can significantly improve users' satisfaction and perception of product recommenders.
machine_learning	Bright field cellular microscopy is a simple and non-invasive method for capturing cytological images. However, the resulting micrographs prove challenging for image segmentation, especially with samples that have tightly clustered or overlapping cells. Filamentous cyanobacteria grow as linearly arranged cells forming chain-like filaments that often touch and overlap. Existing bright field cell segmentation methods perform poorly with these bacteria, and are incapable of identifying the filaments. Existing filament tracking methods are rudimentary, and cannot reliably account for overlapping or parallel touching filaments. We propose a new approach for identifying filaments in bright field micrographs by combining information about both filaments and cells. This information is used by an evolutionary strategy to iteratively construct a continuous spline representation that tracks the medial line of the filaments. We demonstrate that overlapping and parallel touching filaments are segmented correctly in many difficult cases.
machine_learning	Binary descriptors have recently become very popular in visual recognition tasks. This popularity is largely due to their low complexity and for presenting similar performances when compared to non binary descriptors, like SIFT. In literature, many researchers have applied binary descriptors in conjunction with mid-level representations (e.g., Bag-of-Words). However, despite these works have demonstrated promising results, their main problems are due to use of a simple mid-level representation and the use of binary descriptors in which rotation and scale invariance are missing. In order to address those problems, we propose to evaluate state-of-the-art binary descriptors, namely BRIEF, ORB, BRISK and FREAK, in a recent mid-level representation, namely BossaNova, which enriches the Bag-of-Words model, while preserving the binary descriptor information. Our experiments carried out in the challenging PASCAL VOC 2007 dataset revealed outstanding performances. Also, our approach shows good results in the challenging real-world application of pornography detection.
machine_learning	This paper presents an evaluation of different methods considering the usually problems in face recognition. We consider variations in illumination, facial expression and facial details to propose a new method combining global and local face image features. This approach combines PCA, 2D-DCT and Gabor Wavelet Transform to obtain the global and local features representation. The Nearest Neighbor using the Euclidean distance performs the classification. The experiments were performed in the classical ORL and Yale face recognition databases. The proposed approach presented interesting results in comparison with the literature methods.
machine_learning	The affordability of digital cameras, storages, processors and the advances in these areas are encouraging people to take hundreds of photos at once. However, managing the large number of photographs involves arduous tasks such as selecting good quality photos and classifying and labeling each photo. Generally, users put their photos into certain user-designated folders on their local PC without considering any classified information. One of the main problems related to this management method is that users do not create their photo folders systematically because they are carelessness and apathetic. This practice results in confusion when users want to find their photos. One method to overcome this problem is to construct a central photo management system that can manage many photos on the user's local PC. It also can provide smart functions such as automated clustering and summarized visualization for many photos. This paper describes an integrated photo management system coupled with a database on the web, which provides users with an automated photo clustering and visualization function that allows photo overlaps. Our system also provides users with an automated photo quality evaluation based on Depth of Field (DOF) and blur. In order to evaluate our system, we conducted a user study on user-friendliness based on a questionnaire.
machine_learning	We introduce a novel image mosaicing algorithm to generate 360? landscape images while also taking into account the presence of people at the boundaries between stitched images. Current image mosaicing techniques tend to fail when there is extreme parallax caused by nearby objects or moving objects at the boundary between images. This parallax causes ghosting or unnatural discontinuities in the image. To address this problem, we present an image mosaicing algorithm that is robust to parallax and misalignment, and is also able to preserve the important human-centric content, specifically faces. In particular, we find an optimal path between the boundary of two images that preserves color continuity and peoples' faces in the scene. Preliminary results show promising results of preserving close-up faces with parallax while also being able to generate a perceptually plausible 360? panoramic image.
machine_learning	This paper appears in the March, 1972, issue of the Communications of the ACM. Its abstract is reproduced below. Five well-known scheduling policies for movable head disks are compared using the performance criteria of expected seek time (system oriented) and expected waiting time (individual I/O request oriented). Both analytical and simulation results are obtained. The variance of waiting time is introduced as another meaningful measure of performance, showing possible discrimination against individual requests. Then the choice of a utility function to measure total performance including system oriented and individual request oriented measures is described. Such a function allows one to differentiate among the scheduling policies over a wide range of input loading conditions. The selection and implementation of a maximum performance two-policy algorithm are discussed.
machine_learning	" Procedural modeling systems allow users to create high quality content through parametric, conditional or stochastic rule sets. While such approaches create an abstraction layer by freeing the user from direct geometry editing, the nonlinear nature and the high number of parameters associated with such design spaces result in arduous modeling experiences for non-expert users. We propose a method to enable intuitive exploration of such high dimensional procedural modeling spaces within a lower dimensional space learned through autoencoder network training. Our method automatically generates a representative training dataset from the procedural modeling rule set based on shape similarity features. We then leverage the samples in this dataset to train an autoencoder neural network, while also structuring the learned lower dimensional space for continuous exploration with respect to shape features. We demonstrate the efficacy our method with user studies where designers create content with more than 10-fold faster speeds using our system compared to the classic procedural modeling interface.
machine_learning	Plants play an important role in both human life and other lives that exist on the earth. Due to environmental deterioration and lack of awareness, many rare plant species are at the margins of extinction. Despite the great advances made in botany, there are many plants yet to be discovered, classified, and utilized; unknown plants are treasures waiting to be found. Leaf classification and recognition for plant identification plays a vital role in all these endeavors. Detection of edges is considered to be one of the vital steps of preprocessing during leaf identification and recognition system. The current need is to have an edge detector which is both fast and efficient in identifying edges of a leaf image. This paper introduced a wavelet base edge detection technique, which used a dyadic wavelet transformation and p-tile thresholding method to find the edges. The proposed method was compared with conventional edge detectors like canny, sobel, prewitt, Roberts and Log. From the experimental results it was found that the proposed method is advantageous and can detect edges.
machine_learning	Reference based analysis (RBA) is a novel data mining tool for exploring a test data set with respect to a reference data set. The power of RBA lies in it ability to transform any complex data type, such as symbolic sequences and multi-variate categorical data instances, into a multivariate continuous representation. The transformed representation not only allows visualization of the complex data, which cannot be otherwise visualized in its original form, but also allows enhanced anomaly detection in the transformed feature space. We demonstrate the application of the RBA framework in analyzing system call traces and show how the transformation results in improved intrusion detection performance over state of art data mining based intrusion detection methods developed for system call traces.
machine_learning	Extraction of interesting colocations in geo-referenced data is one of the major tasks in spatial pattern mining. The goal is to find sets of spatial object-types with instances located in the same neighborhood. In this context, the main drawback is the visualization and interpretation of extracted patterns by domain experts. Indeed, common textual representation of colocations loses important spatial information such as the position, the orientation or the spatial distribution of the patterns. To overcome this problem, we propose a new clustering-based visualization technique deeply integrated in the colocation mining algorithm. This new simple, concise and intuitive cartographic visualization considers both spatial information and expert practices. This proposition has been integrated in a Geographic Information System and experimented on a real-world geological data set. Domain experts confirm the added-value of this visualization approach.
machine_learning	We propose DOBRO, a light online learning module, which is equipped with a smart correction policy helping making decision to correct or not the given prediction depending on how likely the correction will lead to a better prediction performance. DOBRO is a standalone module requiring nothing more than a time series of prediction errors and it is flexible to be integrated into any black-box model to improve its performance under drifts. We performed evaluation in a real-world application with bus arrival time prediction problem. The obtained results show that DOBRO improved prediction performance significantly meanwhile it did not hurt the accuracy when drift does not happen.
machine_learning	Discovering pattern sets or global patterns is an attractive issue from the pattern mining community in order to provide useful information. By combining local patterns satisfying a joint meaning, this approach produces patterns of higher level and thus more useful for the end-user than the usual local patterns. In parallel, recent works investigating relationships between data mining and constraint programming (CP) show that the CP paradigm is a powerful framework to model and mine patterns in a declarative and generic way. We present a constraint-based language which enables us to define queries in a declarative way addressing patterns sets and global patterns. By specifying what the task is, rather than providing how the solution should be computed, it is easy to process by stepwise refinements to successfully discover global patterns. The usefulness of the approach is highlighted by several examples coming from the clustering based on associations. All primitive constraints of the language are modeled and solved using the SAT framework. We illustrate the efficiency of our approach through several experiments.
machine_learning	Due to their popularity and widespread use, blogs have become an important medium through which to communicate and exchange information on the World Wide Web. The advent of the blogosphere may provide opportunities for establishing a new business model that investigates social relationships. In Korea, there are many blogospheres that appear to maintain different characteristics from foreign blogospheres on the Internet. Consequently, it is inappropriate to apply analysis methods used for the foreign blogosphere directly to Korean blogospheres. To establish successful business policies in Korean blogospheres, the characteristics of Korean blogospheres and the behavioral patterns of bloggers should be understood. In this paper, we analyze the characteristics of the Korean blog network, wherein each blogger forms a node and scraps by bloggers as edges. First, we demonstrate that the Korean blog network is also a scale-free network, like the World Wide Web. Second, we compare the Bow-tie structure of the Korean blog network with that of the World Wide Web. We expect that these analysis results will be helpful in developing effective algorithms and in establishing new business models targeted at the Korean blogosphere.
machine_learning	To work well, machine-learning-based approaches to information extraction and ontology population often require a large number of manually selected and annotated examples. In this paper, we propose ListReader which provides a way to train the structure and parameters of a Hidden Markov Model (HMM) without requiring any labeled training data. The induced HMM is a wrapper---a function that hides within it the complexities of low-level processing---in ListReader's case the complexities of information extraction from OCRed historical documents. The HMM wrapper is capable of recognizing lists of records in text documents and associating subsets of identical fields across related record templates. The algorithmic training method we employ is based on a novel unsupervised active grammar-induction framework. The training produces an HMM wrapper and uses an efficient active sampling process to complete the mapping from wrapper to ontology by requesting annotations from a user for automatically-selected examples. We measure performance of the final HMM in terms of F-measure of extracted information and manual annotation cost and show that ListReader learns faster and better than a state-of-the-art baseline and an alternate version of ListReader that induces a regular-expression wrapper.
machine_learning	This paper presents an XML partitioning technique that allows main-memory query engines to process a class of XQuery queries, that we dub iterative queries, on arbitrarily large input documents. We provide a static analysis technique to recognize these queries. The static analysis is based on paths extracted from queries and does not need additional schema information. We then provide an algorithm using path information for partitioning the input documents of iterative queries. This algorithm admits a streaming implementation, whose effectiveness is experimentally validated.
machine_learning	Data integration systems based on Peer-to-Peer environments have been developed to integrate dynamic, autonomous and heterogeneous data sources on the Web. Some of these systems adopt semantic approaches for clustering their data sources, reducing the search space. However, the clusters may become overloaded and traditional strategies of load balance are not suitable to semantic clusters. In this paper, we discuss limitations of load balance strategies in semantic clusters. In addition, we propose a solution for this load balance and we present some experimental results.
machine_learning	This paper concerns the integration of the Case Based Reasoning (CBR) paradigm in query processing, providing a way to optimize queries when there is no prior knowledge on queried data sources and certainly no related metadata such as data statistics. Our Query Optimization by Learning (QOL) approach optimizes queries using cases generated from the evaluation of similar past queries. A query case comprises: (i) the query, (ii) the query plan and (iii) the measures (computational resources consumed) of the query plan. The work also concerns the way the CBR process interacts with the query plan generation process. This process uses classical heuristics and makes decisions randomly (e.g. when there is no statistics for join ordering and selection of algorithms, routing protocols); It also (re)uses cases (existing query plans) for similar queries parts, improving the query optimization and evaluation efficiency.
machine_learning	While there has been a growing body of work in child-robot interaction, we still have very little knowledge regarding young children's speaking and listening dynamics and how a robot companion should decode these behaviors and encode its own in a way children can understand. In developing a backchannel prediction model based on observed nonverbal behaviors of 4-6 year-old children, we investigate the effects of an attentive listening robot on a child's storytelling. We provide an extensive analysis of young children's nonverbal behavior with respect to how they encode and decode listener responses and speaker cues. Through a collected video corpus of peer-to-peer storytelling interactions, we identify attention-related listener behaviors as well as speaker cues that prompt opportunities for listener backchannels. Based on our findings, we developed a backchannel opportunity prediction (BOP) model that detects four main speaker cue events based on prosodic features in a child's speech. This rule-based model is capable of accurately predicting backchanneling opportunities in our corpora. We further evaluate this model in a human-subjects experiment where children told stories to an audience of two robots, each with a different backchanneling strategy. We find that our BOP model produces contingent backchannel responses that conveys an increased perception of an attentive listener, and children prefer telling stories to the BOP model robot.
machine_learning	Head motion occurs naturally and in synchrony with speech during human dialogue communication, and may carry paralinguistic information, such as intentions, attitudes and emotions. Therefore, natural-looking head motion by a robot is important for smooth human-robot interaction. Based on rules inferred from analyses of the relationship between head motion and dialogue acts, this paper proposes a model for generating head tilting and nodding, and evaluates the model using three types of humanoid robot (a very human-like android, "Geminoid F", a typical humanoid robot with less facial degrees of freedom, "Robovie R2", and a robot with a 3-axis rotatable neck and movable lips, "Telenoid R2"). Analysis of subjective scores shows that the proposed model including head tilting and nodding can generate head motion with increased naturalness compared to nodding only or directly mapping people's original motions without gaze information. We also find that an upwards motion of a robot's face can be used by robots which do not have a mouth in order to provide the appearance that utterance is taking place. Finally, we conduct an experiment in which participants act as visitors to an information desk attended by robots. As a consequence, we verify that our generation model performs equally to directly mapping people's original motions with gaze information in terms of perceived naturalness.
machine_learning	Learning from demonstration is an invaluable skill for a robot acting in a human populated natural environment, allowing the teaching of new skills without tedious and complex manual programming. Physical human-robot interaction, where the human is in a physical contact with the robot, is a promising approach for teaching especially manipulation skills. This paper studies the human side of physical human-robot interaction, in the context of a human physically guiding a robot through the desired set of motions. The paper addresses the question, which kind of response of the robot is preferable for the human user. In addition, different approaches for the guidance are described and relevant technical challenges are discussed. The main finding of the user study is that there is a need for a trade-off between the conflicting goals of naturalness of motion and positioning accuracy.
machine_learning	Simultaneous Localization and Mapping (SLAM) is the process of learning about both the environment and about a robot's location with respect to the environment and is essential for robots to autonomously navigate. A variety of algorithms using many different sensors such as RGB-D cameras, laser range finders, ultrasonic sensors and others have been proposed to perform SLAM. However, these algorithms face common challenges are that of computational complexity, wrong loop closure detection and failure to localize correctly when robot loses state (kidnapped robot problem). In this work, we utilize Wi-Fi signal strength sensing to aid the SLAM process in indoor environments and address the challenges mentioned above.
machine_learning	Cloud computing is a name given to a set of systems for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. Cloud computing is aimed at making an organization more agile and cost effective. Due to the rapid evolution of Cloud Computing in the recent past, it is relevant to investigate the key areas of research of this technology. In this paper, we present a systematic review of research intensive areas in the field of cloud computing. Research papers in the period from 2009 to 2012 were gathered. A total of 36 research papers were reviewed systematically and categorized into four broad categories based on the issues addressed by them. We identified that the majority of the research papers focused on Cloud Security. By systematically analyzing the work accomplished so far, the gaps and yet to be explored areas in this field are brought to light.
machine_learning	This paper introduces a performance evaluation method for algorithms that generates a depth map using an image from a stereo endoscopic camera for image processing of laparoscope operations. The depth image was created by using a space-time stereo method by illuminating various patterns on scenes consisting of models of the 3D-printed organ model and actual organs from a pig, and the ground truth image was generated for each sub-pixel unit to achieve high accuracy and high precision. Different algorithms were evaluated using the ground truth image data. The number of effective depth pixels compared to the ground truth and the distance error was measured from algorithms based on an edge-preserving filter as real-time algorithms and quasi-dense algorithms. This paper presents an analysis of each algorithm from its evaluation indices to determine which algorithm is appropriate to compute the depth map from laparoscopic images.
machine_learning	Web servers provide immunity against Man In The Middle (MITM) attacks and eavesdropping by using HTTP Strict Transport Security (HSTS) to force user agents to communicate only over HTTPS connections. However, the initial connection request from a user is made over an insecure HTTP connection. This issue was addressed by user agents; Google Chrome and Firefox, implicitly, by including a static list of URLs to be accessed only over secure HTTPS connections. Since, these user agents maintain their lists independently, the URLs used by one user agent are invisible to another. A user is prone to MITM attacks, especially in public hotspot environments, when accessing a URL present in the list of secure URLs of one browser but not in another, since the initial handshake from that user agent is insecure. Attacks can be initiated by modifying the outgoing HTTP packets and also the HTTPS response packets from the webserver. This motivated us to propose a solution independent of user agents, by merging the static URL lists of different user agents and enforcing HTTPS for all those URLs. In this paper, we propose a solution, SHSHTTPS Enforcer that introduces a local daemon to enforce URL redirection before the request flows out of the client for the URLs in a list compiled from multiple sources. The proposed solution has been demonstrated through a prototype implementation of the Squid Proxy server as our local daemon. The experiment was conducted by providing a URL, which was not present in one browser's list but was present in another browser's list. It was evident that SHS-HTTPS Enforcer enforced HTTPS successfully and MITM attacks were prevented.
machine_learning	This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a recommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention and medium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.
machine_learning	Bounding the price of stability of undirected network design games with fair cost allocation is a challenging open problem in the Algorithmic Game Theory research agenda. Even though the generalization of such games in directed networks is well understood in terms of the price of stability (it is exactly Hn, the n-th harmonic number, for games with n players), far less is known for network design games in undirected networks. The upper bound carries over to this case as well, while the best known lower bound is 2:245. For more restricted but interesting variants of such games, such as broadcast and multicast games, sublogarithmic upper bounds are known, while the best known lower bounds are 1:818 and 1:862, respectively. In this letter, we discuss a recent breakthrough in this field of research: an O(1) upper bound on the price of stability for undirected broadcast games.
machine_learning	The present article describes a robust approach for abbreviating terms. First, in order to incorporate non-local information into abbreviation generation tasks, we present both implicit and explicit solutions: the latent variable model and the label encoding with global information. Although the two approaches compete with one another, we find they are also highly complementary. We propose a combination of the two approaches, and we will show the proposed method outperforms all of the existing methods on abbreviation generation datasets. In order to reduce computational complexity of learning non-local information, we further present an online training method, which can arrive the objective optimum with accelerated training speed. We used a Chinese newswire dataset and a English biomedical dataset for experiments. Experiments revealed that the proposed abbreviation generator with non-local information achieved the best results for both the Chinese and English languages.
machine_learning	During the course to deploy Ipv6, we found several critical implementation and operational issues which distract user and possibly prevent wide deployment of Ipv6. Among the problems we noticed stateless IP autoconfiguration, which provides for each nodes connected to the network an IP address. However, two nodes may have the same address, and a network conflict appears. In this case a manual IP address, which depends on restarting Network Interface Card, is required. In this paper, we present a technique that, independently of rebooting NIC, maintain the IP address fix. Validation is done by using Fedoracore Operating System.
machine_learning	This paper reviews 802.16 2001's MAC layer QoS metric. It explains the importance of QoS and its parameter set; defines types of services supported by this standard; explores the main entity of the MAC layer used for transportation that is service flow and how the QoS metric is associated with it, as well as the relationship of service class and service flow. It further explains the authorization model and two phase activation. In the end it explains dynamic service flow and dynamic service messages in detail
machine_learning	In order to play an influential role in research and practice, UCD communities should implement new methods and tools into current practices and advance the boundaries of the field.
machine_learning	This review paper gives a brief insight about Mobile IP, its features and entities constituting Mobile IP environment. The paper explores the working and routing mechanism of Mobile IP with a deep insight about triangle routing in Mobile IP Version 4(MIPv4) and route optimization in Mobile IP Version 6(MIPv6). This paper discusses several security issues regarding Mobile IP implementation. This paper also highlights route optimization problems and focuses on the solution of those problems.
machine_learning	Wireless IP network has attracted significant interest due to their ability to support both voice and data transfer in mobile communication. One of the main issues concerning such network is the analysis and design of mobility function particularly the location management. In this paper, we focus on modelling location update function in wireless network standard GPRS (General Packet Radio Service) by using an agent approach [1].
machine_learning	Recently, vehicle detection methods have been popularly used in the field of intelligent vehicles. The performance and processing time of vehicle detection is very important because it is associated with the life of a driver. However, all vehicle detection methods generate missing detections and false detections because of different vehicle appearances. However, in a general road environment, the appearance of most of these vehicles has a front and a rear. In this paper, we propose a training method to detect the front and rear of the vehicle. Our vehicle detection integrates state-of-the-art feature-based detection.
machine_learning	In this interview conducted by Ubiquity editor Walter Tichy, Prof. Thomas Fahringer of the Institute of Computer Science, University of Innsbruck (Austria) discusses the difficulty in predicting the performance of parallel programs, and the subsequent popularity of auto-tuning to automate program optimization.
machine_learning	The unlimited and pervasive use of the Internet by young people raises many concerns about child safety. What solutions are available and why aren't they being used?
machine_learning	The co-founder of the Internet recalls the non-commercial early days and looks at today's issues of fair use, privacy and the need for security.
machine_learning	Understanding the behavior of Runge-Kutta codes when stability considerations restrict the stepsize provides useful information for stiffness detection and other implementation details. Analysis of equilibrium states on test problems is presented which provides predictions and insights into this behavior. The implications for global error are also discussed.
machine_learning	A recent study by Akl indicates that Mifsud's algorithm, which involves unnecessary searching operations, is the fastest existing combination generator. A modified Page and Wilson's algorithm, which is essentially similar to Mifsud's algorithm, is presented. A theoretical analysis of the modified algorithm is also given.
machine_learning	We present a novel hybrid communication protocol that guarantees mobile users� anonymity against a wide-range of adversaries by exploiting the capability of handheld devices to connect to both WiFi and cellular networks. Unlike existing anonymity schemes, we consider all parties that can intercept communications between a mobile user and a server as potential privacy threats. We formally quantify the privacy exposure and the protection of our system in the presence of malicious neighboring peers, global WiFi eavesdroppers, and omniscient mobile network operators, which possibly collude to breach user�s anonymity or disrupt the communication. We also describe how a micropayment scheme that suits our mobile scenario can provide incentives for peers to collaborate in the protocol. Finally, we evaluate the network overhead and attack resiliency of our protocol using a prototype implementation deployed in Emulab and Orbit, and our probabilistic model.
machine_learning	The number of wireless-enabled devices owned by a user has had huge growth over the past few years. Over one third of adults in the United States currently own three wireless devices: a smartphone, laptop, and tablet. This article provides a study of the network usage behavior of today�s multidevice users. Using data collected from a large university campus, we provide a detailed multidevice user (MDU) measurement study of more than 30,000 users. The major objective of this work is to study how the presence of multiple wireless devices affects the network usage behavior of users. Specifically, we characterize the usage pattern of the different device types in terms of total and intermittent usage, how the usage of different devices overlap over time, and uncarried device usage statistics. We also study user preferences of accessing sensitive content and device-specific factors that govern the choice of WiFi encryption type. The study reveals several interesting findings about MDUs. We see how the use of tablets and laptops are interchangeable and how the overall multidevice usage is additive instead of being shared among the devices. We also observe how current DHCP configurations are oblivious to multiple devices, which results in inefficient utilization of available IP address space. All findings about multidevice usage patterns have the potential to be utilized by different entities, such as app developers, network providers, security researchers, and analytics and advertisement systems, to provide more intelligent and informed services to users who have at least two devices among a smartphone, tablet, and laptop.
machine_learning	Witnessing the wide spread of malicious information in large networks, we develop an efficient method to detect anomalous diffusion sources and thus protect networks from security and privacy attacks. To date, most existing work on diffusion sources detection are based on the assumption that network snapshots that reflect information diffusion can be obtained continuously. However, obtaining snapshots of an entire network needs to deploy detectors on all network nodes and thus is very expensive. Alternatively, in this article, we study the diffusion sources locating problem by learning from information diffusion data collected from only a small subset of network nodes. Specifically, we present a new regression learning model that can detect anomalous diffusion sources by jointly solving five challenges, that is, unknown number of source nodes, few activated detectors, unknown initial propagation time, uncertain propagation path and uncertain propagation time delay. We theoretically analyze the strength of the model and derive performance bounds. We empirically test and compare the model using both synthetic and real-world networks to demonstrate its performance.
machine_learning	Practicing and playing a sport causes athletes' bodies to adapt to the movements they regularly perform. Unfortunately, this can cause muscle imbalances, which might impair performance or worse, cause an injury. It is always best to find the root cause of a muscle imbalance, and to make a precise effort to fix it. Muscle imbalance shouldn't be taken lightly-it could create bigger problems, from posture to spinal positioning, which can ultimately lead to issues in walking, sitting and even lying down, as time progresses. However, muscle imbalances can't be easily evaluated using X-rays, CT scans, or other high-tech devices. But it's possible to address the problem in other ways. In general, the "strong" muscle is measured against the "weaker" muscle. Using the infrared (IR) camera, Kinect can recognize users and track their skeletons in the field of view of the sensor. Kinect sensor can locate the joints of the tracked users in space and track their movements over time. This allows Kinect sensor to recognize people (postures) and follow their actions (movements). Hence, the primary aim of this research is to investigate patterns of muscle imbalance among athletes and evaluate those patterns based on the posture, balance, gait and movement variations using Kinect sensor. Ideally the expected outcome of this research would be a physically meaningful & robust method to identify the muscle imbalance of an athlete.
machine_learning	The choice of cloud providers whose offers best fit the requirements of a particular application is a complex issue due to the heterogeneity of the services in terms of resources, costs, technology, and service levels that providers ensure. This article investigates the effectiveness of multiobjective genetic algorithms to resolve a multicloud brokering problem. Experimental results provide clear evidence about how such a solution improves the choice made manually by users returning in real time optimal alternatives. It also investigates how the optimality depends on different genetic algorithms and parameters, problem type, and time constraints.
machine_learning	Many projects have tried to analyze the structure and dynamics of application overlay networks on the Internet using packet analysis and network flow data. While such analysis is essential for a variety of network management and security tasks, it is infeasible on many networks: either the volume of data is so large as to make packet inspection intractable, or privacy concerns forbid packet capture and require the dissociation of network flows from users� actual IP addresses. Our analytical framework permits useful analysis of network usage patterns even under circumstances where the only available source of data is anonymized flow records. Using this data, we are able to uncover distributions and scaling relations in host-to-host networks that bear implications for capacity planning and network application design. We also show how to classify network applications based entirely on topological properties of their overlay networks, yielding a taxonomy that allows us to accurately identify the functions of unknown applications. We repeat this analysis on a more recent dataset, allowing us to demonstrate that the aggregate behavior of users is remarkably stable even as the population changes.
machine_learning	We model the design and impact of Internet pricing plans with data caps. We consider a monopoly ISP that maximizes its profit by setting tier prices, tier rates, network capacity, data caps, and overage charges. We show that when data caps are used to maximize profit, a monopoly ISP will keep the basic tier price the same, increase the premium tier rate, and decrease the premium tier price and the basic tier rate. We give analytical and numerical results to illustrate the increase in ISP profit, and the corresponding changes in user tier choices, user surplus, and social welfare.
machine_learning	Reliable network demographics are quickly becoming a much sought-after digital commodity. However, as the need for more refined Internet demographics has grown, so too has the tension between privacy and utility. Unfortunately, current techniques lean too much in favor of functional requirements over protecting the privacy of users. For example, the most prominent proposals for measuring the relative popularity of a Web site depend on the deployment of client-side measurement agents that are generally perceived as infringing on users� privacy, thereby limiting their wide-scale adoption. Moreover, the client-side nature of these techniques also makes them susceptible to various manipulation tactics that undermine the integrity of their results. In this article, we propose a new estimation technique that uses DNS cache probing to infer the density of clients accessing a given service. Compared to earlier techniques, our scheme is less invasive as it does not reveal user-specific traits, and is more robust against manipulation. We demonstrate the flexibility of our approach through two important security applications. First, we illustrate how our scheme can be used as a lightweight technique for measuring and verifying the relative popularity rank of different Web sites. Second, using data from several hundred botnets, we apply our technique to indirectly measure the infected population of this increasing Internet phenomenon.
machine_learning	In order to improve scalability and to reduce the maintenance overhead for structured peer-to-peer (P2P) networks, researchers have proposed architectures based on several interconnection networks with a fixed-degree and a logarithmical diameter. Among existing fixed-degree interconnection networks, the Kautz digraph has many distinctive topological properties compared to others. It, however, requires that the number of peers have the some given values, determined by peer degree and network diameter. In practice, we cannot guarantee how many peers will join a P2P network at a given time, since a P2P network is typically dynamic with peers frequently entering and leaving. To address such an issue, we propose the balanced Kautz tree and Kautz ring structures. We further design a novel structured P2P system, called BAKE, based on the two structures that has the logarithmical diameter and constant degree, even the number of peers is an arbitrary value. By keeping a total ordering of peers and employing a robust locality-preserved resource placement strategy, resources that are similar in a single or multidimensional attributes space are stored on the same peer or neighboring peers. Through analysis and simulation, we show that BAKE achieves the optimal diameter and as good a connectivity as the Kautz digraph does (almost achieves the Moore bound), and supports the exact as well as the range queries efficiently. Indeed, the structures of balanced Kautz tree and Kautz ring we propose can also be applied to other interconnection networks after minimal modifications, for example, the de Bruijn digraph.
machine_learning	With the proliferation of mobile devices and sensors, complex event proceesing (CEP) is becoming increasingly important to scalably detect situations in real time. Current CEP systems are not capable of dealing efficiently with highly dynamic mobile consumers whose interests change with their location. We introduce the distributed mobile CEP (MCEP) system which automatically adapts the processing of events according to a consumer's location. MCEP significantly reduces latency, network utilization, and processing overhead by providing on-demand and opportunistic adaptation algorithms to dynamically assign event streams and computing resources to operators of the MCEP system.
mathematical_optimization	Universities and training organizations increasingly use technology to record and distribute original material, bringing on a new class of technological and legal issues.
mathematical_optimization	Extended Collaborative Less-is-More Filtering xCLiMF is a learning to rank model for collaborative filtering that is specifically designed for use with data where information on the level of relevance of the recommendations exists, e.g. through ratings. xCLiMF can be seen as a generalization of the Collaborative Less-is-More Filtering (CLiMF) method that was proposed for top-N recommendations using binary relevance (implicit feedback) data. The key contribution of the xCLiMF algorithm is that it builds a recommendation model by optimizing Expected Reciprocal Rank, an evaluation metric that generalizes reciprocal rank in order to incorporate user feedback with multiple levels of relevance. Experimental results on real-world datasets show the effectiveness of xCLiMF, and also demonstrate its advantage over CLiMF when more than two levels of relevance exist in the data.
mathematical_optimization	Past consumption of items affect current choices and influence the perceived quality. The order in which items are consumed can affect the score that a user might give to them. In this work we present two simple models that take advantage of the temporal order of choices and ratings by the user in order to improve the quality of the recommendation. Our model exploits the collaborative effects in the data while also taking into account the order in which items are seen by the users. Experiments show that our approach outperforms standard Matrix Factorization models.
mathematical_optimization	Real world large-scale recommender systems are always dynamic: new users and items continuously enter the system, and the status of old ones (e.g., users' preference and items' popularity) evolve over time. In order to handle such dynamics, we propose a recommendation framework consisting of an online component and an offline component, where the newly arrived items are processed by the online component such that users are able to get suggestions for fresh information, and the influence of longstanding items is captured by the offline component. Based on individual users' rating behavior, recommendations from the two components are combined to provide top-N recommendation. We formulate recommendation problem as a ranking problem where learning to rank is applied to extend upon matrix factorization to optimize item rankings by minimizing a pairwise loss function. Furthermore, to better model interactions between users and items, Latent Dirichlet Allocation is incorporated to fuse rating information and textual information. Real data based experiments demonstrate that our approach outperforms the state-of-the-art models by at least 61.21% and 50.27% in terms of mean average precision (MAP) and normalized discounted cumulative gain (NDCG) respectively.
mathematical_optimization	Given the rise of long term conditions, and focus on living independently in the community, we launched a free crowd sourcing community app called 'ifOnly' to encourage people with disabilities to share the problems they encounter in everyday life. The app allows people to record, upload and share videos and audios that demonstrate everyday problems they face at home. These were then shared on the 'ifOnly' website - www.ifonlyitworked.com. Designers, recruited nationally, were asked to come up with innovative design solutions via a competition hosted by the University of Bath. The designs were evaluated for novelty and commercial potential by a panel of stakeholders.
mathematical_optimization	In many application domains of recommender systems, explicit rating information is sparse or non-existent. The preferences of the current user have therefore to be approximated by interpreting his or her behavior, i.e., the implicit user feedback. In the literature, a number of algorithm proposals have been made that rely solely on such implicit feedback, among them Bayesian Personalized Ranking (BPR). In the BPR approach, pairwise comparisons between the items are made in the training phase and an item i is considered to be preferred over item j if the user interacted in some form with i but not with j. In real-world applications, however, implicit feedback is not necessarily limited to such binary decisions as there are, e.g., different types of user actions like item views, cart or purchase actions and there can exist several actions for an item over time. In this paper we show how BPR can be extended to deal with such more fine-granular, graded preference relations. An empirical analysis shows that this extension can help to measurably increase the predictive accuracy of BPR on realistic e-commerce datasets.
mathematical_optimization	Reciprocal recommender is a class of recommender systems that is important for tasks where people are both the subject and the object of the recommendation; one such task is online dating. We have implemented RECON, a reciprocal recommender for online dating, and we have evaluated it on a major dating website. Results show an improved success rate for recommendations that consider reciprocity in comparison to recommendations that only consider the preferences of the users receiving the recommendations.
mathematical_optimization	In this paper, an effective collaborative filtering algorithm for top-N item recommendation with implicit feedback is proposed. The task of top-N item recommendation is to predict a ranking of items (movies, books, songs, or products in general) that can be of interest for a user based on earlier preferences of the user. We focus on implicit feedback where preferences are given in the form of binary events/ratings. Differently from state-of-the-art methods, the method proposed is designed to optimize the AUC directly within a margin maximization paradigm. Specifically, this turns out in a simple constrained quadratic optimization problem, one for each user. Experiments performed on several benchmarks show that our method significantly outperforms state-of-the-art matrix factorization methods in terms of AUC of the obtained predictions.
mathematical_optimization	Recent work has shown that collaborative filter-based recommender systems can be improved by incorporating side information, such as natural language reviews, as a way of regularizing the derived product representations. Motivated by the success of this approach, we introduce two different models of reviews and study their effect on collaborative filtering performance. While the previous state-of-the-art approach is based on a latent Dirichlet allocation (LDA) model of reviews, the models we explore are neural network based: a bag-of-words product-of-experts model and a recurrent neural network. We demonstrate that the increased flexibility offered by the product-of-experts model allowed it to achieve state-of-the-art performance on the Amazon review dataset, outperforming the LDA-based approach. However, interestingly, the greater modeling power offered by the recurrent neural network appears to undermine the model's ability to act as a regularizer of the product representations.
mathematical_optimization	Contests and challenges have energized researchers and focused attention in many fields recently, including recommender systems. At the 2008 RecSys conference, winners were announced for a contest proposing new startup companies. The 2009 conference featured a panel reflecting on the then recently completed Netflix challenge. Would additional contests help move the field of recommender systems forward? Or would they just draw attention from the most important problems to problems that are most easily formulated as contests? If contests would be useful, what should the tasks be and how should performance be evaluated? The panel will begin with short presentations by the panelists. Following that, the panelists will respond to brief sketches of possible new contests. In addition to prediction and ranking tasks, tasks might include making creative use of the outputs of a fixed recommender engine, or eliciting inputs for a recommender engine.
mathematical_optimization	The problem of information overload has been a relevant and active research topic for the past twenty years. Since then, numerous algorithms and recommendation approaches have been proposed, which gives rise to a new type of problem: recommendation algorithm overload. Although hybrid recommendation techniques, which combine the strengths of individual recommenders, have become well-accepted, the procedure of building and tuning a hybrid recommender is still a tedious and time-consuming process. In our work, we focus on dynamically building personalized hybrid recommender systems on an individual user basis. By means of a dynamic online learning strategy we combine the most appropriate recommendation algorithms for a user based on realtime relevance feedback. Learning effectiveness of genetic algorithms, machine learning techniques and other optimization approaches will be studied in both an offline and online setting.
mathematical_optimization	Novelty and diversity have been identified as key dimensions of recommendation utility in real scenarios, and a fundamental research direction to keep making progress in the field. Yet recommendation novelty and diversity remain a largely open area for research. The DiveRS workshop gathered researchers and practitioners interested in the role of these dimensions in recommender systems. The workshop seeks to advance towards a better understanding of what novelty and diversity are, how they can improve the effectiveness of recommendation methods and the utility of their outputs. The workshop pursued the identification of open problems, relevant research directions, and opportunities for innovation in the recommendation business.
mathematical_optimization	The tremendous popularity of Online Social Networks (OSN) has led to situations, where users have their profiles spread across multiple networks. These partial profiles reflect different user characteristics, depending mainly on the nature of the network, e.g., Facebook's social vs. LinkedIn's professional focus. Combining data gathered by multiple networks may benefit individual users, and the community as a whole, as this could facilitate the provision of more accurate services and recommendations. This paper reports on an exploratory study of the process of making such recommendations using a unique multi-network dataset containing user interests across multiple domains, e.g., music, books, and movies. We represent the data using a graph model and generate recommendations using a set of features extracted from and populated by the model. We assess the contribution of various network- and domain-related features to the accuracy of the recommendations and motivate future work into automated feature selection.
mathematical_optimization	Despite, or exactly because of the wealth of video content available on the Web, it is often cumbersome to obtain access to exactly the content you like. It is an active process of filtering and making choices, often requiring searches through long lists of alternatives. Building upon the concept of personal TV channels, we demonstrate how Web video content can be seamlessly integrated with broadcast video content, thereby also providing personalized Web content.
mathematical_optimization	In this demo we present a recommender benchmark framework that serves as an infrastructure for comparing and examining the performance and feasibility of different recommender algorithms on various datasets with a variety of measures. The extendable infrastructure aims to provide easy plugging of novel recommendation-algorithms, datasets and compare their performance using visual tools and metrics with other algorithms in the benchmark. It also aims at generating a WEKA-type workbench [1] for the recommender systems field to enable usage and application of common recommender systems (RS) algorithms for research and practice. The demo movie is available at: http://www.youtube.com/watch?v=fsDITf6s0WY
mathematical_optimization	This talk will focus on our experience in managing the complexity of Sibyl, a large scale machine learning system that is widely used within Google. We believe that a large fraction of the challenges faced by Sibyl are inherent to large scale production machine learning and that other production systems are likely to encounter them as well [1]. Thus, these challenges present interesting opportunities for future research. The Sibyl system is complex for a number of reasons. We have learnt that a complete end-to-end machine learning solution has to have subsystems to address a variety of different needs: data ingestion, data analysis, data verification, experimentation, model analysis, model serving, configuration, data transformations, support for different kinds of loss functions and modeling, machine learning algorithm implementations, etc. Machine learning algorithms themselves constitute a relatively small fraction of the overall system. Each subsystem consists of a number of distinct components to support the variety of product needs. For example, Sibyl supports more than 5 different model serving systems, each with its own idiosyncrasies and challenges. In addition, Sibyl configuration contains more lines of code than the core Sibyl learner itself. Finally existing solutions for some of the challenges don't feel adequate and we believe these challenges present opportunities for future research. Though the overall system is complex, our users need to be able to deploy solutions quickly. This is because a machine learning deployment is typically an iterative process of model improvements. At each iteration, our users experiment with new features, find those that improve the model's prediction capability, and then "launch" a new model with those improved features. A user may go through 10 or more such productive launches. Not only is speed of iteration crucial to our users, but they are often willing to sacrifice the improved prediction quality of a high quality but cumbersome system for the speed of iteration of a lower quality but nimble system. In this talk I will give an example of how simplification drives systems design and sometimes the design of novel algorithms.
mathematical_optimization	We present a method for learning potentially intransitive preference relations from pairwise comparison and matchup data. Unlike standard preference-learning models that represent the properties of each item/player as a single number, our method infers a multi-dimensional representation for the different aspects of each item/player's strength. We show that our model can represent any pairwise stochastic preference relation and provide a comprehensive evaluation of its predictive performance on a wide range of pairwise comparison tasks and matchup problems from online video games and sports, to peer grading and election. We find that several of these task -- especially matchups in online video games -- show substantial intransitivity that our method can model effectively.
mathematical_optimization	Analyzing and modeling the temporal diffusion of information on social media has mainly been treated as a diffusion process on known graphs or proximity structures. The underlying phenomenon results however from the interactions of several actors and media and is more complex than what these models can account for and cannot be explained using such limiting assumptions. We introduce here a new approach to this problem whose goal is to learn a mapping of the observed temporal dynamic onto a continuous space. Nodes participating to diffusion cascades are projected in a latent representation space in such a way that information diffusion can be modeled efficiently using a heat diffusion process. This amounts to learning a diffusion kernel for which the proximity of nodes in the projection space reflects the proximity of their infection time in cascades. The proposed approach possesses several unique characteristics compared to existing ones. Since its parameters are directly learned from cascade samples without requiring any additional information, it does not rely on any pre-existing diffusion structure. Because the solution to the diffusion equation can be expressed in a closed form in the projection space, the inference time for predicting the diffusion of a new piece of information is greatly reduced compared to discrete models. Experiments and comparisons with baselines and alternative models have been performed on both synthetic networks and real datasets. They show the effectiveness of the proposed method both in terms of prediction quality and of inference speed.
mathematical_optimization	Link prediction on knowledge graphs is useful in numerous application areas such as semantic search, question answering, entity disambiguation, enterprise decision support, recommender systems and so on. While many of these applications require a reasonably quick response and may operate on data that is constantly changing, existing methods often lack speed and adaptability to cope with these requirements. This is aggravated by the fact that knowledge graphs are often extremely large and may easily contain millions of entities rendering many of these methods impractical. In this paper, we address the weaknesses of current methods by proposing Random Semantic Tensor Ensemble (RSTE), a scalable ensemble-enabled framework based on tensor factorization. Our proposed approach samples a knowledge graph tensor in its graph representation and performs link prediction via ensembles of tensor factorization. Our experiments on both publicly available datasets and real world enterprise/sales knowledge bases have shown that our approach is not only highly scalable, parallelizable and memory efficient, but also able to increase the prediction accuracy significantly across all datasets.
mathematical_optimization	Many real-world problems, such as web image analysis, document categorization and product recommendation, often exhibit dual-heterogeneity: heterogeneous features obtained in multiple views, and multiple tasks might be related to each other through one or more shared views. To address these Multi-Task Multi-View (MTMV) problems, we propose a tensor-based framework for learning the predictive multilinear structure from the full-order feature interactions within the heterogeneous data. The usage of tensor structure is to strengthen and capture the complex relationships between multiple tasks with multiple views. We further develop efficient multilinear factorization machines (MFMs) that can learn the task-specific feature map and the task-view shared multilinear structures, without physically building the tensor. In the proposed method, a joint factorization is applied to the full-order interactions such that the consensus representation can be learned. In this manner, it can deal with the partially incomplete data without difficulty as the learning procedure does not simply rely on any particular view. Furthermore, the complexity of MFMs is linear in the number of parameters, which makes MFMs suitable to large-scale real-world problems. Extensive experiments on four real-world datasets demonstrate that the proposed method significantly outperforms several state-of-the-art methods in a wide variety of MTMV problems.
mathematical_optimization	The performance of clustering is a crucial challenge, especially for pattern recognition. The models aggregation has a positive impact on the efficiency of Data clustering. This technique is used to obtain more cluttered decision boundaries by aggregating the resulting clustering models. In this paper, we study an aggregation scheme to improve the stability and accuracy of clustering, which allows to find a reliable and robust clustering model. We demonstrate the advantages of our aggregation method by running Fuzzy C-Means (FCM) clustering on Reuters-21578 corpus. Experimental studies showed that our scheme optimized the bias-variance on the selected model and achieved enhanced clustering for unstructured textual resources.
mathematical_optimization	We propose a Deep Neural Network (DNN) structure for RFID-based activity recognition. RFID data collected from several reader antennas with overlapping coverage have potential spatiotemporal relationships that can be used for object tracking. We augmented the standard fully-connected DNN structure with additional pooling layers to extract the most representative features. For model training and testing, we used RFID data from 12 tagged objects collected during 25 actual trauma resuscitations. Our results showed 76% recognition micro-accuracy for 7 resuscitation activities and 85% average micro-accuracy for 5 resuscitation phases, which is similar to existing system that, however, require the user to wear an RFID antenna.
mathematical_optimization	Most of the supervised classification algorithms are proposed to classify newly seen instances based on their learned label space. However, in the case of data streams, concept-evolution is inevitable. In this paper we propose a support vector based approach for classification beyond the learned label space in data streams with regard to other challenges in data streams like concept-drift and infinite-length. We maintain the boundaries of observed classes through the stream by utilizing a support vector based method (SVDD). Newly arrived instances located outside these boundaries will be analyzed by constructing neighborhood graph to detect the emergence of a class beyond the learned label space (novel class). Our method is more accurate to model intricate-shape class boundaries than existing method since it utilizes support vector data description method. Dynamically maintaining boundaries by shrinking, enlarging and merging spheres in the kernel space, helps our method to adapt both dramatic and gradual changes of underlying distribution of data, and also be more memory efficient than the existing methods. Conducted experiments on both real and synthetic benchmark data sets show the superiority of the proposed method over the state-of-the-art methods in this area.
mathematical_optimization	Dementia is a growing healthcare problem, and it has become necessary to find a way to reduce the prevalence of this disease. Mild cognitive impairment is a risk factor for dementia, and being able to detect the onset of mild cognitive impairment gives healthcare professionals the chance to reduce the effect of dementia on the society. In this paper, we propose a system that seeks to detect mild cognitive impairment in otherwise healthy persons. We are particularly interested in being able to gather data for monitoring without interfering with the subject's daily living.
mathematical_optimization	Decision rules are one of the most interpretable and flexible models for data mining prediction tasks. Till now, few works presented online, any-time and one-pass algorithms for learning decision rules in the stream mining scenario. A quite recent algorithm, the Very Fast Decision Rules (VFDR), learns set of rules, where each rule discriminates one class from all the other. In this work we extend the VFDR algorithm by decomposing a multi-class problem into a set of two-class problems and inducing a set of discriminative rules for each binary problem. The proposed algorithm maintains all properties required when learning from stationary data streams: online and any-time classifiers, processing each example once. Moreover, it is able to learn ordered and unordered rule sets. The new approach is evaluated on various real and artificial datasets. The new algorithm improves the performance of the previous version and is competitive with the state-of-the-art decision tree learning method for data streams.
mathematical_optimization	Sleep monitoring is an increasingly popular practice, both for medical and lifestyle purposes. In the case of infant safety monitoring, however, most of the devices used are inapplicable due to the utilisation of wires, cords, obtrusive sensors, constant radio wave transmission, low sensitivity and specificity. We proposed and are currently developing the second generation of a portable, unobtrusive infant safety system that can be fitted to most existing cots and can wirelessly tele-alert the infant's carers in case of emergency or other pre-defined circumstances. The MAIA system is based on the real-time algorithmic fusion of data obtained from multiple sensors distributed around the infant's cot, as part of a reasonably priced system which is quick to install, requires no alteration of existing infant care routines and demonstrates a high level of sensitivity and specificity.
mathematical_optimization	As in batch learning, one may identify a class of streaming real-world problems which require the modeling of several targets simultaneously. Due to the dependencies among the targets, simultaneous modeling can be more successful and informative than creating independent models for each target. As a result one may obtain a smaller model able to simultaneously explain the relations between the input attributes and the targets. This problem has not been addressed previously in the streaming setting. We propose an algorithm for inducing multi-target model trees with low computational complexity, based on the principles of predictive clustering trees and probability bounds for supporting splitting decisions. Linear models are computed for each target separately, by incremental training of perceptrons in the leaves of the tree. Experiments are performed on synthetic and real-world datasets. The multi-target regression tree algorithm produces equally accurate and smaller models for simultaneous prediction of all the target attributes, as compared to a set of independent regression trees built separately for each target attribute. When the regression surface is smooth, the linear models computed in the leaves significantly improve the accuracy for all of the targets.
mathematical_optimization	Understanding the relationship between neuronal activation patterns of specific brain areas resulting from sensorial experiences is a challenging problem. In this context, we analyzed the levels of similarity between neuronal activation patterns using a semi-supervised method and data acquired from microelectrode arrays implanted on specific brain areas of rats, during an experiment of tactile exploration of four classes of physical objects in the dark. Eight factors were considered (animal, brain region, pair of objects, clustering algorithm, clustering evaluation metric, bin size, window size and contact interval), resulting in 294.912 similarity measurements. Hypotheses regarding the relationship of each of the factors were statistically tested. Not all degrees of similarity between the patterns extracted from pairs of different exploration intervals, for two different objects, were equivalent to a given treatment. This indicated that the similarity between the patterns is sensitive to all the factors analyzed and provides evidence about the complexity of neuronal coding in the brain.
mathematical_optimization	We describe DDE-BIFTOOL, a Matlab package for numerical bifurcation analysis of systems of delay differential equations with several fixed, discrete delays. The package implements continuation of steady state solutions and periodic solutions and their stability analysis. It also computes and continues steady state fold and Hopf bifurcations and, from the latter, it can switch to the emanating branch of periodic solutions. We describe the numerical methods upon which the package is based and illustrate its usage and capabilities through analysing three examples: two models of coupled neurons with delayed feedback and a model of two oscillators coupled with delay.
mathematical_optimization	In processing networks, ordinary network constraints are supplemented by proportional flow restrictions on arcs entering or leaving some nodes. This paper describes a new primal partitioning algorithm for solving pure processing networks using a working basis of variable dimension. In testing against MPSX/370 on a class of randomly generated problems, a FORTRAN implementation of this algorithm was found to be an order-of-magnitude faster. Besides indicating the use of our methods in stand-alone fashion, the computational results also demonstrate the desirability of using these methods as a high-level module in a mathematical programming system.
mathematical_optimization	A collection of subroutines and examples of their uses are described for the quadrature method developed in the companion article. These allow the exact evaluation (up to computer truncation and rounding errors) of integrals of polynomials with two general types of logarithmic weights, and also with the corresponding nonlogarithmic weights. The recurrence coefficients for the related nonclassical orthogonal polynomials with logarithmic weight functions can also be obtained. Tests of accuracy on various platforms are presented. The routines are usable from Fortran, C, and C++ programs conforming to any of at least six international programming-language standards.
mathematical_optimization	A new pseudorandom number generator, the Multiple Prime Random Number Generator, has been developed; it is efficient, conceptually simple, flexible, and easy to program. The generator utilizes cycles around prime numbers to guarantee the length of the period, which can easily be programmed to surpass the maximum period of any other presently available random number generator. There are minimum limits placed on the seed values of the variables because the period of the generator is not a function of the initial values of the variables. The generator passes thirteen standard random number generator tests. It requires only about fifteen lines of FORTRAN code to program and utilizes programming language constructs found in most major languages. Finally, it compares very favorably to the fastest of the other available generators.
mathematical_optimization	A procedure for generating non-differentiable, continuously differentiable, and twice continuously differentiable classes of test functions for multiextremal multidimensional box-constrained global optimization is presented. Each test class consists of 100 functions. Test functions are generated by defining a convex quadratic function systematically distorted by polynomials in order to introduce local minima. To determine a class, the user defines the following parameters: (i) problem dimension, (ii) number of local minima, (iii) value of the global minimum, (iv) radius of the attraction region of the global minimizer, (v) distance from the global minimizer to the vertex of the quadratic function. Then, all other necessary parameters are generated randomly for all 100 functions of the class. Full information about each test function including locations and values of all local minima is supplied to the user. Partial derivatives are also generated where possible.
mathematical_optimization	This remark describes an improvement and a correction to Algorithm 778. It is shown that the performance of the algorithm can be improved significantly by making a relatively simple modification to the subspace minimization phase. The correction concerns an error caused by the use of routine dpmeps to estimate machine precision.
mathematical_optimization	We present a Fortran 90 module, which computes the solutions and their derivatives of Airy's differential equation, both on the real line and in the complex plane. The module also computes the zeros and associated values of the solutions and their derivatives, and the modulus and phase functions on the negative real axis. The computational methods are numerical integration of the differential equation and summation of asymptotic expansions for large argument. These methods were chosen because they are simple, adaptable to any precision, and amenable to rigorous error analysis. The module can be used to validate other codes or as a component in programs that require Airy functions.
mathematical_optimization	SFSDP is a Matlab package for solving sensor network localization (SNL) problems. These types of problems arise in monitoring and controlling applications using wireless sensor networks. SFSDP implements the semidefinite programming (SDP) relaxation proposed in Kim et al. [2009] for sensor network localization problems, as a sparse version of the full semidefinite programming relaxation (FSDP) by Biswas and Ye [2004]. To improve the efficiency of FSDP, SFSDP exploits the aggregated and correlative sparsity of a sensor network localization problem. As a result, SFSDP can handle much larger problems than other software as well as three-dimensional anchor-free problems. SFSDP analyzes the input data of a sensor network localization problem, solves the problem, and displays the computed locations of sensors. SFSDP also includes the features of generating test problems for numerical experiments.
mathematical_optimization	An algorithm is presented for finding a root of a real function. The algorithm combines bisection with second and third order methods using derivatives estimated from objective function values. Globaql convergence is ensured and the number of function evaluations is bounded by four times the number needed by bisection. Numerical comparisons with existing algorithms indicate the superiority of the new algorithm in all classes of problems.
mathematical_optimization	A method is presented for the generation of test problems for global optimization algorithms. Given a bounded polyhedron in R and a vertex, the method constructs nonconvex quadratic functions (concave or indefinite) whose global minimum is attained at the selected vertex. The construction requires only the use of linear programming and linear systems of equations.
mathematical_optimization	Methods are presented for evaluating the performance of programs for the functions &Ggr;(x), ln &Ggr;(x), and &psgr;(x). Accuracy estimates are based on comparisons using the manipulation theorem. Ideas for checking robustness are also given, and details on specific implementations of test programs are included.
mathematical_optimization	It has recently been shown that cancellation errors in a quasi-Newton method can increase without bound as the method converges. A simple test is presented to determine when cancellation errors could lead to significant contamination of the approximating matrix.
mathematical_optimization	An algorithm for accurately computing the lower incomplete gamma function ?(a, t) in the case where a = n + 1/2, n ? Z and t < 0 is described. Series expansions and analytic continuation are employed to compute the function for certain critical values of n, and these results are used to initiate stable recurrence. The algorithm has been implemented in Fortran 2003, with precompuations carried out in Maple.
mathematical_optimization	We present the fast approximation of multivariate functions based on Chebyshev series for two types of Chebyshev lattices and show how a fast Fourier transform (FFT) based discrete cosine transform (DCT) can be used to reduce the complexity of this operation. Approximating multivariate functions using rank-1 Chebyshev lattices can be seen as a one-dimensional DCT while a full-rank Chebyshev lattice leads to a multivariate DCT. We also present a MATLAB/Octave toolbox which uses this fast algorithms to approximate functions on a axis aligned hyper-rectangle. Given a certain accuracy of this approximation, interpolation of the original function can be achieved by evaluating the approximation while the definite integral over the domain can be estimated based on this Chebyshev approximation. We conclude with an example for both operations and actual timings of the two methods presented.
mathematical_optimization	Object-oriented programming is a relatively new tool in the development of optimization software. The code extensibility and the rapid algorithm prototyping capability enabled by this programming paradigm promise to enhance the reliability, utility, and ease of use of optimization software. While the use of object-oriented programming is growing, there are still few examples of general purpose codes written in this manner, and a common approach is far from obvious. This paper describes OPT++, a C++ class library for nonlinear optimization. The design is predicated on the concept of distinguishing between an algorithm-independent class hierarchy for nonlinear optimization problems and a class hierarchy for nonlinear optimization methods that is based on common algorithmic traits. The interface is designed for ease of use while being general enough so that new optimization algorithms can be added easily to the existing framework. A number of nonlinear optimization algorithms have been implemented in OPT++ and are accessible through this interface. Furthermore, example applications demonstrate the simplicity of the interface as well as the advantages of a common interface in comparing multiple algorithms.
mathematical_optimization	We present two new adaptive quadrature routines. Both routines differ from previously published algorithms in many aspects, most significantly in how they represent the integrand, how they treat nonnumerical values of the integrand, how they deal with improper divergent integrals, and how they estimate the integration error. The main focus of these improvements is to increase the reliability of the algorithms without significantly impacting their efficiency. Both algorithms are implemented in MATLAB and tested using both the �families� suggested by Lyness and Kaganove and the battery test used by Gander and Gautschi and Kahaner. They are shown to be more reliable, albeit in some cases less efficient, than other commonly-used adaptive integrators.
mathematical_optimization	In this paper we present a new binary-programming formulation for the Steiner problem in graphs (SPG), which is well known to be NP-hard. We use this formulation to generate test problems with known optimal solutions. The technique uses the KKT optimality conditions on the corresponding quadratically constrained optimization problem.
mathematical_optimization	The algorithm computes exponential integrals En(z) for integer orders n ? 1 and complex z in -&pgr; < arg z ? &pgr;. Both single and double precision subroutines are provided. Exponential scaling and stable sequence generation are auxiliary options.
mathematical_optimization	In recent years many results have been obtained in the field of the numerical inversion of Laplace transforms. Among them, a very accurate and general method is due to Talbot: this method approximates the value of the inverse Laplace transform f(t), for t fixed, using the complex values of the Laplace transform F(s) sampled on a suitable contour of the complex plane. On the basis of the interest raised by Talbot's method implementation, the author has been induced to investigate more deeply the possibilities of this method and has been able to generalize Talbot's method, to approximate simultaneously several values of f(t) using the same sampling values of the Laplace transform. In this way, the only unfavorable aspect of the classical Talbot method, that is, that of recomputing all of the samples of F(s) for each t, has been eliminated.
mathematical_optimization	We report on careful implementations of seven algorithms for solving the problem of finding a maximum transversal of a sparse matrix. We analyze the algorithms and discuss the design choices. To the best of our knowledge, this is the most comprehensive comparison of maximum transversal algorithms based on augmenting paths. Previous papers with the same objective either do not have all the algorithms discussed in this article or they used nonuniform implementations from different researchers. We use a common base to implement all of the algorithms and compare their relative performance on a wide range of graphs and matrices. We systematize, develop, and use several ideas for enhancing performance. One of these ideas improves the performance of one of the existing algorithms in most cases, sometimes significantly. So much so that we use this as the eighth algorithm in comparisons.
mathematical_optimization	A stochastic algorithm is presented for finding the global optimum of a function of n variables subject to general constraints. The algorithm is intended for moderate values of n, but it can accommodate objective and constraint functions that are discontinuous and can take advantage of parallel processors. The performance of this algorithm is compared to that of the Nelder-Mead Simplex algorithm and a Simulated Annealing algorithm on a variety of nonlinear functions. In addition, one-, two-, four-, and eight-processor versions of the algorithm are compared using 64 of the nonlinear problems with constraints collected by Hock and Schittkowski. In general, the algorithm is more robust than the Simplex algorithm, but computationally more expensive. The algorithm appears to be as robust as the Simulated Annealing algorithm, but computationally cheaper. Issues discussed include algorithm speed and robustness, applicability to both computer and mathematical models, and parallel efficiency.
mathematical_optimization	An algorithm for computing the quotient of two complex numbers is modified to make it more robust in the presence of underflows.
mathematical_optimization	In this paper, we propose a new reference-based data compression method for efficient compressing of genome sequencing data in FASTQ format. With the advance of the next sequencing technology, the genome data can be generated faster and cheaper, which brings the challenges for efficient storage of these data when used in cloud computing. In order to efficiently store these types of genome data in cloud, content-aware compressing methods have to be developed to make use of the specific file structures. Compared with existing genome-specific compression methods, our proposed content-aware method focused on high compression ratio by taking advantages of repetitive nature of DNA sequence, and using reference genomes in compressing the sequences inside the FASTQ files. The benchmark results of 8 datasets show that our method can achieve highest compression ratio compared with existing FASTQ file compressors.
mathematical_optimization	In this paper, we study methods for improving the utility and privacy of reputation scores for online auctions, such as used in eBay, so as to reduce the effectiveness of feedback extortion. The main idea behind our techniques is to escrow reputations scores until appropriate external events occur. Depending on the degree of utility and privacy needed, these external techniques could depend on the number and type of reputation scores collected. Moreover, if additional privacy protection is needed, then random sampling can be used with respect reputation scores in such a way that reputation aggregates remain useful, but individual reputation scores are probabilistically hidden from users. Finally, we show that if privacy is also desired with respect to the the reputation aggregator, then we can use zero-knowledge proofs for reputation comparisons.
mathematical_optimization	In this paper, we revisit the problem of the link privacy attack in online social networks. In the link privacy attack, it turns out that by bribing or compromising a small number of nodes (users) in the social network graph, it is possible to obtain complete link information for a much larger fraction of other non-bribed nodes in the graph. This can constitute a significant privacy breach in online social networks where the link information of nodes is kept private or accessible only to closely related nodes. We show that the link privacy attack can be made even more effective with degree inference. Since online social networks typically have high degree, the link privacy attack becomes quite feasible even with an in-lookahead neighborhood of one (only friends can see a user's links/profile). To reduce the effect of the link privacy attack, we present several practical mitigation strategies -- non-uniform user privacy settings, approximation of the node degree information and a non-constant cost model for the attack. All the strategies are able to mitigate the privacy link attack by either reducing the effectiveness of the attack or by making it more expensive to mount. Interestingly, some of the more efficient strategies now become worse than the RANDOM strategy and the effect of a larger neighborhood which would otherwise make the attack even more efficient can be mitigated.
mathematical_optimization	The course timetabling problem essentially involves the assignment of weekly lectures to the time periods and lecture rooms. One of the most common scheduling problems at educational institutions is that of scheduling courses or preparing timetable, such that there is no overlap in the schedule. There are various additional constraints come into play. Course scheduling problems consist of scheduling a certain number of resources such as classes, teachers and classrooms to a number of time slots that are distributed over a period of time. The goal is use Genetic Algorithm for finding course schedule. The problem belongs to class known as NP -- Complete. It may possible that there are several solutions with no overlapping. Main goal of algorithm is minimize conflicts.
mathematical_optimization	Despite the importance and widespread use of range data, e.g., time intervals, spatial ranges, etc., little attention has been devoted to study the processing and querying of range data in the context of big data. The main challenge relies in the nature of the traditional index structures e.g., B-Tree and R-Tree, being centralized by nature, and hence are almost crippled when deployed in a distributed environment. To address this challenge, this paper presents Kangaroo, a system built on top of Hadoop to optimize the execution of range queries over range data. The main idea behind Kangaroo is to split the data into non-overlapping partitions in a way that minimizes the query execution time. Kangaroo is query workload-aware, i.e., results in partitioning layouts that minimize the query processing time of given query patterns. In this paper, we study the design challenges Kangaroo addresses in order to be deployed on top of a distributed file system, i.e., HDFS. We also study four different partitioning schemes that Kangaroo can support. With extensive experiments using real range data of more than one billion records and real query workload of more than 30,000 queries, we show that the partitioning schemes of Kangaroo can significantly reduce the I/O of range queries on range data.
mathematical_optimization	Most graph decomposition procedures seek to partition a graph into disjoint sets of vertices. Motivated by applications of clustering in distributed computation, we describe a graph decomposition algorithm for the paradigm where the partitions intersect. This algorithm covers the vertex set with a collection of overlapping clusters. Each vertex in the graph is well-contained within some cluster in the collection. We then describe a framework for distributed computation across a collection of overlapping clusters and describe how this framework can be used in various algorithms based on the graph diffusion process. In particular, we focus on two illustrative examples: (i) the simulation of a randomly walking particle and (ii) the solution of a linear system, e.g. PageRank. Our simulation results for these two cases show a significant reduction in swapping between clusters in a random walk, a significant decrease in communication volume during a linear system solve in a geometric mesh, and some ability to reduce the communication volume during a linear system solve in an information network.
mathematical_optimization	Most online social networks provide a mechanism for users to broadcast messages to their personalized network through actions like shares, likes and tweets. Receiving positive feedback from the network such as likes, comments and retweets in response to such actions can provide a strong incentive for users to broadcast more often in the future. We call such feedback by the network, that influences a user to perform certain desirable future actions, social incentives. For example, after a user shares an article to her social network, receiving positive feedback such as a ''like'' from a friend can potentially encourage her to continue sharing more regularly. Typically, for every user's visit to an online social network site, good messages need to be ranked and selected by a recommender system from a large set of candidate messages (broadcasted by the user's network). In this paper, we propose a novel recommendation problem: How should we recommend messages to users to incentivize neighbors in their personal network to perform desirable actions in the future with high likelihood, without significantly hurting overall engagement for the entire system? For instance, messages could be content shared by neighbors. The goal in this case would be to encourage more content shares in the future. We call this problem social incentive optimization and study an instance of it for LinkedIn's news feed. We observe that a user who receives positive social feedback from neighbors has a higher likelihood of broadcasting more frequently. Using this observation, we develop a novel recommendation framework that incentivize users to broadcast more often, without significantly hurting overall feed engagement. We demonstrate the effectiveness of our approach through causal analysis on retrospective data and online A/B experiments.
mathematical_optimization	This paper presents an artificial intelligence (AI) for the board game 'Quarto!' in Java. The program uses depth-first search for decision making. To improve runtime performance, we used alphabeta pruning, a transposition table, and a Java constraint solver. The result of our work is an open-source Java program capable of beating human opponents in real-time.
mathematical_optimization	The Priesthood of the Singularity posits a fast approaching prospect of machines overtaking human abilities (Ray Kurzweil's The Singularity is Near, Viking Press, 2006) on the basis of the exponential rate of electronic integration---memory and processing power. In fact, they directly correlate the growth of computing technology with that of machine intelligence as if the two were connected in some simple-to-understand and predictable way. Here we present a different view based upon the fundamentals of intelligence and a more likely relationship. We conclude that machine intelligence is growing in a logarithmic (or at best linear fashion) rather than the assumed exponential rate.
mathematical_optimization	So far, Computer-Assisted Language Learning (CALL) comes in many different flavors. Our research work focuses on developing an integrated e-learning environment that allows improving language skills in specific contexts. Integrated e-learning environment means that it is a Web-based solution that performs language learning tasks using common working environments like, for instance, Web browsers or Email clients. It should be accessible on different platforms, even on mobile devices. Natural Language Processing (NLP) forms the technological basis for developing such a learning framework. The paper gives an overview of the state-of-the-art in this area. Therefore, on the one hand, it explains creation processes for NLP resources and gives an overview of corpora. On the other hand, it describes existing NLP standards. Based on our requirements, the paper gives special attention to the evaluation and comparison of toolkits that can suitably support the planned implementation. An outlook at the end points out necessary developments in e-learning to keep in mind.
mathematical_optimization	In the medical context, causal knowledge usually refers to causal relations between diseases and symptoms, living habits and diseases, symptoms which get better and therapy, drugs and side-effects, etc [3]. All these causal relations are usually in medical literature, forum and clinical cases and compose the core part of medical diagnosis. Therefore, mining these causal knowledge to predict disease and recommend therapy is of great value for assisting patients and professionals. The task of mining these causal knowledge for diagnosis assistance can be decomposed into four constitutes: (1) mining medical causality from text; (2) medical treatment effectiveness measurement; (3) disease prediction and (4) explicable medical treatment recommendation. However, these tasks have never been systemically studied before. For my PhD thesis, I plan to formally define the problem of mining medical domain causality for diagnosis assistance and propose methods to solve this problem. 1. Ming these textual causalities can be very useful for discovering new knowledge and making decisions. Many studies have been done for causal extraction from the text [1, 4, 5]. However, all these studies are based on pattern or causal triggers, which greatly limit their power to extract causality and rarely consider the frequency of co-occurrence and contextual semantic features. Besides, none of them take the transitivity rules of causality leading to reject those causalities which can be easily get by simple inference. Therefore, we formally define the task of mining causality via frequency of event co-occurrence, semantic distance between event pairs and transitivity rules of causality, and present a factor graph to combine these three resources for causality mining. 2. Treatment effectiveness analysis is usually taken as a subset of causal analysis on observational data. For such real observational data, PSM and RCM are two dominant methods. On one hand, it is usually difficult for PSM to find the matched cases due to the sparsity of symptom. On the other hand, we should check every possible (symptom, treatment) pair by exploiting RCM, leading to make the characteristic of exploding up, especially when we want to check the causal relation between a combination of symptoms and a combination of drugs. Besides, the larger number of symptom or treatment in the combination the less number of patient case retrieved, which lead to the lack of statistical significance. Specifically, patients tend to take tens of herbs as the treatment each time in Traditional Chinese Medicine (TCM). Therefore, how to evaluate the effectiveness of herbs separately and jointly is really a big challenge. This is also a very fundamental research topic supporting many downstream applications. 3. Both hospitals and on-line forums have accumulated sheer amount of records, such as clinical text data and online diagnosis Q&A pairs. The availability of such data in large volume enables automatic disease prediction. There are some papers on disease prediction with electronic health record (EHR) [2], but the research on disease prediction with raw symptoms is still necessary and challenging. Therefore, we propose a general new idea of using the rich contextual information of diseases and symptoms to bridge the gap of disease candidates and symptoms, and detach it from the specific way of implementing the idea using network embedding. 4. Recommendation in medical domain is usually a decision-making issue, which requires the ability of explaining "why". The ability of explaining "why" are basically from two paths. Consider the recommendation suggest you eat more vegetables. You probably do not believe it if there is nothing attached. But if the recommendation gives the literally reasons why eating more vegetables is good you might like to take this suggestion. Consider another scenario, if the recommendation gives you the data of the contrast which show that people who eat more vegetables are healthier than those eat less, it is certain that you also want to take this recommendation. Based on these two intuitions, we present a recommendation model based on proofs which are either literally reasons or difference from contrast. This work was supported by the 973 program (No. 2014CB340503) and the NSFC (No. 61133012 and No. 61472107).
mathematical_optimization	The English language offers a complex and ambiguous grammar that is readily understood by its natural users, but at times can be difficult to grasp by beginners/learners, and no less, by machines. This paper discusses research and implementations of several techniques towards algorithmically analyzing and verifying the grammatical correctness of sentences in written English.
mathematical_optimization	This demonstration will present automatic knowledge extraction from documents in different languages. The semantic extraction is done according to an ontology describing persons and organizations, their relations and the activities of persons. The result of the extracted knowledge is stored in a triplestore. An interface is used to explore interactively the RDF extracted from the document. The text is presented with colored entities and actions, pronouns and their ant�cedent are linked, attributes of entities and description of events can be popped-up by clicking on words.
mathematical_optimization	At present, product lifecycle data is provided, used, and archived only for very specific purposes. Textual data generated in product use can be very comprehensive and contain valuable information beyond its original purpose, e.g. for product development or improvement. This kind of information, however, is not fed back systematically, as its evaluation is currently not possible due to a lack of suitable methods and tools in product development. The paper in hand presents a concept designed to support product developers by introducing maintenance, service, and customer data, which has undergone restructuring into textual form in the product use phase, into product development. To process the restructured textual data, customized text mining methods are generated as a part of knowledge management in product development. These methods aim at identifying products, product generations, and failure descriptions, as well as the relationships among them. The knowledge gained through text mining is visualized in the work environment familiar to the product developer, and implements utilitarian visualization methods. Visualization is integrated into existing IT systems and provides knowledge required for product improvement or development in a target-oriented way. This extension to knowledge-based product development facilitates more efficient improvement of current and future products.
mathematical_optimization	Designers of IVR systems often shy away from using speech prompts; preferring, where they can, to use keypad input. Part of the reason is that speech processing is expensive and often error prone. This work attempts to address this problem by offering guidelines for prompt design based on field experiments. It is shown, specifically, that accuracy can be influenced by prompt examples, depending on the nature of the information requested.
mathematical_optimization	Technical writing in professional environments, such as user manual authoring for new products, is a task that relies heavily on reuse of content. Therefore, technical content is typically created following a strategy where modular units of text have references to each other. One of the main challenges faced by technical authors is to avoid duplicating existing content, as this adds unnecessary effort, generates undesirable inconsistencies, and dramatically increases maintenance and translation costs. However, there are few computational tools available to support this activity. This paper investigates the use of different similarity methods for the task of identification of reuse opportunities in technical writing. We evaluated our results using existing ground truth as well as feedback from technical authors. Finally, we also propose a tool that combines text similarity algorithms with interactive visualizations to aid authors in understanding differences in a collection of topics and identifying reuse opportunities.
mathematical_optimization	Transcription of handwritten historical documents is one of the main topics in document analysis systems, due to cultural reasons. State-of-the-art handwritten text recognition systems allow to speed up the transcription task. Currently, this automatic transcription is far from perfect, and human expert revision is required in order to obtain the actual transcription. In this context, crowdsourcing emerged as a powerful tool for massive transcription at a relatively low cost, since the supervision effort of professional transcribers may be dramatically reduced. However, current transcription crowdsourcing platforms are mainly limited to the use of non-mobile devices, since the use of keyboards in mobile devices is not friendly enough for most users. This work presents the alternative of using speech dictation of handwritten text lines as transcription source in a crowdsourcing platform. The experiments explore how an initial handwritten text recognition hypothesis can be improved by using the contribution of speech recognition from several speakers, providing as a final result a better hypothesis to be amended by a professional transcriber with less effort.
mathematical_optimization	In this paper, we extend a trivial wear-leveling algorithm to reduce the management overhead of leveraging the number of write operations over all data blocks. By exploiting access locality, most parts of a mapping table of a flash translation layer (FTL) keep unchanged since wear leveling is adopted only when a block tends to be unreliable. Because the mapping table is changed rarely, we can reduce the mounting time by record most portion of the mapping table on the flash drive. Furthermore, we can give short codes to the data blocks that erased frequently (because the locality) when recording the erase counts in a log structure. We observe that the algorithm can extend the lifetime of a flash drive and reduce mounting time in most situations.
mathematical_optimization	Driven by rapid development of cloud computing, virtualized environments are becoming popular in data center. Frequent communication among multiple virtual machines is required by a large amount of applications. Although many virtualization acceleration techniques have been proposed, the network performance is still a hot research topic due to the complicated and costly implementations of I/O virtualization mechanism. Some previous research focuses on improving the efficiency of communication among virtual machines in the same host. But studying how to accelerate cross-node virtual machine communication is also necessary. On the other hand, many high efficient, tight-coupling interconnects have been proposed as data center interconnects. They have advantages in performance and efficiency, while traditional Ethernet and InfiniBand have good scalability. However, these two kinds of interconnects can coexist very well. Tight-coupling protocol is suitable for connecting small-scale data center nodes, which we call super-node, while super-node is connected by traditional interconnect. In our opinion, data center with such hybrid interconnect architecture is one of important trends. Targeting the hybrid interconnect architecture, this paper proposes an efficient mechanism, named as TCNet (abbreviation for tight-coupling network), to accelerate cross-node virtual machine communication. To verify the acceleration mechanism, we build a prototype system which chooses PCIe (for inner-super-node interconnect) and Ethernet (for inter-super-node interconnect) as the hybrid interconnect and use KVM as software environments. We use several benchmarks to evaluate the mechanism. The latency of TCNet is 23% shorter than that of Gigabit Ethernet on average and the bandwidth is 1.14 times as large as that of Gigabit Ethernet on average. Besides, we use Specweb2006 to evaluate its web service ability. TCNet can support 20% more clients simultaneously than that of Ethernet and response requests 19% faster. The results demonstrate that TCNet has great potential to accelerate cross-node virtual machine communication for data center with hybrid interconnect.
mathematical_optimization	A log service provides efficient storage and retrieval of data that is written sequentially (append-only) and not subsequently modified. Application programs and subsystems use log services for recovery, to record security audit trails, and for performance monitoring. Ideally, a log service should accommodate very large, long-lived logs, and provide efficient retrieval and low space overhead.In this paper, we describe the design and implementation of the Clio log service. Clio provides the abstraction of log files: readable, append-only files that are accessed in the same way as conventional files. The underlying storage medium is required only to be append-only; more general types of write access are not necessary. We show how log files can be implemented efficiently and robustly on top of such storage media?in particular, write-once optical disk.In addition, we describe a general application software storage architecture that makes use of log files.This work was supported in part by the Defense Advanced Research Projects Agency under contracts N00039-84-C-0211 and N00039-86-K-0431, by National Science Foundation grant DCR-83-52048, and by Digital Equipment Corporation, Bell-Northern Research and AT&T Information Systems.
mathematical_optimization	Automated localization and detection of the optic disc (OD) is an essential step in the analysis of digital diabetic retinopathy systems. Accurate localization and detection of optic disc boundary is very useful in proliferative diabetic retinopathy where fragile vessels develop in the retina. In this paper, we propose an automated system for optic disk localization and detection. Our method localizes optic disk using average filter and thresholding, extracts the region of interest (ROI) containing optic disk to save time and detects the optic disk boundary using Hough transform. This method can be used in computerized analysis of retinal images, e.g., in automated screening for diabetic retinopathy. The technique is tested on publicly available DRIVE, STARE, diarectdb0 and diarectdb1 databases of manually labeled images which have been established to facilitate comparative studies on localization and detection of optic disk in retinal images. The proposed method achieves an average accuracy of 96.7% for localization and an average area under the receiver operating characteristic curve of 0.958 for optic detection.
mathematical_optimization	Although active networks have generated much debate in the research community, on the whole there has been little hard evidence to inform this debate. This paper aims to redress the situation by reporting what we have learned by designing, implementing and using the ANTS active network toolkit over the past two years. At this early stage, active networks remain an open research area. However, we believe that we have made substantial progress towards providing a more flexible network layer while at the same time addressing the performance and security concerns raised by the presence of mobile code in the network. In this paper, we argue our progress towards the original vision and the difficulties that we have not yet resolved in three areas that characterize a "pure" active network: the capsule model of programmability; the accessibility of that model to all users; and the applications that can be constructed in practice.
mathematical_optimization	Most operating systems provide protection and isolation to user processes, but not to critical system components such as device drivers or other system code. Consequently, failures in these components often lead to system failures. VirtuOS is an operating system that exploits a new method of decomposition to protect against such failures. VirtuOS exploits virtualization to isolate and protect vertical slices of existing OS kernels in separate service domains. Each service domain represents a partition of an existing kernel, which implements a subset of that kernel's functionality. Unlike competing solutions that merely isolate device drivers, or cannot protect from malicious and vulnerable code, VirtuOS provides full protection of isolated system components. VirtuOS's user library dispatches system calls directly to service domains using an exceptionless system call model, avoiding the cost of a system call trap in many cases. We have implemented a prototype based on the Linux kernel and Xen hypervisor. We demonstrate the viability of our approach by creating and evaluating a network and a storage service domain. Our prototype can survive the failure of individual service domains while outperforming alternative approaches such as isolated driver domains and even exceeding the performance of native Linux for some multithreaded workloads. Thus, VirtuOS may provide a suitable basis for kernel decomposition while retaining compatibility with existing applications and good performance.
mathematical_optimization	We analyzed the user-level file access patterns and caching behavior of the Sprite distributed file system. The first part of our analysis repeated a study done in 1985 of the: BSD UNIX file system. We found that file throughput has increased by a factor of 20 to an average of 8 Kbytes per second per active user over 10-minute intervals, and that the use of process migration for load sharing increased burst rates by another factor of six. Also, many more very large (multi-megabyte) files are in use today than in 1985. The second part of our analysis measured the behavior of Sprite's main-memory file caches. Client-level caches average about 7 Mbytes in size (about one-quarter to one-third of main memory) and filter out about 50% of the traffic between clients and servers. 35% of the remaining server traffic is caused by paging, even on workstations with large memories. We found that client cache consistency is needed to prevent stale data errors, but that it is not invoked often enough to degrade overall system performance.
mathematical_optimization	Input and output are often viewed as complementary operations, and it is certainly true that the direction of data flow during input is the reverse of that during output. However, in a conventional operating system, the direction of control flow is the same for both input and output: the program plays the active role, while the operating system transput primitives are always passive. Thus there are four primitive transput operations, not two: the corresponding pairs are passive input and active output, and active input and passive output. This paper explores the implications of this idea in the context of an object oriented operating system. This work is supported in part by the National Science Foundation under Grant No. MCS-8004111. Computing equipment and technical support are provided in part under a cooperative research agreement with Digital Equipment Corporation.
mathematical_optimization	This study addresses the robot that waits for users while they shop. In order to wait, the robot needs to understand which locations are appropriate for waiting. We investigated how people choose locations for waiting, and revealed that they are concerned with "disturbing pedestrians" and "disturbing shop activities". Using these criteria, we developed a classifier of waiting locations. "Disturbing pedestrians" are estimated from statistics of pedestrian trajectories, which is observed with a human-tracking system based on laser range finders. "Disturbing shop activities" are estimated based on shop visibility. We evaluated this autonomous waiting behavior in a shopping-assist scenario. The experimental results revealed that users found the autonomous waiting robot chose appropriate waiting locations for waiting more than a robot with random choice or one controlled manually by the user him or herself.
mathematical_optimization	This paper presents a methodology to identifying a facial expression of human being based on the information theory approach of coding. This task is consists of two major phases: Extraction of appropriate facial features and consequent recognition of the user's emotional state that can be robust to facial expression variations among different users is the topic of this paper. 1) Identifying maximum matching face from database. 2) Extracting a facial expression from matched image. First phase consist of feature extraction using Principal Component analysis & face recognition using feed forward back propagation Neural Network with the use of eigen vector for calculating eigen values of images. The architecture considered for implementation is a Neuro-Fuzzy system with concepts of artificial intelligence. The approach chosen for the implementation is Soft Computing which is basically a synergistic integration computing paradigms: neural networks, fuzzy logic to provide a flexible framework to construct computationally intelligent systems. A Evolving Emotional Intelligence may contains simulated emotions, including sadness, joy, anger, fear, hope, relief, disappointment, gratitude, pride, shame, love and hate. Research on human psychology had long considered the notion of an emotion (e.g., happy) to be a matter of degree; however, Using fuzzy modeling proved to produce a more representative picture of the emotional process. By testing the above task over 1000 to 10000 images including both color, grayscale images of same & different human faces & may get the 80 to 90% accurate result.
mathematical_optimization	This paper focuses on different aspects of Virtualization that will help the IT & development organization to become more strategic to their business. The Dynamic IT strategy helps businesses enhance the capability and efficiency of their people, processes, and IT infrastructure. Virtualization is a key component of Dynamic IT. It delivers an environment that helps organizations anticipate and respond to the ever changing business challenges and opportunities, and is a key enabler for making IT more dynamic and efficient.
mathematical_optimization	Anomaly detection involves identifying observations that deviate from the normal behavior of a system. One of the ways to achieve this is by identifying the phenomena that characterize �normal� observations. Subsequently, based on the characteristics of data learned from the �normal� observations, new observations are classified as being either �normal� or not. Most state-of-the-art approaches, especially those which belong to the family of parameterized statistical schemes, work under the assumption that the underlying distributions of the observations are stationary. That is, they assume that the distributions that are learned during the training (or learning) phase, though unknown, are not time-varying. They further assume that the same distributions are relevant even as new observations are encountered. Although such a �stationarity� assumption is relevant for many applications, there are some anomaly detection problems where stationarity cannot be assumed. For example, in network monitoring, the patterns which are learned to represent normal behavior may change over time due to several factors such as network infrastructure expansion, new services, growth of user population, and so on. Similarly, in meteorology, identifying anomalous temperature patterns involves taking into account seasonal changes of normal observations. Detecting anomalies or outliers under these circumstances introduces several challenges. Indeed, the ability to adapt to changes in nonstationary environments is necessary so that anomalous observations can be identified even with changes in what would otherwise be classified as �normal� behavior. In this article we propose to apply a family of weak estimators for anomaly detection in dynamic environments. In particular, we apply this theory to spam email detection. Our experimental results demonstrate that our proposal is both feasible and effective for the detection of such anomalous emails.
mathematical_optimization	The Domain Name System (DNS) allows clients to use resolvers, sometimes called caches, to query a set of authoritative servers to translate host names into IP addresses. Prior work has proposed using the interaction between these DNS resolvers and the authoritative servers as an access control mechanism. However, while prior work has examined the DNS from many angles, the resolver component has received little scrutiny. Essential factors for using a resolver in an access control system, such as whether a resolver is part of an ISP�s infrastructure or running on an end-user�s system, have not been examined. In this study, we examine DNS resolver behavior and usage, from query patterns and reactions to nonstandard responses to passive association techniques to pair resolvers with their client hosts. In doing so, we discover evidence of security protocol support, misconfigured resolvers, techniques to fingerprint resolvers, and features for detecting automated clients. These measurements can influence the implementation and design of these resolvers and DNS-based access control systems.
mathematical_optimization	Firewalls are the cornerstones of the security infrastructure for most enterprises. They have been widely deployed for protecting private networks. The quality of the protection provided by a firewall directly depends on the quality of its policy (i.e., configuration). Due to the lack of tools for analyzing firewall policies, many firewalls used today have policy errors. A firewall policy error either creates security holes that will allow malicious traffic to sneak into a private network or blocks legitimate traffic and disrupts normal business processes, which in turn could lead to irreparable, if not tragic, consequences. A major cause of policy errors are policy changes. Firewall policies often need to be changed as networks evolve and new threats emerge. Users behind a firewall often request the firewall administrator to modify rules to allow or protect the operation of some services. In this article, we first present the theory and algorithms for firewall policy change-impact analysis. Our algorithms take as input a firewall policy and a proposed change, then output the accurate impact of the change. Thus, a firewall administrator can verify a proposed change before committing it. We implemented our firewall change-impact analysis algorithms, and tested them on both real-life and synthetic firewall policies. The experimental results show that our algorithms are effective in terms of ensuring firewall policy correctness and efficient in terms of computing the impact of policy changes. Thus, our tool can be practically used in the iterative process of firewall policy design and maintenance. Although the focus of this article is on firewalls, the change-impact analysis algorithms proposed in this article are not limited to firewalls. Rather, they can be applied to other rule-based systems, such as router access control lists (ACLs), as well.
mathematical_optimization	In this article we present a distributed system that stores name-to-address bindings and provides name resolution to a network of computers. This name system consists of a network of name services that are individually self-configuring and self-administering. The name service consists of an agent program that works in conjunction with the current implementation of the Domain Name System (DNS) program. The DNS agent program automatically configures the Berkeley Internet Name Domain (BIND) process during start-up and dynamically reconfigures and administers the BIND process based on the changing state of the network. The proposed name system offers high scalability and fault-tolerance capabilities and communicates using standard Internet protocols.
mathematical_optimization	Pedestrian detection requires both reliable performance and fast processing. Stereo-based pedestrian detectors meet these requirements due to a hypotheses generation processing. However, noisy depth images increase the difficulty of robustly estimating the road line in various road environments. This problem results in inaccurate candidate bounding boxes and complicates the correct classification of the bounding boxes. In this letter, we propose a dynamic ground plane estimation method to manage this problem. Our approach estimates the ground plane optimally using a posterior probability that combines a prior probability and several uncertain observations due to cluttered road environments. Our approach estimates a ground plane optimally using a posterior probability which combines a prior probability and several uncertain observations due to cluttered road environments. The experimental results demonstrate that the proposed method estimates the ground plane robustly and accurately in noisy depth images and also a stereo-based pedestrian detector using the proposed method outperforms previous state-of-the art detectors with less complexity.
mathematical_optimization	There is a continuous struggle for control of resources at every organization that is connected to the Internet. The local organization wishes to use its resources to achieve strategic goals. Some external entities seek direct control of these resources, for purposes such as spamming or launching denial-of-service attacks. Other external entities seek indirect control of assets (e.g., users, finances), but provide services in exchange for them. Using a year-long trace from an edge network, we examine what various external organizations know about one organization. We compare the types of information exposed by or to external organizations using either active (reconnaissance) or passive (surveillance) techniques. We also explore the direct and indirect control external entities have on local IT resources.
mathematical_optimization	Traffic management practices of ISPs are an issue of public concern. We propose a framework for classification of traffic management practices as reasonable or unreasonable. We present a survey of traffic management techniques and examples of how these techniques are used by ISPs. We suggest that whether a traffic management practice is reasonable rests on the answers to four questions regarding the techniques and practices used. We propose a framework that classifies techniques as unreasonable if they are unreasonably anticompetitive, cause undue harm to consumers, or unreasonably impair free speech. We propose alternatives to unreasonable or borderline congestion management practices.
mathematical_optimization	While subjective measurements are the most natural for assessing the user-perceived quality of a media stream, there are issues with their scalability and their context accuracy. We explore techniques to select application-layer measurements, collected by an instrumented media player, that most accurately predict the subjective quality rating that a user would assign to a stream. We consider three feature subset selection techniques that reduce the number of features (measurements) under consideration to ones most relevant to user-perceived stream quality. Two of the three techniques mathematically consider stream characteristics when selecting measurements, while the third is based on observation. We apply the reduced feature sets to two nearest-neighbor algorithms for predicting user-perceived stream quality. Our results demonstrate that there are clear strategies for estimating the quality rating that work well in specific circumstances such as video-on-demand services. The results also demonstrate that neither of the mathematically-based feature subset selection techniques identify a single set of features that is unambiguously influential on user-perceived stream quality, but that ultimately a combination of retransmitted and/or lost application-layer packets is most accurate for predicting stream quality.
mathematical_optimization	Emerging cloud services, including mobile offices, Web-based storage services, and content delivery services, run diverse workloads under various device platforms, networks, and cloud service providers. They have been realized on top of SSL/TLS, which is the de facto protocol for end-to-end secure communication over the Internet. In an attempt to achieve a cognitive SSL/TLS with heterogeneous environments (device, network, and cloud) and workload awareness, we thoroughly analyze SSL/TLS-based data communication and identify three critical mismatches in a conventional SSL/TLS-based data transmission. The first mismatch is the performance of loosely coupled encryption-compression and communication routines that lead to underutilized computation and communication resources. The second mismatch is that the conventional SSL/TLS only provides a static compression mode, irrespective of the dynamically changing status of each SSL/TLS connection and the computing power gap between the cloud service provider and diverse device platforms. The third is the memory allocation overhead due to frequent compression switching in the SSL/TLS. As a remedy to these rudimentary operations, we present a system called an Adaptive Cryptography Plugged Compression Network (ACCENT) for SSL/TLS-based cloud services. It is comprised of the following three novel mechanisms, each of which aims to provide an optimal SSL/TLS communication and maximize the network transfer performance of an SSL/TLS protocol stack: tightly-coupled threaded SSL/TLS coding, floating scale-based adaptive compression negotiation, and unified memory allocation for seamless compression switching. We implemented and tested the mechanisms in OpenSSL-1.0.0. ACCENT is integrated into the Web-interface layer and SSL/TLS-based secure storage service within a real cloud computing service, called iCubeCloud, as the key primitive for SSL/TLS-based data delivery over the Internet.
mathematical_optimization	We analyze UK and US experiences as they relate to two central net neutrality questions: (1) whether competition serves as a deterrent to the discriminatory treatment of Internet traffic, and (2) whether discrimination creates barriers to application development and innovation. Relying on consumer switching behavior to provide more comprehensive competitive discipline was insufficient for a variety of reasons, including the presence of switching costs. The process of correcting errors in the technology used for application-specific management revealed that such management creates costs for application developers and innovators, regardless of whether their products are targeted for traffic management.
mathematical_optimization	Trust is a central component of social interactions among humans. Many applications motivate the consideration of trust evaluation in online social networks (OSNs). Some work has been proposed based on a trusted graph. However, it is still an open challenge to construct a trusted graph, especially in terms of selecting proper recommenders, which can be used to predict the trustworthiness of an unknown target efficiently and effectively. Based on the intuition that people who are close to and influential to us can make more proper and acceptable recommendations, we present the idea of recommendation-aware trust evaluation (RATE). We further model the recommender selection problem as an optimization problem, with the objectives of higher accuracy, lower risk (uncertainty), and lower cost. Four metrics: trustworthiness, expertise, uncertainty, and cost, are identified to measure and adjust the quality of recommenders. We focus on a 1-hop recommender selection, for which we propose the FluidTrust model to better illustrate the trust--decision making process of a user. We also discuss the extension of multihop scenarios and multitarget scenarios. Experimental results, with the real social network datasets of Epinions and Advogato, validate the effectiveness of RATE: it can predict trust with higher accuracy (it gains about 20% higher accuracy in Epinions), lower risk, and less cost (about a 30% improvement).
mathematical_optimization	Net neutrality represents the idea that Internet users are entitled to service that does not discriminate on the basis of source, destination, or ownership of Internet traffic. The United States Congress is considering legislation on net neutrality, and debate over the issue has generated intense lobbying. Congressional action will substantially affect the evolution of the Internet and of future Internet research. In this article, we argue that neither the pro nor anti net neutrality positions are consistent with the philosophy of Internet architecture. We develop a net neutrality policy founded on a segmentation of Internet services into infrastructure services and application services, based on the Internet's layered architecture. Our net neutrality policy restricts an Internet service Provider's ability to engage in anticompetitive behavior while simultaneously ensuring that it can use desirable forms of network management. We illustrate the effect of this policy by discussing acceptable and unacceptable uses of network management.
mathematical_optimization	This paper presents a novel convolution neural network for classifying the orientation (or viewpoint) of a vehicle in a given image. Current equipping sensors in self-driving car is able to produce bounding box of vehicles in the proximity, but it does not recognize the viewpoint of them. Analyzing surrounding cars' direction in very complex environment has a significant role for autonomous driving. Utilizing nothing but a captured image, the purpose of this research is to classify viewpoint of vehicle: (1) front; (2) rear; (3) side; (4) front-side; and (5) rear-side. Deep convolutional neural network is used as the tool in performing classification task. The approach involves examining different CNN architectures using a large scale car dataset. In addition to that, the goal of the model is to be small and fast enough for limited hardware resource. We are able to achieve 95% accuracy, 57ms inference time on Nvidia GRID K520 GPU, and 1.6 MB Caffe model size.
mathematical_optimization	The Internet of Things (IoT) has gained worldwide attention in recent years. It transforms the everyday objects that surround us into proactive actors of the Internet, generating and consuming information. An important issue related to the appearance of such a large-scale self-coordinating IoT is the reliability and the collaboration between the objects in the presence of environmental hazards. High failure rates lead to significant loss of data. Therefore, data survivability is a main challenge of the IoT. In this article, we have developed a compartmental e-Epidemic SIR (Susceptible-Infectious-Recovered) model to save the data in the network and let it survive after attacks. Furthermore, our model takes into account the dynamic topology of the network where natural death (crashing nodes) and birth are defined and analyzed. Theoretical methods and simulations are employed to solve and simulate the system of equations developed and to analyze the model.
mathematical_optimization	This paper describe briefly how the Ad hoc On-demand Distance Vector (AODV) routing protocol can be used for internetworking between wireless ad hoc networks and the IPv6 Internet. This solution relies on the signalling of AODV to find an access providing Internet Gateway that is able to distribute a globally routable prefix for the ad hoc network. We have tried to evaluate this solution by means of simulation in order to stress the design and find areas that need improvement. We have observed that a critical factor in this design is how to determine whether the node is located in the ad hoc network, or located on the Internet. In addition to these design considerations we have also investigated common performance metrics such as end-to-end delay, delivery ratio and routing overhead and verified that these are not affected in a negative way by the design.
mathematical_optimization	Location-enhanced content while useful for mobile users can also be harmful for their privacy. We present feasibility analysis and preliminary evaluation of Cach�, an approach for preserving mobile users' privacy by caching location-enhanced content in advance.
mathematical_optimization	This paper proposes a new QoS-aware medium access control (MAC) protocol in mobile ad hoc networks (MANETs). This takes the unique challenges of MANETs into consideration, and works in conjunction with the location-based forwarding strategy. This novel protocol is based on the legacy IEEE 802.11, and thus can be relatively easily integrated into existing systems. It is adaptive and network-aware depending on the type and intensity of traffic, and relative mobility patterns of nodes. In addition, it makes use of the point-coordination-function (PCF) of IEEE 802.11 in a distributed fashion for the first time in multihop MANETs. Our strategy enables two-way admission control for improved performance, whereby the forwarder-node selection algorithm allows previous hop nodes to perform implicit admission control using locally available information, while a selected forwarder-node performs explicit admission control depending on its current load. Analytical results confirm the performance improvement of our strategy.
mathematical_optimization	IEEE 802.11-based Enterprise WMNs (Wireless Mesh Networks) is one of possible practical extensions of WMNs. However, the wide adoption of Enterprise WMNs is limited by the flow unfairness problem due to the complicated interference relationship between adjacent nodes. To overcome this limitation, we propose FlowNet that centrally coordinates flows to implement a centralized source rate control algorithm in the Enterprise WMNs. By the cooperative operation between flow signalling and flow balancing methods, FlowNet provides proportional fairness among multiple flows in the Enterprise WMNs.
mathematical_optimization	The movement pattern of mobile users plays an important role in performance analysis of wireless computer and communication networks. In this paper, we first give an overview and classification of mobility models used for simulation-based studies. Then, we present an enhanced random mobility model, which makes the movement trace of mobile stations more realistic than common approaches for random mobility. Our movement concept is based on random processes for speed and direction control in which the new values are correlated to previous ones. Upon a speed change event, a new target speed is chosen, and an acceleration is set to achieve this target speed. The principles for direction changes are similar. Finally, we discuss strategies for the stations' border behavior (i.e., what happens when nodes move out of the simulation area) and show the effects of certain border behaviors and mobility models on the spatial user distribution.
mathematical_optimization	We are interested in the sensor networks for scientific applications to cover and measure statistics on the sea surface. Due to flows and waves, the sensor nodes may gradually lose their positions; leaving the points of interest uncovered. Manual readjustment is costly and cannot be performed in time. We argue that a network of mobile sensor nodes which can perform self-adjustment is the best candidate to maintain the coverage of the surface area. A key observation of our scheme is that the motion of the flows is not only a curse but should also be considered as a fortune. The sensor nodes can be pushed by free to some locations that potentially improve the overall coverage. To this end, we present a dominating set maintenance scheme to maximally exploit the uncontrollable mobility and balance the energy consumption among all the sensor nodes. We proved that the coverage is guaranteed in our scheme. The simulation demonstrates that the network lifetime can be significantly extended, compared to a straight forward back-to-original reposition scheme.
mathematical_optimization	Dynamic spectrum access has been studied to exploit instantaneous spectrum availability by opening licensed spectrum to secondary users. To achieve high spectrum efficiency, secondary unlicensed users need to continuously sense spectrum to detect the presence of primary licensed users. Cooperative spectrum sensing has been recognized as a powerful solution to improve spectrum sensing performance, which requires nearby wireless nodes to share sensing results with each other. However, information sharing is achieved through broadcasting in wireless networks, which can provide free-riding opportunity for selfish nodes. Selfish nodes can benefit from receiving the sensing results from its neighbors by free without sharing. Therefore, appropriate strategies are essential to enforce and sustain the cooperation among neighboring nodes. In this paper we model cooperative spectrum sensing as an N-player horizontal infinite game, and study varies strategies for it. In wireless networks, the frequently occurred collisions make the cooperation enforcement problem quite challenging as it is hard to tell whether the information lost is due to nodes' selfishness or wireless collision. In this paper, we prove that Grim Trigger strategy, a classical strategy to stimulate cooperation in an infinite game, can result in poor performance due to random errors. We then propose a strategy basing on Carrot-and-Stick strategy, which can recover cooperation among multiple players from deviation. We prove that if nodes are sufficiently far-sight, or equivalently the entire system runs sufficiently long, the Nash Equilibrium of the proposed strategy for spectrum sensing game is still mutual cooperation, even under collision situation. We also prove that the proposed strategy is robust to collisions and colluding cheat.
mathematical_optimization	The performance of the semantic concept detection method depends on, the selection of the low-level visual features used to represent key-frames of a shot and the selection of the feature-fusion method. This paper proposes a set of low-level visual features of considerably smaller size and also proposes novel 'hybrid-fusion' and 'mixed-hybrid-fusion' approaches which are formulated by combining contemporary early and late-fusion strategies. In the proposed hybrid-fusion approach, the features from the same feature group are combined using early-fusion before classifier training; and the concept probability scores from multiple classifiers are merged using late-fusion approach, to get final detection scores. A feature group is defined as the features from the same feature family like color moments. The hybrid-fusion approach is refined and the 'mixed-hybrid-fusion' approach is proposed additionally to further improve the detection rate. Neural Network is used to build classifiers that produce concept probabilities for a test frame. The proposed approaches are evaluated on TRECVID development dataset which contains multi-labeled key-frames. Results show that, the proposed approaches outperform early-fusion and late-fusion approaches by large margins with respect to feature set dimensionality and mean Average Precision (mAP) values.
programming_language	Intelligent Transport Systems (ITS) are becoming a reality, driven by navigation safety requirements and by the investments of car manufacturers and Public Transport Authorities all around the world. ITS make it possible to imagine a future in which cars will be able to foresee and avoid collisions, navigate the quickest route to their destination, making use of up-to-the minute traffic reports, identify the nearest available parking slot and minimize their carbon emissions. Also demand for voice, data and multimedia services, while moving in car increase the importance of broadband wireless systems [1]. Efforts are being imparted towards the convergence of mobile communications, computing and remote sensing. Spread spectrum based digital RADAR can be utilized as a remote sensing device in ITS. This paper reviews the development of DSSS (Direct Sequence Spread Spectrum) based digital RADAR. It is quite capable of detecting target in the open field. The experiments carried out for different standard target like flat plates, spheres etc. are also reviewed. The operational digital RADAR is capable of rejecting interference, but fails in a strong multipath scenario. Again RAKE processing is established in communication. The approach is implementing RAKE processing at the RADAR receiver to exploit multipath.
programming_language	Collaborative Filtering (CF) based recommender systems often suffer from the sparsity problem, particularly for new and inactive users when they use the system. The emerging trend of social networking sites and their accommodation in other sites like e-commerce can potentially help alleviate the sparsity problem with their provided social relation data. In this paper, we have particularly explored a new kind of social relation, the membership, and its combined effect with friendship. The two type of heterogeneous social relations are fused into the CF recommender via a factorization process. Due to the two relations' respective properties, we adopt different fusion strategies: regularization was leveraged for friendship and collective matrix factorization (CMF) was proposed for incorporating membership. We further developed a unified model to combine the two relations together and tested it with real large-scale datasets at five sparsity levels. The experiment has not only revealed the significant effect of the two relations, especially the membership, in augmenting recommendation accuracy in the sparse data condition, but also identified the ability of our fusing model in achieving the desired fusion performance.
programming_language	Implicit feedback is a key source of information for many recommendation and personalization approaches. However, using it typically requires multiple episodes of interaction and roundtrips to a recommendation engine. This adds latency and neglects the opportunity of immediate personalization for a user while the user is navigating recommendations. We propose a novel strategy to address the above problem in a principled manner. The key insight is that as we observe a user's interactions, it reveals much more information about her desires. We exploit this by inferring the within-session user intent on-the-fly based on navigation interactions, since they offer valuable clues into a user's current state of mind. Using navigation patterns and adapting recommendations in real-time creates an opportunity to provide more accurate recommendations. By prefetching a larger amount of content, this can be carried out entirely in the client (such as a browser) without added latency. We define a new Bayesian model with an efficient inference algorithm. We demonstrate significant improvements with this novel approach on a real-world, large-scale dataset from Netflix on the problem of adapting the recommendations on a user's homepage.
programming_language	For the emergency medical service a reliable sharing and transmitting of medical data and electronic patient records between the ambulance and the hospital is of great importance for the quality of patient care. During the last years new mobile technologies have evolved with much higher bandwidths than before. Nevertheless, the available network bandwidth strongly varies from area to area. In this paper we present an approach to investigate the predictability of the mobile network performance based on the statistical evaluation of the gathered transmission data on frequently used routes. This allows one to pro-actively adapt the available bandwidth to improve the utilization of the mobile network capacity and achieve a more reliable transmission of the medical data. We performed data rate measurements on different routes and derived predictions of the available mobile resources. As a first approach, the predictions are based on the average of former measurements, but taking into account the current position and temporal variation of the mobile resources. Compared with the results obtained through the straightforward reference approach our predictions are more accurate and precise. Consequently, our empirical studies confirm that despite high variations of the available wireless bandwidth reasonable predictions for known routes are possible.
programming_language	In this paper we present an ongoing research to develop a distributed reinforcement learning approach for mission survivability that combines two basic strategies for mission resilience: a) mission decomposition and distribution with replication of critical components, and b) differential task allocation based on estimated level of threat. Level of threat is estimated from a locally perceived attack, or the possibility of an attack, based on threat information that is shared between similar nodes.
programming_language	Today XML has reached a wide acceptance as the data exchange format for e-commerce. Unfortunately, XML covers the syntactic level, but lacks semantics. Ontology can represent shared domain knowledge and enable semantic interoperability. Therefore, in this paper, we propose an approach for representing and reasoning on XML with ontologies. The formal definitions of XML Schemas and ontologies are given. On this basis, for representing XML with ontologies, we propose an approach which can translate the XML Schema into the ontology. Based on the constructed ontologies, we study how to reason on XML by means of ontologies, so that the reasoning problems of XML (e.g., conformance, inclusion, and equivalence) may be reasoned through the reasoning mechanism of ontologies.
programming_language	In this paper we view interactive constraint acquisition as the process of learning constraints from examples and focus on the roles played by both the user and the system during an interactive session. We consider our user as a teacher who provides positive examples to an automated constraint acquisition system. Each positive example represents a solution to the target constraint network we are trying to acquire. In this paper we compare a number of ways in which users can choose examples to be presented to a constraint acquisition system and identify the best strategy for the user to adopt. We recognize that not every user will naturally be able to assume the best profile and therefore present an assistant that can help a user construct good examples. We show that the assistant helps, in a significant manner, a human user trying to describe a target constraint network using a very small number of examples.
programming_language	OWL ontologies are nowadays a quite popular way to describe structured knowledge in terms of classes, relations among classes and class instances. In this paper, given an OWL target class T, we address the problem of inducing EL(D) concept descriptions that describe sufficient conditions for being an individual instance of T. To do so, we use a Foil-based method with a probabilistic candidate ensemble estimation. We illustrate its effectiveness by means of an experimentation.
programming_language	matcont is a matlab continuation package for the interactive numerical study of a range of parameterized nonlinear dynamical systems, in particular ODEs, that allows to compute curves of equilibria, limit points, Hopf points, limit cycles, flip, fold and torus bifurcation points of limit cycles. It is now possible to continue homoclinic-to-hyperbolic-saddle and homoclinic-to-saddle-node orbits in matcont. The implementation is done using the continuation of invariant subspaces, with the Riccati equations included in the defining system. A key feature is the possibility to initiate both types of homoclinic orbits interactively, starting from an equilibrium point and using a homotopy method. All known codimension-two homoclinic bifurcations are tested for during continuation. The test functions for inclination-flip bifurcations are implemented in a new and more efficient way. Heteroclinic orbits can now also be continued and an analogous homotopy method can be used for the initialization.
programming_language	This article describes an efficient and robust algorithm and implementation for the evaluation of the Wright ? function in IEEE double precision arithmetic over the complex plane.
programming_language	Analysing changes of the behaviour of an occupant who lives in an Ambient Intelligence (AmI) environment is addressed in this paper. Changes in Activities of Daily Living (ADL) are indicators of the social and health status of the occupant. This research therefore aims to identify trends in ADL and interpret them in a suitable form for carers. It is essential for this purpose to have access to relatively long-term monitoring data of the occupant using appropriate sensory devices. Different trend analysis techniques are investigated and compared. These techniques include; Seasonal Kendall Test (SKT), Simple Moving Mean Average (SMA), and Exponentially Weighted Moving Average (EWMA), which are used to detect trends in the time-series data representing occupancy duration in different areas of a home environment for an elderly person living independently.
programming_language	Direct methods for solving large sparse linear systems of equations are popular because of their generality and robustness. Their main weakness is that the memory they require usually increases rapidly with problem size. We discuss the design and development of the first release of a new symmetric direct solver that aims to circumvent this limitation by allowing the system matrix, intermediate data, and the matrix factors to be stored externally. The code, which is written in Fortran and called HSL_MA77, implements a multifrontal algorithm. The first release is for positive-definite systems and performs a Cholesky factorization. Special attention is paid to the use of efficient dense linear algebra kernel codes that handle the full-matrix operations on the frontal matrix and to the input/output operations. The input/output operations are performed using a separate package that provides a virtual-memory system and allows the data to be spread over many files; for very large problems these may be held on more than one device. Numerical results are presented for a collection of 30 large real-world problems, all of which were solved successfully.
programming_language	At the heart of a frontal or multifrontal solver for the solution of sparse symmetric sets of linear equations, there is the need to partially factorize dense matrices (the frontal matrices) and to be able to use their factorizations in subsequent forward and backward substitutions. For a large problem, packing (holding only the lower or upper triangular part) is important to save memory. It has long been recognized that blocking is the key to efficiency and this has become particularly relevant on modern hardware. For stability in the indefinite case, the use of interchanges and 2 � 2 pivots as well as 1 � 1 pivots is equally well established. In this article, the challenge of using these three ideas (packing, blocking, and pivoting) together is addressed to achieve stable factorizations of large real-world symmetric indefinite problems with good execution speed. The ideas are not restricted to frontal and multifrontal solvers and are applicable whenever partial or complete factorizations of dense symmetric indefinite matrices are needed.
programming_language	An implementation using systolic array logic of Aitken's method of iterated interpolation is described. The proposed design has a simple, linear topology, requires no clock, and makes only modest demands on the host computer. By overlapping the computation of successive function values, a processing element utilization of approximately 1/2 is achieved. The paper illustrates how �mathematical hardware� packages, as well as software library routines, may be part of the mathematical problem solver's tool kit in the future.
programming_language	A SemiDefinite Programming (SDP) problem is one of the most central problems in mathematical optimization. SDP provides an effective computation framework for many research fields. Some applications, however, require solving a large-scale SDP whose size exceeds the capacity of a single processor both in terms of computation time and available memory. SDPARA (SemiDefinite Programming Algorithm paRAllel package) [Yamashita et al. 2003b] was designed to solve such large-scale SDPs. Its parallel performance is outstanding for general SDPs in most cases. However, the parallel implementation is less successful for some sparse SDPs obtained from applications such as Polynomial Optimization Problems (POPs) or Sensor Network Localization (SNL) problems, since this version of SDPARA cannot directly handle sparse Schur Complement Matrices (SCMs). In this article we improve SDPARA by focusing on the sparsity of the SCM and we propose a new parallel implementation using the formula-cost-based distribution along with a replacement of the dense Cholesky factorization. We verify numerically that these features are key to solving SDPs with sparse SCMs more quickly on parallel computing systems. The performance is further enhanced by multithreading and the new SDPARA attains considerable scalability in general. It also finds solutions for extremely large-scale SDPs arising from POPs which cannot be obtained by other solvers.
programming_language	The performance of a sparse direct solver is dependent upon the pivot sequence that is chosen before the factorization begins. In the case of symmetric indefinite systems, it may be necessary to modify this sequence during the factorization to ensure numerical stability. These modifications can have serious consequences in terms of time as well as the memory and flops required for the factorization and subsequent solves. This study focuses on hard-to-solve sparse symmetric indefinite problems for which standard threshold partial pivoting leads to significant modifications. We perform a detailed review of pivoting strategies that are aimed at reducing the modifications without compromising numerical stability. Extensive numerical experiments are performed on a set of tough problems arising from practical applications. Based on our findings, we make recommendations on which strategy to use and, in particular, a matching-based approach is recommended for numerically challenging problems.
programming_language	XML (Extensible Mark-up Language) is a well established format which is often used for modeling of semi-structured data. XPath and XQuery are de facto standards among XML query languages and searching for occurrences of a twig pattern query (TPQ) in an XML document is one of their core tasks. There is a large number of different approaches addressing the TPQ matching problem. The aim of this article is to compare the state-of-the-art techniques and give an overview which can help to understand the relationships between different methodologies used in this area. We distinguish three main areas of a TPQ processing: (1) index data structures and XML document partitioning, (2) join algorithms, and (3) cost-based optimizations. We cover the most important techniques in each area and explain their relationships and possible combinations.
programming_language	In many applications, it is convenient to substitute a large data graph with a smaller homomorphic graph. This paper investigates approaches for summarising massive data graphs. In general, massive data graphs are processed using a shared-nothing infrastructure such as MapReduce. However, accurate graph summarisation algorithms are suboptimal for this kind of environment as they require multiple iterations over the data graph. We investigate approximate graph summarisation algorithms that are efficient to compute in a shared-nothing infrastructure. We define a quality assessment model of a summary with regards to a gold standard summary. We evaluate over several datasets the trade-offs between efficiency and precision of the algorithms. With regards to an application, experiments highlight the need to trade-off the precision and volume of a graph summary with the complexity of a summarisation technique.
programming_language	The Integrated Intelligent Computer-Assisted Language Learning (iiCALL) environment enables language learning within common working environment like Web browsers or e-mail clients. Based on a generic data model, we developed a software framework for generic and transparent information exchange within those environments. The applicability of both is proven empirically by implementation of a corresponding prototype. The software framework constitutes a major improvement compared to current research. It enables a decoupled realization of individual systems within an iiCALL environment, while still ensuring interoperability during runtime.
programming_language	In this paper, we present a new approach for extracting the high quality (HQ) parallel corpora from multilingual resources. The original of our research compared to the previous works is the approach for gaining HQ data using for the Machine Translation domain. Almost previous approaches allowed to quickly acquire raw corpora, but not allow to gain HQ data. Our approach is a semi-automatic process including in a serial of steps that can automatically detect and download good multilingual Websites and parallel web pages to construct parallel corpora whose quality is well validated, revised, and enhanced collaboratively.
programming_language	User's query sometimes does not fully reflect the semantics, so the information retrieval systems return no results as expected results, it is necessary to add semantics to the user's query. The paper represents how to add semantics into a query in Vietnamese language based on Ontology of Object Member Property (OOMP) by improved Similar Noun Phrase Expansion (iSNPE) algorithm.
programming_language	The Integrated Intelligent Computer-Assisted Language Learning (iiCALL) environment offers options to learn natural languages with the use of common working environments like Web browsers or e-mail clients. Therefore, we designed a generic data model and developed a software framework handling language learning processes and information exchanges. A corresponding prototype, implemented as Firefox plug-in, shows the applicability of the generic data model to a specific learning scenario within an iiCALL environment. For developers, the paper proves extensibility by describing the framework and the way of extending iiCALL to add new learning scenarios and functionalities.
programming_language	The Integrated Intelligent Computer-Assisted Language Learning (iiCALL) environment is a Web-based e-learning platform that enables language learning in common working environments like Web browsers or email clients. Based on learning theories, it offers context-related learning scenarios for different learning types and different levels of learners. So far, the prototype implements the server architecture which runs inside an Apache Tomcat using Hibernate and MySQL for persistence purposes and which uses the General Architecture for Text Engineering (GATE) framework for Natural Language Processing (NLP) tasks. The prototype also implements a client as a Mozilla Firefox extension by using the XML User Interface Language (XUL) and JavaScript. It exemplarily shows a context-related vocabulary trainer, a learning scenario for language learners on a beginner's level. The paper explains technology of the prototype and points out open issues and future work.
programming_language	Government agencies must often quickly organize and analyze large amounts of textual information, for example comments received as part of notice and comment rulemaking. Hierarchical organization is popular because it represents information at different levels of detail and is convenient for interactive browsing. Good hierarchical clustering algorithms are available, but there are few good solutions for automatically labeling the nodes in a cluster hierarchy.This paper presents a simple algorithm that automatically assigns labels to hierarchical clusters. The algorithm evaluates candidate labels using information from the cluster, the parent cluster, and corpus statistics. A trainable threshold enables the algorithm to assign just a few high-quality labels to each cluster. Experiments with Open Directory Project (ODP) hierarchies indicate that the algorithm creates cluster labels that are similar to labels created by ODP editors.
programming_language	The linguistic modes during its acquisition are identified as complementary pairs (King & Quigley, 1985; Marschark, 1993; Rondal, 1980). These pairs are named active and reactive based depending on its functional relationship. The acquisition of the reactive modes precedes the active one and it is necessary to give feedback to the precision and efficiency of the active modes. The pairs gesticulating/pointing-observing, writing-reading and speaking-listening can therefore, be identified. The reactive modes represent functions of the individual such as reader listener and observer. The active modes work as mediators of other modes. They represent the individual's actions such as: pointer/gesticulator, speaker and writer (Ribes, 1990; G�mez Fuentes, & Ribes, 2008).
programming_language	OpenFlow networks require installation of flow rules in a limited capacity switch memory (Ternary Content Addressable Memory or TCAMs, in particular) from a logically centralized controller. A controller can manage the switch memory in an OpenFlow network through events that are generated by the switch at discrete time intervals. Recent studies have shown that data centers can have up to 10,000 network flows per second per server rack today. Increasing the TCAM size to accommodate these large number of flow rules is not a viable solution since TCAM is costly and power hungry. Current OpenFlow controllers handle this issue by installing flow rules with a default idle timeout after which the switch automatically evicts the rule from its TCAM. This results in inefficient usage of switch memory for short lived flows when the timeout is too high and in increased controller workload for frequent flows when the timeout is too low. In this context, we present SmartTime - an OpenFlow controller system that combines an adaptive timeout heuristic to compute efficient idle timeouts with proactive eviction of flow rules, which results in effective utilization of TCAM space while ensuring that TCAM misses (or controller load) does not increase. To the best of our knowledge, SmartTime is the first real implementation of an intelligent flow management strategy in an OpenFlow controller that can be deployed in current OpenFlow networks. In our experiments using multiple real data center packet traces and cache sizes, SmartTime adaptive policy consistently outperformed the best performing static idle timeout policy or random eviction policy by up to 58% in terms of total cost.
programming_language	As multithreaded server applications and runtime systems prevail, garbage collection is becoming an essential feature to support high performance systems. The fundamental issue of garbage collector (GC) design is to maximize the recycled space with minimal time overhead. This paper proposes two innovative solutions: one to improve space efficiency, and the other to improve time efficiency. To achieve space efficiency, we propose the Space Tuner that utilizes the novel concept of allocation speed to reduce wasted space. Conventional static space partitioning techniques often lead to inefficient space utilization. The Space Tuner adjusts the heap partitioning dynamically such that when a collection is triggered, all space partitions are fully filled. To achieve time efficiency, we propose a novel parallelization method that reduces the compacting GC parallelization problem into a tree traversal parallelization problem. This method can be applied for both normal and large object compaction. Object compaction is hard to parallelize due to strong data dependencies such that the source object can not be moved to its target location until the object originally in the target location has been moved out. Our proposed algorithm overcomes the difficulties by dividing the heap into equal-sized blocks and parallelizing the movement of the independent blocks. It is noteworthy that these proposed algorithms are generic such that they can be utilized in different GC designs.
programming_language	This paper presents MultiHype, a novel architecture that supports multiple hypervisors (or virtual machine monitors) on a single physical platform by leveraging many-core based cloud-on-chip architecture. A MultiHype platform consists of a control plane and multiple hypervisors created on-demand, each can further create multiple guest virtual machines. Supported at architectural level, a single platform using MultiHype can behave as a distributed system with each hypervisor and its virtual machines running independently and concurrently. As a direct consequence, vulnerabilities of one hypervisor or its guest virtual machine can be confined within its own domain, which makes the platform more resilient to malicious attacks and failures in a cloud environment. Towards defending against resource exhaustion attacks, MultiHype further implements a new cache eviction policy and memory management scheme for preventing resource monopolization on shared cache, and defending against denial of resource exploits on physical memory resource launched from malicious virtual machines on shared platform. We use Bochs emulator and cycle based x86 simulation to evaluate the effectiveness and performance of MultiHype.
programming_language	The performance and power benefits of Flash memory have paved its adoption in mass storage devices in the form of Solid-State Disks (SSDs). Despite these benefits, Flash memory's limited write endurance remains a big impediment to its wide adoption in the enterprise server market. Existing research efforts have mostly focused on proposing various mechanisms and algorithms to improve SSD's performance and reliability. However, there is still a lack of flexible tools that allow characterizing SSD endurance (i.e., wear-out behavior) and investigating its impact on applications without affecting the lifetime of the real SSD device. To address this issue, SolidSim, a kernel-level simulator has been enhanced with capabilities to simulate state-of-the-art wear-leveling, garbage-collection and other advanced internal management techniques of an SSD. These extensions have further increased SolidSim's flexibility to study both SSD performance and endurance characteristics. Our approach allows investigating these characteristics without requiring any changes to applications or gathering any workload traces. The paper presents insights into wear-out behavior including logical, physical and translation characteristics, and correlates them with application behavior and SSD life-times using a set of representative workloads.
programming_language	What fundamental opportunities for scalability are latent in interfaces, such as system call APIs? Can scalability opportunities be identified even before any implementation exists, simply by considering interface specifications? To answer these questions this paper introduces the following rule: Whenever interface operations commute, they can be implemented in a way that scales. This rule aids developers in building more scalable software starting from interface design and carrying on through implementation, testing, and evaluation. To help developers apply the rule, a new tool named Commuter accepts high-level interface models and generates tests of operations that commute and hence could scale. Using these tests, Commuter can evaluate the scalability of an implementation. We apply Commuter to 18 POSIX calls and use the results to guide the implementation of a new research operating system kernel called sv6. Linux scales for 68% of the 13,664 tests generated by Commuter for these calls, and Commuter finds many problems that have been observed to limit application scalability. sv6 scales for 99% of the tests.
programming_language	We have improved the performance of the Mach 3.0 operating system by redesigning its internal thread and interprocess communication facilities to use continuations as the basis for control transfer. Compared to previous versions of Mach 3.0, our new system consumes 85% less space per thread. Cross-address space remote procedure calls execute 14% faster. Exception handling runs over 60% faster.In addition to improving system performance, we have used continuations to generalize many control transfer optimizations that are common to operating systems, and have recast those optimizations in terms of a single implementation methodology. This paper describes our experiences with using continuations in the Mach operating system.
programming_language	This paper appears in the March, 1972, issue of the Communications of the ACM. Its abstract is reproduced below. An operating system which is organized as a small supervisor and a set of independent processes are described. The supervisor handles I/O with external devices - the file and directory system - schedules active processes and manages memory, handles errors, and provides a small set of primitive functions which it will execute for a process. A process is able to specify a request for a complicated action on the part of the supervisor (usually a wait on the occurrence of a compound event in the system) by combining these primitives into a ?supervisory computer program.? The part of the supervisor which executes these programs may be viewed as a software implemented ?supervisory computer.? The paper develops these concepts in detail, outlines the remainder of the supervisor, and discusses some of the advantages of this approach.
programming_language	The performance improvements brought by demand paging policies with swapped working-sets depend on several factors, among which the scheduling policy, the behaviour of the programs running in the system and the secondary memory latency characteristics are the more noticeable. We present in this paper a modelling approach to quantify the effects of these factors on the performance of a system running with a swapped working-sets policy. A preliminary analysis, conducted in the virtual time of the programs, shows their influence on the paging behaviour of programs. The results of this analysis are then used within a detailed queueing network of a multiprogrammed system. Computationnaly simple expressions for the CPU time spent in user state and in supervisor state are obtained for a class of paging policies ranging from pure demand paging to demand paging with swapped working-sets. Numerical examples illustrate the analysis, and these results are compared with measurements made on a real system running with swapped working-sets policies.
programming_language	This paper proposes and evaluates soft timers, a new operating system facility that allows the efficient scheduling of software events at a granularity down to tens of microseconds. Soft timers can be used to avoid interrupts and reduce context switches associated with network processing without sacrificing low communication delays.More specifically, soft timers enable transport protocols like TCP to efficiently perform rate-based clocking of packet transmissions. Experiments show that rate-based clocking can improve HTTP response time over connections with high bandwidth-delay products by up to 89% and that soft timers allow a server to employ rate-based clocking with little CPU overhead (2-6%) at high aggregate bandwidths.Soft timers can also be used to perform network polling, which eliminates network interrupts and increases the memory access locality of the network subsystem without sacrificing delay. Experiments show that this technique can improve the throughput of a Web server by up to 25%.
programming_language	Mach is a multiprocessor operating system being implemented at Carnegie-Mellon University. An important component of the Mach design is the use of memory objects which can be managed either by the kernel or by user programs through a message interface. This feature allows applications such as transaction management systems to participate in decisions regarding secondary storage management and page replacement.This paper explores the goals, design and implementation of Mach and its external memory management facility. The relationship between memory and communication in Mach is examined as it relates to overall performance, applicability of Mach to new multiprocessor architectures, and the structure of application programs.
programming_language	We present MaWi - a smart phone based scalable indoor localization system. Central to MaWi is a novel framework combining two self-contained but complementary localization techniques: Wi-Fi and Ambient Magnetic Field. Combining the two techniques, MaWi not only achieves a high localization accuracy, but also effectively reduces human labor in building fingerprint databases: to avoid war-driving, MaWi is designed to work with low quality fingerprint databases that can be efficiently built by only one person. Our experiments demonstrate that MaWi, with a fingerprint database as scarce as one data sample at each spot, outperforms the state-of-the-art proposals working on a richer fingerprint database.
programming_language	In this paper, we present simulation and experimental studies of multiple bird source separation based on the Voxnet acoustic array node. The Approximate Maximum Likelihood (AML) method is used to estimate blindly the direction-of-arrivals(DOAs) of the sources to generate the steering vectors in order to separate the sources via beamforming. Simulation and measured data confirmed the proper operations of the AML beamforming algorithm and the Voxnet hardware node.
programming_language	Leveraging the abilities of multiple affordable robots as a swarm is enticing because of the resulting robustness and emergent behaviors of a swarm. However, because swarms are composed of many different agents, it is difficult for a human to influence the swarm by managing individual agents. Instead, we propose that human influence should focus on (a)~managing the higher level attractors of the swarm system and (b)~managing trade-offs that appear in mission-relevant performance. We claim that managing attractors theoretically allows a human to abstract the details of individual agents and focus on managing the collective as a whole. Using a swarm model with two attractors, we demonstrate this concept by showing how limited human influence can cause the swarm to switch between attractors. We further claim that using quorum sensing allows a human to manage trade-offs between the scalability of interactions and mitigating the vulnerability of the swarm to agent failures.
programming_language	Microbial communities that live on the outside and inside of the human body dramatically influence human health and diseases. In recent years, major progress has been made in understanding the human microbiome communities through projects such as the Human Microbiome Project (http://commonfund.nih.gov/hmp/), using next generation sequencing technologies and metagenomic approaches. In this paper, we describe a comparative computational analysis of 183 human gut microbiome sequence datasets, drawn from healthy individuals as well as those with autoimmune diseases. About 2.4 TB of Illumina deep sequencing metagenomic data were analyzed using computational workflows we developed, which run multiple steps of data- and computing-intensive analyses such as mapping, sequence assembly, gene identification, clustering and functional annotations. The analyses were carried out on the Gordon supercomputer at the San Diego Supercomputer Center (SDSC), using ~180,000 core hours and tens of TB storage space. Our analysis reveals the detailed microbial composition, dynamics, and functional profiles of the samples and provides new insight into how to correlate microbial profiles with human health and disease states.
programming_language	Maguro is a system for efficiently searching very large collections of text content of up to 1 trillion documents at low cost. Search engines span across content that is very dynamic and highly augmented with metadata to the tail content of the web. A long tail distribution of content calls for different trade-offs in the design space for good efficiency across the entire index range. Maguro is designed for the long tail of content with less dynamics and less metadata, but very good cost efficiency. Maguro is part of the serving stack in Bing and allows us to scale the index significantly better.
programming_language	Reasoning about the correctness of multithreaded programs is complicated by the potential for unexpected interference between threads. Previous work on controlling thread interference focused on verifying race freedom and/or atomicity. Unfortunately, race freedom is insufficient to prevent unintended thread interference. The notion of atomic blocks provides more semantic guarantees, but offers limited benefits for non-atomic code and it requires bi-modal sequential/multithreaded reasoning (depending on whether code is inside or outside an atomic block). This paper proposes an alternative strategy that uses yield annotations to control thread interference, and we present an effect system for verifying the correctness of these yield annotations. The effect system guarantees that for any preemptively-scheduled execution of a well-formed program, there is a corresponding cooperative execution with equivalent behavior in which context switches happen only at yield annotations. This effect system enables cooperative reasoning: the programmer can adopt the simplifying assumption of cooperative scheduling, even though the program still executes with preemptive scheduling and/or true concurrency on multicore processors. Unlike bimodal sequential/multithreaded reasoning, cooperative reasoning can be applied to all program code.
programming_language	Physical activity is important at all ages, and for the oldest it is also essential to stay fit to manage daily life. Exergames can make physical activity fun and motivational, and we see that senior centres and retirement homes start offering exergames to seniors. We wanted to find out whether seniors will keep on playing over time and what motivates them to play on. In a five month study we found that seniors still enjoy playing given that they have games that they enjoy, and that they ask for new challenges and new games when they master the basic gameplay.
programming_language	The wide spread interest in slicing of object-oriented software has led to the birth of number of algorithms. Nowadays, slicing of object-oriented programs has picked up the momentum as most of the real world programs are object-oriented in nature. The algorithms which are available of-the-self, address different issues in their own ways. In this paper, we propose a new algorithm which incorporates graph coloring technique. But in order to compute the dynamic slice, we have contradicted some key constraints of graph coloring algorithm. The advantage of our algorithm is that it is faster and the process of computing slice can be optimized further.
programming_language	Many computational tasks require the cooperation of many processors in the network, but prohibit certain processors pair (or large group) from operating simultaneously. This symmetry breaking technique play a major role in distributed network algorithm. Two symmetry breaking task of "localized" nature are coloring (Vertex Coloring) and Maximal Independence Set (MIS). In this paper, we present an experimental analysis of simple and elegant randomized distributed algorithm for vertex coloring problem
programming_language	The concept of inter-professional collaboration to optimize solutions for complex rehabilitation problems is not novel. However, the processes involved in successful and optimal collaboration between rehabilitation therapists and computer scientists is not well studied. In this paper, we examine strategies to connect technology driven problems and solutions in the lab to clinically driven problems and constraints for usable solutions in the field. We highlight gaps in collaboration such as differences in discipline language, hypothesis driven vs. function driven outcomes and understanding of the 'end-user'. We also discuss future ideas for successful collaboration to optimize usability of rehabilitation technology through creative problem solving.
programming_language	Feed ranking's goal is to provide perople with over a billion personalized experiences. We strive to provide the most compelling content to each person, personalized to them so that they are most likely to see the content that is most interesting to them. Similar to a newspaper, putting the right stories above the fold has always been critical to engaging customers and interesting them in the rest of the paper. In feed ranking, we face a similar challenge, but on a grander scale. Each time a person visits, we need to find the best piece of content out of all the available stories and put it at the top of feed where people are most likely to see it. To accomplish this, we do large-scale machine learning to model each person, figure out which friends, pages and topics they care about and pick the stories each particular person is interested in. In addition to the large-scale machine learning problems we work on, another primary area of research is understanding the value we are creating for people and making sure that our objective function is in alignment with what people want.
programming_language	The Berkeley AMPLab is creating a new approach to data analytics. Launching in early 2011, the lab aims to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and in crowds). The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the four years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. BDAS features prominently in many industry discussions of the future of the Big Data analytics ecosystem -- a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving "up the stack" to better integrate and support advanced analytics and to make people a full-fledged resource for making sense of data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe the current state of BDAS with an emphasis on our newest efforts, including some or all of: the GraphX graph processing system, the Velox and MLBase machine learning platforms, and the SampleClean framework for hybrid human/computer data cleaning. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.
programming_language	This paper presents CMod, a novel tool that provides a sound module system for C. CMod works by enforcing a set of four rules that are based on principles of modular reasoning and on current programming practice. CMod's rules flesh out the convention that .h header files are module interfaces and .c source files are module implementations. Although this convention is well-known, developing CMod's rules revealed there are many subtleties in applying the basic pattern correctly. We have proven formally that CMod's rules enforce both information hiding and type-safe linking. We evaluated CMod on a number of benchmarks, and found that most programs obey CMod's rules, or can be made to with minimal effort, while rule violations reveal brittle coding practices including numerous information hiding violations and occasional type errors.
programming_language	This paper describes the rationale behind the ongoing development of Sprite Catcher -- a handheld, tangible interactive tool for treating depression and anxiety. The current design, which is intended to encourage the user to practice self-reflection and mindfulness, is the product of participatory design conducted with counsellors from a local mental health charity and with a university psychology researcher. Through a review of previous work in this area, a description of a use scenario and an overview of the design's functions and concepts, we illustrate where the project is heading and which research questions we aim to respond to.
programming_language	Two different ways of defining ad-hoc polymorphic operations commonly occur in programming languages. With the first form polymorphic operations are defined inductively on the structure of types while with the second form polymorphic operations are defined for specific sets of types.In intensional type analysis operations are defined by induction on the structure of types. Therefore no new cases are necessary for user-defined types, because these types are eQuivalent to their underlying structure. However, intensional type analysis is "closed" to extension, as the behavior of the operations cannot be differentiated for the new types, thus destroying the distinctions that these types are designed to express.Haskell type classes on the other hand define polymorphic operations for sets of types. Operations defined by class instances are considered "open"---the programmer can add instances for new types without modifying existing code. However, the operations must be extended with specialized code for each new type, and it may be tedious or even impossible to add extensions that apply to a large universe of new types.Both approaches have their benefits, so it is important to let programmers decide which is most appropriate for their needs. In this paper, we define a language that supports both forms of ad-hoc polymorphism, using the same basic constructs.
programming_language	We present CoreLinks, a call-by-value variant of System F with row polymorphism, row-based effect types, and implicit subkinding, which forms the basis for the Links web programming language. We focus on extensions to CoreLinks for database programming. The effect types support abstraction over database queries, while ensuring that queries are translated predictably to idiomatic and efficient SQL at run-time. Subkinding statically enforces the constraint that queries must return a list of records of base type. Polymorphism over the presence of record labels supports abstraction over database queries, inserts, deletes and updates.
programming_language	Proof assistants such as the Coq system have traditionally been a laboratory for exotic type features such as dependent and computable types, and first-class type classes. Although these features have been introduced independently, it turns out they can be com-bined in novel and nontrivial ways to solve some of the more challenging problems posed by the formalization of advanced mathematics. Many of these patterns could also be useful for general programming.
programming_language	In this paper, we prove the safety of integrating region-based memory management and Cheney-style copying garbage collection. The safety property relies on a refinement of the region typing rules that forbids dangling pointers during evaluation.To accommodate the triggering of garbage collection at any step in the evaluation process, we base our type-safety result for the region-based system on a small-step contextual semantics and show that whenever a well-typed expression reduces to another expression, possibly by deallocating a region, then no dangling pointer is introduced. Because there are no dangling pointers in the initial heap, no dangling pointers appear during evaluation.Although in principle, the refinement of the region typing rules leads to less flexibility and can cause worse memory behavior than when dangling pointers are permitted, experiments show that, for a range of benchmark programs, the refinement has little effect on overall memory behavior.
programming_language	In this paper, we introduce a tuple notation for noncompliance that represents certification problems when meeting security controls in distributed, multi-component software systems. The security controls are adopted from NIST SP800-53 and DoD 8500.2 documents. We derive tuples from component policies and interactions, along with the risks associated with violating the security controls. Tuples can be clustered from different perspectives, reasoned about to target the cause and strength of noncompliance. They can also be mapped to specific security concerns and weaknesses in the multi-component architecture.
programming_language	A popular research topic as of late has been dynamic partial FPGA (Field Programmable Gate Array) reconfiguration. This concept allows on-the-fly reconfiguration of digital systems where only parts of the circuit change, providing application acceleration and allowing static modules to continue processing unaffected by the dynamic elements. Design characteristics which benefit from this progressive approach include increased system flexibility, increased performance, and a reduction in circuit complexity. One characteristic receiving limited focus thus far has been the increased security that could result from these changing circuits. This benefit is innate to the design and makes reverse engineering of the system a much more ambitious task. In an effort to further enhance this passive security feature, a new partial reconfiguration technique has been proposed that changes connectivity between generic modules. This extended abstract introduces the method, model, and design flow for this dynamic partial FPGA reconfiguration technique and addresses the security implications of such a design.
programming_language	This paper proposes efficient object-based implementations for Software Transactional Memory (stm). The proposed implementations are based on using multiple versions of the transactional objects to increase the concurrency of stm execution. In this work, we propose 3 algorithms for enhancing concurrency in stm. We also propose a new approach for determining the optimal number of versions to keep. Finally, we present experimental results that show the efficiency of the proposed implementations.
programming_language	Applications that work on network-oriented data often use property graph models. Although their graph data is represented by an object-oriented model, current approaches cannot define statically typed vertex and edge sets. Thus, custom graph operations use untyped input and output sets and cannot exploit crucial concepts like polymorphism. Not only do illegal calling contexts or arguments result in runtime errors or unexpected query results, but also the resulting code tends to be error prone, unclear, and thus hard to maintain. To solve these problems, we extend the property graph model with typed graph classes and open it up to polymorphism. Our approach is an internal domain specific language for graph traversals based on the object-oriented and functional programming language Scala. A case study emphasizes the usability of our framework.
programming_language	With the advent of the multicore era, it is clear that future growth in application performance will primarily come from increased parallelism. We believe parallelism should be introduced early into the Computer Science curriculum to educate students on the fundamentals of parallel computation. In this paper, we introduce the newly-created Habanero-Java library (HJlib), a pure Java 8 library implementation of the pedagogic parallel programming model [12]. HJlib has been used in teaching a sophomore-level course titled "Fundamentals of Parallel Programming" at Rice University. HJlib adds to the Java ecosystem a powerful and portable task parallel programming model that can be used to parallelize both regular and irregular applications. By relying on simple orthogonal parallel constructs with important safety properties, HJlib allows programmers with a basic knowledge of Java to get started with parallel programming concepts by writing or refactoring applications to harness the power of multicore architecture. The HJlib APIs make extensive use of lambda expressions and can run on any Java 8 JVM. HJlib runtime feedback capabilities, such as the abstract execution metrics and the deadlock detector, help the programmer to obtain feedback on theoretical performance as well as the presence of potential bugs in their program. Being an implementation of a pedagogic programming model, HJlib is also an attractive tool for both educators and researchers. HJlib is actively being used in multiple research projects at Rice and also by external independent collaborators. These projects include exploitation of homogeneous and heterogeneous multicore parallelism in big data applications written for the Hadoop platform [20, 43].
programming_language	Traits have been designed as units of fine-grained behavior reuse in the object-oriented paradigm. In this paper, we present the language Sugared Welterweight Record-Trait Java (SWRTJ), a Java dialect with records and traits. Records have been devised to complement traits for fine-grained state reuse. Records and traits can be composed by explicit linguistic operations, allowing code manipulations to achieve fine-grained code reuse. Classes are assembled from (composite) records and traits and instantiated to generate objects. We present the prototypical implementation of SWRTJ using Xtext, an Eclipse framework for the development of programming languages as well as other domain-specific languages. Our implementation comprises an Eclipse-based editor for SWRTJ with typical IDE functionalities, and a stand-alone compiler, which translates SWRTJ programs into standard Java programs.
programming_language	The Java Native Interface (JNI) allows Java programmers to inter-operate with code written in other languages like C and C++. One reason to use JNI is to get higher performance. Other reasons are to access low-level implementation features not available in pure Java and facilitate the reuse of existing code and libraries. However, the drawback is that native code can be used to compromise the security of the rest of Java. In this paper, we propose JNICodejail, which sandboxes the native code used in JNI. JNICodejail ensures that the native code is unable to affect the rest of Java (except what is allowed through JNI) and is confined only with the appropriate system privileges. However, native code is allowed to read memory outside its sandbox, thus, it is possible to share data which is read-only with the sandbox for improved efficiency. A recent alternative for sandboxing JNI native code is Arabica. We demonstrate that our JNICodejail prototype can have reasonable performance with respect to both normal un-sandboxed JNI execution and sandboxing with Arabica.
programming_language	At runtime, how objects have to be handled frequently depends on how they were used before. But with current programming-language support, selecting objects according to their previous usage patterns often results in scattered and tangled code. In this study, we propose a new kind of pointcut, called Instance Pointcuts, for maintaining sets that contain objects with a specified usage history. Instance pointcut specifications can be reused, by refining their selection criteria, e.g., by restricting the scope of an existing instance pointcut; and they can be composed, e.g., by set operations. These features make instance pointcuts easy to evolve according to new requirements. Our approach improves modularity by providing a fine-grained mechanism and a declarative syntax to create and maintain usage-specific object sets.
programming_language	When Java was introduced to the world at large 20 years ago, it brought many interesting features and capabilities into the mainstream computing environment. A Virtual Machine based approach with a just-in-time compiler that supported sandboxing, dynamic class loading, and introspection enabled a number of novel and innovative network-based applications to be developed. While many of these capabilities existed in some fashion in other prototype and experimental languages, the combination of all of them in a popular general purpose language opened up the possibility of building real systems that could leverage these capabilities. Applets, Jini, JXTA, and many other innovative concepts were introduced over the course of time, building on top of the basic capabilities of Java. This talk will present some personal experiences with using Java in distributed computing environments ranging from mobile software agents to distributed resource sharing to process integrated mechanisms. The basis for many of these capabilities is the Aroma Virtual Machine, a custom Java compatible VM with state capture, migration, and resource control capabilities. Motivations behind the Aroma VM will be discussed, along with design choices and some results. Finally, the talk will discuss a wish list of features that would be nice to have in future versions of Java to enable many more novel applications to be developed.
programming_language	The demand for portable mainstream programming models supporting scalable, reactive and versatile distributed computing is growing dramatically with the proliferation of manycore/heterogeneous processors on portable devices and cloud computing clusters that can be elastically and dynamically allocated. With such changes, distributed software systems and applications are shifting towards service oriented architectures (SOA) that consist of largely decoupled, dynamically replaceable components and connected via loosely coupled, interactive networks that may exhibit more complex coordination and synchronization patterns. In this paper, we propose the Distributed Selector (DS) model, to address the aforementioned requirements via a simple easy-to-use API. Our implementation of this model runs on distributed JVMs, and features automated bootstrap and global termination. We focus on the Selector Model (a generalization of the actor model) as a foundation for creating distributed programs and introduce a unified runtime system that supports both shared memory and distributed multi-node execution of such programs. The multiple guarded mailboxes, a unique and novel property of Selectors, enable the programmer to easily specify coordination patterns that are strictly more general than those supported by the Actor model. We evaluate the performance of our selector-based distributed implementation using benchmarks from the Savina benchmark suite [13]. Our results show promising scalability performance for various message exchange patterns. We also demonstrate high programming productivity arising from high-level abstraction and location transparency in the HJ Distributed Selector Runtime library (as evidenced by minimal differences between single-node and multi-node implementations of a selector-based application), as well as the contribution of automated system bootstrap and global termination capabilities.
programming_language	The existence of large API libraries contributes significantly to the programming productivity and quality of Java programmers. The vast number of available library APIs, however, presents a learning challenge for Java programmers. Most Java programmers do not know all the APIs. Whenever their programming task requires API methods they do not yet know, they have to be able to find what they need and learn how to use them on demand. This paper describes a tool called STeP_IN_Java (a <u>S</u>ocio-<u>Te</u>chnical <u>P</u>latform for <u>I</u>n situ <u>N</u>etworking of <u>Java</u> programmers) that helps Java programmers find APIs, and learn from both examples and experts how to use them on demand. STeP_IN_Java features a sophisticated yet easy-to-use search interface that enables programmers to conduct a personalized search and to progressively refine their search by limiting search scopes. Example programs are provided and embedded to assist programmers in using APIs. Furthermore, if a programmer still has questions about a particular API method, he or she can ask peer programmers. The STeP_IN_Java system automatically routes the question to a group of experts who are chosen based on two criteria: they have high expertise on the particular API method and they have a good social relationship with the programmer who is requesting the information.
programming_language	Parallelism dominates modern hardware design, from multi-core CPUs to SIMD and GPGPU. This bring with it, however, a need to program this hardware in a programmer-friendly manner. Traditionally, managed languages like Java have struggled to take advantage of data-parallel hardware, but projects like Aparapi provide a programming model that lets the programmer easily express the parallelism within their code, while still programming in a high-level language. This work takes advantage of this programmer-specified parallelism to perform source-level auto-vectorization, an optimization that is rarely considered in Java compilation. This is done using a source-to-source auto-vectorization transformation on the Aparapi Java program and a JNI vector library that is pre-compiled to take advantage of available SIMD instructions. This replaces the existing Aparapi fallback path, for when no OpenCL device exists or if that device has insufficient memory for the program. We show that for all ten benchmarks tested the auto-vectorization tool produced an implementation that was able to beat the default Aparapi fallback path by a factor of 4.56x or 3.24x on average for a desktop and a server system respectively. In addition it was found that this improved fallback path even outperformed the GPU implementation for six of the ten benchmarks.
programming_language	Profilers help programmers analyze their programs and identify performance bottlenecks. We implement a profiler framework that helps to compare and analyze programs implementing the same algorithms written in different languages. Profiler implementers replicate common functionalities in their language profilers. We focus on building a generic profiler framework for dynamic languages to minimize the recurring implementation effort. We implement our profiler in a framework that optimizes abstract syntax tree (AST) interpreters using a just-in-time (JIT) compiler. We evaluate it on ZipPy and JRuby+Truffle, Python and Ruby implementations in this framework, respectively. We show that our profiler runs faster than the existing profilers in these languages and requires modest implementation effort. Our profiler serves three purposes: 1) helps users to find the bottlenecks in their programs, 2) helps language implementers to improve the performance of their language implementation, 3) helps to compare and evaluate different languages on cross-language benchmarks.
programming_language	Despite its importance for many software engineering tasks, dynamic program analysis is only insufficiently supported on the Java platform [2]. Existing Java Virtual Machines (JVMs) as well as Android's Dalvik Virtual Machine (DVM) lack dedicated mechanisms for expressing arbitrary dynamic program analysis tasks at a high abstraction level, for ensuring complete code coverage of the analysis, and for isolating analysis tasks from the observed program to prevent interference. For example, the JVM Tool Interface requires analysis tasks to be written in low-level native code, and some virtual machines (e.g., DVM) do not support it. As a consequence, dynamic program analysis tools are often implemented using low-level mechanisms, resulting in error-prone code that is difficult to maintain, and support only a particular virtual machine. Moreover, many analysis tools produce unsound profiles (due to interference of the analysis with the observed program) or incomplete profiles (due to limited code coverage). We present a novel dynamic program analysis framework that offers high-level abstractions for comprehensive, multi-platform analysis for the JVM and DVM. Our framework ensures complete bytecode coverage and isolates the execution of the analysis code from the observed program. It is based on the concepts developed for DiSL [4], ShadowVM [3], and FRANC [1]. The domain-specific aspect language DiSL is used to specify the events of interest for an analysis. The events captured during program execution are transmitted to the ShadowVM, where the user-defined analysis code processes the events. Different event-ordering semantics are supported, avoiding synchronization for analyses that do not require global event ordering across all threads. In addition to events specified by DiSL code, our framework also generates lifecycle events and inter-process communication events. The latter is particularly important for the analysis of Android applications, as they typically involve multiple processes. Several state-of-the-art analysis tools have already been ported to our framework, including code coverage testing tools, calling-context profilers, and object liftetime profilers.
programming_language	Traits provide a mechanism for fine-grained code reuse to overcome the limitations of class-based inheritance. A trait is a set of methods which is completely independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. We present: (i) a formulation of traits which aims to achieve complete compatibility and interoperability with the Java platform without reducing the flexibility of traits, and (ii) an integration with Eclipse which aims to support an incremental adoption of traits in existing Java projects. Indeed, the proposed trait language can coexist with Java code. Single parts of a project can be refactored to use traits, without requiring a complete rewrite of the whole existing code-base.
programming_language	Star is a functional, multi-paradigm and extensible programming language that runs on the Java platform. Starview Inc. developed the language as an integral part of the Starview Enterprise Platform, a framework for real-time business applications such as factory scheduling and data analytics. Star borrows from many languages, with obvious heritage from Haskell, ML, and April, but also contains new approaches to some design aspects, such as syntax and syntactic extensibility, actors, and queries. Its texture is quite different from that of other languages on the Java platform. Despite this, the learning curve for Java programmers is surprisingly shallow. The combination of a powerful type system (which includes type inference, constrained polymorphism, and existentials) and syntactic extensibility make the Star well-suited to producing embedded domain-specific languages. This paper gives an overview of the language, and reports on some aspects of its design process, on our experience on using it in industrial projects, and on our experience implementing Star on the Java platform.
programming_language	Many Computer Science and Engineering curricula contain core modules on computer programming and programming languages. An increasing number of institutions choose to introduce undergraduates to programming through object oriented languages. As part of a longitudinal phenomenographic study we have set out to investigate the understanding of programming concepts that first year undergraduate students have when learning to program and think in the object oriented paradigm.The conceptions that students have developed on what learning to program really means and their perception of program correctness are explored; providing an insight into the levels of abstraction and complexity of the learners' understanding. Our findings suggest that the way students experience learning to program is related to their perception of what constitutes program correctness.
programming_language	In this work, we consider low power, wearable pulse oximeter sensors for ambulatory, remote vital signs monitoring applications. It is extremely important for such sensors to maintain clinical accuracy and yet provide power savings to enable non-intrusive, long lasting sensors. Our contributions in this work include sub-Nyquist, random sampling of evanescent red and infra red (IR) photoplethysmograph (PPG) signals in real time under the Compressed Sensing (CS) paradigm. We describe the real time platform and demonstrate that the SpO2 accuracy is not compromised due to aliasing of ambient light artifacts, even when average number of measurements is much below that of Nyquist rate. We briefly discuss the various modules contributing to overall power consumption of a wireless pulse oximeter sensor and show that 10x reductions in LED power and radio power are possible, without sacrificing the SpO2 accuracy.
programming_language	We present a set of techniques for reducing the memory consumption of object-oriented programs. These techniques include analysis algorithms and optimizations that use the results of these analyses to eliminate fields with constant values, reduce the sizes of fields based on the range of values that can appear in each field, and eliminate fields with common default values or usage patterns. We apply these optimizations both to fields declared by the programmer and to implicit fields in the runtime object header. Although it is possible to apply these techniques to any object-oriented program, we expect they will be particularly appropriate for memory-limited embedded systems.We have implemented these techniques in the MIT FLEX compiler system and applied them to the programs in the SPECjvm98 benchmark suite. Our experimental results show that our combined techniques can reduce the maximum live heap size required for the programs in our benchmark suite by as much as 40%. Some of the optimizations reduce the overall execution time; others may impose modest performance penalties.
programming_language	DNA microarray technology has helped us to understand the biological system because of its ability to monitor the expression levels of thousands of genes simultaneously. Since DNA microarray experiments provide us with huge amount of gene expression data, they should be analyzed with statistical methods to extract the meanings of experimental results.For visualization and class prediction of gene expression data, we have developed a new SVM-based method called multidimensional SVMs, that generate multiple orthogonal axes. This method projects high dimensional data into lower dimensional space to exhibit properties of the data clearly and to visualize the distribution of the data roughly. Furthermore, the multiple axes can be used for class prediction. The basic properties of conventional SVMs are retained in our method: solutions of mathematical programming are sparse, the optimal solutions can always be found due to its convexity, and nonlinear classification is implemented implicitly through the use of kernel functions. The application of our method to the experimentally obtained gene expression datasets for patients' samples indicates that our algorithm is efficient and useful for visualization and class prediction.
programming_language	Embedded computer systems are characterized by the presence of a processor running application specific dedicated software. A large number of these systems must satisfy real-time constraints. This paper examines the problem of determining the extreme (best and worst) case bounds on the running time of a given program on a given processor. This has several applications in the design of embedded systems with real-time constraints. An important aspect of this problem is determining which paths in the program are exercised in the extreme cases. The state of the art solution here relies on an explicit enumeration of program paths. This runs out of steam rather quickly since the number of feasible program paths is typically exponential in the size of the program. We present a solution for this problem that does not require an explicit enumeration of program paths, i.e., the paths are considered implicitly. This solution is implemented in the program cinderella1 which currently targets a popular embedded processor --- the Intel i960. The preliminary results of using this tool are also presented here.
programming_language	Now that the use of garbage collection in languages like Java is becoming widely accepted due to the safety and software engineering benefits it provides, there is significant interest in applying garbage collection to hard real-time systems. Past approaches have generally suffered from one of two major flaws: either they were not provably real-time, or they imposed large space overheads to meet the real-time bounds.Our previous work [3] presented the Metronome, a mostly non-copying real-time collector. The Metronome achieves worst-case pause times of 6 milliseconds while maintaining consistent mutator CPU utilization rates of 50% with only 1.5-2.1 times the maximum heap space required by the application, which is comparable with space requirements for stop-the-world collectors.However, that algorithm assumed a constant collection rate, ignored program-dependent characteristics, and lacked a precise specification for when to trigger collection or how much defragmentation to perform. This paper refines the model by taking into account program properties such as pointer density, average object size, and locality of object size. This allows us to bound both the time for collection and consequently the space overhead required much more tightly. We show experimentally that most parameters usually are not subject to large variation, indicating that a small number of parameters will be sufficient to predict the time and space requirements accurately.Our previous work also did not present the details of our approach to avoiding and undoing fragmentation. In this paper we present a more detailed analysis of fragmentation than in previous work, and show how our collector is able to bound fragmentation to acceptable limits.
programming_language	Cooperative safety and infotainment were the key motivating factors behind the advent of vehicular communication architecture. In this paper, the Context-aware cooperative collision avoidance braking system is proposed. The vehicles moving in the same lane, closer than the safety distance (SD) are prompted to reduce their speed or increase the inter-vehicular distance. The SD is set according to the available parameters and road infrastructure, and is further tuned with the help of context-aware SD adjustment system. The system proposed is self-learning and alert messages are directed to only pertinent vehicles with the help of clustering. The hardware system is implemented in vehicle on highway for SD calculation.
programming_language	Resource sharing and interferences of multiple threads of one, but even worse between multiple application programs running concurrently on a Multi-Processor System-on-a-Chip (MPSoC) today make it very hard to provide any timing or throughput-critical applications with time bounds. Additional interferences result from the interaction of OS functions such as thread multiplexing and scheduling as well as complex resource (e.g., cache) reservation protocols used heavily today. Finally, dynamic power and temperature management on a chip might also throttle down processor speed at arbitrary times leading to additional variations and jitter in execution time. This may be intolerable for many safety-critical applications such as medical imaging or automotive driver assistance systems. Static solutions to provide the required isolation by allocating distinct resources to safety-critical applications may not be feasible for reasons of cost and due to the lack of efficiency and inflexibility. Also, shutting off or restricting temperature and power management might not be tolerable. In this keynote, we propose new techniques for adaptive isolation of resources including processor, I/O, memory as well as communication resources on demand on an MPSoC based on the paradigm of Invasive Computing. In Invasive Computing, a programmer may specify bounds on the execution quality of a program or even single segments of a program followed by an invade command. This system returns a constellation of exclusive resources called a claim that is subsequently used in a by-default non-shared way until being released again by the invader. Through this principle, it becomes possible to isolate applications automatically and in an on-demand manner. In invasive computing, isolation is supported on all levels of hardware and software including an invasive OS. In case of an abundant number of cores available on an MPSoC today, the problem still becomes how to find suitable claims that will guarantee a performance bound in a negligible amount of time? For a broad class of streaming applications, we propose a combined static/dynamic approach based on a static design space exploration phase to extract a set of satisfying claim characteristics for which program execution is guaranteed to stay within the desired performance bounds. For a class of compositional and heterogeneous MPSoC systems, only very little information must then be passed to the OS for run-time claim search in the form of so-called CCGs (claim constraint graphs). A special role here plays a compositional Network-on-a-Chip (NoC) architecture that allows to invade guaranteed bandwith between processor, memory and I/O tiles independently from other applications. We demonstrate the above concepts for a complex object detection application algorithm chain taken from robot vision to show jitter-minimized implementations become possible, even for statically unknown arrivals of other concurrent applications.
programming_language	Process Networks (PNs) are used for modeling streaming-oriented applications with changing behavior, which must be mapped on a concurrent architecture to meet the performance and energy constraints of embedded devices. Finding an optimal mapping of Process Networks to the constrained architecture presumes that the behavior of the PN is statically known. In this paper we present a static analysis for synchronous PNs that partitions the state space according to extract run-time modes based on a Data Augmented Control Flow Automaton (DACFA). The result is a mode automaton whose nodes describe identified program modes and whose edges represent transitions among them. Optimizing back-ends mapping from PNs to concurrent architectures can be guided by these analysis results.
programming_language	Gesture provides a new design space for in-vehicle human-machine interaction. It could potentially mitigate emerging conflicts between the increasing functionality of today's vehicles and the very limited space that is available for implementing these functions within the driver's reach. However, because gesture requires manual input, it may cause unintended consequences for drivers rather than supporting concurrent driving tasks as it is meant to do. This workshop will explore the potential of in-vehicle gesture interaction, as well as the cautions that must be exercised during its implementation. Participants will contribute to the discussion of design guidelines for gesture interaction; discussion of the advantages and disadvantages of gesture interaction based on properties of secondary tasks; will propose promising uses of gesture for in-vehicle interaction; and will advise systematic approaches for guiding the development of gesture interactions that will minimize the impact to or even facilitate primary driving tasks. The discussion within this workshop will also consider the different phases of automation as a design factor and discuss how to adapt gesture interactions to the changing demands in manual driving control.
programming_language	In this paper, we propose a methodology for generating users' tailored video abstracts. First, video frames are scored by a group of video experts (operators) according to audio, visual and textual content of the video. Then, a matrix that contains the relevancy scores of each video scene into a number of pre-defined categories is computed using Scaled Invariant Feature Transform (SIFT) features, which are computed pairwise for representative keyframes and delegate images from the training collection. Later, for profiling purposes, an end-user's interest levels towards those high-level visual concepts (categories) are captured in the form of a vector. As a result of combining these two groups of data, the user's priorities in regards to different video segments can be determined. In the next stage, the initial averaged scores of the frames are updated based on the end-users' generated profiles. Eventually, the highest scored video frames alongside the auditory and textual content are inserted into final digest. The effectiveness of this approach has been evaluated by comparing the video summaries generated by this system against the results from a number of automatic and semi-automatic summarization tools that use different modalities for abstraction.
programming_language	In the past years recognition by biometric information has been increasingly adopted. This paper presents a new approach to biometric recognition based on hand geometry. A database with 100 individuals and samples of both sides of the hands was used. The process prioritizes user comfort during capture and produces segmentation of hands and fingers with high precision. Altogether, 54 features have been extracted and different classification and training methods were evaluated. Tests using cross-validation and stratified random subsampling techniques were performed. The experiments demonstrated competitive results when compared to other state-of-the-art methods. The proposed approach obtained 100% accuracy using the Logist Boost together with Random Forest learning strategy and Bagging together with FLR combination.
programming_language	One of the main issues related to the treatment of soybean seeds concerns its vigor definition. The great majority of vigor analysis is performed by a human specialist leading to an extremely tiresome and subjective approach, highly susceptible to failures. In order to deal with this problem the present paper proposed a methodology joining content-based image retrieval techniques with the specialist perception to retrieve soybean seeds according to his/her expectation and aiding the seed vigor definition. The experiments performed showed that the proposed approach presented several notable contributions to the soybean vigor definition process, reaching up to 100% of precision regarding the similarity queries. Moreover, it not only improves in a great extent the entire process from planting to harvesting, but also enables to automate and accelerate it.
programming_language	We present an emotion recognition system based on a probabilistic approach to adaptive processing of Facial Emotion Tree Structures (FETS). FETS are made up of localized Gabor features related to the facial components according to the Facial Action Coding System. The proposed model is an extension of the probabilistic based recursive neural network model applying in face recognition by Cho and Wong [1]. The robustness of the model in an emotion recognition system is evaluated by testing with known and unknown subjects with different emotions. The experiment results shows that the proposed model significantly improved the recognition rate in terms of generalization.
programming_language	The main drawback regarding agriculture commodities such as soybeans is that they must be uniform in quality considering the companies that produce and sell it. Thus, to reach such requirements it is necessary to establish strict parameters and quality control processes. The most important test to accomplish this task is based on the seed vigor definition. However, the great majority of seed vigor analysis is performed by a human specialist leading to an extremely tiresome and subjective approach, highly susceptible to failures, as well as, certain types of deliberate adulteration, based on monetary interests. In order to deal with this problem, the present paper proposed an image analysis framework for effective classification of seeds according to their vigor. The experiments performed showed that the proposed framework presented several notable contributions to the soybean vigor definition process, reaching up to 81% of classification accuracy. Moreover, it not only improves in a great extent the entire process from planting to harvesting, but also enables to automate and accelerate it, as well as, can be used as a counterevidence of the specialist classification.
programming_language	In this paper, we make the study on the unique solution for the P3P problem. After partitioning the space into several regions, the parametric function and target function for each region are presented. Under the condition of knowing the approximate relative position between center of perspective and control points, the unique solution can be obtained.
programming_language	Traditional recommender systems minimize prediction error with respect to users' choices. Recent studies have shown that recommender systems have a positive effect on the provider's revenue. In this paper we show that by providing a set of recommendations different than the one perceived best according to user acceptance rate, the recommendation system can further increase the business' utility (e.g. revenue), without any significant drop in user satisfaction. Indeed, the recommendation system designer should have in mind both the user, whose taste we need to reveal, and the business, which wants to promote specific content. We performed a large body of experiments comparing a commercial state-of-the-art recommendation engine with a modified recommendation list, which takes into account the utility (or revenue) which the business obtains from each suggestion that is accepted by the user. We show that the modified recommendation list is more desirable for the business, as the end result gives the business a higher utility (or revenue). To study possible reduce in satisfaction by providing the user worse suggestions, we asked the users how they perceive the list of recommendation that they received. Differences in user satisfaction between the lists is negligible, and not statistically significant. We also uncover a phenomenon where movie consumers prefer watching and even paying for movies that they have already seen in the past than movies that are new to them.
programming_language	Data Matrix is a 2D barcode with high information density and wide applicability. When printed in documents or used for labeling products, the barcode may be subject to deformations. This paper looks at the recognition of Data Matrix barcodes is under scaling, rotation and cylindrical warping. The results suggest that the readability of Data Matrix barcodes is immune to rotation transformations and responds well to scaling, while it is more seriously impaired by cylindrical warping.
programming_language	This paper deals with the problem of shape-based retrieval in time-series databases. The shape-based retrieval is defined as the operation that searches for the (sub)sequences whose shapes are similar to that of a given query sequence. In this paper, we propose an effective and efficient approach for shape-based retrieval of subsequences. We first introduce a new similarity model for shape-based retrieval that supports various combinations of transformations such as shifting, scaling, moving average, and time warping. For efficient processing of the shape-based retrieval, we also propose the indexing and query processing methods. To verify the superiority of our approach, we perform extensive experiments with the real-world S&P 500 stock data. The results reveal that our approach successfully finds all the subsequences that have the shapes similar to that of the query sequence, and also achieves significant speedup over the sequential scan method.
programming_language	Background modeling and foreground object detection is the first step in visual surveillance system. The task becomes more difficult when the background scene contains significant variations, such as water surface, waving trees and sudden illumination conditions, etc. Recently, subspace learning model such as Robust Principal Component Analysis (RPCA) provides a very nice framework for separating the moving objects from the stationary scenes. However, due to its batch optimization process, high dimensional data should be processed. As a result, huge computational complexity and memory problems occur in traditional RPCA based approaches. In contrast, Online Robust PCA (OR-PCA) has the ability to process such large dimensional data via stochastic manners. OR-PCA processes one frame per time instance and updates the subspace basis accordingly when a new frame arrives. However, due to the lack of features, the sparse component of OR-PCA is not always ro-bust to handle various background modeling challenges. As a consequence, the system shows a very weak performance, which is not desirable for real applications. To handle these challenges, this paper presents a multi-feature based OR-PCA scheme. A multi-feature model is able to build a ro-bust low-rank background model of the scene. In addition, a very nice feature selection process is designed to dynamically select a useful set of features frame by frame, according to the weighted sum of total features. Experimental results on challenging datasets such as Wallflower, I2R and BMC 2012 show that the proposed scheme outperforms the state of the art approaches for the background subtraction task.
programming_language	Lesion segmentation plays an important role in medical image processing and analysis. There exist several successful dynamic programming (DP) based segmentation methods for general images. In those methods, the gradient is used as an important factor in the cost function to attract the contours to the boundaries. Since medical images have their characteristics such as low contrast, blurred edges and high noises, the gradient operator cannot work well enough to achieve a satisfactory performance for boundary detection. We define the local intra-class variance and combine it with the dynamic programming method to replace the traditional gradient operation. Experiments on synthetic and X-ray images are carried out and the results are compared with Canny and fast multilevel fuzzy edge detection (FMFED) algorithms. It is demonstrated that the proposed method performs better on medical images with low contrast, blurred edges and high noises. In addition, 483 regions of interest of mammograms randomly extracted from DDSM of the University of South Florida are used to compare our proposed method with the plane-fitting and dynamic programming method (PFDP), and the normalized cut segmentation method (Ncut). The results demonstrate that our method is more accurate and robust than PFDP and Ncut.
programming_language	The Bag-of-Visual-Words approach has been successfully used for video and image analysis by encoding local features as visual words, and the final representation is a histogram of the visual words detected in the image. One limitation of this approach relies on its inability of encoding spatial distribution of the visual words within an image, which is important for similarity measurement between images. In this paper, we present a novel technique to incorporate spatial information, called Global Spatial Arrangement (GSA). The idea is to split the image space into quadrants using each detected point as origin. To ensure rotation invariance, we use the information of the gradient of each detected point to define each quarter of the quadrant. The final representation uses only two extra information into the final feature vector to encode the spatial arrangement of visual words, with the advantage of being invariant to rotation. We performed representative experimental evaluations using several public datasets. Compared to other techniques, such as the Spatial Pyramid (SP), the proposed method needs 90% less information to encode spatial information of visual words. The results in image retrieval and classification demonstrated that our proposed approach improved the retrieval accuracy compared to other traditional techniques, while being the most compact descriptor.
programming_language	Salient points are very important for image description because they are related to the visually most important parts of the image, leading to a compact and more discriminative representation close to human perception. Based on these promising features, in this paper we propose a new shape descriptor, namely Bag-of-Salience-Points (BoSP), using the shape salience points combined with the Bag-of-Visual-Words modeling approach. Each salience point, after extracted from the shape contour, is represented by its curvature value using a multi-scale procedure proposed in this work. Taking advantage of this representation, each salience is assigned to a visual word according to a Dictionary of Curvatures. The final shape representation is given by computing a histogram of visual words detected in the shape, combined with a spatial pooling approach that encodes the distance distribution of the visual words in relation the shape centroid. This proposed new shape description allows to analyze the dissimilarity between shapes using fast distance functions, such as the City-block distance, even if two shapes have different number of salience points. This is a powerful asset to reduce the computational complexity when retrieving images. Compared to other shape descriptors, the BoSP descriptor has the advantage of proving a powerful shape description with high recognition accuracy, a compact representation invariant to geometric transformations while demanding a low computational cost to measure the dissimilarity of shapes.
programming_language	We develop an effective method for improving the segmentation result based on the Multi-Stencils Fast Marching method (MSFM). In MSFM, the gradient information of the image plays a vital role for calculating edges. It is straightforward to obtain the edge of good quality images; however, MSFM may not have robust edge maps available for images with spurious edges. Thus, a special multi-direction circumscribed circle filter is proposed to calculate the image gradient information which is then used in the MSFM. Using the new gradient information, better image contours can be obtained with MSFM. The size of the radius used in our circle filter is constant even the standard deviation of zero-mean Gaussian noise changes while the parameters of mean filter and Canny filter for gradient computation have to be correctly selected according to different noisy images. Our proposed method shows that it is effective through the experiments of image segmentation.
programming_language	In this paper, we present a framework for evaluating segmentation algorithms for Web pages. Web page segmentation consists in dividing a Web page into coherent fragments, called blocks. Each block represents one distinct information element in the page. We define an evaluation model that includes different metrics to evaluate the quality of a segmentation obtained with a given algorithm. Those metrics compute the distance between the obtained segmentation and a manually built segmentation that serves as a ground truth. We apply our framework to four state-of-the-art segmentation algorithms (BOM, Block Fusion, VIPS and JVIPS) on several categories (types) of Web pages. Results show that the tested algorithms usually perform rather well for text extraction, but may have serious problems for the extraction of geometry. They also show that the relative quality of a segmentation algorithm depends on the category of the segmented page.
programming_language	We have developed a vision-based interface based on body position for viewpoint control in an immersive projection display. The 3D position of the arms and head of the user are tracked by image processing without the need to attach devices to the user. This provides freedom of movement and increases the immersion in the virtual environment. Images are captured using two high-sensitivity monochrome cameras suitable for the dark condition in the projection display. The edges of both hands and the top of the head are tracked in order to control the user's viewpoint by estimating the variance of the position. We evaluated the utility of the interface in an experiment where subjects performed walk-through trials in 3D space, comparing the body-position interface with a joystick. The results indicate that the performance of the body-position interface is comparable to that of the joystick in terms of viewpoint control, but enhances the sensation of speed. The developed system is applicable as a non-contact user interface for moving around in a virtual environment.
programming_language	This paper presents a stereo vision-based autonomous navigation system using a GPS and the VFH algorithm. In order to obtain a high-accuracy disparity map and meet the time constraints of the real time navigation system, this work proposes the use of a semi-global stereo method. By not suffering the same issues of the regularly used local stereo methods, the employed stereo technique enables the generation of a highly dense, efficient, and accurate disparity map. Obstacles are detected using a method that checks for relative slopes and heights differences. Experimental tests using an electric vehicle in an urban environment were performed to validate the proposed approach.
programming_language	Nowadays, document image retrieval systems are increasingly applicable by various businesses, governmental and academic organizations. ELEPAP (Hellenic Protection and Rehabilitation Centre for Disabled Children) is an organization which needs more efficient ways of managing its huge volume of archived documents. This paper deals with the preprocessing procedures of well-known OCR systems in order to extract specific features from ELEPAP's patients' cards. It is shown that our proposed methodology can provide good IT solutions for ELEPAP in order to extract information from its old archives.
programming_language	Several K-View based algorithms have been made for developing image texture classification. These are K-View-Template algorithm (K-View-T), K-View-Datagram algorithm (K-View-D), Fast Weighted K-View-Voting algorithm (K-View-V), and K-View Using Rotation-Invariant Feature algorithm (K-View-R). In this paper, we review those K-View based algorithms and perform an empirical study for comparison by using classification accuracy, efficiency and stability. In addition, we propose a new K-View algorithm Using Gray Level Co-Occurrence Matrix algorithm (K-View-G) which also is compared with other K-View algorithms to demonstrate its effectiveness in image texture classification.
programming_language	The digitalization of bound documents either using flatbed scanners or digital cameras often yield images with non-straight text-lines due to a geometrical warp. This paper presents a new algorithm for text-line segmentation for documents captured by digital cameras or scanners. The proposed method reached 97.84% correct segmentation, while the best results offered by its predecessors in the literature yields 95.21%.
programming_language	We develop an effective method for the study of cervical vertebra maturation (CVM) for bone age evaluation. Such studies need an accurate X-ray radiographs segmentation of cervical vertebra. It is difficult to have a good segmentation on this type of images. Current segmentation methods do not work well on scanned images from analog image X-ray radiographs of cervical vertebra. A new method for analysis of cervical bone age is proposed. Two key techniques are developed in this proposed segmentation algorithm: (1) a fitting weight matrix is built to reduce the effect of subjective factors entered by the user when fast marching method is used to obtain the initial rough outline of cervical vertebra, and (2) apply a curve fitting method based on rotating and overlapping parabolic curves to derive the final segments of cervical vertebra. Furthermore, the user can calculate corresponding parameters from segmented results to assess the bone age. Experimental results using the proposed algorithm show that our algorithm is more accurate than those of fast marching method (FMM) and radiologists through repetition. It also shows that the proposed method has a higher accuracy on the correlation of the skeletal maturity indicators (SMI) and quantitative cervical vertebral maturation (QCVM).
programming_language	Ensuring the correctness of multithreaded programs is difficult, due to the potential for unexpected and nondeterministic interactions, between threads. Previous work addressed this problem by devising tools for detecting race conditions, a situation where two threads simultaneously access the same data variable, and at least one of the accesses is a write. Unfortunately, verifying the absence of such simultaneous-access race conditions is neither necessary nor sufficient to ensure the absence of errors due to unexpected thread interactions.We propose that a stronger non-interference property is required, namely the atomicity of code blocks, and we present a type system for specifying and verifying such atomicity properties. The type system allows statement blocks and functions to be annotated with the keyword atomic. If the program type checks, then the type system guarantees that for any arbitrarily-interleaved program execution, there is a corresponding execution with equivalent behavior in which the instructions of each atomic block executed by a thread are not interleaved with instructions from other threads. This property allows programmers to reason about the behavior of well-typed programs at a higher level of granularity, where each atomic block is executed "in one step", thus signi .cantly simplifying both formal and informal reasoning.Our type system is sufficient to verify a number of interesting examples. For example,it can prove that many methods of java.util. Vector are atomic, even though some methods have benign race conditions, and would be rejected by earlier type systems for race detection.